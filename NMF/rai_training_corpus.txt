############ Machine Learning for High-risk Applications ###########

Praise for Machine Learning for High-Risk Applications


Machine Learning for High-Risk Applications is a practical, opinionated, and timely book. Readers of all stripes will find rich insights into this fraught subject, whether you?re a data scientist interested in better understanding your models, or a manager responsible for ensuring compliance with existing standards, or an executive trying to improve your
organization?s risk controls.
?Agus Sudjianto, PhD, EVP, Head of Corporate Model Risk,
Wells Fargo

  Don?t miss out on this must-read! Packed with a winning combination of cutting-edge theory and real-world expertise, this book is a game-changer for anyone grappling with the complexities of AI interpretability, explainability, and security. With expert guidance on managing bias and much more, it?s the ultimate guide to mastering the buzzword bonanza of the AI world. Don?t let the competition get ahead?get your hands on this
indispensable resource today!
?Mateusz Dymczyk, Software Engineer,
Machine Learning, Meta

The book is a comprehensive and timely guide for anyone working on machine learning when the stakes are high. The authors have done an excellent job providing an overview of regulatory aspects, risk management, interpretability, and many other topics while providing practical advice and code examples. Highly recommended for anyone who prefers diligence over disaster when deploying machine learning models.
?Christoph Molnar, Author of Interpretable Machine Learning

 Machine learning applications need to account for fairness, accountability, transparency, and ethics in every industry to be successful. Machine Learning for High-Risk Applications lays the foundation for such topics and gives valuable insights that can be utilized for various use cases. I highly recommend this book for any machine learning practitioners.
?Navdeep Gill, Engineering Manager, H2O.ai

Responsible AI?explained simply.
?Hariom Tatsat, Coauthor of Machine Learning & Data
Science Blueprints for Finance

    Machine Learning for High-Risk Applications is a highly needed book responding to the growing demand for in-depth analysis of predictive models. The book is very practical and gives explicit advice on how to look at different aspects, such as model debugging, bias, transparency, and explainability analysis. The authors share their huge experience in analyzing different classes of models, for both tabular and image data.
I recommend this book to anyone wishing to work responsibly with complex models,
not only in high-risk applications.
?Przemys?aw Biecek, Professor at the Warsaw University of Technology

A refreshingly thoughtful and practical guide to responsible use of machine learning. This book has the potential to prevent AI accidents and harms before they happen.
?Harsh Singhal, Senior AI Solution Director,
Financial Services, C3.ai

    This book stands out for its uniquely tactical approach to addressing system risks in ML. The authors emphasize the critical importance of addressing potential harms as necessary to the delivery of desired outcomes?noted as key to the very success of ML. Especially helpful is the focus on ensuring that the right roles are in the room when making decisions about ML. By taking a nuanced approach to derisking ML, this book offers readers a valuable resource for successfully deploying ML systems in a responsible
and sustainable manner.
?Liz Grennan, Associate Partner and Global Co-Lead for
Digital Trust, McKinsey & Company

This book is a comprehensive review of both social and technical approaches to high-risk AI applications and provides practitioners with useful techniques to bridge their day-to-
day work with core concepts in Responsible AI.
?Triveni Gandhi, PhD, Responsible AI Lead, Dataiku

Unlocking the full potential of machine learning and AI goes beyond mere accuracy of models. This book delves into the critical yet often overlooked aspects of explainable, bias-free, and robust models. In addition, it offers invaluable insights into the cultural and organizational best practices for organizations to ensure the success
of their AI initiatives. With technology advancing at an unprecedented pace and regulations struggling to keep up, this timely and comprehensive guide serves as
an indispensable resource for practitioners.
?Ben Steiner, Columbia University

 Machine learning models are very complex in nature and their development is fraught with pitfalls. Mistakes in this field can cost many?s reputation and millions or even billions of dollars. This book contains must-have knowledge for any machine learning practitioner who wants to design, develop, and deploy robust machine learning models that avoid failing like so many other ML endeavors over the past years.
?Szilard Pafka, PhD, Chief Scientist, Epoch

Saying this book is timely is an understatement. People who do machine learning models need a text like this to help them consider all the possible biases and repercussions that arise from the models they create. The best part is that Patrick, James, and Parul do a wonderful job in making this book readable and digestible. This book is needed on any
machine learning practitioner?s bookshelf.
?Aric LaBarr, PhD, Associate Professor of Analytics

This is an extremely timely book. Practitioners of data science and AI need to seriously consider the real-world impact and consequences of models. The book motivates and helps them to do so. It not only provides solid technical information, but weaves
a cohesive tapestry with legislation, security, governance, and ethical threads. Highly
recommended as reference material.
?Jorge Silva, PhD, Director of AI/Machine Learning Server, SAS

       With the ever-growing applications of AI affecting every facet of our lives, it is important to ensure that AI applications, especially the ones that are safety critical, are developed responsibly. Patrick Hall and team have done a fantastic job in articulating the key aspects and issues in developing safety-critical applications in this book in
   a pragmatic way. I highly recommend this book, especially if you are involved in building AI applications that are high stakes, critical, and need to be developed and
tested systematically and responsibly!
?Sri Krishnamurthy, QuantUniversity

If you?re looking for direction from a trusted advisor as you venture into the use of AI in your organization, this book is a great place to start. The authors write from a position of both knowledge and experience, providing just the right mix of baseline education in technology and common pitfalls, coverage of regulatory and societal issues, relevant and
relatable case studies, and practical guidance throughout.
?Brett Wujek, PhD, Head of AI Product Management, SAS









 Machine Learning for High-Risk Applications
Approaches to Responsible AI












Patrick Hall, James Curtis, and Parul Pandey
Foreword by Agus Sudjianto, PhD












Beijing	Boston	Farnham	Sebastopol	Tokyo


Machine Learning for High-Risk Applications
by Patrick Hall, James Curtis, and Parul Pandey
Copyright ? 2023 Patrick Hall, James Curtis, and Parul Pandey. All rights reserved. Printed in the United States of America.
Published by O?Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.
O?Reilly books may be purchased for educational, business, or sales promotional use. Online editions are also available for most titles (http://oreilly.com). For more information, contact our corporate/institutional sales department: 800-998-9938 or corporate@oreilly.com.

Acquisition Editors: Rebecca Novack and Nicole Butterfield Development Editor: Michele Cronin Production Editor: Gregory Hyman Copyeditor: Liz Wheeler

Proofreader: Kim Cofer
Indexer: WordCo Indexing Services, Inc.
Interior Designer: David Futato Cover Designer: Karen Montgomery Illustrator: Kate Dullea


April 2023:	First Edition

Revision History for the First Edition
2023-04-17: First Release

See http://oreilly.com/catalog/errata.csp?isbn=9781098102432 for release details.

The O?Reilly logo is a registered trademark of O?Reilly Media, Inc. Machine Learning for High-Risk Applications, the cover image, and related trade dress are trademarks of O?Reilly Media, Inc.
The views expressed in this work are those of the authors, and do not represent the publisher?s views. While the publisher and the authors have used good faith efforts to ensure that the information and instructions contained in this work are accurate, the publisher and the authors disclaim all responsibility for errors or omissions, including without limitation responsibility for damages resulting from the use of or reliance on this work. Use of the information and instructions contained in this work is at your own risk. If any code samples or other technology this work contains or describes is subject to open source licenses or the intellectual property rights of others, it is your responsibility to ensure that your use thereof complies with such licenses and/or rights.
This work is part of a collaboration between O?Reilly and Dataiku. See our statement of editorial independence (https://oreil.ly/editorial-independence).







978-1-098-10243-2 [LSI]



Table of Contents



Foreword.	ix
Preface.	xi
Part I.	Theories and Practical Applications of AI Risk Management
1. Contemporary Machine Learning Risk Management.	3
A Snapshot of the Legal and Regulatory Landscape
4
The Proposed EU AI Act
4
US Federal Laws and Regulations
5
State and Municipal Laws
5
Basic Product Liability
6
Federal Trade Commission Enforcement
7
Authoritative Best Practices
8
AI Incidents
11
Cultural Competencies for Machine Learning Risk Management
13
Organizational Accountability
13
Culture of Effective Challenge
14
Diverse and Experienced Teams
15
Drinking Our Own Champagne
15
Moving Fast and Breaking Things
16
Organizational Processes for Machine Learning Risk Management
16
Forecasting Failure Modes
17
Model Risk Management Processes
18
Beyond Model Risk Management
22
Case Study: The Rise and Fall of Zillow?s iBuying
27
Fallout
28

iii
Lessons Learned	28
Resources	31
2. Interpretable and Explainable Machine Learning.	33
Important Ideas for Interpretability and Explainability	34
Explainable Models	39
Additive Models	39
Decision Trees	44
An Ecosystem of Explainable Machine Learning Models	47
Post Hoc Explanation	50
Feature Attribution and Importance	51
Surrogate Models	63
Plots of Model Performance	68
Cluster Profiling	71
Stubborn Difficulties of Post Hoc Explanation in Practice	71
Pairing Explainable Models and Post Hoc Explanation	75
Case Study: Graded by Algorithm	77
Resources	80
3. Debugging Machine Learning Systems for Safety and Performance.	81
Training	83
Reproducibility	83
Data Quality	85
Model Specification for Real-World Outcomes	88
Model Debugging	91
Software Testing	92
Traditional Model Assessment	93
Common Machine Learning Bugs	95
Residual Analysis	103
Sensitivity Analysis	107
Benchmark Models	110
Remediation: Fixing Bugs	112
Deployment	114
Domain Safety	114
Model Monitoring	116
Case Study: Death by Autonomous Vehicle	120
Fallout	120
An Unprepared Legal System	120
Lessons Learned	121
Resources	122




iv |  Table of Contents

4. Managing Bias in Machine Learning.	123
ISO and NIST Definitions for Bias	126
Systemic Bias	126
Statistical Bias	126
Human Biases and Data Science Culture	127
Legal Notions of ML Bias in the United States	128
Who Tends to Experience Bias from ML Systems	131
Harms That People Experience	133
Testing for Bias	135
Testing Data	135
Traditional Approaches: Testing for Equivalent Outcomes	137
A New Mindset: Testing for Equivalent Performance Quality	141
On the Horizon: Tests for the Broader ML Ecosystem	143
Summary Test Plan	146
Mitigating Bias	147
Technical Factors in Mitigating Bias	148
The Scientific Method and Experimental Design	148
Bias Mitigation Approaches	149
Human Factors in Mitigating Bias	153
Case Study: The Bias Bug Bounty	156
Resources	158
5. Security for Machine Learning.	159
Security Basics	161
The Adversarial Mindset	161
CIA Triad	162
Best Practices for Data Scientists	163
Machine Learning Attacks	166
Integrity Attacks: Manipulated Machine Learning Outputs	166
Confidentiality Attacks: Extracted Information	171
General ML Security Concerns	173
Countermeasures	175
Model Debugging for Security	175
Model Monitoring for Security	178
Privacy-Enhancing Technologies	179
Robust Machine Learning	182
General Countermeasures	182
Case Study: Real-World Evasion Attacks	184
Evasion Attacks	184
Lessons Learned	185
Resources	186
Table of Contents  |	v
Part II.	Putting AI Risk Management into Action
6. Explainable Boosting Machines and Explaining XGBoost.	189
Concept Refresher: Machine Learning Transparency	190
Additivity Versus Interactions	190
Steps Toward Causality with Constraints	191
Partial Dependence and Individual Conditional Expectation	191
Shapley Values	194
Model Documentation	195
The GAM Family of Explainable Models	196
Elastic Net?Penalized GLM w/ Alpha and Lambda Search	196
Generalized Additive Models	200
GA2M and Explainable Boosting Machines	205
XGBoost with Constraints and Post Hoc Explanation	208
Constrained and Unconstrained XGBoost	208
Explaining Model Behavior with Partial Dependence and ICE	214
Decision Tree Surrogate Models as an Explanation Technique	217
Shapley Value Explanations	221
Problems with Shapley values	224
Better-Informed Model Selection	228
Resources	229
7. Explaining a PyTorch Image Classifier	231
Explaining Chest X-Ray Classification	232
Concept Refresher: Explainable Models
and Post Hoc Explanation Techniques	233
Explainable Models Overview	233
Occlusion Methods	234
Gradient-Based Methods	234
Explainable AI for Model Debugging	235
Explainable Models	235
ProtoPNet and Variants	236
Other Explainable Deep Learning Models	237
Training and Explaining a PyTorch Image Classifier	238
Training Data	238
Addressing the Dataset Imbalance Problem	239
Data Augmentation and Image Cropping	240
Model Training	242
Evaluation and Metrics	244
Generating Post Hoc Explanations Using Captum	244
Evaluating Model Explanations	250
The Robustness of Post Hoc Explanations	252
vi |  Table of Contents
Conclusion	258
Resources	259
8. Selecting and Debugging XGBoost Models.	261
Concept Refresher: Debugging ML	262
Model Selection	262
Sensitivity Analysis	262
Residual Analysis	264
Remediation	265
Selecting a Better XGBoost Model	266
Sensitivity Analysis for XGBoost	271
Stress Testing XGBoost	272
Stress Testing Methodology	273
Altering Data to Simulate Recession Conditions	274
Adversarial Example Search	276
Residual Analysis for XGBoost	280
Analysis and Visualizations of Residuals	281
Segmented Error Analysis	285
Modeling Residuals	287
Remediating the Selected Model	290
Overemphasis of PAY_0	291
Miscellaneous Bugs	293
Conclusion	295
Resources	296
9. Debugging a PyTorch Image Classifier	297
Concept Refresher: Debugging Deep Learning	299
Debugging a PyTorch Image Classifier	302
Data Quality and Leaks	303
Software Testing for Deep Learning	305
Sensitivity Analysis for Deep Learning	306
Remediation	314
Sensitivity Fixes	321
Conclusion	325
Resources	326
10. Testing and Remediating Bias with XGBoost.	327
Concept Refresher: Managing ML Bias	328
Model Training	331
Evaluating Models for Bias	335
Testing Approaches for Groups	335
Individual Fairness	345
Table of Contents  |	vii
Proxy Bias	349
Remediating Bias	350
Preprocessing	350
In-processing	355
Postprocessing	359
Model Selection	362
Conclusion	366
Resources	368
11. Red-Teaming XGBoost.	369
Concept Refresher	370
CIA Triad	370
Attacks	371
Countermeasures	373
Model Training	375
Attacks for Red-Teaming	379
Model Extraction Attacks	379
Adversarial Example Attacks	383
Membership Attacks	386
Data Poisoning	387
Backdoors	390
Conclusion	394
Resources	395
Part III.	Conclusion
12. How to Succeed in High-Risk Machine Learning.	399
Who Is in the Room?	400
Science Versus Engineering	402
The Data-Scientific Method	403
The Scientific Method	404
Evaluation of Published Results and Claims	405
Apply External Standards	407
Commonsense Risk Mitigation	410
Conclusion	413
Resources	414
Index.	415






viii |  Table of Contents



Foreword



Renowned statistician George Box once famously stated, ?All models are wrong, but some are useful.? Acknowledgment of this fact forms the foundation of effective risk management. In a world where machine learning increasingly automates important decisions about our lives, the consequences of model failures can be catastrophic. It?s critical to take deliberate steps to mitigate risk and avoid unintended harm.
Following the 2008 financial crisis, regulators and financial institutions recognized the importance of managing model risk in ensuring the safety of banks, refining the practice of model risk management (MRM). As AI and machine learning gain widespread adoption, MRM principles are being applied to manage their risk. The National Institute of Standards and Technology?s AI Risk Management Framework serves as an example of this evolution. Proper governance and control of the entire process, from senior management oversight to policy and procedures, including organizational structure and incentives, are crucial to promoting a culture of model risk management.
In Machine Learning for High-Risk Applications, Hall, Curtis, and Pandey have pre? sented a framework for applying machine learning to high-stakes decision making. They provide compelling evidence through documented cases of model failures and emerging regulations that highlight the importance of strong governance and cul? ture. Unfortunately, these principles are still rarely implemented outside of regulated industries, such as banks. The book covers important topics ranging across model transparency, governance, security, bias management, and more.
Performance testing alone is not enough in machine learning, where very different models can have the same performance due to model multiplicity. Models must also be explainable, secure, and fair. This is the first book that emphasizes inherently interpretable models and their recent development and application, particularly in cases where models impact individuals, such as in consumer finance. In these scenarios, where explainability standards and regulations are particularly stringent,



ix

the explainable AI (XAI) post hoc explainability approach often faces significant challenges.
Developing reliable and safe machine learning systems also requires a rigorous evalu? ation of model weaknesses. This book presents two thorough examples alongside a methodology for model debugging, including identifying model flaws through error or residual slicing, evaluating model robustness under input corruption, assessing the reliability or uncertainty of model outputs, and testing model resilience under distribution shift through stress testing. These are crucial topics for developing and deploying machine learning in high-risk settings.
Machine learning models have the potential to disproportionately harm historically marginalized groups, and to deliver this harm rapidly and at scale through automa? tion. Biased model decisions have detrimental impacts on protected groups, perpet? uating social and economic disparities. In this book, the reader will learn how to approach the issue of model fairness through a sociotechnical lens. The authors also detail a thorough study of the effects of model debiasing techniques, and give practical advice on the application of these techniques within different regulated verticals.
Machine Learning for High-Risk Applications is a practical, opinionated, and timely book. Readers of all stripes will find rich insights into this fraught subject, whether you?re a data scientist interested in better understanding your models, or a manager responsible for ensuring compliance with existing standards, or an executive trying to improve your organization?s risk controls.

                   ? Agus Sudjianto, PhD EVP, Head of Corporate Model Risk, Wells Fargo
















x  |  Foreword



Preface



Today, machine learning (ML) is the most commercially viable subdiscipline of arti? ficial intelligence (AI). ML systems are used to make high-risk decisions in employ? ment, bail, parole, lending, security, and in many other high-impact applications throughout the world?s economies and governments. In a corporate setting, ML systems are used in all parts of an organization?from consumer-facing products, to employee assessments, to back-office automation, and more. Indeed, the past decade has brought with it even wider adoption of ML technologies. But it has also proven that ML presents risks to its operators, consumers, and even the general public.
Like all technologies, ML can fail?whether by unintentional misuse or intentional abuse. As of 2023, there have been thousands of public reports of algorithmic discrimination, data privacy violations, training data security breaches, and other harmful incidents. Such risks must be mitigated before organizations, and the pub? lic, can realize the true benefits of this exciting technology. Addressing ML?s risks requires action from practitioners. While nascent standards, to which this book aims to adhere, have begun to take shape, the practice of ML still lacks broadly accepted professional licensing or best practices. That means it?s largely up to individual practitioners to hold themselves accountable for the good and bad outcomes of their technology when it?s deployed into the world. Machine Learning for High-Risk Appli? cations will arm practitioners with a solid understanding of model risk management processes and new ways to use common Python tools for training explainable models and debugging them for reliability, safety, bias management, security, and privacy issues.








xi

We adapt a definition of AI from Stuart Russell and Peter Norvig?s book, Artificial Intelligence: A Modern Approach (https://oreil.ly/ oosZs): The designing and building of intelligent systems that receive signals from the environment and take actions that affect that environment (2020). For ML, we use the common definition attributed?perhaps apocryphally?to Arthur Samuel: [A] field of study that gives computers the ability to learn without being explic? itly programmed (circa 1960).

Who Should Read This Book
This is a mostly technical book for early-to-middle career ML engineers and data scientists who want to learn about the responsible use of ML or ML risk management. The code examples are written in Python. That said, this book probably isn?t for every data scientist and engineer out there coding in Python. This book is for you if you want to learn some model governance basics and update your workflow to accommodate basic risk controls. This book is for you if your work needs to comply with certain nondiscrimination, transparency, privacy, or security standards. (Although we can?t guarantee compliance or provide legal advice!) This book is for you if you want to train explainable models, and learn to edit and debug them. Finally, this book is for you if you?re concerned that your work in ML may be leading to unintended consequences relating to sociological biases, data privacy violations, security vulnerabilities, or other known problems caused by automated decision making writ large?and you want to do something about it.
Of course, this book may be of interest to others. If you?re coming to ML from a field like physics, econometrics, or psychometrics, this book can help you learn how to blend newer ML techniques with established domain expertise and notions of validity or causality. This book may give regulators or policy professionals some insights into the current state of ML technologies that may be used in an effort to comply with laws, regulations, or standards. Technical risk executives or risk managers may find this book helpful in providing an updated overview of newer ML approaches suited for high-stakes applications. And expert data scientists or ML engineers may find this book educational too, but they may also find it challenges many established data science practices.
What Readers Will Learn
Readers of this book will be exposed to both traditional model risk management and how to blend it with computer security best practices like incident response, bug bounties, and red-teaming, to apply battle-tested risk controls to ML workflows and systems. This book will introduce a number of older and newer explainable models,



xii  |  Preface

and explanation techniques that make ML systems even more transparent. Once we?ve set up a solid foundation of highly transparent models, we?ll dig into testing models for safety and reliability. That?s a lot easier when we can see how our model works! We?ll go way beyond quality measurements in holdout data to explore how to apply well-known diagnostic techniques like residual analysis, sensitivity analysis, and benchmarking to new types of ML models. We?ll then progress to structuring models for bias management, testing for bias, and remediating bias from an organ? izational and technical perspective. Finally, we?ll discuss security for ML pipelines and APIs.

The Draft European Union AI Act categorizes the following ML applications as high risk: biometric identification; management of critical infrastructure; education; employment; essential services, both public (e.g., public assistance) and private (e.g., credit lend? ing); law enforcement; immigration and border control; criminal justice; and the democratic process. These are the types of ML use cases we have in mind when we refer to high-risk applications, and that?s why we?ve chosen to focus the code examples in this book on computer vision and tree-based models for tabular data.

Readers should also be aware that in this first edition we focus on more established ML methods for estimation and decision making. We do not address unsupervised learning, search, recommendation systems, reinforcement learning, and generative AI in great depth. There are several reasons for this:
? These systems are not the most common commercial production systems, yet.
? Before moving on to more sophisticated unsupervised, recommendation, and reinforcement learning or generative approaches, it is imperative that we master the fundamentals. This first edition of the book focuses on the basics that will enable readers to take on more sophisticated projects later.
? Risk management for these systems is not as well understood as it is for the types of supervised models we concentrate on in this book. To be direct?as we often are in the remainder of the book?using models for which failure modes, mitigants, and controls are not well known can increase risk.
We do hope to return to these topics in the future and we acknowledge they are affecting billions of people today?positively and negatively. We also note that with a little creativity and elbow grease many of the techniques, risk mitigants, and risk management frameworks in this book can and should be applied to unsupervised models, search, recommendation, and generative AI.




Preface  |  xiii

Cutting-edge generative AI systems, like ChatGPT and GitHub Copilot, are an exciting way ML is impacting our lives. These sys? tems appear to have addressed some of the bias issues that plagued earlier generations of similar systems. However, they still pose risks when working in high-stakes applications. If we?re working with them and have concerns, we should consider the following simple guardrails:
Don?t copy and paste from or into the user interface.
Not using generated content directly and not pasting our own content directly into the interface can limit intellectual prop? erty and data privacy risks.
Check all generated content.
These systems continue to generate wrong, offensive, or other? wise problematic content.
Avoid automation complacency.
Generally, these systems are better suited to content genera? tion than to decision support. We should be careful not to let them unintentionally make decisions for us.

Alignment with the NIST AI Risk Management Framework
In an attempt to follow our own advice, and to make the book even more practical for those working on high-risk applications, we will highlight where the proposed approaches in the book align to the nascent National Institute of Standards and Technology (NIST) AI Risk Management Framework (RMF). Application of external standards is a well-known risk management tactic, and NIST has an incredible track record for authoritative technical guidance. The AI RMF has many components, but two of the most central are the characteristics for trustworthiness in AI and the core RMF guidance. The characteristics for trustworthiness establish the basic principles of AI risk management, while the core RMF guidance provides advice for the implementation of risk controls. We will use vocabulary relating to NIST?s characteristics for AI trustworthiness throughout the book: validity, reliability, safety, security, resiliency, transparency, accountability, explainability, interpretability, bias management, and enhanced privacy. At the beginning of each chapter in Part I, we?ll also use a callout box to break down how and where the content aligns to specific aspects of the core NIST AI RMF map, measure, manage, and govern functions. We hope alignment to the NIST AI RMF improves the usability of the book, making it a more effective AI risk management tool.





xiv  |  Preface

NIST does not review, approve, condone, or otherwise address any content in this book, including any claims relating to the AI RMF. All AI RMF content is simply the authors? opinions and in no way reflects an official position of NIST or any official or unofficial relationship between NIST and the book or the authors.

Book Outline
The book is broken into three parts. Part I discusses issues from a practical applica? tion perspective, with dashes of theory where necessary. Part II contains long-form Python coding examples, addressing the topics in Part I from both structured and unstructured data perspectives. Part III imparts hard-won advice on how to succeed in real-world high-risk use cases.
Part I
Chapter 1 begins with a deep dive into pending regulations, discussions of product liability, and a thorough treatment of traditional-model risk management. Because many of these practices assume a somewhat staid and professional approach to modeling?a far cry from today?s common ?move fast and break things? ethos?we?ll also discuss how to incorporate computer security best practices that assume failure into model governance.
Chapter 2 presents the burgeoning ecosystem of explainable models. We cover the generalized additive model (GAM) family in the most depth, but also discuss many other types of high-quality and high-transparency estimators. Chapter 2 also outlines many different post hoc explanation techniques, but with an eye toward rigor and known problems with this somewhat overhyped subfield of responsible ML techniques.
Chapter 3 tackles model validation, but in a way that actually tests models? assump? tions and real-world reliability. We?ll go over software testing basics as well as touch on highlights from the field of model debugging.
Chapter 4 overviews the sociotechnical aspects of fairness and bias in ML before tran? sitioning to technical bias measurement and remediation approaches. Chapter 4 then treats bias testing in some detail, including tests for disparate impact and differential validity. Chapter 4 also addresses both established and conservative methods for bias remediation, and more cutting-edge dual-objective, adversarial, and pre-, in-, and postprocessing remediation techniques.
Chapter 5 closes Part I by laying out how to red-team ML systems, starting with the basics of computer security and moving into discussions of common ML attacks, adversarial ML, and robust ML.



Preface  |  xv

Each chapter in Part I closes with a case discussion relating to topics like Zillow?s iBuying meltdown, the A-level scandal in the UK, the fatal crash of a self-driving Uber, Twitter?s inaugural bias bug bounty, and real-world ML evasion attacks. Each chapter will also outline alignment between content and the NIST AI RMF.
Part II
Part II expands on the ideas in Part I with a series of thorough code example chapters. Chapter 6 puts explainable boosting machines (EBMs), XGBoost, and explainable AI techniques through their paces in a basic consumer finance example.
Chapter 7 applies post hoc explanation techniques to a PyTorch image classifier.
In Chapter 8, we?ll debug our consumer finance models for performance problems, and do the same for our image classifier in Chapter 9.
Chapter 10 contains detailed examples relating to bias testing and bias remediation, and Chapter 11 provides examples of ML attacks and countermeasures for tree-based models.
Part III
We end the book in Chapter 12 with more general advice for how to succeed in high-risk ML applications. It?s not by moving fast and breaking things. For some low-risk use cases, it might be fine to apply a quick and dirty approach. But as ML becomes more regulated and is used in more high-risk applications, the consequences of breaking things are becoming more serious. Chapter 12 caps off the book with hard-won practical advice for applying ML in high-stakes scenarios.
Our hope with the first edition of this text is to provide a legitimate alternative to the opaque and compressed-time-frame workflows that are common in ML today. This book should provide a set of vocabulary, ideas, tools, and techniques that enable practitioners to be more deliberate in their very important work.
Example Datasets
We rely on two primary datasets in this book, to explain techniques or to demon? strate approaches and discuss their results. These are example datasets, not fit for training models in high-risk applications, but they are well known and easily accessi? ble. Their shortcomings also allow to us to point out various data, modeling, and interpretation pitfalls. We will refer to these datasets many times in subsequent chapters, so be sure to get a feel for them before diving into the rest of the book.





xvi  |  Preface

Taiwan Credit Data
For the structured data chapters?Chapters 6, 8, 10, and 11?we used a slightly modified version of the Taiwan credit data available from the University of California Irvine Machine Learning Repository (https://oreil.ly/xJ5u2) or Kaggle (https://oreil.ly/ DmAWe). The credit card default data contains demographic and payment informa? tion about credit card customers in Taiwan in the year 2005. In general, the goal with this dataset is to use past payment statuses (PAY_*), past payment amounts (PAY_AMT*), and bill amounts (BILL_AMT*) as inputs to predict whether a customer will meet their next payment (DELINQ_NEXT = 0). Currency amounts are reported in Taiwanese dollars. We?ve added simulated SEX and RACE markers to this dataset to illustrate bias testing and remediation approaches. We use the payment information as input features, and following best practices to manage bias in ML systems, we do not use the demographic information as model inputs. The complete data dictionary is available in Table P-1.
Table P-1. Data dictionary for the credit card default data

ID	ID	Int	Unique row identifier
LIMIT_BAL	Input	Float	Amount of previously awarded credit

SEX	Demographic
information
RACE	Demographic
information
EDUCATION	Demographic information
MARRIAGE	Demographic information
AGE	Demographic
information

Int	1 = male; 2 = female

Int	1 = Hispanic; 2 = Black; 3 = white;a 4 = Asian

Int	1 = graduate school; 2 = university; 3 = high school; 4 = others
Int	1 = married; 2 = single; 3 = others

Int	Age in years

PAY_0, PAY_2? PAY_6







BILL_AMT1? BILL_AMT6

Input	Int	History of past payment; PAY_0 = the repayment status in September, 2005; PAY_2
= the repayment status in August, 2005; ?;
PAY_6 = the repayment status in April, 2005. The measurement scale for the repayment status is: ?1 = pay duly; 1 = payment delay for one month; 2 = payment delay for two months;
?; 8 = payment delay for eight months;
9 = payment delay for nine months and above
Input	Float	Amount of bill statement; BILL_AMT1 = amount of bill statement in September, 2005; BILL_AMT2 = amount of bill statement in August, 2005; ?; BILL_AMT6 = amount of bill statement in April, 2005


Preface  |  xvii




PAY_AMT1? PAY_AMT6

Input	Float	Amount of previous payment; PAY_AMT1
= amount paid in September, 2005; PAY_AMT2
= amount paid in August, 2005; ?; PAY_AMT6
= amount paid in April, 2005

DELINQ_NEXT	Target	Int	Whether a customer?s next payment is delinquent
(late), 1 = late; 0 = on-time
a There is an ongoing debate as to whether ?White? should be capitalized alongside ?Black? when referring to racial demographic groups. Throughout this book, we have generally followed the lead of many authoritative voices (https://oreil.ly/ 3iKFj) in publishing and academia by capitalizing ?Black,? out of recognition of a shared history and cultural identity.

As readers will see in the following chapters, this dataset encodes some pathological flaws. It?s too small to train usable high-capacity ML estimators and nearly all of the signal for DELINQ_NEXT is encoded in PAY_0. As the book progresses, we?ll attempt to deal with these issues and uncover others.
Kaggle Chest X-Ray Data
For the deep learning chapters?Chapters 6 and 9?we will be using the Kaggle Chest X-Ray Images dataset (https://oreil.ly/TsoGB). This dataset is composed of roughly 5,800 images of two classes, pneumonia and normal. These labels were determined by human domain experts. The images are deidentified chest X-rays, taken during rou? tine care visits to Guangzhou Women and Children?s Medical Center in Guangzhou, China. See Figure P-1 for an example pneumonia image.

Figure P-1. An example pneumonia image from the Kaggle Chest X-ray dataset
The main issues we?ll face with this dataset are small size?even for transfer learning tasks, misalignment between the images in the dataset, visual artifacts that can give rise to shortcut learning, and the need for domain expertise to validate modeling



xviii  |  Preface

outcomes. As with the Taiwan credit data, we?ll work through these issues and discover more in the later chapters of this book.
Conventions Used in This Book
The following typographical conventions are used in this book:
Italic
Indicates new terms or important ideas.
Constant width
Used for program listings, as well as within paragraphs to refer to program elements such as variable or function names, databases, data types, environment variables, statements, and keywords.

This element signifies a general note or suggestion.



This element indicates a warning or caution.



Online Figures
You can find larger, color versions of some figures at https://oreil.ly/MLHA-figures. Links to each figure also appear in their captions.
Using Code Examples
Supplemental material (code examples, exercises, etc.) is available for download at
https://oreil.ly/machine-learning-high-risk-apps-code.

Over time, code examples may change from those printed in the book.


If you have a technical question or a problem using the code examples, please send email to bookquestions@oreilly.com.


Preface  |  xix

This book is here to help you get your job done. In general, if example code is offered with this book, you may use it in your programs and documentation. You do not need to contact us for permission unless you?re reproducing a significant portion of the code. For example, writing a program that uses several chunks of code from this book does not require permission. Selling or distributing examples from O?Reilly books does require permission. Answering a question by citing this book and quoting example code does not require permission. Incorporating a significant amount of example code from this book into your product?s documentation does require permission.
We appreciate, but generally do not require, attribution. An attribution usually includes the title, author, publisher, and ISBN. For example: ?Machine Learning for High-Risk Applications by Patrick Hall, James Curtis, and Parul Pandey (O?Reilly). Copyright 2023 Patrick Hall, James Curtis, and Parul Pandey, 978-1-098-10243-2.?
If you feel your use of code examples falls outside fair use or the permission given above, feel free to contact us at permissions@oreilly.com.
O?Reilly Online Learning
For more than 40 years, O?Reilly Media has provided technol? ogy and business training, knowledge, and insight to help companies succeed.

Our unique network of experts and innovators share their knowledge and expertise through books, articles, and our online learning platform. O?Reilly?s online learning platform gives you on-demand access to live training courses, in-depth learning paths, interactive coding environments, and a vast collection of text and video from O?Reilly and 200+ other publishers. For more information, visit http://oreilly.com.
How to Contact Us
Please address comments and questions concerning this book to the publisher:
O?Reilly Media, Inc.
1005 Gravenstein Highway North Sebastopol, CA 95472
800-998-9938 (in the United States or Canada) 707-829-0515 (international or local)
707-829-0104 (fax)




xx  |  Preface

We have a web page for this book, where we list errata, examples, and any additional information. You can access this page at https://oreil.ly/machine-learning-high-risk- apps.
Email bookquestions@oreilly.com to comment or ask technical questions about this book.
For news and information about our books and courses, visit https://oreilly.com. Find us on LinkedIn: https://linkedin.com/company/oreilly-media
Follow us on Twitter: https://twitter.com/oreillymedia
Watch us on YouTube: https://youtube.com/oreillymedia
Acknowledgments
The authors thank our editors and production staff at O?Reilly, particularly Michele Cronin and Gregory Hyman; our copyeditor, Liz Wheeler; and our acquisitions edi? tors, Rebecca Novack and Nicole Butterfield. Thanks also to our technical reviewers, Navdeep Gill, Collin Starkweather, Hariom Tatstat, and Laura Uzc tegui.
Patrick Hall
Thanks to Lisa and Dylan for love and support throughout the long drafting and editing process. Thanks also to my colleagues over the past decade at the Institute for Advanced Analytics, SAS Institute, the George Washington School of Business, H2O.ai, SolasAI, the AI Incident Database, NIST, and BNH.AI.
James Curtis
For my wife, Lindsey, whose unyielding love forms the bedrock of my life. My children, Isaac and Micah, did not help much with the writing of this book, but I appreciate them nonetheless. Finally, I must thank my former colleagues at SolasAI, especially Nick and Chris, for the many hours of insightful discussions.
Parul Pandey
I am incredibly grateful to have had the love and support of my husband, Manoj, and my son, Agrim, while writing this book. Both of them not only encouraged me to take up this mammoth task but also were deeply understanding when I would spend hours in my study working on the book.





Preface  |  xxi





PART I

Theories and Practical Applications
of AI Risk Management





CHAPTER 1

Contemporary Machine Learning
Risk Management



Building the best machine learning system starts with cultural competencies and business processes. This chapter presents numerous cultural and procedural approaches we can use to improve ML performance and safeguard our organizations? ML against real-world safety and performance problems. It also includes a case study that illustrates what happens when an ML system is used without proper human oversight. The primary goal of the approaches discussed in this chapter is to create better ML systems. This might mean improved in silico test data performance. But it really means building models that perform as expected once deployed in vivo, so we don?t lose money, hurt people, or cause other harms.

In vivo is Latin for ?within the living.? We?ll sometimes use this term to mean something closer to ?interacting with the living,? as in how ML models perform in the real world when interacting with human users. In silico means ?by means of computer modeling or computer simulation,? and we?ll use this term to describe the testing data scientists often perform in their development environ? ments before deploying ML models.

The chapter begins with a discussion of the current legal and regulatory landscape for ML and some nascent best-practice guidance, to inform system developers of their fundamental obligations when it comes to safety and performance. We?ll also introduce how the book aligns to the National Institute of Standards and Technology (NIST) AI Risk Management Framework (https://oreil.ly/Or940) (RMF) in this part of the chapter. Because those who do not study history are bound to repeat it, the chapter then highlights AI incidents, and discusses why understanding AI incidents is important for proper safety and performance in ML systems. Since many ML safety

3

concerns require thinking beyond technical specifications, the chapter then blends model risk management (MRM), information technology (IT) security guidance, and practices from other fields to put forward numerous ideas for improving ML safety culture and processes within organizations. The chapter will close with a case study focusing on safety culture, legal ramifications, and AI incidents.
None of the risk management approaches discussed in this chapter are a silver bullet. If we want to manage risk successfully, we?ll need to pick from the wide variety of available controls those that work best for our organization. Larger organizations will typically be able to do more risk management than smaller organizations. Readers at large organizations may be able to implement many controls across various depart? ments, divisions, or internal functions. Readers at smaller organizations will have to choose their risk management tactics judiciously. In the end, a great deal of tech? nology risk management comes down to human behavior. Whichever risk controls an organization implements, they?ll need to be paired with strong governance and policies for the people that build and maintain ML systems.
A Snapshot of the Legal and Regulatory Landscape
It?s a myth that ML is unregulated. ML systems can and do break the law. Forgetting or ignoring the legal context is one of the riskiest things an organization can do with respect to ML systems. That said, the legal and regulatory landscape for ML is complicated and changing quickly. This section aims to provide a snapshot of important laws and regulations for overview and awareness purposes. We?ll start by highlighting the pending EU AI Act. We?ll then discuss the many US federal laws and regulations that touch on ML, US state and municipal laws for data privacy and AI, and the basics of product liability, then end the section with a rundown of recent Federal Trade Commission (FTC) enforcement actions.

The authors are not lawyers and nothing in this book is legal advice. The intersection of law and AI is an incredibly complex topic that data scientists and ML engineers are not equipped to handle alone. You may have legal concerns about ML systems that you work on. If so, seek real legal advice.

The Proposed EU AI Act
The EU has proposed sweeping regulations for AI that are expected to be passed in 2023. Known as the EU AI Act (https://oreil.ly/x5dLT) (AIA), they would prohibit certain uses of AI like distorting human behavior, social credit scoring, and real-time biometric surveillance. The AIA deems other uses to be high risk, including appli? cations in criminal justice, biometric identification, employment screening, critical infrastructure management, law enforcement, essential services, immigration, and


4  |  Chapter 1: Contemporary Machine Learning Risk Management

others?placing a high documentation, governance, and risk management burden on these. Other applications would be considered limited or low risk, with fewer compliance obligations for their makers and operators. Much like the EU General Data Protection Regulation (GDPR) has changed the way companies handle data in the US and around the world, EU AI regulations are designed to have an outsized impact on US and other international AI deployments. Whether we?re working in the EU or not, we may need to start familiarizing ourselves with the AIA. One of the best ways is to read the Annexes (https://oreil.ly/0k_TQ), especially Annexes 1 and 3?8, that define terms and layout documentation and conformity requirements.
US Federal Laws and Regulations
Because we?ve been using algorithms in one form or another for decades in our government and economy, many US federal laws and regulations already touch on AI and ML. These regulations tend to focus on social discrimination by algorithms, but also treat transparency, privacy, and other topics. The Civil Rights Acts of 1964 and 1991, the Americans with Disabilities Act (ADA), the Equal Credit Opportunity Act (ECOA), the Fair Credit Reporting Act (FCRA), and the Fair Housing Act (FHA) are some of the federal laws that attempt to prevent discrimination by algorithms in areas like employment, credit lending, and housing. ECOA and FCRA, along with their more detailed implementation in Regulation B, attempt to increase transparency in ML-based credit lending and guarantee recourse rights for credit consumers. For a rejected credit application, lenders are expected to indicate the reasons for the rejection, i.e., an adverse action, and describe the features in the ML model that drove the decision. If the provided reasoning or data is wrong, consumers should be able to appeal the decision.
The practice of MRM, defined in part in the Federal Reserve?s SR 11-7 guidance (https://oreil.ly/xpr5P), forms a part of regulatory examinations for large US banks, and sets up organizational, cultural, and technical processes for good and reliable performance of ML used in mission-critical financial applications. Much of this chap? ter is inspired by MRM guidance, as it?s the most battle-tested ML risk management framework out there. Laws like the Health Insurance Portability and Accountability Act of 1996 (HIPAA) and the Family Educational Rights and Privacy Act (FERPA) set up serious data privacy expectations in healthcare and for students. Like the GDPR, HIPAA?s and FERPA?s interactions with ML are material, complex, and still debated. These are not even all the US laws that might affect our use of ML, but hopefully this brief listing provides an idea of what the US federal government has decided is important enough to regulate.
State and Municipal Laws
US states and cities have also taken up laws and regulations for AI and ML. New York City (NYC) Local Law 144, which mandates bias audits for automated employment

A Snapshot of the Legal and Regulatory Landscape |  5

decision tools, was initially expected to go into effect in January 2023. Under this law, every major employer in NYC will have to conduct bias testing of automated employ? ment software and post the results on their website. Washington DC?s proposed Stop Discrimination by Algorithms Act attempts to replicate federal expectations for nondiscrimination and transparency, but for a much broader set of applications, for companies that operate in DC or use the data of many DC citizens.
Numerous states passed their own data privacy laws as well. Unlike the older HIPAA and FERPA federal laws, these state data privacy laws are often intentionally designed to partially regulate the use of AI and ML. States like California, Colorado, Virginia, and others have passed data privacy laws that mention increased transparency, decreased bias, or both, for automated decision-making systems. Some states have put biometric data or social media in their regulatory crosshairs too. For example, Illinois? Biometric Information Privacy Act (BIPA) outlaws many uses of biometric data, and IL regulators have already started enforcement actions. The lack of a federal data privacy or AI law combined with this new crop of state and local laws makes the AI and ML compliance landscape very complicated. Our uses of ML may or may not be regulated, or may be regulated to varying degrees, based on the specific application, industry, and geography of the system.
Basic Product Liability
As makers of consumer products, data scientists and ML engineers have a fundamen? tal obligation to create safe systems. To quote a recent Brookings Institute report, ?Products Liability Law as a Way to Address AI Harms? (https://oreil.ly/2K_R6), ?Manufacturers have an obligation to make products that will be safe when used in reasonably foreseeable ways. If an AI system is used in a foreseeable way and yet becomes a source of harm, a plaintiff could assert that the manufacturer was negligent in not recognizing the possibility of that outcome.? Just like car or power tool manufacturers, makers of ML systems are subject to broad legal standards for negligence and safety. Product safety has been the subject of large amounts of legal and economic analysis, but this subsection will focus on one of the first and simplest standards for negligence: the Hand rule. Named after Judge Learned Hand, and coined in 1947, it provides a viable framework for ML product makers to think about negligence and due diligence. The Hand rule says that a product maker takes on a burden of care, and that the resources expended on that care should always be greater than the cost of a likely incident involving the product. Stated algebraically:

Burden ò Risk = Probability of lossLoss size

In more plain terms, organizations are expected to apply care, i.e., time, resources, or money, to a level commensurate to the cost associated with a foreseeable risk. Otherwise liability can ensue. In Figure 1-1, Burden is the parabolicly increasing line,

6  |  Chapter 1: Contemporary Machine Learning Risk Management

and risk, or Probability multiplied by Loss, is the parabolicly decreasing line. While these lines are not related to a specific measurement, their parabolic shape is meant to reflect the last-mile problem in removing all ML system risk, and shows that the application of additional care beyond a reasonable threshold leads to diminishing returns for decreasing risk as well.

Figure 1-1. The Hand rule. Adapted from ?Economic Analysis of Alternative Standards of Liability in Accident Law? (https://oreil.ly/9_u8H).

A fairly standard definition for the risk of a technology incident is the estimated likelihood of the incident multiplied by its estimated cost. More broadly, the International Organization for Standardi? zation (ISO) defines risk in the context of enterprise risk manage? ment as the ?effect of uncertainty on objectives.?

While it?s probably too resource intensive to calculate the quantities in the Hand rule exactly, it is important to think about these concepts of negligence and liability when designing an ML system. For a given ML system, if the probability of an incident is high, if the monetary or other loss associated with a system incident is large, or if both quantities are large, organizations need to spend extra resources on ensuring safety for that system. Moreover, organizations should document to the best of their ability that due diligence exceeds the estimated failure probabilities multiplied by the estimated losses.
Federal Trade Commission Enforcement
How might we actually get in trouble? If you?re working in a regulated industry, you probably know your regulators. But if we don?t know if our work is regulated or who might be enforcing consequences if we cross a legal or regulatory red line, it?s probably the US Federal Trade Commission we need to be most concerned with. The

A Snapshot of the Legal and Regulatory Landscape |  7

FTC is broadly focused on unfair, deceptive, or predatory trade practices, and they have found reason to take down at least three prominent ML algorithms in three years. With their new enforcement tool, algorithmic disgorgement, the FTC has the ability to delete algorithms and data, and, typically, prohibit future revenue genera? tion from an offending algorithm. Cambridge Analytica (https://oreil.ly/cM3V8) was the first firm to face this punishment, after their deceptive data collection practices surrounding the 2016 election. Everalbum (https://oreil.ly/05SO5) and WW (https:// oreil.ly/PMOq0), known as Weight Watchers, have also faced disgorgement.
The FTC has been anything but quiet about its intention to enforce federal laws around AI and ML. FTC commissioners have penned lengthy treatises on algorithms and economic justice (https://oreil.ly/v8Z4y). They have also posted at least two blogs providing high-level guidance for companies who would like to avoid the unpleasant? ness of enforcement actions. These blogs highlight a number of concrete steps organ? izations should take. For example, in ?Using Artificial Intelligence and Algorithms? (https://oreil.ly/066Y-), the FTC makes it clear that consumers should not be misled into interacting with an ML system posing as a human. Accountability is another prominent theme in ?Using Artificial Intelligence and Algorithms,? ?Aiming for Truth, Fairness, and Equity in Your Company?s Use of AI? (https://oreil.ly/XMqKo), and other related publications. In ?Aiming for Truth, Fairness, and Equity in Your Company?s Use of AI,? the FTC states, ?Hold yourself accountable?or be ready for the FTC to do it for you? (emphasis added by the original author). This extremely direct language is unusual from a regulator. In ?Using Artificial Intelligence and Algorithms,? the FTC puts forward, ?Consider how you hold yourself accountable, and whether it would make sense to use independent standards or independent expertise to step back and take stock of your AI.? The next section introduces some of the emerging independent standards we can use to increase accountability, make better products, and decrease any potential legal liability.
Authoritative Best Practices
Data science mostly lacks professional standards and licensing today, but some authoritative guidance is starting to appear on the horizon. ISO is beginning to outline technical standards for AI (https://oreil.ly/8ZeJQ). Making sure our models are in line with ISO standards would be one way to apply an independent standard to our ML work. Particularly for US-based data scientists, the NIST AI RMF is a very important project to watch.
Version 1 of the AI RMF was released in January 2023. The framework puts forward characteristics of trustworthiness in AI systems: validity, reliability, safety, security, resiliency, transparency, accountability, explainability, interpretability, bias management, and enhanced privacy. Then it presents actionable guidance across four organizational functions?map, measure, manage, and govern?for achieving


8  |  Chapter 1: Contemporary Machine Learning Risk Management

trustworthiness. The guidance in the map, measure, manage, and govern functions is subdivided into more detailed categories and subcategories. To see these categories of guidance, check out the RMF (https://oreil.ly/kxq-G) or the AI RMF playbook (https:// oreil.ly/dn4xs), which provides even more detailed suggestions.

The NIST AI Risk Management Framework is a voluntary tool for improving the trustworthiness of AI and ML systems. The AI RMF is not regulation and NIST is not a regulator.


To follow our own advice, and that of regulators and publishers of authoritative guidance, and to make this book more useful, we?ll be calling out how we believe the content of each chapter in Part I aligns to the AI RMF. Following this paragraph, readers will find a callout box that matches the chapter subheadings with AI RMF subcategories. The idea is that readers can use the table to understand how employing the approaches discussed in each chapter may help them adhere to the AI RMF. Because the subcategory advice may, in some cases, sound abstract to ML practition? ers, we provide more practice-oriented language that matches the RMF categories; this will be helpful in translating the RMF into in vivo ML deployments. Check out the callout box to see how we think Chapter 1 aligns with the AI RMF, and look for similar tables at the start of each chapter in Part I.


Authoritative Best Practices | 9




?Forecasting Failure Modes? on page 17

GOVERN 1.2, GOVERN 1.3, GOVERN 4.2, MAP 1.1, MAP 2.3, MAP 3.2,
MANAGE 1.4, MANAGE 2

?Risk tiering? on page 18	GOVERN 1.2, GOVERN 1.3, GOVERN 1.4, GOVERN 5.2, MAP 1.5, MAP 4, MAP
5.1, MANAGE 1.2, MANAGE 1.3, MANAGE 1.4

?Model documentation? on page 19

GOVERN 1, GOVERN 2.1, GOVERN 4.2, MAP, MEASURE 1.1, MEASURE 2,
MEASURE 3, MEASURE 4, MANAGE

?Model monitoring? on page 20	GOVERN 1.2, GOVERN 1.3, GOVERN 1.4, GOVERN 1.5, MAP 2.3, MAP 3.5,
MAP 4, MAP 5.2, MEASURE 1.1, MEASURE 2.4, MEASURE 2.6, MEASURE
2.7, MEASURE 2.8, MEASURE 2.10, MEASURE 2.11, MEASURE 2.12, MEASURE
3.1, MEASURE 3.3, MEASURE 4, MANAGE 2.2, MANAGE 2.3, MANAGE 2.4,
MANAGE 3, MANAGE 4
?Model inventories? on page 21	GOVERN 1.2, GOVERN 1.3, GOVERN 1.4, GOVERN 1.6, MAP 3.5, MAP 4,
MANAGE 3

?System validation and process auditing? on page 21

GOVERN 1.2, GOVERN 1.3, GOVERN 1.4, GOVERN 2.1, GOVERN 4.1, GOVERN
4.3, GOVERN 6.1, MAP 2.3, MAP 3.5, MAP 4, MEASURE, MANAGE 1

?Change management? on page 22	GOVERN 1.2, GOVERN 1.3, GOVERN 1.4, GOVERN 1.7, GOVERN 2.2, GOVERN
4.2, MAP 3.5, MAP 4, MEASURE 1.2, MEASURE 2.13, MEASURE 3.1, MANAGE
4.1, MANAGE 4.2

?Model audits and assessments? on page 22

GOVERN 1.2, GOVERN 1.3, GOVERN 1.4, GOVERN 2.1, GOVERN 4.1, MAP 3.5,
MAP 4, MEASURE, MANAGE 1

?Impact assessments? on page 23	GOVERN 1.2, GOVERN 1.3, GOVERN 1.4, GOVERN 2.1, GOVERN 4.1, GOVERN
4.2, GOVERN 5.2, MAP 1.1, MAP 2.2, MAP 3.1, MAP 3.2, MAP 3.5, MAP 5,
MEASURE 3, MANAGE 1

?Appeal, override, and opt out? on page 24
?Pair and double programming? on page 24
?Security permissions for model deployment? on page 24

GOVERN 1.1, GOVERN 1.2, GOVERN 1.4, GOVERN 1.5, GOVERN 3.2, GOVERN 5,
MAP 3.5, MAP 5.2, MEASURE 2.8, MEASURE 3.3, MANAGE 4.1
GOVERN 4.1, GOVERN 5.2, MAP 3.5

GOVERN 1.4, GOVERN 4.1, GOVERN 5.2, MAP 3.5, MAP 4.2

?Bug bounties? on page 25	GOVERN 5, MAP 3.5, MAP 5.2, MEASURE 3
?AI incident response? on page 25	GOVERN 1.2, GOVERN 1.5, GOVERN 4.3, GOVERN 6.2, MAP 3.5, MAP 4, MAP
5, MEASURE 3.1, MANAGE 2.3, MANAGE 4.1, MANAGE 4.3

? Applicable AI trustworthiness characteristics include: Valid and Reliable, Safe, Managed Bias, Secure and Resilient, Transparent and Accountable, Explainable and Interpretable, Enhanced Privacy
? See also:
? NIST AI Risk Management Framework (https://oreil.ly/1YAoU)
? NIST AI Risk Management Framework Playbook (https://oreil.ly/6_TUM)
? Full crosswalk table (not an official resource) (https://oreil.ly/61TXd)



10  |  Chapter 1: Contemporary Machine Learning Risk Management

AI Incidents
In many ways, the fundamental goal of ML safety processes and related model debugging, also discussed in Chapter 3, is to prevent and mitigate AI incidents. Here, we?ll loosely define AI incidents as any outcome of the system that could cause harm. As becomes apparent when using the Hand rule as a guide, the severity of an AI incident is increased by the loss the incident causes, and decreased by the care taken by the operators to mitigate those losses.
Because complex systems drift toward failure, there is no shortage of AI incidents to discuss as examples. AI incidents can range from annoying to deadly?from mall security robots falling down stairs (https://oreil.ly/fLHU1), to self-driving cars killing pedestrians (https://oreil.ly/vFW_-), to mass-scale diversion of healthcare resources (https://oreil.ly/2e8WQ) away from those who need them most. As pictured in Fig? ure 1-2, AI incidents can be roughly divided into three buckets:
Abuses
AI can be used for nefarious purposes, apart from specific hacks and attacks on other AI systems. The day may already have come when hackers use AI to increase the efficiency and potency of their more general attacks. What the future could hold is even more frightening. Specters like autonomous drone attacks and ethnic profiling by authoritarian regimes are already on the horizon.
Attacks
Examples of all major types of attacks?confidentiality, integrity, and availability attacks (see Chapter 5 for more information)?have been published by research? ers. Confidentiality attacks involve the exfiltration of training data or model logic from AI system endpoints. Integrity attacks include adversarial manipulation of training data or model outcomes, either through adversarial examples, evasion, impersonation, or poisoning. Availability attacks can be conducted through more standard denial-of-service approaches, through sponge examples that overuse system resources, or via algorithmic discrimination induced by some adversary to deny system services to certain groups of users.
Failures
AI system failures tend to involve algorithmic discrimination, safety and perfor? mance lapses, data privacy violations, inadequate transparency, or problems in third-party system components.
AI incidents are a reality. And like the systems from which they arise, AI incidents can be complex. AI incidents have multiple causes: failures, attacks, and abuses. They also tend to blend traditional notions of computer security with concerns like data privacy and algorithmic discrimination.



AI Incidents | 11



Figure 1-2. A basic taxonomy of AI incidents. Adapted from ?What to Do When AI Fails" (https://oreil.ly/AHfmK).
The 2016 Tay chatbot incident (https://oreil.ly/a-DhB) is an informative example. Tay was a state-of-the-art chatbot trained by some of the world?s leading experts at Microsoft Research for the purpose of interacting with people on Twitter to increase awareness about AI. Sixteen hours after its release?and 96,000 tweets later?Tay had spiraled into writing as a neo-nazi pornographer and had to be shut down. What happened? Twitter users quickly learned that Tay?s adaptive learning system could easily be poisoned. Racist and sexual content tweeted at the bot was incorporated into its training data, and just as quickly resulted in offensive output. Data poisoning is an integrity attack, but due to the context in which it was carried out, this attack resulted in algorithmic discrimination. It?s also important to note that Tay?s designers, being world-class experts at an extremely well-funded research center, seemed to have put some guardrails in place. Tay would respond to certain hot-button issues with precanned responses. But that was not enough, and Tay devolved into a public security and algorithmic discrimination incident for Microsoft Research.
Think this was a one-off incident? Wrong. Just recently, again due to hype and failure to think through performance, safety, privacy, and security risks systematically, many of Tay?s most obvious failures were repeated in Scatter Lab?s release of its Lee Luda chatbot (https://oreil.ly/5OLXV). When designing ML systems, plans should be compared to past known incidents in hope of preventing future similar incidents. This is precisely the point of recent AI incident database efforts (https://oreil.ly/vvLbp) and associated publications (https://oreil.ly/59yaY).
AI incidents can also be an apolitical motivator for responsible technology develop? ment. For better or worse, cultural and political viewpoints on topics like algorithmic discrimination and data privacy can vary widely. Getting a team to agree on ethical considerations can be very difficult. It might be easier to get them working to pre? vent embarrassing and potentially costly or dangerous incidents, which should be a baseline goal of any serious data science team. The notion of AI incidents is central to understanding ML safety; a central theme of this chapter?s content is cultural


12  |  Chapter 1: Contemporary Machine Learning Risk Management

competencies and business processes that can be used to prevent and mitigate AI incidents. We?ll dig into those mitigants in the next sections and take a deep dive into a real incident to close the chapter.

Cultural Competencies for Machine Learning Risk Management
An organization?s culture is an essential aspect of responsible AI. This section will dis? cuss cultural competencies like accountability, drinking our own champagne, domain expertise, and the stale adage ?move fast and break things.?
Organizational Accountability
A key to the successful mitigation of ML risks is real accountability within organi? zations for AI incidents. If no one?s job is at stake when an ML system fails, gets attacked, or is abused for nefarious purposes, then it?s entirely possible that no one in that organization really cares about ML safety and performance. In addition to devel? opers who think through risks, apply software quality assurance (QA) techniques, and model debugging methods, organizations need individuals or teams who validate ML system technology and audit associated processes. Organizations also need some? one to be responsible for AI incident response plans. This is why leading financial institutions, whose use of predictive modeling has been regulated for decades, employ a practice known as model risk management. MRM is patterned off the Federal Reserve?s SR 11-7 model risk management guidance (https://oreil.ly/xpr5P), which arose out the of the financial crisis of 2008. Notably, implementation of MRM often involves accountable executives and several teams that are responsible for the safety and performance of models and ML systems.
Implementation of MRM standards usually requires several different teams and exec? utive leadership. These are some of the key tenets that form the cultural backbone for MRM:
Written policies and procedures
The organizational rules for making and using ML should be written and avail? able for all organizational stakeholders. Those close to ML systems should have trainings on the policies and procedures. These rules should also be audited to understand when they need to be updated. No one should be able to claim ignorance of the rules, the rules should be transparent, and the rules should not change without approval. Policies and procedures should include clear mech? anisms for escalating serious risks or problems to senior management, and likely should put forward whistleblower processes and protections.




Cultural Competencies for Machine Learning Risk Management  |  13

Effective challenge
Effective challenge dictates that experts with the capability to change a system, who did not build the ML system being challenged, perform validation and auditing. MRM practices typically distribute effective challenge across three ?lines of defense,? where conscientious system developers make up the first line of defense and independent, skilled, and empowered technical validators and process auditors make up the second and third lines, respectively.
Accountable leadership
A specific executive within an organization should be accountable for ensuring AI incidents do not happen. This position is often referred to as chief model risk officer (CMRO). It?s also not uncommon for CMRO terms of employment and compensation to be linked to ML system performance. The role of CMRO offers a very straightforward cultural check on ML safety and performance. If our boss really cares about ML system safety and performance, then we start to care too.
Incentives
Data science staff and management must be incentivized to implement ML responsibly. Often, compressed product timelines can incentivize the creation of a minimum viable product first, with rigorous testing and remediation relegated to the end of the model lifecycle immediately before deployment to production. Moreover, ML testing and validation teams are often evaluated by the same criteria as ML development teams, leading to a fundamental misalignment where testers and validators are encouraged to move quickly rather than assure quality. Aligning timeline, performance evaluation, and pay incentives to team function helps solidify a culture of responsible ML and risk mitigation.
Of course, small or young organizations may not be able to spare an entire full-time employee to monitor ML system risk. But it?s important to have an individual or group held accountable if ML systems cause incidents and rewarded if the systems work well. If an organization assumes that everyone is accountable for ML risk and AI incidents, the reality is that no one is accountable.
Culture of Effective Challenge
Whether our organization is ready to adopt full-blown MRM practices, or not, we can still benefit from certain aspects of MRM. In particular, the cultural competency of effective challenge can be applied outside of the MRM context. At its core, effec? tive challenge means actively challenging and questioning steps taken throughout the development of ML systems. An organizational culture that encourages serious questioning of ML system designs will be more likely to develop effective ML systems or products, and to catch problems before they explode into harmful incidents. Note that effective challenge cannot be abusive, and it must apply equally to all personnel developing an ML system, especially so-called ?rockstar? engineers and


14  |  Chapter 1: Contemporary Machine Learning Risk Management

data scientists. Effective challenge should also be structured, such as weekly meetings where current design thinking is questioned and alternative design choices are seri? ously considered.
Diverse and Experienced Teams
Diverse teams can bring wider and previously uncorrelated perspectives to bear on the design, development, and testing of ML systems. Nondiverse teams often do not. Many have documented the unfortunate outcomes that can arise as a result of data scientists not considering demographic diversity in the training or results of ML systems. A potential solution to these kinds of oversights is increasing demographic diversity on ML teams from its current woeful levels (https://oreil.ly/7M9uB). Busi? ness or other domain experience is also important when building teams. Domain experts are instrumental in feature selection and engineering, and in the testing of system outputs. In the mad rush to develop ML systems, domain-expert participation can also serve as a safety check. Generalist data scientists often lack the experience necessary to deal with domain-specific data and results. Misunderstanding the mean? ing of input data or output results is a recipe for disaster that can lead to AI incidents when a system is deployed. Unfortunately, when it comes to data scientists forgetting or ignoring the importance of domain expertise, the social sciences deserve a special emphasis. In a trend referred to as ?tech?s quiet colonization of the social sciences? (https://oreil.ly/IcIBi), several organizations have pursued regrettable ML projects that seek to replace decisions that should be made by trained social scientists (https:// oreil.ly/xI9Jv) or that simply ignore the collective wisdom of social science domain expertise altogether (https://oreil.ly/KvVSv).
Drinking Our Own Champagne
Also known as ?eating our own dog food,? the practice of drinking our own cham? pagne refers to using our own software or products inside of our own organization. Often a form of prealpha or prebeta testing, drinking our own champagne can iden? tify problems that emerge from the complexity of in vivo deployment environments before bugs and failures affect customers, users, or the general public. Because serious issues like concept drift, algorithmic discrimination, shortcut learning, and under? specification are notoriously difficult to identify using standard ML development processes, drinking our own champagne provides a limited and controlled, but also realistic, test bed for ML systems. Of course, when organizations employ demograph? ically and professionally diverse teams and include domain experts in the field where the ML system will be deployed, drinking our own champagne is more likely to catch a wide variety of problems. Drinking our own champagne also brings the classical Golden Rule into AI. If we?re not comfortable using a system on ourselves or in our own organization, then we probably shouldn?t deploy that system.



Cultural Competencies for Machine Learning Risk Management  |  15

One important aspect to consider about deployment environments is the impact of our ML systems on ecosystems and the planet?for example:
? The carbon footprint of ML models
? The possibility that an ML system could damage the environ? ment by causing an AI incident
If we?re worried about the environmental impacts of our model, we should loop in ML governance with broader environmental, social, and governance efforts at our organization.

Moving Fast and Breaking Things
The mantra ?move fast and break things? is almost a religious belief for many ?rock- star? engineers and data scientists. Sadly, these top practitioners also seem to forget that when they go fast and break things, things get broken. As ML systems make more high-impact decisions that implicate autonomous vehicles, credit, employment, university grades and attendance, medical diagnoses and resource allocation, mort? gages, pretrial bail, parole, and more, breaking things means more than buggy apps. It can mean that a small group of data scientists and engineers causes real harm at scale to many people. Participating in the design and implementation of high-impact ML systems requires a mindset change to prevent egregious performance and safety problems. Practitioners must change from prioritizing the number of software fea? tures they can push, or the test data accuracy of an ML model, to recognizing the implications and downstream risks of their work.

Organizational Processes for Machine Learning Risk Management
Organizational processes play a key role in ensuring that ML systems are safe and performant. Like the cultural competencies discussed in the previous section, organi? zational processes are a key nontechnical determinant of reliability in ML systems. This section on processes starts out by urging practitioners to consider, document, and attempt to mitigate any known or foreseeable failure modes for their ML systems. We then discuss more about MRM. While ?Cultural Competencies for Machine Learning Risk Management? on page 13 focused on the people and mindsets neces? sary to make MRM a success, this section will outline the different processes MRM uses to mitigate risks in advanced predictive modeling and ML systems. While MRM is a worthy process standard to which we can all aspire, there are additional important process controls that are not typically part of MRM. We?ll look beyond traditional MRM in this section and highlight crucial risk control processes like pair or double programming and security permission requirements for code deployment.


16  |  Chapter 1: Contemporary Machine Learning Risk Management

This section will close with a discussion of AI incident response. No matter how hard we work to minimize harms while designing and implementing an ML system, we still have to prepare for failures and attacks.
Forecasting Failure Modes
ML safety and ethics experts roughly agree on the importance of thinking through, documenting, and attempting to mitigate foreseeable failure modes for ML systems. Unfortunately, they also mostly agree that this is a nontrivial task. Happily, new resources and scholarship on this topic have emerged in recent years that can help ML system designers forecast incidents in more systematic ways. If holistic categories of potential failures can be identified, it makes hardening ML systems for better real-world performance and safety a more proactive and efficient task. In this sub? section, we?ll discuss one such strategy, along with a few additional processes for brainstorming future incidents in ML systems.
Known past failures
As discussed in ?Preventing Repeated Real World AI Failures by Cataloging Inci? dents: The AI Incident Database? (https://oreil.ly/BfMJC), one the most efficient ways to mitigate potential AI incidents in our ML systems is to compare our system design to past failed designs. Much like transportation professionals investigating and cataloging incidents, then using the findings to prevent related incidents and test new technologies, several ML researchers, commentators, and trade organizations have begun to collect and analyze AI incidents in hopes of preventing repeated and related failures. Likely the most high-profile and mature AI incident repository is the AI Incident Database (https://oreil.ly/H8nmd). This searchable and interactive resource allows registered users to search a visual database with keywords and locate different types of information about publicly recorded incidents.
Consult this resource while developing ML systems. If a system similar to the one we?re currently designing, implementing, or deploying has caused an incident in the past, this is one of strongest indicators that our new system could cause an incident. If we see something that looks familiar in the database, we should stop and think about what we?re doing a lot more carefully.
Failures of imagination
Imagining the future with context and detail is never easy. And it?s often the context in which ML systems operate, accompanied by unforeseen or unknowable details, that lead to AI incidents. In a recent workshop paper, the authors of ?Overcom? ing Failures of Imagination in AI Infused System Development and Deployment? (https://oreil.ly/veB5T) put forward some structured approaches to hypothesize about those hard-to-imagine future risks. In addition to deliberating on the who (e.g.,


Organizational Processes for Machine Learning Risk Management |  17

investors, customers, vulnerable nonusers), what (e.g., well-being, opportunities, dig? nity), when (e.g., immediately, frequently, over long periods of time), and how (e.g., taking an action, altering beliefs) of AI incidents, they also urge system designers to consider the following:
? Assumptions that the impact of the system will be only beneficial (and to admit when uncertainty in system impacts exists)
? The problem domain and applied use cases of the system, as opposed to just the math and technology
? Any unexpected or surprising results, user interactions, and responses to the system
Causing AI incidents is embarrassing, if not costly or illegal, for organizations. AI incidents can also hurt consumers and the general public. Yet, with some foresight, many of the currently known AI incidents could have been mitigated, if not wholly avoided. It?s also possible that in performing the due diligence of researching and conceptualizing ML failures, we find that our design or system must be completely reworked. If this is the case, take comfort that a delay in system implementation or deployment is likely less costly than the harms our organization or the public could experience if the flawed system was released.
Model Risk Management Processes
The process aspects of MRM mandate thorough documentation of modeling systems, human review of systems, and ongoing monitoring of systems. These processes represent the bulk of the governance burden for the Federal Reserve?s SR 11-7 MRM guidance, which is overseen by the Federal Reserve and the Office of the Comptroller of the Currency for predictive models deployed in material consumer finance appli? cations. While only large organizations will be able to fully embrace all that MRM has to offer, any serious ML practitioner can learn something from the discipline. The following section breaks MRM processes down into smaller components so that readers can start thinking through using aspects of MRM in their organization.
Risk tiering
As outlined in the opening of this chapter, the product of the probability of a harm occurring and the likely loss resulting from that harm is an accepted way to rate the risk of a given ML system deployment. The product of risk and loss has a more for? mal name in the context of MRM, materiality. Materiality is a powerful concept that enables organizations to assign realistic risk levels to ML systems. More importantly, this risk-tiering allows for the efficient allocation of limited development, validation, and audit resources. Of course, the highest materiality applications should receive the greatest human attention and review, while the lowest materiality applications


18  |  Chapter 1: Contemporary Machine Learning Risk Management

could potentially be handled by automatic machine learning (AutoML) systems and undergo minimal validation. Because risk mitigation for ML systems is an ongoing, expensive task, proper resource allocation between high-, medium-, and low-risk systems is a must for effective governance.
Model documentation
MRM standards also require that systems be thoroughly documented. First, docu? mentation should enable accountability for system stakeholders, ongoing system maintenance, and a degree of incident response. Second, documentation must be standardized across systems for the most efficient audit and review processes. Doc? umentation is where the rubber hits the road for compliance. Documentation tem? plates, illustrated at a very high level by the following section list, are documents that data scientists and engineers fill in as they move through a standardized workflow or in the later stages of model development. Documentation templates should include all the steps that a responsible practitioner should conduct to build a sound model. If parts of the document aren?t filled out, that points to sloppiness in the training process. Since most documentation templates and frameworks also call for adding one?s name and contact information to the finished model document, there should be no mystery about who is not pulling their weight. For reference, the following section list is a rough combination of typical sections in MRM documentation and the sections recommended by the Annexes to the EU Artificial Intelligence Act (https://oreil.ly/p_Cqt):
? Basic Information
? Names of Developers and Stakeholders
? Current Date and Revision Table
? Summary of Model System
? Business or Value Justification
? Intended Uses and Users
? Potential Harms and Ethical Considerations
? Development Data Information
? Source for Development Data
? Data Dictionary
? Privacy Impact Assessment
? Assumptions and Limitations
? Software Implementation for Data Preprocessing
? Model Information
? Description of Training Algorithm with Peer-Reviewed References

Organizational Processes for Machine Learning Risk Management |  19

? Specification of Model
? Performance Quality
? Assumptions and Limitations
? Software Implementation for Training Algorithm
? Testing Information
? Quality Testing and Remediation
? Discrimination Testing and Remediation
? Security Testing and Remediation
? Assumptions and Limitations
? Software Implementation for Testing
? Deployment Information
? Monitoring Plans and Mechanisms
? Up- and Downstream Dependencies
? Appeal and Override Plans and Mechanisms
? Audit Plans and Mechanisms
? Change Management Plans
? Incident Response Plans
? References (If we?re doing science, then we?re building on the shoulders of giants and we?ll have several peer-reviewed references in a formatted bibliography!)
Of course, these documents can be hundreds of pages long, especially for high- materiality systems. The proposed datasheet (https://oreil.ly/mjKjy) and model card (https://oreil.ly/DmMp4) standards may also be helpful for smaller or younger organi? zations to meet these goals. If readers are feeling like lengthy model documentation sounds impossible for their organization today, then maybe these two simpler frame? works might work instead.
Model monitoring
A primary tenant of ML safety is that ML system performance in the real world is hard to predict and, accordingly, performance must be monitored. Hence, deployed- system performance should be monitored frequently and until a system is decommis? sioned. Systems can be monitored for any number of problematic conditions, the most common being input drift. While ML system training data encodes information about a system?s operating environment in a static snapshot, the world is anything but static. Competitors can enter markets, new regulations can be promulgated, con? sumer tastes can change, and pandemics or other disasters can happen. Any of these


20  |  Chapter 1: Contemporary Machine Learning Risk Management

can change the live data that?s entering our ML system away from the characteristics of its training data, resulting in decreased, or even dangerous, system performance. To avoid such unpleasant surprises, the best ML systems are monitored both for drifting input and output distributions and for decaying quality, often known as model decay. While performance quality is the most common quantity to monitor, ML systems can also be monitored for anomalous inputs or predictions, specific attacks and hacks, and for drifting fairness characteristics.
Model inventories
Any organization that is deploying ML should be able to answer straightforward questions like:
? How many ML systems are currently deployed?
? How many customers or users do these systems affect?
? Who are the accountable stakeholders for each system?
MRM achieves this goal through the use of model inventories. A model inventory is a curated and up-to-date database of all an organization?s ML systems. Model invento? ries can serve as a repository for crucial information for documentation, but should also link to monitoring plans and results, auditing plans and results, important past and upcoming system maintenance and changes, and plans for incident response.
System validation and process auditing
Under traditional MRM practices, an ML system undergoes two primary reviews before its release. The first review is a technical validation of the system, where skilled validators, not uncommonly PhD-level data scientists, attempt to poke holes in system design and implementation, and work with system developers to fix any discovered problems. The second review investigates processes. Audit and compli? ance personnel carefully analyze the system design, development, and deployment, along with documentation and future plans, to ensure all regulatory and internal process requirements are met. Moreover, because ML systems change and drift over time, review must take place whenever a system undergoes a major update or at an agreed upon future cadence.
Readers may be thinking (again) that their organization doesn?t have the resources for such extensive reviews. Of course that is a reality for many small or younger organizations. The keys for validation and auditing, that should work at nearly any organization, are having technicians who did not develop the system test it, having a function to review nontechnical internal and external obligations, and having sign-off oversight for important ML system deployments.



Organizational Processes for Machine Learning Risk Management |  21

Change management
Like all complex software applications, ML systems tend to have a large number of different components. From backend ML code, to application programming inter? faces (APIs), to graphical user interfaces (GUIs), changes in any component of the system can cause side effects in other components. Add in issues like data drift, emergent data privacy and anti-discrimination regulations, and complex dependen? cies on third-party software, and change management in ML systems becomes a serious concern. If we?re in the planning or design phase of a mission-critical ML system, we?ll likely need to make change management a first-class process control. Without explicit planning and resources for change management, process or techni? cal mistakes that arise through the evolution of the system, like using data without consent or API mismatches, are very difficult to prevent. Furthermore, without change management, such problems might not even be detected until they cause an incident.
We?ll circle back to MRM throughout the book. It?s one the most battle-tested frame? works for governance and risk management of ML systems. Of course, MRM is not the only place to draw inspiration for improved ML safety and performance processes, and the next subsection will draw out lessons from other practice areas.

Reading the 21-page SR 11-7 model risk management guidance (https://oreil.ly/0By87) is a quick way to up-skill yourself in ML risk management. When reading it, pay special attention to the focus on cultural and organizational structures. Managing technology risks is often more about people than anything else.

Beyond Model Risk Management
There are many ML risk management lessons to be learned from financial audit, data privacy, and software development best practices and from IT security. This subsection will shine a light on ideas that exist outside the purview of traditional MRM: model audits, impact assessments, appeals, overrides, opt outs, pair or double programming, least privilege, bug bounties, and incident response, all from an ML safety and performance perspective.
Model audits and assessments
Audit is a common term in MRM, but it also has meanings beyond what it is typically known as?the third line of defense in a more traditional MRM scenario. The phrase model audit has come to prominence in recent years. A model audit is an official testing and transparency exercise focusing on an ML system that tracks adherence to some policy, regulation, or law. Model audits tend to be conducted by independent



22  |  Chapter 1: Contemporary Machine Learning Risk Management

third parties with limited interaction between auditor and auditee organizations. For a good breakdown of model audits, check out the recent paper ?Algorithmic Bias and Risk Assessments: Lessons from Practice? (https://oreil.ly/eHxBb). The paper ?Closing the AI Accountability Gap: Defining an End-to-End Framework for Internal Algorithmic Auditing? (https://oreil.ly/vO9cH) puts forward a solid framework for audits and assessments, even including worked documentation examples. The related term, model assessment, seems to mean a more informal and cooperative testing and transparency exercise that may be undertaken by external or internal groups.
ML audits and assessments may focus on bias issues or other serious risks including safety, data privacy harms, and security vulnerabilities. Whatever their focus, audits and auditors have be fair and transparent. Those conducting audits should be held to clear ethical or professional standards, which barely exist as of 2023. Without these kinds of accountability mechanisms or binding guidelines, audits can be an ineffective risk management practice, and worse, tech-washing exercises that certify harmful ML systems. Despite flaws, audits are an en vogue favorite risk control tactic of policy-makers and researchers, and are being written into laws?for example, the aforementioned New York City Local Law 144.
Impact assessments
Impact assessments are a formal documentation approach used in many fields to forecast and record the potential issues a system could cause once implemented. Likely due to their use in data privacy (https://oreil.ly/1OdKa), impact assessments are starting to show up in organizational ML policies and proposed laws (https://oreil.ly/ waTek). Impact assessments are an effective way to think through and document the harms that an ML system could cause, increasing accountability for designers and operators of AI systems. But impact assessments are not enough on their own. Remembering the definition of risk and materiality previously put forward, impact is just one factor in risk. Impacts must be combined with likelihoods to form a risk measure, then risks must be actively mitigated, where the highest-risk applications are accorded the most oversight. Impact assessments are just the beginning of a broader risk management process. Like other risk management processes, they must be performed at a cadence that aligns to the system being assessed. If a system changes quickly, it will need more frequent impact assessments. Another potential issue with impact assessments is caused when they are designed and implemented by the ML teams that are also being assessed. In this case, there will be a temptation to diminish the scope of the assessment and downplay any potential negative impacts. Impact assessments are an important part of broader risk management and gover? nance strategies, but they must be conducted as often as required by a specific system, and likely conducted by independent oversight professionals.




Organizational Processes for Machine Learning Risk Management |  23

Appeal, override, and opt out
Ways for users or operators to appeal and override inevitable wrong decisions should be built into most ML systems. It?s known by many names across disciplines: actiona? ble recourse, intervenability, redress, or adverse action notices. This can be as simple as the ?Report inappropriate predictions? function in the Google search bar, or it can be as sophisticated as presenting data and explanations to users and enabling appeal processes for demonstrably wrong data points or decision mechanisms. Another similar approach, known as opt out, is to let users do business with an organization the old-fashioned way without going through any automated processing. Many data privacy laws and major US consumer finance laws address recourse, opt out, or both. Automatically forcing wrong decisions on many users is one of the clearest ethical wrongs in ML. We shouldn?t fall into an ethical, legal, and reputational trap that?s so clear and so well-known, but many systems do. That?s likely because it takes planning and resources for both processes and technology, laid out from the beginning of designing an ML system, to get appeal, override, and opt out right.
Pair and double programming
Because they tend to be complex and stochastic, it?s hard to know if any given ML algorithm implementation is correct! This is why some leading ML organizations implement ML algorithms twice as a QA mechanism. Such double implementation is usually achieved by one of two methods: pair programming or double programming. In the pair programming approach, two technical experts code an algorithm without collaborating. Then they join forces and work out any discrepancies between their implementations. In double programming, the same practitioner implements the same algorithm twice, but in very different programming languages, such as Python (object-oriented) and SAS (procedural). They must then reconcile any differences between their two implementations. Either approach tends to catch numerous bugs that would otherwise go unnoticed until the system was deployed. Pair and double programming can also align with the more standard workflow of data scientists prototyping algorithms, while dedicated engineers harden them for deployment. However, for this to work, engineers must be free to challenge and test data science prototypes and should not be relegated to simply recoding prototypes.
Security permissions for model deployment
The concept of least privilege (https://oreil.ly/0qP9-) from IT security states that no system user should ever have more permissions than they need. Least privilege is a fundamental process control that, likely because ML systems touch so many other IT systems, tends to be thrown out the window for ML build-outs and for so-called ?rock star? data scientists. Unfortunately, this is an ML safety and performance anti? pattern. Outside the world of overhyped ML and rock star data science, it?s long been understood that engineers cannot adequately test their own code and that others in a


24  |  Chapter 1: Contemporary Machine Learning Risk Management

product organization?product managers, attorneys, or executives?should make the final call as to when software is released.
For these reasons, the IT permissions necessary to deploy an ML system should be distributed across several teams within IT organizations. During development sprints, data scientists and engineers certainly must retain full control over their development environments. But, as important releases or reviews approach, the IT permissions to push fixes, enhancements, or new features to user-facing products are transferred away from data scientists and engineers to product managers, testers, attorneys, executives, or others. Such process controls provide a gate that prevents unapproved code from being deployed.
Bug bounties
Bug bounties are another concept we can borrow from computer security. Tradition? ally, a bug bounty is when an organization offers rewards for finding problems in its software, particularly security vulnerabilities. Since ML is mostly just software, we can do bug bounties for ML systems. While we can use bug bounties to find security problems in ML systems, we can also use them to find other types of problems related to reliability, safety, transparency, explainability, interpretability, or privacy. Through bug bounties, we use monetary rewards to incentivize community feedback in a standardized process. As we?ve highlighted elsewhere in the chapter, incentives are crucial in risk management. Generally, risk management work is tedious and resource consuming. If we want our users to find major problems in our ML systems for us, we need to pay them or reward them in some other meaningful way. Bug bounties are typically public endeavors. If that makes some organizations nervous, internal hackathons in which different teams look for bugs in ML systems may have some of the same positive effects. Of course, the more participants are incentivized to participate, the better the results are likely to be.
AI incident response
According to the vaunted SR 11-7 guidance (https://oreil.ly/E7G2R), ?even with skilled modeling and robust validation, model risk cannot be eliminated.? If risks from ML systems and ML models cannot be eliminated, then such risks will eventu? ally lead to incidents. Incident response is already a mature practice in the field of computer security. Venerable institutions like NIST (https://oreil.ly/glkOX) and SANS (https://oreil.ly/gU-Vo) have published computer security incident response guidelines for years. Given that ML is a less mature and higher-risk technology than general- purpose enterprise computing, formal AI incident response plans and practices are a must for high-impact or mission-critical AI systems.
Formal AI incident response plans enable organizations to respond more quickly and effectively to inevitable incidents. Incident response also plays into the Hand rule discussed at the beginning of the chapter. With rehearsed incident response plans

Organizational Processes for Machine Learning Risk Management |  25

in place, organizations may be able to identify, contain, and eradicate AI incidents before they spiral into costly or dangerous public spectacles. AI incident response plans are one of the most basic and universal ways to mitigate AI-related risks. Before a system is deployed, incident response plans should be drafted and tested. For young or small organizations that cannot fully implement model risk management, AI incident response is a cost-effective and potent AI risk control to consider. Borrowing from computer incident response, AI incident response can be thought of in six phases:
Phase 1: Preparation
In addition to clearly defining an AI incident for our organization, preparation for AI incidents includes personnel, logistical, and technology plans for when an incident occurs. Budget must be set aside for response, communication strategies must be put in place, and technical safeguards for standardizing and preserving model documentation, out-of-band communications, and shutting down AI sys? tems must be implemented. One of the best ways to prepare and rehearse for AI incidents are tabletop discussion exercises, where key organizational personnel work through a realistic incident. Good starter questions for an AI incident tabletop include the following:
? Who has the organizational budget and authority to respond to an AI incident?
? Can the AI system in question be taken offline? By whom? At what cost? What upstream processes will be affected?
? Which regulators or law enforcement agencies need to be contacted? Who will contact them?
? Which external law firms, insurance agencies, or public relation firms need to be contacted? Who will contact them?
? Who will manage communications? Internally, between responders? Exter? nally, with customers or users?
Phase 2: Identification
Identification is when organizations spot AI failures, attacks, or abuses. Identifi? cation also means staying vigilant for AI-related abuses. In practice, this tends to involve more general attack identification approaches, like network intrusion monitoring, and more specialized monitoring for AI system failures, like concept drift or algorithmic discrimination. Often the last step of the identification phase is to notify management, incident responders, and others specified in incident response plans.





26  |  Chapter 1: Contemporary Machine Learning Risk Management

Phase 3: Containment
Containment refers to mitigating the incident?s immediate harms. Keep in mind that harms are rarely limited to the system where the incident began. Like more general computer incidents, AI incidents can have network effects that spread throughout an organizations? and its customers? technologies. Actual con? tainment strategies will vary depending on whether the incident stemmed from an external adversary, an internal failure, or an off-label use or abuse of an AI system. If necessary, containment is also a good place to start communicating with the public.
Phase 4: Eradication
Eradication involves remediating any affected systems. For example, sealing off any attacked systems from vectors of in- or ex-filtration, or shutting down a discriminatory AI system and temporarily replacing it with a trusted rule-based system. After eradication, there should be no new harms caused by the incident.
Phase 5: Recovery
Recovery means ensuring all affected systems are back to normal and that con? trols are in place to prevent similar incidents in the future. Recovery often means retraining or reimplementing AI systems, and testing that they are performing at documented preincident levels. Recovery can also require careful analysis of technical or security protocols for personnel, especially in the case of an acciden? tal failure or insider attack.
Phase 6: Lessons learned
Lessons learned refers to corrections or improvements of AI incident response plans based on the successes and challenges encountered while responding to the current incident. Response plan improvements can be process- or technology- oriented.
When reading the following case, think about the phases of incident response, and whether an AI incident response plan would have been an effective risk control for Zillow.
Case Study: The Rise and Fall of Zillow?s iBuying
In 2018, the real estate tech company Zillow entered the business of buying homes and flipping them for a profit, known as iBuying. The company believed that its proprietary, ML-powered Zestimate algorithm could do more than draw eyeballs to its extremely popular web products. As reported by Bloomberg, Zillow employed domain experts to validate the numbers generated by their algorithms when they first started to buy homes. First, local real estate agents would price the property. The numbers were combined with the Zestimate, and a final team of experts vetted each offer before it was made.


Case Study: The Rise and Fall of Zillow?s iBuying | 27

According to Bloomberg (https://oreil.ly/LQQg3), Zillow soon phased out these teams of domain experts in order to ?get offers out faster,? preferring the speed and scale of a more purely algorithmic approach. When the Zestimate did not adapt to a rapidly inflating real estate market in early 2021, Zillow reportedly intervened to increase the attractiveness of its offers. As a result of these changes, the company began acquiring properties at a rate of nearly 10,000 homes per quarter. More flips means more staff and more renovation contractors, but as Bloomberg puts it, ?Zillow?s humans couldn?t keep up.? Despite increasing staffing levels by 45% and bringing on ?armies? of contractors, the iBuying system was not achieving profitability. The combination of pandemic staffing and supply challenges, the overheated housing market, and complexities around handling large numbers of mortgages were just too much for the iBuying project to manage.
In October of 2021, Zillow announced that it would stop making offers through the end of the year. As a result of Zillow?s appetite for rapid growth, as well as labor and supply shortages, the company had a huge inventory of homes to clear. To solve its inventory problem, Zillow was posting most homes for resale at a loss. Finally, on November 2, Zillow announced that it was writing down its inventory by over $500 million. Zillow?s foray into the automated house-flipping business was over.
Fallout
In addition to the huge monetary loss of its failed venture, Zillow announced that it would lay off about 2,000 employees?a full quarter of the company. In June of 2021, Zillow was trading at around $120 per share. At the time of this writing, nearly one year later, shares are approximately $40, erasing over $30 billion in stock value. (Of course, the entire price drop can?t be attributed to the iBuying incident, but it certainly factored into the loss.) The downfall of Zillow?s iBuying is rooted in many interwoven causes, and cannot be decoupled from the pandemic that struck in 2020 and upended the housing market. In the next section, we?ll examine how to apply what we?ve learned in this chapter about governance and risk management to Zillow?s misadventures.
Lessons Learned
What does this chapter teach us about the Zillow iBuying saga? Based on the public reporting, it appears Zillow?s decision to sideline human review of high-materiality algorithms was probably a factor in the overall incident. We also question whether Zillow had adequately thought through the financial risk it was taking on, whether appropriate governance structures were in place, and whether the iBuying losses might have been handled better as an AI incident. We don?t know the answers to many of these questions with respect to Zillow, so instead we?ll focus on insights readers can apply at their own organizations:


28  |  Chapter 1: Contemporary Machine Learning Risk Management

Lesson 1: Validate with domain experts.
In this chapter, we stressed the importance of diverse and experienced teams as a core organizational competency for responsible ML development. Without a doubt, Zillow has internal and external access to world-class expertise in real estate markets. However, in the interest of speed and automation?sometimes referred to as ?moving fast and breaking things? or ?product velocity??Zillow phased the experts out of the process of acquiring homes, choosing instead to rely on its Zestimate algorithm. According to follow-up reporting by Bloom? berg (https://oreil.ly/boQye) in May 2022, ?Zillow told its pricing experts to stop questioning the algorithms, according to people familiar with the process.? This choice may have proven fatal for the venture, especially in a rapidly changing, pandemic-driven real estate market. No matter the hype, AI is not smarter than humans yet. If we?re making high-risk decisions with ML, keep humans in the loop.
Lesson 2: Forecast failure modes.
The coronavirus pandemic of 2020 created a paradigm shift in many domains and markets. ML models, which usually assume that the future will resemble the past, likely suffered across the board in many verticals. We shouldn?t expect a company like Zillow to see a pandemic on the horizon. But as we?ve discussed, rigorously interrogating the failure modes of our ML system constitutes a crucial competency for ML in high-risk settings. We do not know the details of Zillow?s model governance frameworks, but the downfall of Zillow?s iBuying stresses the importance of effective challenge and asking hard questions, like ?What happens if the cost of performing renovations doubles over the next two years?? and ?What will be the business cost of overpaying for homes by two percent over the course of six months?? For such a high-risk system, probable failure modes should be enumerated and documented, likely with board of directors oversight, and the actual financial risk should have been made clear to all senior decision- makers. At our organization, we need to know the cost of being wrong with ML and that senior leadership is willing to tolerate those costs. Maybe senior leaders at Zillow were accurately informed of iBuying?s financial risks, maybe they weren?t. What we know now is that Zillow took a huge risk, and it did not pay off.
Lesson 3: Governance counts.
Zillow?s CEO is famous for risk-taking, and has a proven track record of winning big bets. But, we simply can?t win every bet we make. This is why we manage and govern risks when conducting automated decision making, especially in high- risk scenarios. SR 11-7 states, ?the rigor and sophistication of validation should be commensurate with the bank?s overall use of models.? Zillow is not a bank, but Bloomberg?s May 2022 postmortem puts it this way: Zillow was ?attempting to pivot from selling online advertising to operating what amounted to a hedge


Case Study: The Rise and Fall of Zillow?s iBuying | 29

fund and a sprawling construction business.? Zillow drastically increased the materiality of its algorithms, but appears not to have drastically increased gov? ernance over those algorithms. As noted, most of the public reporting points to Zillow decreasing human oversight of its algorithms during its iBuying pro? gram, not increasing oversight. A separate risk function, empowered with the organizational stature to stop models from moving into production, and with the appropriate budget and staff levels, that reports directly to the board of directors and operates independently from business and technology functions headed by the CEO and CTO, is common in major consumer finance organization. This organizational structure, when it works as intended, allows for more objective and risk-based decisions about ML model performance, and avoids the conflicts of interest and confirmation bias that tend to occur when business and technol? ogy leaders evaluate their own systems for risk. We don?t know if Zillow had an independent model governance function?it is quite rare these days outside of consumer finance. But we do know that no risk or oversight function was able to stop the iBuying program before losses became staggering. While it?s a tough battle to fight as a single technician, helping our organization apply independent audits to its ML systems is a workable risk mitigation practice.
Lesson 4: AI incidents occur at scale.
Zillow?s iBuying hijinks aren?t funny. Money was lost. Careers were lost?thou? sands of employees were laid off or resigned. This looks like a $30 billion AI inci? dent. From the incident response lens, we need to be prepared for systems to fail, we need to be monitoring for systems to fail, and we need to have documented and rehearsed plans in place for containment, eradication, and recovery. From public reporting, it does appear that Zillow was aware of its iBuying problems, but its culture was more focused on winning big than preparing for failure. Given the size of the financial loss, Zillow?s containment efforts could have been more effective. Zillow was able to eradicate its most acute problems with the declaration of the roughly half-billion dollar write-off in November of 2021. As for recovery, Zillow?s leadership has plans for a new real estate super app, but given the stock price at the time of this writing, recovery is a long way off and investors are weary. Complex systems drift toward failure. Perhaps a more disciplined incident-handling approach could save our organization when it bets big with ML.
The final and most important lesson we can take away from the Zillow Offers saga is at the heart of this book. Emerging technologies always come with risks. Early automobiles were dangerous. Planes used to crash into mountainsides much more frequently. ML systems can perpetuate discriminatory practices, pose security and privacy risks, and behave unexpectedly. A fundamental difference between ML and other emerging technologies is that these systems can make decisions quickly and at huge scales. When Zillow leaned into its Zestimate algorithm, it could scale up its


30  |  Chapter 1: Contemporary Machine Learning Risk Management

purchasing to hundreds of homes per day. In this case, the result was a write-down of half of a billion dollars, even larger stock losses, and the loss of thousands of jobs. This phenomenon of rapid failure at scale can be even more directly devastating when the target of interest is access to capital, social welfare programs, or the decision of who gets a new kidney.
Resources
Further Reading
? ISO standards for AI (https://oreil.ly/N3WTp)
? NIST AI Risk Management Framework (https://oreil.ly/mQ8aW)
? SR 11-7 model risk management guidance (https://oreil.ly/AANIg)





CHAPTER 2

Interpretable and Explainable
Machine Learning



Scientists have been fitting models to data to learn more about observed patterns for centuries. Explainable machine learning models and post hoc explanation of ML models present an incremental, but important, advance in this long-standing practice. Because ML models learn about nonlinear, faint, and interacting signals more easily than traditional linear models, humans using explainable ML models and post hoc explanation techniques can now also learn about nonlinear, faint, and interacting signals in their data with more ease.
In this chapter, we?ll dig into important ideas for interpretation and explanation before tackling major explainable modeling and post hoc explanation techniques. We?ll cover the major pitfalls of post hoc explanation too?many of which can be overcome by using explainable models and post hoc explanation together. Next we?ll discuss applications of explainable models and post hoc explanation, like model doc? umentation and actionable recourse for wrong decisions, that increase accountability for AI systems. The chapter will close with a case discussion of the so called ?A-level scandal? in the United Kingdom (UK), where an explainable, highly documented model made unaccountable decisions, resulting in a nationwide AI incident. The discussion of explainable models and post hoc explanation continues in Chapters 6 and 7, where we explore two in-depth code examples related to these topics.








33



Important Ideas for Interpretability and Explainability
Before jumping into the techniques for training explainable models and generating post hoc explanations, we need to discuss the big ideas behind the math and code. We?ll start by affirming that transparency does not equate to trust. We can trust things that we don?t understand, and understand things we don?t trust. Stated even more simply: transparency enables understanding, and understanding is different from trust. In fact, greater understanding of poorly built ML systems may actually decrease trust.
Trustworthiness in AI is defined by the National Institute of Standards and Technol? ogy (NIST) using various characteristics: validity, reliability, safety, managed bias, security, resiliency, transparency, accountability, explainability, interpretability, and enhanced privacy. Transparency makes it easier to achieve other desirable trustwor? thiness characteristics and makes debugging easier. However, human operators must take these additional governance steps. Trustworthiness is often achieved in practice through testing, monitoring, and appeal processes (see Chapters 1 and 3). Increased transparency enabled by explainable ML models and post hoc explanation should facilitate the kinds of diagnostics and debugging that help to make traditional linear models trustworthy. It also means that regulated applications that have relied on



34  |  Chapter 2: Interpretable and Explainable Machine Learning

linear models for decades due to per-consumer explanation and general documenta? tion requirements are now likely ripe for disruption with accurate and transparent ML models.

Being interpretable, explainable, or otherwise transparent does not make a model good or trustworthy. Being interpretable, explaina? ble, or otherwise transparent enables humans to make a highly informed decision as to whether a model is good or trustworthy.

Redress for subjects of wrong ML-based decisions via prescribed appeal and override processes is perhaps the most important trust-enhancing use of explainable models and post hoc explanation. The ability to logically appeal automated decisions is sometimes called actionable recourse. It?s very hard for consumers?or job applicants, patients, prisoners, or students?to appeal automated and unexplainable decisions. Explainable ML models and post hoc explanation techniques should allow for ML- based automated decisions to be understood by decision subjects, which is the first step in a logical appeal process. Once users can demonstrate that either their input data or the logic of their decision is wrong, operators of ML systems should be able to override the initial errant decision.

Mechanisms that enable appeal and override of automated deci? sions should always be deployed with high-risk ML systems.


It?s also important to differentiate between interpretability and explanation. In the groundbreaking study ?Psychological Foundations of Explainability and Interpreta? bility in Artificial Intelligence? (https://oreil.ly/yz6GF), researchers at NIST were able to differentiate between interpretability and explainability using widely accepted notions of human cognition. According to NIST researchers, the similar, but not identical, concepts of interpretation and explanation are defined as follows:
Interpretation
A high-level, meaningful mental representation that contextualizes a stimulus and leverages human background knowledge. An interpretable model should provide users with a description of what a data point or model output means in context.
Explanation
A low-level, detailed mental representation that seeks to describe a complex pro? cess. An ML explanation is a description of how a particular model mechanism or output came to be.


Important Ideas for Interpretability and Explainability | 35

Interpretability is a much higher bar to reach than explainability. Achieving interpretability means putting an ML mechanism or result in context, and this is rarely achieved through models or post hoc explanations. Interpretability is usually achieved through plainly written explanations, compelling visualizations, or inter? active graphical user interfaces. Interpretability usually requires working with experts both within the applicable domain and in user interaction and experience, as well as other interdisciplinary professionals.

Let?s get more granular now, and touch on some of what makes an ML model interpretable or explainable. It?s very difficult for models achieve transparency if their input data is a mess. Let?s start our discussion there. When considering input data and transparency, think about the following:
Explainable feature engineering
We should avoid overly complex feature engineering if our goal is transparency. While deep features from autoencoders, principal components, or high-degree interactions might make our model perform better in test data, explaining such features is going to be difficult even if we feed them into an otherwise explainable model.
Meaningful features
Using a feature as an input to some model function assumes it is related to the function output, i.e., the model?s predictions. Using nonsensical or loosely related features because they improve test data performance violates a fundamental assumption of the way explaining models works. For example, if along with other features we use eye color to predict credit default, the model will likely train to some convergence criterion and we can calculate Shapley additive explanation (SHAP) values for eye color. But eye color has no real validity in this context and is not be causally related to credit default. While eye color may serve as a proxy for underlying systemic biases, claiming that eye color explains credit default is invalid. Apply commonsense?or even better, causal?discovery approaches to increase the validity of models and associated explanations.
Monotonic features
Monotonicity helps with explainability. When possible, use features that have a monotonic relationship with the target variable. If necessary, apply a feature engineering technique such as binning to induce monotonicity.
If we can rely on our data being usable for explainability and interpretability pur? poses, then we can use concepts like additive independence of inputs, constraints, linearity and smoothness, prototypes, sparsity, and summarization to ensure our model is as transparent as possible:


36  |  Chapter 2: Interpretable and Explainable Machine Learning

Additivity of inputs
Keeping inputs in an ML model separate, or limiting their interaction to a small group, is crucial for transparency. Traditional ML models are (in)famous for combining and recombining input features into an undecipherable tangle of high-degree interactions. In stark contrast, traditional linear models treat inputs in an independent and additive fashion. The output decision from a linear model is typically the simple linear combination of learned model parameters and input feature values. Of course, performance quality is often noticeably worse for traditional linear models as compared to traditional opaque ML models.
Enter the generalized additive model (GAM). GAMs keep input features inde? pendent, enabling transparency, but also allow for arbitrarily complex modeling of each feature?s behavior, dramatically increasing performance quality. We?ll also be discussing GAM?s close descendants, GA2Ms and explainable boosting machines (EBMs), in the next section. They all work by keeping inputs inde? pendent and enabling visualization of the sometimes complex manner in which they treat those individual inputs. In the end, they retain high transparency because users don?t have to disentangle inputs and how they affect one another, and the output decision is still a linear combination of learned model parameters and some function applied to data input values.
Constraints
Traditional unexplainable ML models are revered for their flexibility. They can model almost any signal-generating function in training data. Yet, when it comes to transparency, modeling every nook and cranny of some observed response function usually isn?t a great idea. Due to overfitting, it turns out it?s also not a great idea for performance on unseen data either. Sometimes what we observe in training data is just noise, or even good old-fashioned wrong. So, instead of overfitting to bad data, it?s often a good idea to apply constraints that both increase transparency and help with performance on unseen data by forcing models to obey causal concepts instead of noise.
Any number of constraints can be applied when training ML models, but some of the most helpful and widely available are sparsity, monotonicity, and interaction constraints. Sparsity constraints, often implemented by L1 regularization (which usually decreases the number of parameters or rules in a model), increases emphasis on a more manageable number of input parameters and internal learn? ing mechanisms. Positive monotonic constraints mean that as a model?s input increases, its output must never decrease. Negative monotonic constraints ensure that as a model?s input increases, its output can never increase. Interaction con? straints keep internal mechanisms of ML models from combining and recombin? ing too many different features. These constraints can be used to encourage models to learn interpretable and explainable causal phenomena, as opposed to


Important Ideas for Interpretability and Explainability | 37

focusing on nonrobust inputs, arbitrary nonlinearities, and high-degree interac? tions that may be drivers of errors and bias in model outcomes.
Linearity and smoothness
Linear functions are monotonic by default and can be described by single numeric coefficients. Smooth functions are differentiable, meaning they can be summarized at any location with a derivative function or value. Basically, linear and smooth functions are better behaved and typically easier to summarize. In contrast, unconstrained and arbitrary ML functions can bounce around or bend themselves in knots in ways that defy human understanding, making the inherent summarization that needs to occur for explanation nearly impossible.
Prototypes
Prototypes refer to well-understood data points (rows) or archetypal features (columns) that can be used to explain model outputs for some other previously unseen data. Prototypes appear in many places in ML, and have been used to explain and interpret model decisions for decades. Think of explaining a k-nearest neighbors prediction using the nearest neighbors or profiling clusters based on centroid locations?those are prototypes. Prototypes also end up being important for counterfactual explanations. Prototypes have even entered into the typically complex and opaque world of computer vision with this-looks-like-that deep learning (https://oreil.ly/zO1kx).
Sparsity
ML models, for better or worse, can now be trained with trillions of parameters (https://oreil.ly/mDwV5). Yet, it?s debatable whether their human operators can reason based on more than a few dozen of those parameters. The volume of information in a contemporary ML model will have to be summarized to be transparent. Generally, the fewer coefficients or rules in an ML model, the more sparse and explainable it is.
Summarization
Summarization can take many forms, including variable importance measures, surrogate models, and other post hoc ML approaches. Visualization is perhaps the most common vehicle for communicating summarized information about ML models, and approximation taken to compress information is the Achil? les? heel (https://oreil.ly/Dzfit) for summarization. Additive, linear, smooth, and sparse models are generally easier to summarize, and post hoc explanation pro? cesses have a better chance of working well in these cases.
Achieving transparency with ML takes some extra elbow grease as compared to popular and commoditized unexplainable approaches. Fear not. The recent paper ?Designing Inherently Interpretable Machine Learning Models? (https://oreil.ly/ Zv6YO) and software packages like InterpretML (https://oreil.ly/rML2n), H2O


38  |  Chapter 2: Interpretable and Explainable Machine Learning

(https://oreil.ly/ysEHE), and PiML (https://oreil.ly/Y2EFl) provide a great framework for training and explaining transparent models, and the remaining sections of the chapter highlight the most effective technical approaches, and common gotchas, for us to consider next time we start to design an AI system.
Explainable Models
For decades, many ML researchers and practitioners alike labored under a seemingly logical assumption that more complex models were more accurate. However, as pointed out by luminary professor Cynthia Rudin, in her impactful ?Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use Interpreta? ble Models Instead? (https://oreil.ly/i4syT), ?It is a myth that there is necessarily a trade-off between accuracy and interpretability.? We?ll dig into the tension around explainable models versus post hoc explanation later in the chapter. For now, let?s concentrate on the powerful idea of accurate and explainable models. They offer the potential for highly accurate decision making coupled with improved human learning from machine learning, actionable recourse processes and regulatory compliance, improved security, and better ability to address various issues, including inaccuracy and bias. These appealing characteristics make explainable ML models a general win-win for practitioners and consumers.
We?re going to go over some of the most popular types of explainable ML models next. We?ll start with the large class of additive models, including penalized regres? sion, GAMs, GA2Ms and EBMs. We?ll also be covering decision trees, constrained tree ensembles, and a litany of other options before moving on to post hoc explana? tion techniques.
Additive Models
Perhaps the most widely used types of explainable ML models are those based on traditional linear models: penalized regression models, GAMs, and GA2Ms (or EBMs). These techniques use contemporary methods to augment traditional model? ing approaches, often yielding noticeable improvements in performance. But they also keep interpretability high by treating input features in a separate and additive manner, or by allowing only a small number of interaction terms. They also rely on straightforward visualization techniques to enhance interpretability.
Penalized regression
We?ll start our discussion of explainable models with penalized regression. Penalized regression updates the typical 19th century approach to regression for the 21st cen? tury. These types of models usually produce linear, monotonic response functions with globally explainable results, like those of traditional linear models, but often


Explainable Models | 39

with a boost in predictive performance. Penalized regression models eschew the assumption-laden Normal Equation (https://oreil.ly/uXdeH) approaches to finding model parameters, and instead use more sophisticated constrained and iterative optimization procedures that allow for handling of correlation, feature selection, and treatment of outliers, all while using validation data to pick a better model automatically.
In Figure 2-1, we can see how a penalized regression model learns optimal coeffi? cients for six input features over more than 80 training iterations. We can see at the beginning of the optimization procedure that all parameter values start very small, and grow as the procedure begins to converge. This happens because it?s typical to start the training procedure with large penalties on input features. These penalties are typically decreased as training proceeds to allow a small number of inputs to enter the model, to keep model parameters artificially small, or both. (Readers may also hear penalized regression coefficients referred to as ?shrunken? on occasion.) At each iteration, as more features enter the model, or as coefficient values grow or change, the current model is applied to validation data. Training continues to a predefined number of iterations or until performance in validation data ceases to improve.

Figure 2-1. Regularization paths for selected features in an elastic net regression model (digital, color version(: https://oreil.ly/dR7Ty))





40  |  Chapter 2: Interpretable and Explainable Machine Learning

In addition to a validation-based early stopping procedure, penalized regression addresses outliers, feature selection, correlated inputs, and nonlinearity by using, respectively, the iteratively reweighted least squares (IRLS) technique; two types of penalties, on the L1 and L2 parameter norms; and link functions:
IRLS
IRLS is a well-established procedure for minimizing the effect of outliers. It starts like an old-fashioned regression, but after that first iteration, the IRLS procedure checks which rows of input data are leading to large errors. It then reduces the weight of those rows in subsequent iterations of fitting model coefficients. IRLS continues this fitting and down-weighting procedure until model parameters converge to stable values.
L1 norm penalty
Also known as least absolute shrinkage and selection operator (LASSO), L1 penalties keep the sum of the absolute model parameters to a minimum. This penalty has the effect of driving unnecessary regression parameters to zero, and selecting a small, representative subset of features for the regression model, while also avoiding the potential multiple comparison problems that arise in older stepwise feature selection. When used alone, L1 penalties are known to increase performance quality in situations where a large number of potentially correlated input features cause stepwise feature selection to fail.
L2 norm penalty
Also known as ridge or Tikhonov regression, L2 penalties minimize the sum of squared model parameters. L2 norm penalties stabilize model parameters, especially in the presence of correlation. Unlike L1 norm penalties, L2 norm penalties do not select features. Instead, they limit each feature?s impact on the overall model by keeping all model parameters smaller than they would be in the traditional solution. Smaller parameters make it harder for any one feature to dominate a model and for correlation to cause strange behavior during model training.
Link functions
Link functions enable linear models to handle common distributions of training data, such as using a logit link function to fit a logistic regression to input data with two discrete outcomes. Other common and useful link functions include the Poisson link function for count data or an inverse link function for gamma-distributed outputs. Matching outcomes to their distribution family and link function, such as a binomial distribution and logit link function for logistic regression, is absolutely necessary for training deployable models. Many ML models and libraries outside of penalized regression packages do not support the necessary link functions and distribution families to address fundamental assumptions in training data.


Explainable Models | 41

Contemporary penalized regression techniques usually combine the following:
? Validation-based early stopping for improved generalization
? IRLS to handle outliers
? L1 penalties for feature selection purposes
? L2 penalties for robustness
? Link functions for various target or error distributions
Readers can learn more about penalized regression in Elements of Statistical Learning (https://oreil.ly/FQjuv) (Springer), but for our purposes, it?s more important to know when we might want to try penalized regression. Penalized regression has been applied widely across many research disciplines, but it is a great fit for business data with many features, even datasets with more features than rows, and for datasets with a lot of correlated variables. Penalized regression models also preserve the basic interpretability of traditional linear models, so we think of them when we have many correlated features or need maximum transparency. It?s also important to know that penalized regression techniques don?t always create confidence intervals, t-statistics, or p-values for regression parameters. These types of measures are typically only available through bootstrapping, which can require extra computing time. The R packages elasticnet (https://oreil.ly/aooJV) and glmnet (https://oreil.ly/lFcpJ) are main? tained by the inventors of the LASSO and elastic net regression techniques, and the H2O generalized linear model (https://oreil.ly/Oeywm) sticks closely to the imple? mentation of the original software, while allowing for much-improved scalability.
An even newer and extremely interesting twist to penalized regression models is the super-sparse linear integer model, or SLIM (https://oreil.ly/2YPmFX1). SLIMs also rely on sophisticated optimization routines, with the objective of creating accurate models that only require simple arithmetic to evaluate. SLIMs are meant to train lin? ear models that can be evaluated mentally by humans working in high-risk settings, such as healthcare. If we?re ever faced with an application that requires the highest interpretability and the need for field-workers to evaluate results quickly, we think of SLIMs. On the other hand, if we?re looking for better predictive performance, to match that of many unexplainable ML techniques, but to keep a high degree of interpretability, think of GAMs?which we?ll be covering next.
Generalized additive models
GAMs are a generalization of linear models that allow a coefficient and function to be fit to each model input, instead of just a coefficient for each input. Training models in this manner allows for each input variable to be treated in a separate but nonlinear fashion. Treating each input separately keeps interpretability high.



42  |  Chapter 2: Interpretable and Explainable Machine Learning

Allowing for nonlinearity improves performance quality. Traditionally GAMs have relied on splines to fit nonlinear shape functions for each input, and most implemen? tations of GAMs generate convenient plots of the fitted shape functions. Depending on our regulatory or internal documentation requirements, we may be able to use the shape functions directly in predictive models for increased performance. If not, we may be able to eyeball some fitted shape functions and switch them out for a more explainable polynomial, log, trigonometric, or other simple function of the input feature that may also increase prediction quality. An interesting twist to GAMs was introduced recently with neural additive models (NAMs) (https://oreil.ly/nnCz5) and GAMI-Nets (https://oreil.ly/G_wCc). In these models, an artificial neural network is used to fit the shape functions. We?ll continue the theme of estimating shape functions with ML when we discuss explainable boosting machines in the next section. The Rudin Group also recently put forward a variant of GAMs in which monotonic step functions are used as shape functions for maximum interpretability. Check out ?Fast Sparse Classification for Generalized Linear and Additive Models? (https://oreil.ly/tCnld) to see those in action.

We?ll use the term shape function to describe the learned nonlinear relationship that GAM-like models apply to each input and interac? tion feature. These shape functions may be traditional splines, or they can be fit by machine learning estimators like boosted trees or neural networks.

The semi-official name for the ability to change out parts of a model to better match reality or human intuition is model editing. Being editable is another important aspect of many explainable models. Models often learn wrong or biased concepts from wrong or biased training data. Explainable models enable human users to spot bugs, and edit them out. The GAM family of models is particularly amenable to model editing, which is another advantage of these powerful modeling approaches. Readers can learn more about GAMs in Elements of Statistical Learning (https:// oreil.ly/5S49T). To try GAMs, look into the R gam (https://oreil.ly/aSAJK) package or the more experimental H2O (https://oreil.ly/5yoAd) or pyGAM (https://oreil.ly/1h-Y7) implementations.
GA2M and explainable boosting machines
The GA2M and EBM represent straightforward and material improvements to GAMs. Let?s address the GA2M first. The ?2? in GA2M refers to the consideration of a small group of pairwise interactions as inputs to the model. Choosing to include a small number of interaction terms in the GAM boosts performance without com? promising interpretability. Interaction terms can be plotted as contours along with



Explainable Models | 43

the standard two-dimensional input feature plots that tend to accompany GAMs. Some astute readers may already be familiar with the EBM, an important variant of GA2M. In an EBM, the shape functions for each input feature are trained iteratively using boosting. These response functions can be splines, decision trees, or even boosted ensembles of decision trees themselves. By training the additive model using boosting, we can usually get a more accurate final model than if we use the typical GAM backfitting method.
Because of these advances, GA2Ms and EBMs now rival, or exceed, unexplainable ML models in performance quality on tabular data, while also presenting the obvi? ous advantages of interpretability and model editing. If your next project is on structured data, try out EBMs using the InterpretML (https://oreil.ly/GMsbK) package from Microsoft Research. EBMs put the final touches on our discussion of additive models. We?ll move on next to decision trees, yet another type of high-quality, high- interpretability model with a track record of success across statistics, data mining, and ML.
Decision Trees
Decision trees are another type of popular predictive model. When used as single trees, and not as ensemble models, they learn highly interpretable flowcharts from training data and tend to exhibit better predictive performance than linear models. When used as ensembles, as with random forests and GBMs, they lose interpretability but tend to be even better predictors. In the following sections, we?ll discuss both single tree models and constrained decision tree ensembles that can retain some level of explainability.
Single decision trees
Technically, decision trees are directed graphs in which each interior node corre? sponds to an input feature. There are graph edges to child nodes for values of the input feature that create the highest target purity, or increased predictive quality, in each child node. Each terminal node or leaf node represents a value of the target feature, given the values of the input features represented by the path from the root to the leaf. These paths can be visualized or explained with simple if-then rules.
In plainer language, decision trees are data-derived flowcharts, just like we see in Figure 2-2. Decision trees are great for training interpretable models on structured data. They are beneficial when the goal is to understand relationships between the input and target variable with Boolean-like ?if-then? logic. They are also great at finding interactions in training data. Parent-child relationships, especially near the





44  |  Chapter 2: Interpretable and Explainable Machine Learning

top of the tree, tend to point toward feature interactions that can be used to better understand drivers of the modeling target, or they can be used as interaction terms to boost predictive accuracy for additive models. Perhaps their main advantage over more straightforward additive models is their ability to train directly on character values, missing data, nonstandardized data, and nonnormalized data. In this age of big (er, bad) data, decision trees enable the construction of models with minimal data preprocessing, which can help eliminate additional sources of human error from ML models.

Figure 2-2. A simple decision tree model forming a data-derived flowchart






Explainable Models | 45

With all this going for them, why shouldn?t we use decision trees? First of all, they?re only interpretable when they?re shallow. Decision trees with more than, say, five levels of if-then branches become difficult to interpret. They also tend to perform poorly on unstructured data like sound, images, video, and text. Unstructured data has become the domain of deep learning and neural networks. Like many models, decision trees require a high degree of tuning. Decision trees have many hyperparameters, or settings, which must be specified using human domain knowledge, grid searches, or other hyperparameter tuning methods. Such methods are time-consuming at a minimum, and at worst, sources of bias and overfitting. Single decision trees are also unstable, in that adding a few rows of data to training or validation data and retrain? ing can lead to a completely rearranged flowchart. Such instability is an Achilles? heel for many ML models.
Decision trees make locally optimal, or greedy, decisions each time they make a new branch or if-then rule. Other ML models use different optimization strategies, but the end result is the same: a model that?s not the best model for the dataset, but instead is one good candidate for the best model out of many, many possible options. This issue of many possible models for any given dataset has at least two names, the multiplicity of good models and the Rashomon effect (https://oreil.ly/nNwFY). (The name Rashomon (https://oreil.ly/wW-k5) is from a famous film in which witnesses describe the same murder differently.) The Rashomon effect is linked to another nasty problem, known as underspecification (https://oreil.ly/V_DaF), in which hyper? parameter tuning and validation-data-based model selection leads to a model that looks good in test scenarios but then flops in the real world. How do we avoid these problems with decision trees? Mainly by training single trees that we can see and examine, and for which we can ensure that their logic will hold up when they are deployed.
There are many variations on the broader theme of decision trees. For example, linear tree models fit a linear model in each terminal node of a decision tree, adding predictive capacity to standard decision trees while keeping all the explainability intact. Professor Rudin?s research group has introduced optimal sparse decision trees (https://oreil.ly/4kGpW) as a response to the issues of instability and underspecifica? tion. Another answer to these problems is manual constraints informed by human domain knowledge. We?ll address constrained decision tree ensembles next, but if readers are ready to try standard decision trees, there are many packages to try, and the R package rpart (https://oreil.ly/XLE1H) is one of the best.
Constrained XGBoost models
The popular gradient boosting package XGBoost now supports both monotonic con? straints (https://oreil.ly/39PhO) and interaction constraints (https://oreil.ly/yVAtK). As described earlier, user-supplied monotonic constraints force the XGBoost model to


46  |  Chapter 2: Interpretable and Explainable Machine Learning

preserve more explainable monotonic relationships between model inputs and model predictions. The interaction constraints can prevent XGBoost from endlessly recom? bining features. These software features turn what would normally be an opaque tree ensemble model replete with underspecification problems into a highly robust model capable of learning from data and accepting causally motivated constraints from expert human users. These newer training options, which enable human domain experts to use causal knowledge to set the direction of modeled relationships and to specify which input features should not interact, combined with XGBoost?s proven track record of scalability and performance, make this a hard choice to overlook when it comes to explainable ML. While not as directly interpretable as EBMs, because gradient boosting machines (GBMs) combine and recombine features into a tangle of nested if-then rules, constrained XGBoost models are amenable to a wide variety of post hoc explainable and visualization techniques. The explanation tech? niques often enable practitioners to confirm the causal knowledge they?ve injected into the model and to understand the inner workings of the complex ensemble.
An Ecosystem of Explainable Machine Learning Models
Beyond additive and tree-based models, there is an entire ecosystem of explainable ML models. Some of these models have been known for decades, some represent tweaks to older approaches, and some are wholly new. Whether they?re new or old, they challenge the status quo of unexplainable ML?and that?s a good thing. If readers are surprised about all the different options for explainable models, imagine a customer?s surprise when we can explain the drivers of our AI system?s decisions, help them confirm or deny hypotheses about their business, and provide explanations to users, all while achieving high levels of predictive quality. Indeed, there are lots of options for explainable models, and there?s sure to be one that might suit our next project well. Instead of asking our colleagues, customers, or business partners to blindly trust an opaque ML pipeline, consider explainable neural networks; k-nearest neighbors; rule-based, causal, or graphical models; or even sparse matrix factorization next time:
Causal models
Causal models, in which causal phenomena are linked to some prediction out? come of interest in a provable manner, are often seen as the gold standard of interpretability. Given that we can often look at a chart that defines how the model works and that they are composed using human domain or causal inference techniques, they?re almost automatically interpretable. They also don?t fit to noise in training data as badly as traditional unexplainable ML models. The only hard part of causal models is finding the data necessary to train them, or the training processes themselves. However, training capabilities for causal mod? els continue to improve, and packages like pyMC3 (https://oreil.ly/XY448) for Bayesian models and dowhy (https://oreil.ly/yacTr) for causal inference have been


Explainable Models | 47

enabling practitioners to build and train causal models for years. More recently, both Microsoft and Uber released a tutorial for causal inference (https://oreil.ly/ wQu72) with real-world use cases using modeling libraries from both companies, EconML (https://oreil.ly/Q448j) and causalml (https://oreil.ly/Tx12a), respectively. If we care about stable, interpretable models, watch this space carefully. Once considered to be nearly impossible to train, causal models are slowly working their way into the mainstream.
Explainable neural networks
Released (https://oreil.ly/xiKH_) and refined (https://oreil.ly/jgu71) by Wells Fargo Bank risk management, explainable neural networks (XNNs) prove that with a little ingenuity and elbow grease, even the most unexplainable models can be made explainable and retain high degrees of performance quality. Explainable neural networks use the same principals as GAMs, GA2Ms, and EBMs to achieve both high interpretability and high predictive performance, but with some twists. Like GAMs, XNNs are simply additive combinations of shape functions. How? ever, they add an additional indexed structure to GAMs. XNNs are an example of generalized additive index models (GAIMs), where GAM-like shape functions are fed by a lower projection layer that can learn interesting higher-degree interactions.
In an XNN, back-propagation is used to learn optimal combinations of variables (see c in online XNN figure(: https://oreil.ly/kBy92)) to act as inputs into shape functions learned via subnetworks (see b in online figure), which are then combined in an additive fashion with optimal weights to form the output of the network (see a in online figure). While likely not the simplest explainable model, XNNs have been refined to be more scalable, they automatically identify important interactions in training data, and post hoc explanations can be used to break down their predictions into locally accurate feature contributions. Another interesting type of transparent neural networks was put forward recently in ?Neural Additive Models: Interpretable Machine Learning with Neural Nets? (https://oreil.ly/ge-fk). NAMs appear similar to XNNs, except they forego the bottom layer that attempts to locate interaction terms.
k-nearest neighbors (k-NN)
The k-NN method uses prototypes, or similar data points, to make predictions. Reasoning in this way does not require training and its simplicity often appeals to human users. If k is set to three, an inference on a new point involves finding the three nearest points to the new point and taking the average or modal label from those three points as the predicted outcome for the new point. This kind of logic is common in our day-to-day lives. Take the example of residential real-estate appraisal. Price per square foot for one home is often evaluated as the average



48  |  Chapter 2: Interpretable and Explainable Machine Learning

price per square foot of three comparable houses. When we say, ?it sounds like,? or ?it feels like,? or ?it looks like,? we?re probably using prototype data points to make inferences about things in our own life. The notion of interpretation by comparison to prototypes is what led the Rudin Group to take on unexplainable computer vision models with this-looks-like-that (https://oreil.ly/qG7yT), a new type of deep learning that uses comparisons to prototypes to explain image classification predictions.
Rule-based models
Extracting predictive if-then rules from datasets is another long-running type of ML modeling. The simple Boolean logic of if-then rules is interpretable, as long as the number of rules, the number of branches in the rules, and the number of entities in the rules are constrained. RuleFit (https://oreil.ly/xc3-B) and skope-rules (https://oreil.ly/3w2BK) are two popular techniques that seek to find predictive and explainable rules from training data. Rule-based models are also the domain of the Rudin Group. Among their greatest hits for interpretable and high-quality rule-based predictors are certifiable optimal rule lists (CORELS) (https://oreil.ly/BgWCt) and scalable Bayesian rule lists (https://oreil.ly/RCu74). Code for these and other valuable contributions from the Rudin Group are available on their public code page (https://oreil.ly/aYDTE).
Sparse matrix factorization
Factoring a large data matrix into two smaller matrices is a common dimension reduction and unsupervised learning technique. Most older matrix factorization techniques reattribute the original columns of data across dozens of derived features, rendering the results of these techniques unexplainable. However, with the introduction of L1 penalties into matrix factorization, it?s now possible to extract new features from a large matrix of data, where only a few of the original columns have large weights on any new feature. By using sparse principal com? ponents analysis (SPCA) (https://oreil.ly/xqWFw), we might find, for example, that when extracting a new feature from customer financial data, that the new feature is composed exclusively of the debt-to-income and revolving account bal? ance features in our original dataset. We could then reason through this feature being related to consumer debt. Or if we find another new feature that has high weights for income, payments, and disbursements, then we could interpret that feature as relating to cash flow. Nonnegative matrix factorization (NMF) (https:// oreil.ly/CKydA) gives similar results but assumes that training data only takes on positive values. For unstructured data like term counts and pixel intensities, that assumption always holds. Hence, NMF can be used to find explainable summaries of topics in documents or to decompose images into explainable




Explainable Models | 49

dictionaries of subcomponents. Whether we use SPCA or NMF, the resulting extracted features can be used as explainable summaries, archetypes for explaina? ble comparisons, axes for visualization, or features in models. And it turns out that just like many explainable supervised learning models are special instances of GAMs, many unsupervised learning techniques are instances of generalized low-rank models (https://oreil.ly/YKUDa), which we can try out in H2O.
Now that we?ve covered the basics of explainable models, we?ll move on to post hoc explanation techniques. But before we do, remember that it?s perfectly fine to use explainable models and post hoc explanation together. Explainable models are often used to incorporate domain knowledge into learning mechanisms, to address inherent assumptions or limitations in training data, or to build functional forms that humans have a chance of understanding. Post hoc explanation is often used for visualization and summarization. While post hoc explanation is often discussed in terms of increasing transparency for traditional opaque ML models, there are lots of reasons to question that application, which we?ll introduce in the following section. Using explainable models and post hoc explanation together, to improve upon and validate the other, may be the best general application for both technologies.
Post Hoc Explanation
We?ll start off by addressing global and local feature attribution measures, then move on to surrogate models and popular types of plots for describing model behavior, and touch on a few post hoc explanation techniques for unsupervised learning. We?ll also discuss the shortcomings of post hoc explanations, which can be broadly summarized in three points that readers should keep in mind as they form their own impressions of and practices with explanations:
? If a model doesn?t make sense, its explanations won?t either. (We can?t explain nonsense.)
? ML models can easily grow so complex they can?t be summarized accurately.
? It?s difficult to convey explanatory information about ML systems to a broad group of users and stakeholders.
Despite these difficulties, post hoc explanation is almost always necessary for inter? pretability and transparency. Even models like logistic regression, thought of as highly transparent, must be summarized to meet regulatory obligations around trans? parency. For better or worse, we?re probably stuck with post hoc explanation and summarization. So let?s try to make it work as well as possible.





50  |  Chapter 2: Interpretable and Explainable Machine Learning

Many models must be summarized in a post hoc fashion to be interpretable. Yet, ML explanations are often incorrect, and check? ing them requires rigorous testing and comparisons with an under? lying explainable model.

Feature Attribution and Importance
Feature attribution is one of the most central aspects of explaining ML models. Feature attribution methods tell us how much an input feature contributed to the predictions of a model, either globally (across an entire dataset) or locally (for one or a few rows of data). Feature attribution values can typically be either positive or negative.
When we discuss feature importance, we mean a global measure of how much each feature contributes toward a model?s predictions. Unlike feature attributions, feature importance values are typically always positive. That is, feature importance values measure how significantly a feature contributed to a model?s overall behavior on a dataset. Feature attributions, on the other hand, give us more detail on the feature?s contribution.
While a handful of the more established global feature importance metrics do not arise from the aggregation of local measures, averaging (or otherwise aggregating) local feature attributions into global feature importances is common today. We?ll start by discussing newer methods for local feature attribution and then move on to global methods.

Global explanations summarize a model mechanism or prediction over an entire dataset or large sample of data. Local explanations perform the same type of summarization, but for smaller segments of data, down to a single row or cell of data.

?Feature importance? can be misleading as a name. We?re just approximating one model?s idea of what is important. Consider, for example, a computer vision secu? rity system that relies on gradient-based methods to detect ?important? aspects of video frames. Without proper training and deployment specifications, such systems will have a hard time picking up individuals wearing camouflage, as newer digital camouflage clothing is specifically designed to blend into various backgrounds, and to keep visual gradients between the fabric and different backgrounds smooth and undetectable. But isn?t people wearing camouflage one of the most important things to detect in a security application? For the rest of this section, keep in mind that ?feature importance? is highly dependent on the model?s understanding and training.



Post Hoc Explanation | 51

Local explanations and feature attribution
In some applications, it is crucial to determine which input features impacted a specific prediction?that is, to measure local feature attributions. Have a look at Figure 2-3 to see local feature attribution in action.

Figure 2-3. Local feature attribution for two explainable models for three individuals at the 10th, 50th, and 90th percentiles of predicted probabilities (digital, color version(: https://oreil.ly/4Y H))




52  |  Chapter 2: Interpretable and Explainable Machine Learning

Figure 2-3 shows two types of local feature attribution values, for two different models, for three different customers in a credit lending example. The first customer sits at the 10th percentile of probability of default, and they are likely to receive the credit product on offer. The second customer sits at the 50th percentile, and are unlikely to receive the product on offer. The third customer sits at the 90th percentile of probability of default and provides an example of an extremely high-risk applicant. The two models being summarized are a penalized generalized linear model (GLM) and a monotonically constrained GBM. Since both models have relatively simple structures that make sense for the problem at hand, we?ve done a good job handling the caveats brought up in our opening remarks about post hoc explanation. These explanations should be trustworthy, and they are simple enough to be accurately summarized.

One practical way to make explanations more interpretable is to compare them with some meaningful benchmark. That?s why we compare our feature attribution values to Pearson correlation and compare our partial dependence and individual conditional expectation (ICE) values to mean model predictions. This should enable viewers of the plot to compare more abstract explanatory values to a more understandable benchmark value.

How are we summarizing the models? local behavior? For the penalized GLM, we?re multiplying the model coefficients by the values of the input feature for each appli? cant. For the GBM, we?re applying SHAP. Both of these local feature attribution tech? niques are additive and locally accurate?meaning they sum to the model prediction. Both are measured from an offset, the GLM intercept or the SHAP intercept, which are similar in value but not equal. (That is not accounted for in Figure 2-3.) And both values can be generated in the same space: either log odds or predicted probability, depending on how the calculation is performed.
What do the plotted values tell us? For the low-risk applicant at the 10th percentile of probability of default, we can see that their most recent bill amount, BILL_AMT1, is highly favorable and driving their prediction downward. The SHAP values for the same customer tell a slightly different story, but the GBM was trained on a different set of features. The SHAP values paint a story of the applicant being lower-risk on all considered attributes. For the applicant at the 50th percentile, we see most local feature attribution values staying close to their respective intercepts, and for the high-risk applicant nearly all local feature attribution values are positive, pushing predictions higher. Both models seem to agree that it?s the applicant?s recent payment statuses (PAY_0, PAY_2, and PAY_3) that are driving risk, with the GBM and SHAP also focusing on payment amount information.



Post Hoc Explanation | 53

These are just two types of local feature attribution, and we?re going to discuss many others. But this small example likely brings up some questions that might get stuck in readers? heads if we don?t address them before moving on. First and foremost, the act of generating explanations does not make these good models; it simply means we get to make a more informed decision about whether the models are good or not. Second, it?s not uncommon for two different models to give different explanations for the same row of data. But just because it?s common, doesn?t make it right. It?s not something we should just accept and move past. While the models in Figure 2-3 appear to show adequate agreement when operating on the same features, this is, unfortunately, a best-case scenario for post hoc explanation, and it happened because we picked relatively simple models and made sure the sign of the GLM coefficients and the direction of the monotonic constraints of the GBM agreed with domain expertise. For complex, unexplainable models trained on hundreds or more correlated features, we?ll likely see much less agreement between model explanations, and that should raise red flags.
This notion that different models should yield similar explanations on the same row of data is called consistency. Consistency is a reasonable goal for high-risk applica? tions in order to bolster trust in outcomes and agreement among multiple important decision-making systems.
As readers are probably starting to pick up, post hoc explanation is complex and fraught, and this is why we suggest pairing these techniques with explainable models, so the models can be used to check the explanations and vice versa. Nevertheless, we push onward to an outline of the major local explanation techniques?counterfactu? als, gradient-based, occlusion, prototypes, SHAP values, and others:
Counterfactuals
Counterfactual explanations tell us what an input feature?s value would have to become to change the outcome of a model prediction. The bigger the swing of the prediction when changing an input variable by some standard amount, the more important that feature is, as measured from the counterfactual perspective. Check out Section 9.3, ?Counterfactual Explanations? (https://oreil.ly/9SNML), in Christoph Molnar?s Interpretable Machine Learning for more details. To try out counterfactual explanations, check out ?DiCE: Diverse Counterfactual Explana? tions for Machine Learning Classifiers? (https://oreil.ly/s_QaL) from Microsoft Research.
Gradient-based feature attribution
Think of gradients as regression coefficients for every little piece of a complex machine-learned function. In deep learning, gradient-based approaches to local explanations are common. When used for image or text data, gradients can often be overlaid on input images and text to create highly visual explanations depicting which parts of the input, if changed, would generate the largest changes


54  |  Chapter 2: Interpretable and Explainable Machine Learning

in the model output. Various tweaks to this idea are said to result in improved explanations, such as integrated gradients (https://oreil.ly/is_C-), layer-wise rele? vance propagation (https://oreil.ly/XKJ4B), deeplift (https://oreil.ly/6rhO0), and Grad-CAM (https://oreil.ly/Zkfeh). For an excellent, highly technical review of gradient-based explanations, see Ancona et al.?s ?Towards Better Understand? ing of Gradient-based Attribution Methods for Deep Neural Networks? (https:// oreil.ly/h7Lde). To see what can go wrong with these techniques, see ?Sanity Checks for Saliency Maps? (https://oreil.ly/a9fQA). We?ll return to these ideas in Chapter 9, when we train a deep learning model and compare various attribution techniques.
Occlusion
Occlusion refers to the simple and powerful idea of removing features from a model prediction and tracking the resulting change in the prediction. A big change may mean the feature is important, a small change may mean it?s less important. Occlusion is the basis of SHAP, leave-one-feature-out (LOFO), and many other explanation approaches, including many in computer vision and natural language processing. Occlusion can be used to generate explanations in complex models when gradients are unavailable. Of course, it?s never simple mathematically to remove inputs from a model, and it takes a lot of care to generate relevant explanations from the results of feature removal. For an authoritative review of occlusion and feature removal techniques, see Covert, Lundberg, and Lee?s ?Explaining by Removing: A Unified Framework for Model Explanation? (https://oreil.ly/662aS), where they cover 25 explanation methods that can be linked back to occlusion and feature removal.
Prototypes
Prototypes are instances of data that are highly representative of larger amounts of data. Prototypes are used to explain by summarization and comparison. A common kind of prototype is k-means (or other) cluster centroids. These prototypes are an average representation of a similar group of data. They can be compared to other points in their own cluster and in other clusters based on distances and in terms of real-world similarity. Real-world data is often highly heterogeneous, and it can be difficult to find prototypes that represent an entire dataset well. Criticisms are data points that are not represented well by prototypes. Together, prototypes and criticisms create a set of points that can be leveraged for summarization and comparison purposes to better understand both datasets and ML models. Moreover, several types of ML models, like k-NN and this-looks-like-that deep learning, are based on the notion of prototypes, which enhances their overall interpretability. To learn more about prototypes, look into Molnar?s chapter on prototypes and criticisms (https://oreil.ly/2IQYd).




Post Hoc Explanation | 55

There are various other local explanation techniques. Readers may have heard of treeinterpreter (https://oreil.ly/VECj5) or eli5 (https://oreil.ly/xOkSX), which generate locally accurate, additive attribution values for ensembles of decision trees. Alethia (https://oreil.ly/ZauV8) provides model summaries and local inference for rectified linear unit (ReLU) neural networks.
Next, we?ll devote a section to the discussion of Shapley values, one of most popular and rigorous types of local explanations available to data scientists. Before moving on, we?ll remind readers once more that these post hoc explanation techniques, including SHAP, are not magic. While the ability to understand which features influence ML model decisions is an incredible breakthrough, there is a great deal of literature pointing toward problems with these techniques. To get the most out of them, approach them with a staid and scientific mindset. Do experiments. Use explainable models and simulated data to assess explanation quality and validity. Does the explanation technique we?ve selected give compelling explanations on ran? dom data? (If so, that?s bad.) Does the technique provide stable explanations when data is mildly perturbed? (That?s a good thing, usually.) Nothing in life or ML is perfect, and that certainly includes local post hoc explanation.
Shapley values. Shapley values were created by the Nobel laureate economist and mathematician Lloyd Shapley. Shapley additive explanations (SHAP) unify (https:// oreil.ly/ilEXW) approaches such as LIME, LOFO, treeinterpreter, deeplift, and oth? ers to generate accurate local feature importance values, and they can be aggrega? ted or visualized to create consistent global explanations. Aside from their own Python package, SHAP (https://oreil.ly/LP4zm), and various R packages, SHAP is supported in popular machine learning software frameworks like H2O, LightGBM, and XGBoost.
SHAP starts out the same as many other explanation techniques, asking the intuitive question: what would the model prediction be for this row without this feature? So why is SHAP different from other types of local explanations? To be exact, in a system with as many complex interactions as a typical ML model, that simple question must be answered using an average of all possible sets of inputs that do not include the feature of interest. Those different groups of inputs are called coalitions. For a simple dataset with twenty columns, that means about half a million different model predictions on different coalitions are considered on the average. Now repeat that process of dropping and averaging for every prediction in our dataset, and we can see why SHAP takes into account more information than most other local feature attribution approaches.
There are many different flavors of SHAP, but the most popular are Kernel SHAP, Deep SHAP, and Tree SHAP. Of these, Tree SHAP is less approximate, and Kernel and Deep SHAP are more approximate. Kernel SHAP has the advantage of being


56  |  Chapter 2: Interpretable and Explainable Machine Learning

usable on any type of model, i.e., being model-agnostic. It?s like local interpretable model-agnostic explanations (LIME) combined with the coalitional game theory approach. However, with more than a few inputs, Kernel SHAP often requires unten? able approximations to achieve tolerable runtimes. Kernel SHAP also requires the specification of background data, or data that is used by an explanation technique during the process of calculating explanations, which can have a large influence on the final explanation values. Deep SHAP also relies on approximations and may be less suitable than easier-to-compute gradient-based explanations, depending on the model and dataset at hand. On the other hand, Tree SHAP is fast and more exact. But as its name suggests, it?s only suitable for tree-based models.

Many explanation techniques rely on ?background data,? which is data separate from the observations being explained that is used to support the calculation of the explanations. For example, when we compute SHAP values, we form coalitions by removing features from the data. When we have to evaluate the model on this coalition, we substitute the missing values by sampling from the background data. Background data can have a large effect on explanations and must be chosen carefully so as not to conflict with statistical assumptions of explanation techniques and to provide the right context for explanations.

Two of the major places where data scientists tend to go wrong with Tree SHAP are in the interpretation of SHAP itself and in failing to understand the assumptions inherent in different parameterizations of the technique. For interpretation, let?s start with recognizing SHAP as an offset from the average model prediction. SHAP values are calculated in reference to that offset, and large SHAP values mean the feature causes the model prediction to depart from the average prediction in some noticeable way. Small SHAP values mean the feature doesn?t move the model prediction too far from the average prediction. We?re often tempted to read more into SHAP than is actually there. We tend to seek causal or counterfactual logic from SHAP values, and this is simply not possible. SHAP values are the weighted average of the feature?s con? tribution to model predictions across a vast number of coalitions. They don?t provide causal or counterfactual explanations, and if we?d like for them to be meaningful at all, the underlying model must also be meaningful.

A SHAP value can be interpreted as the difference in model out? come away from the average prediction attributed to a certain input feature value.





Post Hoc Explanation | 57

Tree SHAP also asks users to make trade-offs. Based on how features that are missing from each coalition are filled in (the perturbation method), we choose between differ? ent philosophies of explanations and different shortcomings.
If no background data is explicitly passed in, the default settings in Tree SHAP use tree_path_dependent perturbations, which use the number of training examples that went down each path of the tree to approximate the background data distribution. If background data is supplied to Tree SHAP, then this data is sampled from to fill in missing feature values in what are known as interventional feature perturbations. The additional flexibility of choosing a background dataset allows explanations to be more targeted, but choosing an appropriate background dataset can be a com? plex exercise, even for experienced practitioners. We?ll talk more about choosing an appropriate background dataset and the effects it can have in Chapter 6.
Besides the added complexity, the main shortcoming of interventional feature perturbations is that they create unrealistic data instances. This means that when we?re evaluating the attribution of a feature, we may be doing so on a bunch of fake observations that would never be observed in the real world. On the other hand, intervening allows us to skirt the need to worry about correlated features. In contrast, tree_path_dependent feature perturbations are more sensitive to correlated features, but they try to only consider data points that are realistic.

Due to general issues with correlation and information overload, good explanations usually require that the underlying model is trained on a smaller number of uncorrelated features with a direct relationship to the modeling target. As said by the authors of the excellent paper ?True to the Model or True to the Data?? (https:// oreil.ly/ze8_z), including Scott Lundberg, the creator of SHAP: ?Currently, the best case for feature attribution is when the features that are being perturbed are independent to start with.?

This web of assumptions and limitations mean that we still have to be careful and thoughtful, even when using Tree SHAP. We can make things easier on us, though. Correlation is the enemy of many explainable models and post hoc explanation tech? niques, and SHAP is no different. The authors like to start with a reasonable number of input features that do not have serious multicollinearity issues. On a good day, we?d have found those features using a causal discovery approach as well. Then we?d use domain knowledge to apply monotonic constraints to input features in XGBoost. For general feature importance purposes, we?d use Tree SHAP with tree_path_dependent feature perturbation. For an application like credit scoring, where the proper context is defined by regulatory commentary, we might use interventional SHAP values and background data. For instance, certain regulatory commentary on the generation of explanations for credit denials in the US suggests (https://oreil.ly/W0VxD) that we


58  |  Chapter 2: Interpretable and Explainable Machine Learning

?identify the factors for which the applicant?s score fell furthest below the average score for each of those factors achieved by applicants whose total score was at or slightly above the minimum passing score.? This means our background dataset should be composed of applicants with predictions just above the cutoff to receive the credit product.
Critical applications of local explanations and feature importance. Local feature attribution values? most mission-critical application is likely meeting regulatory requirements. The primary requirement now in the US is to explain credit denials with adverse action notices. The key technical component for the adverse action reporting process are reason codes. Reason codes are plain-text explanations of a model prediction described in terms of a model?s input features. They are a step beyond local feature attributions, in which raw local feature attribution values are matched to reasons a product can be denied. Consumers should then be allowed to review the reason codes for their negative prediction and follow a prescribed appeal process if data inputs or decision factors are demonstrably wrong.
Adverse action reporting is a specific instance of a more high-level notion known as actionable recourse, where transparent model decisions are based on factors users have control over and can be appealed by model users and overridden by model operators. Many forthcoming and proposed regulations, such as those in Cal? ifornia (https://oreil.ly/Wc25G), Washington, DC (https://oreil.ly/jh5xG), and the EU (https://oreil.ly/kuoEI), are likely to introduce similar requirements for explanation or recourse. When working under regulatory scrutiny, or just to do the right thing when making important decisions for other human beings, we?ll want our explanations to be as accurate, consistent, and interpretable as possible. While we expect that local feature attributions will be one of the most convenient technical tools to generate the raw data needed to comply, we make the best explanations when combining local feature attributions with explainable models and other types of explanations, like those described in the subsequent sections of this chapter.
Global feature importance
Global feature importance methods quantify the global contribution of each input feature to the predictions of a complex ML model over an entire dataset, not just for one individual or row of data. Global feature importance measures sometimes give insight into the average direction that a variable pushes a trained ML function, and sometimes they don?t. At their most basic, they simply state the magnitude of a feature?s relationship with the response as compared to other input features. This is hardly ever a bad thing to know, and since most global feature importance measures are older approaches, they are often expected by model validation teams. Figure 2-4 provides an example of feature importance plots, in which we are comparing the global feature importance of two models.


Post Hoc Explanation | 59



Figure 2-4. Global feature importance for two explainable models compared to Pearson correlation (digital, color version(: https://oreil.ly/C2dF0))
Charts like this help us answer questions like the following:
? Does one ordering of feature importance make more sense than the other?
? Does this plot reflect patterns we know the models should have learned from training data?
? Are the models placing too much emphasis on just one or two features?
Global feature importance is a straightforward way to conduct such basic checks. In Figure 2-4, we compare feature importance to Pearson correlation to have some baseline understanding of which features should be important. Between Pearson correlation and the two models, we can see that everyone agrees that PAY_0 is the most important feature. However, the GLM places nearly all of its decision-making importance on PAY_0, while the GBM spreads importance over a larger set of inputs. When models place too much emphasis on one feature, as the GLM in Figure 2-4 does, it can make them unstable in new data if the distribution of the most important feature drifts, and it makes adversarial manipulation of a model easy. For the GLM in Figure 2-4, a bad actor would only have to alter the value of a single feature, PAY_0, to drastically change the model?s predictions.




60  |  Chapter 2: Interpretable and Explainable Machine Learning

Global feature importance metrics can be calculated in many ways. Many data scien? tists are first introduced to feature importance when learning about decision trees. A common feature importance method for decision trees is to sum up the change in the splitting criterion for every split in the tree based on a certain feature. For instance, if a decision tree (or tree ensemble) is trained to maximize information gain for each split, the feature importance assigned to some input feature is the total information gain associated with that feature every time it is used in the tree(s). Perturbation-based feature importance is another common type of feature importance measurement, and it?s a model-agnostic technique, meaning it can be used for almost all types of ML models. In perturbation-based feature importance, an input feature of interest is shuffled (sorted randomly) and predictions are made. The difference in some original score, usually the model prediction or something like mean squared error (MSE), before and after shuffling the feature of interest is the feature importance. Another similar approach is known as leave-one-feature-out (LOFO, or leave-one-covariate-out, LOCO). In the LOFO method, a feature is some? how dropped from the training or prediction of a model?say, by retraining without the feature and making predictions, or by setting the feature to missing and making predictions. The difference in the relevant score between the model with the feature of interest and without the feature of interest is taken to be the LOFO importance.
While permutation and LOFO are typically used to measure the difference in predic? tions or the difference in an accuracy or error score, they have the advantage of being able to estimate the impact of a feature on nearly anything associated with a model. For instance, it?s quite possible to calculate permutation- or LOFO-based contributions to a fairness metric, allowing us to gain insight into which specific features are contributing to any detected sociological bias. This same motif can be reapplied for any number of measures of interest about a model?error functions, security, privacy, and more.

Techniques like perturbation feature importance and LOFO can be used to estimate contributions to many quantities besides model predictions.


Because these techniques are well-established, we can find a great deal of related information and software packages. For a great discussion on split-based feature importance, check out Chapter 3 (https://oreil.ly/P2gEb) of an Introduction to Data Mining (Pearson). Section 10.13.1 of Elements of Statistical Learning (https://oreil.ly/ jQOX6) introduces split-based feature importance, and Section 15.3.2 provides a brief introduction to permutation-based feature importance in the context of ran? dom forests. The R package vip (https://oreil.ly/7Jo9s) provides a slew of variable importance plots, and we can try LOFO with the Python package lofo-importance


Post Hoc Explanation | 61

(https://oreil.ly/jP5jV). Of course, there are drawbacks and weaknesses to most global feature importance techniques. Split-based feature importance has serious consis? tency problems, and like so much of post hoc explainable AI (XAI), correlation will lead us astray with permutation-based and LOFO approaches. The paper ?There Is No Free Variable Importance? (https://oreil.ly/bx6QA) gets into more details of the sometimes disqualifying issues related to global feature importance. But, as is repeated many times in this chapter, constrained models with a reasonable number of noncorrelated and logical inputs will help us avoid the worst problems with global feature importance.
SHAP also has a role to play in global feature importance. SHAP is by nature a local feature attribution method, but it can be aggregated and visualized to create global feature importance information. Of the many benefits SHAP presents over more traditional feature importance measures, its interpretation is perhaps the most impor? tant. With split-based, permutation, and LOFO feature importance, oftentimes we only see a relative ordering of the importance of the input features, and maybe some qualitative notions of how a feature actually contributes to model predictions. With SHAP values, we can calculate the average absolute value of the feature attributions across a dataset, and this measure of feature importance has a clear and quantitative relationship with model predictions on individual observations. SHAP also provides many levels of granularity for feature importance. While SHAP can be directly aggre? gated into a feature importance value, that process can average out important local information. SHAP opens up the option of examining feature importance values anywhere from the most local level?a single row?to the global level. For instance, aggregating SHAP across important segments, like US states or different genders, or using the numerous visualizations in the SHAP package, can provide a view of feature importance that might be more informative and more representative than a single average absolute value. Like permutation and LOFO feature importance, SHAP can also be used to estimate importance of quantities besides model predictions. It?s capable of estimating contributions to model errors (https://oreil.ly/oYG5d) and to fairness metrics, like demographic parity (https://oreil.ly/4aHtK).
This concludes our discussion of feature importance. Whether it?s global or local, fea? ture importance will probably be the first post hoc XAI technique we encounter when we?re building models. As this section shows, there?s a lot more to feature importance than just bar charts or running SHAP. To get the best results with feature importance, we?ll have to be familiar with the strengths and weaknesses of the many approaches. Next, we?ll be covering surrogate models?another intriguing explanation approach, but also one that requires thought and caution.






62  |  Chapter 2: Interpretable and Explainable Machine Learning

Surrogate Models
Surrogate models are simple models of complex models. If we can build a simple, interpretable model of a more complex model, we can use the explainable character? istics of the surrogate model to explain, summarize, describe, or debug the more complex model. Surrogate models are generally model agnostic. We can use them for almost any ML model. The problem with surrogate models is they are mostly a trick-of-the-trade technique, with few mathematical guarantees that they truly represent the more complex model they are attempting to summarize. That means we have to be careful when using surrogate models, and at a minimum, check that they are accurate and stable representations of the more complex models they seek to summarize. In practice, this often means looking at different types of accuracy and error measures on many different data partitions to ensure that fidelity to the more complex model?s predictions is high, and that it remains high in new data and stable during cross-validation. Surrogate models also have many names. Readers may have heard about model compression, model distillation, or model extraction. All of these either are surrogate modeling techniques or are closely related. Like feature importance, there are also many different types of surrogate models. In the sections that follow, we?ll start out with decision tree surrogate models, which are typically used to construct global explanations, then transition into LIME and anchors, which are typically used to generate local explanations.
Decision tree surrogates
Decision tree surrogate models are usually created by training a decision tree on the original inputs and predictions of a complex model. Feature importance, trends, and interactions displayed in the surrogate model are then assumed to be indicative of the internal mechanisms of the complex model. There are no theoretical guarantees that the simple surrogate model is highly representative of the more complex model. But, because of the structure of decision trees, these surrogate models create very interpretable flowcharts of a more complex model?s decision-making processes, as is visible in Figure 2-5. There are prescribed methods for training decision tree surrogate models, for example those explored in ?Extracting Tree-Structured Repre? sentations of Trained Networks? (https://oreil.ly/ewW3O) and ?Interpretability via Model Extraction? (https://oreil.ly/RnFep).









Post Hoc Explanation | 63



Figure 2-5. A decision tree surrogate model creates a flowchart for a monotonic gradient boosting machine

Decision tree surrogate models can be highly interpretable when they create flowcharts of more complex models.







64  |  Chapter 2: Interpretable and Explainable Machine Learning

In practice, it usually suffices to measure the fidelity of the surrogate tree?s predictions to the complex model?s predictions in the data partition of interest with metrics like logloss, root mean square error (RMSE), or R2, and to measure the stability of those predictions with cross-validation. If a surrogate decision tree fails to provide high fidelity with respect to the more complex model, more sophisticated explainable models, like EBMs or XNNs, can be considered as surrogates instead.
The surrogate model in Figure 2-5 was trained on the same inputs as the more com? plex GBM it seeks to summarize, but instead of training on the original target that indicates payment delinquency, it is trained instead on the predictions of the GBM. When interpreting this tree, features that are higher or used more frequently are considered to be more important in the explained GBM. Features that are above and below one another can have strong interactions in the GBM, and other techniques discussed in this chapter, such as explainable boosting machines and a comparison between partial dependence and ICE, can be used to confirm the existence of those interactions in training data or in the GBM model.

Decision tree surrogates can be used to find interactions for use in linear models or for LIMEs. EBMs and differences between partial dependence and ICE can also be used to find interactions in data and models.

The decision paths in the tree can also be used to gain some understanding of how the more complex GBM makes decisions. Tracing the decision path from the root node in Figure 2-5 to the average predictions at the bottom of the tree, we can see that those who have good statuses for their most recent (PAY_0) and second (PAY_2) most recent repayments and have somewhat large fifth most recent payment amounts (PAY_AMT5) are most likely not to have future delinquency, according to the original model. Those customers who have unfavorable most recent and fifth most recent repayment statuses appear most likely to have future payment problems. (The PAY_3 splits exhibit a large amount of noise and are not interpreted here.) In both cases, the GBM appears to be considering both recent and past repayment behaviors to come to its decision about future payments. This prediction behavior is logical but should be confirmed by other means when possible. Like most surrogate models, decision tree surrogates are useful and highly interpretable, but should not be used for important explanation or interpretation tasks on their own.
Linear models and local interpretable model-agnostic explanations
LIME is one of the earliest, the most famous, and the most criticized post hoc explanation techniques. As the name indicates, it?s most often used for generating



Post Hoc Explanation | 65

local explanations, by fitting a linear model to the predictions of some small region of a more complex model?s predictions. While this is its most common usage, it?s a reductionist take on the technique.
When first introduced in the 2016 article ??Why Should I Trust You?? Explaining the Predictions of Any Classifier? (https://oreil.ly/e9WL2), LIME was presented as a framework with several admirable qualities. The most appealing of these was a sparsity requirement for local explanations. If our model has one thousand features and we apply SHAP, we will get back one thousand SHAP values for every prediction we want to explain. Even if SHAP is perfect for our data and model, we?ll still have to sort through one thousand values every time we want to explain a prediction. The framework of LIME circumvents this problem by requiring that generated explana? tions are sparse, meaning that they key into the small handful of locally important features instead of all the features included in the model to be explained.

A LIME value can be interpreted as the difference between a LIME prediction and the associated LIME intercept attributed to a certain input feature value.


The rest of the framework of LIME specifies fitting an interpretable surrogate model to some weighted local region of another model?s predictions. And that?s a more faith? ful description of the LIME framework?a locally weighted interpretable surrogate model with a penalty to induce sparsity, fit to some arbitrary, more complex model?s predictions. These ideas are useful and quite reasonable.

Always ensure LIME fits the underlying response function well with fit statistics and visualizations, and that the local model inter? cept is not explaining the most salient phenomenon driving a given prediction.

It?s the popular implementation of LIME that gets inexperienced users into trouble and that presents security problems. For tabular data, the software package lime (https://oreil.ly/dKYHV) asks users to select a row to be explained, generates a fairly simplistic sample of data based on a specified input dataset, weights the sample by the user-selected row, fits a LASSO regression between the weighted sample and the more complex model?s predictions on the sample, and finally, uses the LASSO regression coefficients to generate explanations for the user-specified row. There are a lot of potential issues in that implementation:




66  |  Chapter 2: Interpretable and Explainable Machine Learning


? The sampling is a problem for real-time explanation because it requires data generation and fitting a model in the midst of a scoring pipeline, and it also opens users up to data poisoning attacks that can alter explanations.
? Generated LIME samples can contain large proportions of out-of-range data that can lead to unrealistic local feature importance values.
? Local feature importance values are offsets from the local GLM intercept, and this intercept can sometimes account for the most important local phenomena.
? Extreme nonlinearity and high-degree interactions in the selected local region of predictions can cause LIME to fail completely.
Because LIME can be used on almost any type of ML model to generate sparse explanations, it can still be a good tool in our kit if we?re willing to be patient and think through the LIME process. If we need to use LIME, we should plot the LIME predictions versus our more complex model predictions and analyze them with RMSE, R2, or similar. We should be careful about the LIME intercept and make sure that it?s not explaining our prediction on its own, rendering the actual LIME values useless. To increase the fidelity of LIME, try LIME on discretized input features and on manually constructed interactions. (We can use decision tree surrogates to guess at those interactions.) Use cross-validation to estimate standard deviations or even confidence intervals for local feature contribution values. And keep in mind that poor fit or inaccuracy of local linear models is itself informative, often indicating extreme nonlinearity or high-degree interactions in that region of predictions.
Anchors and rules
On the heels of LIME, probably with some lessons in mind, the same group of researchers released another model-agnostic local post hoc explanation technique named anchors. Anchors generates high-fidelity sets of plain-language rules to describe a machine learning model prediction, with a special focus on finding the most important features for the prediction at hand. Readers can learn more about anchors in ?Anchors: High-Precision Model-Agnostic Explanations? (https://oreil.ly/ V1rFJ) and the software package anchor (https://oreil.ly/KNGF3). While anchors is a prescribed technique with documented strengths and weaknesses, it?s just one special instance of using rule-based models as surrogate models. As discussed in the first part of the chapter, rule-based models have good learning capacity for nonlinearities and interactions, while still being generally interpretable. Many of the rule-based models highlighted previously could be evaluated as surrogate models.






Post Hoc Explanation | 67

Plots of Model Performance
In addition to feature importance and surrogate models, partial dependence, individ? ual conditional expectation, and accumulated local effects (ALE) plots have become popular for describing trained model behaviors with respect to input features. In this section, we?ll go over partial dependence and ICE, how partial dependence should really only be used with ICE, and discuss ALE as a more contemporary replacement for partial dependence.
Partial dependence and individual conditional expectation
Partial dependence plots show us the estimated average manner in which machine- learned response functions change based on the values of one or two input features of interest, while averaging out the effects of all other input features. Remember the averaging-out part. We?ll circle back to that. Partial dependence plots can show the nonlinearity, nonmonotonicity, and two-way interactions in complex ML models and can be used to verify monotonicity of response functions trained under monotonic constraints. Partial dependence is introduced along with tree ensembles in Elements of Statistical Learning (https://oreil.ly/35vig), Section 10.13. ICE plots are a newer, local, and less well-known adaptation of partial dependence plots. They depict how a model behaves for a single row of data as one feature is changed. ICE pairs nicely with partial dependence in the same plot to provide more local information to augment the more global information provided by partial dependence. ICE plots were introduced in the paper ?Peeking Inside the Black Box: Visualizing Statistical Learning with Plots of Individual Conditional Expectation? (https://oreil.ly/Vuraz). There are lots of software packages for us to try partial dependence and ICE. For Python users, check out PDPbox (https://oreil.ly/RbzII) and PyCEbox (https://oreil.ly/ KcdET). For R users, there are the pdp (https://oreil.ly/pE0Ue) and ICEbox (https:// oreil.ly/QbsDA) packages. Also, many modeling libraries support partial dependence without having to use an external package.
Partial dependence should be paired with ICE plots, as ICE plots can reveal inaccur? acies in partial dependence due to the averaging-out of strong interactions or the presence of correlation. When ICE curves diverge from partial dependence curves, this can indicate strong interactions between input features, which is another advan? tage of using them together. We can then use EBMs or surrogate decision trees to confirm the existence of the interaction in training data or the model being explained. One more trick is to plot partial dependence and ICE with a histogram of the feature of interest. That gives good insight into whether any plotted prediction is trustworthy and supported by training data. In Figure 2-6, partial dependence, ICE, and a histogram of PAY_0 are used to summarize the behavior of a monotonic GBM.




68  |  Chapter 2: Interpretable and Explainable Machine Learning



Figure 2-6. Partial dependence and ICE for an important input variable with accom? panying histogram and mean target value overlay for a monotonic gradient boosting machine (digital, color version(: https://oreil.ly/zFr70))

Due to multiple known weaknesses, partial dependence should not be used without ICE, or ALE should be used in place of partial dependence.


On the top, we can see a histogram of PAY_0 and that there is simply not much data for customers who are more than two months late on their most recent payment. On the bottom, we see partial dependence and ICE curves for customers at the deciles of predicted probability of default. Partial dependence and ICE help us confirm the monotonicity of the constrained GBM?s response function for PAY_0. The model appears to behave reasonably, even when there is not much data to learn from for


Post Hoc Explanation | 69

higher values of PAY_0. Probability of default increases in a monotonic fashion as customers become more late on their most recent payment, and probability of default is stable for higher values of PAY_0, even though there is almost no data to support the classifier in that region. It might be tempting to believe that every time we use monotonic constraints we would be protected against our ML models learning silly behaviors when there isn?t much training data for them to learn from, but this is not true. Yes, monotonic constraints help with stability and underspecification problems, and partial dependence and ICE help us spot these problems if they occur, but we got lucky here. The truth is we need to check all our models for unstable behavior in sparse domains of the training data, and be prepared to have specialized models, or even human case workers, ready to make good predictions for these difficult rows of data.

Comparing explainable model shape functions, partial dependence, ICE, or ALE plots with a histogram can give a basic qualitative measure of uncertainty in model outcomes, by enabling visual discovery of predictions that are based on only small amounts of training data.

Here?s one more word of advice before moving onto ALE plots: like feature impor? tance, SHAP, LIME, and all other explanation techniques that operate on a back? ground dataset, we have to think through issues of context with partial dependence and ICE. They both use sneaky implicit background data. For partial dependence, it?s whatever dataset we?re interested in with all the values of the feature being plotted set to a certain value. This alters patterns of interactions and correlation, and although it?s an exotic concern, it opens us up to data poisoning attacks as addressed in ?Fooling Partial Dependence via Data Poisoning? (https://oreil.ly/SVFmU). For ICE, the implicit background dataset is a single row of data with the feature of interest set to a certain value. Watch out for the ICE values being plotted being too unrealistic in combination with the rest of the observed data in that row.
Accumulated local effect
ALE is a newer and highly rigorous method for representing the behavior of an ML model across the values of an input feature, introduced in ?Visualizing the Effects of Predictor Variables in Black Box Supervised Learning Models? (https://oreil.ly/ TFFTK). Like partial dependence plots, ALE plots show the shape?i.e., nonlinearity or nonmonotonicity?of the relationship between predictions and input feature val? ues. ALE plots are especially valuable when strong correlations exist in the training data, a situation where partial dependence is known to fail. ALE is also faster to calculate than partial dependence. Try it with ALEPlot (https://oreil.ly/4o0wH) in R, and the Python edition ALEPython (https://oreil.ly/_qKs8).


70  |  Chapter 2: Interpretable and Explainable Machine Learning

Cluster Profiling
While a great deal of focus has been placed on explaining supervised learning models, sometimes we need to use unsupervised techniques. Feature extraction and clustering are two of the most common unsupervised learning tasks. We discussed how to make feature extraction more explainable with sparse methods like SPCA and NMF when we covered explainable models. And with the application of very much established post hoc methods of profiling, clustering can often be made more transparent too. The simplest approach is to use means and medians to describe cluster centroids, or to create prototypical members of a dataset based on clusters. From there, we can use concepts associated with prototypes such as summarization, comparison, and criticisms to better understand our clustering solution. Another technique is to apply feature extraction, particularly sparse methods, to project a higher-dimensional clustering solution into two or three dimensions for plotting. Once plotted on sparse interpretable axes, it?s easier to use our domain knowledge to understand and check a group of clusters. Distributions of features can also be employed to understand and describe clusters. The density of a feature within a cluster can be compared to its density in other clusters or to its overall distribution. Features with the most dissimilar distributions versus other clusters or the entire training data can be seen as more important to the clustering solution. Finally, surrogate models can be applied to explain clusters. Using the same inputs as the clustering algorithm and the cluster labels as the target, we fit an interpretable classifier like a decision tree to our clusters and use the surrogate model?s interpretable characteristics to gain insight into our clustering solution.
Stubborn Difficulties of Post Hoc Explanation in Practice
Unless we?re careful, we can get into very murky areas with post hoc explanation. We?ve taken care to discuss the technical drawbacks of the techniques, but there is even more to consider when working with these techniques on real-world high-risk applications. As a refresher, Professor Rudin?s ?Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use Interpretable Models Instead? (https://oreil.ly/vR4Xl) lays out the primary criticisms of post hoc explanation for opaque ML models and high-risk uses. According to Rudin, explanations for tradi? tional ML models are:
? Premised on the wrong belief that unexplainable models are more accurate than explainable models
? Not faithful enough to the actual inner workings of complex models
? Often nonsensical




Stubborn Difficulties of Post Hoc Explanation in Practice |  71

? Difficult to calibrate against external data
? Unnecessarily complicated
That?s why in this chapter we advocate for the use of post hoc explanation with inter? pretable models, where the model and the explanations can act as process controls for one another. Even using explanations in this more risk-aware manner, there are still serious issues to address. This section will highlight the concerns we see the most in practice. We?ll close this section by highlighting the advantages of using explainable models and post hoc explanations in combination. But as the transparency case will show, even if we get things mostly right on the technical side of transparency, human factors are still immensely important to the final success, or failure, of a high-stakes ML application.
Christoph Molnar has not only been prolific in teaching us how to use explanations; he and coauthors have also been quite busy researching their drawbacks. If read? ers would like to dig into details of issues with common explanation approaches, we?d suggest both ?General Pitfalls of Model-Agnostic Interpretation Methods for Machine Learning Models? (https://oreil.ly/DeZ0J) and the earlier Limitations of Inter? pretable Machine Learning Methods (https://oreil.ly/HYgJ6). Next we?ll outline the issues we see the most in practice?confirmation bias, context, correlation and local dependence, hacks, human interpretation, inconsistency, and explanation fidelity:
Confirmation bias
For most of this chapter, we?ve discussed increased transparency as a good thing. While it certainly is, increased human understanding of ML models, and the ability to intervene in the functioning of those models, does open cracks for confirmation bias to sneak into our ML workflow. For example, let?s say we?re convinced a certain interaction should be represented in a model based on past experience in similar projects. However, that interaction just isn?t appearing in our explainable model or post hoc explanation results. It?s extremely hard to know if our training data is biased, and missing the known important interaction, or if we?re biased. If we intervene in the mechanisms of our model to somehow inject this interaction, we could simply be succumbing to our own confirmation bias.
Of course, a total lack of transparency also allows confirmation bias to run wild, as we can spin a model?s behavior in whatever way we like. The only real way to avoid confirmation bias is to stick to the scientific method and battle-tested scientific principles like transparency, verification, and reproducibility.
Context
?Do not explain without context? (https://oreil.ly/A-GxX), say Dr. Przemys?aw Biecek and team. In practice, this means using logical and realistic background data to generate explanations, and making sure background data cannot be


72  |  Chapter 2: Interpretable and Explainable Machine Learning

manipulated by adversaries. Even with solid background data for explanations, we still need to ensure our underlying ML models are operating in a logical context as well. For us, this means a reasonable number of uncorrelated input features, all with direct relationships to the modeling target.
Correlation and dependencies
While correlation may not prevent ML algorithms from training and generating accurate in silico predictions in many cases, it does make explanation and inter? pretation very difficult. In large datasets, there are typically many correlated features. Correlation violates the principle of independence, meaning we can?t realistically interpret features on their own. When we attempt to remove a fea? ture, as many explanation techniques do, another correlated feature will swoop in and take its place in the model, nullifying the effect of the attempted removal and the removal?s intended meaning as an explanation tool. We also rely on perturbing features in explanations, but if features are correlated, it makes very little sense to perturb just one of them to derive an explanation. Worse, when dealing with ML models, they can learn local dependencies, meaning different correlation-like relationships, on a row-by-row basis. It?s almost impossible to think through the complexities of how correlation corrupts explanations, much less how complex local dependencies might do the same.
Hacks
Explanation techniques that use background data can be altered by adversaries. These include LIME and SHAP, as explored in ?Fooling LIME and SHAP: Adver? sarial Attacks on Post hoc Explanation Methods? (https://oreil.ly/ljDkp), and par? tial dependence, as described in ?Fooling Partial Dependence via Data Poisoning? (https://oreil.ly/MJUz7). While these hacks are likely an exotic concern for now, we don?t want to be part of the first major hack on ML explanations. Make sure that the code used to generate background data is kept secure and that background data cannot be unduly manipulated during explanation calculations. Data poisoning, whether against training data or background data, is easy for inside attackers. Even if background data is safe, explanations can still be mis? interpreted in malicious ways. In what?s known as fairwashing (https://oreil.ly/ RBwSb), explanations for a sociologically biased ML model are made to look fair, abusing explanations to launder bias while still exposing model users to real harm.
Human interpretation
ML is difficult to understand, sometimes even for experienced practitioners and researchers. Yet, the audience for ML explanations is much broader than just industry experts. High-risk applications of ML often involve important decisions for other human beings. Even if those other human beings are highly educated, we cannot expect them to understand a partial dependence and ICE plot or an array of SHAP values. To get transparency right for high-stakes situations,

Stubborn Difficulties of Post Hoc Explanation in Practice |  73

we?ll need to work with psychologists, domain experts, designers, user interaction experts, and others. It will take extra time and product iterations, with extensive communications between technicians, domain experts, and users. Not doing this extra work can result in abject failure, even if technical transparency goals are met, as ?Case Study: Graded by Algorithm? on page 77 discusses.
Inconsistency
Consistency refers to stable explanations across different models or data samples. Consistency is difficult to achieve and very important for high-stakes ML appli? cations. In situations like credit or pretrial release decisions, people might be sub? ject to multiple automated decisions and associated explanations, and especially so in a more automated future. If the explanations provide different reasons for the same outcome decision, this will complicate already difficult situations. To increase consistency, explanations need to key into real, generalizable phenom? ena in training data and in the application domain. To achieve consistency, we need to train our models on a reasonable number of independent features. The models themselves also need to be parsimonious, that is, constrained to obey real-world relationships. Conversely, consistent explanations are impossible for complex, underspecified, uninterpretable models with numerous and correlated inputs.
Measuring explanation quality
Imagine training a model, eyeballing the results, and then assuming it?s work? ing properly and deploying it. That?s likely a bad idea. But that?s how we all work with post hoc explanations. Given all the technical concerns raised in previous sections, we obviously need to test explanations and see how they work for a given data source and application, just like we do with any other ML technique. Like our efforts to measure model quality before we deploy, we should be making efforts to measure explanation quality too. There are published proposals for such measurements and commonsense testing techniques we can apply. ?Towards Robust Interpretability with Self-Explaining Neural Networks? (https://oreil.ly/t4322) puts forward explicitness, i.e., whether explanations are immediately understandable; faithfulness, i.e., whether explanations are true to known important factors; and stability, i.e., whether explanations are consistent with neighboring data points. ?On the (In)fidelity and Sensitivity of Explana? tions? (https://oreil.ly/kSiSS) introduces related eponymous tests. Beyond these proposals for formal measurement, we can check that explainable model mecha? nisms and post hoc explanations confirm one another if we use both explainable models and explanations. If older trustworthy explanations are available, we can use those as benchmarks against which to test new explanations for fidelity. Also, stability tests, in which the data or model is perturbed in small ways, should generally not lead to major changes in post hoc explanations.



74  |  Chapter 2: Interpretable and Explainable Machine Learning

Test explanations before deploying them in high-risk use cases. While the lack of ground truth for explanations is a difficult barrier, explanations should be compared to interpretable model mechanisms. Comparisons to benchmark explanations, explicit? ness and fidelity measures, perturbation, comparison to nearest neighbors, and simulated data can also be used to test explanation quality.

There?s no denying the appeal of explaining any model, no matter how complex, simply by applying some postprocessing. But given all the technical and worldly problems we?ve just been over, hopefully we?ve convinced readers that traditional ML model explanation is kind of a pipe dream. Explaining the unexplainable might not be impossible, but it?s technically difficult today, and once we consider all the human factors required to achieve real-world transparency, it becomes even more difficult.
Pairing Explainable Models and Post Hoc Explanation
As we end the chapter?s more technical discussions, we?d like to highlight new research that helps elucidate why explaining traditional ML models is so difficult and leave readers with an example of combining explainable models and post hoc explanations. Two recent papers relate the inherent complexity of traditional ML models to transparency difficulties. First, ?Assessing the Local Interpretability of Machine Learning Models? (https://oreil.ly/FQQd_) proxies complexity with the number of runtime operations associated with an ML model decision, and shows that as the number of operations increases, interpretability decreases. Second, ?Quantify? ing Model Complexity via Functional Decomposition for Better Post-Hoc Interpret? ability? (https://oreil.ly/DWTRF) uses number of features, interaction strength, and main effect complexity to measure the overall complexity of ML models, and shows that models that minimize these criteria are more reliably interpretable. In summary, complex models are hard to explain and simpler models are easier to explain, but certainly not easy. Figure 2-7 provides an example of augmenting a simple model with explanations, and why even that is difficult.
Figure 2-7 contains a trained three-level decision tree with a highlighted decision path, and the Tree SHAP values for a single row of data that follows that decision path. While Figure 2-7 looks simple, it?s actually illustrative of several fundamental problems in ML and ML explanation. Before we dive into the difficulties presented by Figure 2-7, let?s bask in the glory of a predictive model with its entire global decision- making mechanism on view, and for which we can generate numeric contributions of the input features to any model prediction. That level of transparency used to be reserved for linear models, but all the new approaches we?ve covered in this chapter make this level of transparency a reality for a much broader class of higher-capacity models. This means that if we?re careful we can train more sophisticated models,


Pairing Explainable Models and Post Hoc Explanation  |  75

that learn more from data, and still be able to interpret and learn from the results ourselves. We can learn more from data, do so reliably, and learn more as humans from the results. That?s a huge breakthrough.

Figure 2-7. A simple explainable model paired with post hoc explanation information

Use explainable models and post hoc explanations together to check one another and to maximize the transparency of ML models.




76  |  Chapter 2: Interpretable and Explainable Machine Learning

Now let?s dig into the difficulties in Figure 2-7 while keeping in mind that these problems always exist when using ML models and post hoc XAI. (We can just see them and think through them in this simple case.) Notice that the decision path for the selected individual considers PAY_0, PAY_6, and PAY_AMT1. Now look at the Tree SHAP values. They give higher weight to PAY_2 than PAY_6, and weigh PAY_5 over PAY_AMT1, but PAY_2 and PAY_5 are not on the decision path. This occurs because the SHAP calculation takes into account artificial observations with different values for PAY_0, PAY_AMT1, and PAY_6, and those observations go down different decision paths. We?d see this behavior whether or not we used tree_path_dependent or interventional feature perturbations.
This phenomenon is unintuitive, but it is correct and not the result of approximation error. We could have used a different package or approach and probably generated local explanations that were true to the single decision path highlighted in Figure 2-7, but then we wouldn?t have the deep theoretical support that accompanies Shapley values and SHAP. At least with SHAP, we know why our explanations show this effect. In general, explaining ML models is very hard, and for many different reasons. Always test explanations before deploying them in high-risk contexts, and make sure you understand the post hoc techniques you?re applying.
Chapter 6 will go more into the details of how SHAP uses background data and calculates feature attributions based on different settings. We all need to understand these subtleties before using feature attribution methods like SHAP for high-risk applications. There?s so much to think about on just the technical side of explana? tions. The following case will dig into some of the human factors of explanations, which might be even more difficult to get right.
Case Study: Graded by Algorithm
Adding transparency into ML models is no easy task. Even if we get the technical aspects of explainable models and post hoc explanations right, there are still many human factors that must be handled carefully. The so-called A-level scandal (https:// oreil.ly/s54hO) in the UK is an object lesson in failing to understand human factors for high-risk ML-based decisions. As COVID lockdowns took hold across the United Kingdom in the spring of 2020, students, teachers, and government officials realized that standardized tests could not take place as usual. As a first attempt to remedy the problems with national standardized testing, teachers were asked to estimate student performance on the important A-level exams that determine college entrance and affect other important life outcomes. Unfortunately, teacher estimates were seen as implausibly positive, to the level that using the estimated student performance would be unfair to past and future students.



Case Study: Graded by Algorithm  |  77

To address teachers? positive biases, the government Office of Qualifications and Examinations Regulation (Ofqual) decided to implement an algorithm to adjust teacher predictions. The statistical methodology of the adjustment algorithm was implemented by experts and a model document (https://oreil.ly/0gM6i) was released after students received their grades. The algorithm was designed to generate a final distribution of grades that was similar to results in previous years. It preserved teacher rankings, but used past school performance to adjust grades downward. Students in Scotland were first to see the results. In that part of the UK, ?35.6 percent of grades were adjusted down by a single grade, while 3.3 percent went down by two grades, and 0.2 went down by three,? according to ZDNet (https://oreil.ly/h47XJ).
Over the next few months, student outcry over likely bias against poorer schools and regions of the country caused a massive, slow-burning AI incident. Even though officials had seen the problems in Scotland, they applied the same process in England, but instated a free appeals process and the right for students to retest at a later date. In the end, irreparable damage to public trust could not be undone, and the transparent but biased notion of adjusting individual scores by past school performance was too much for many to stomach. In the end, the UK government decided to use the origi? nal teacher estimates. According to Wired (https://oreil.ly/DoV4O), ?the government has essentially passed the administrative buck to universities, who will now have to consider honouring thousands more offers?they have said that despite u-turn, it will not be possible to honour all original offers.? The same article also pointed out that teacher estimates of student performance had shown racial bias in the past. What a mess.
Shockingly, other institutions have also adopted the idea of algorithmic scores for life-changing college entrance exams. The International Baccalaureate (IB) is an elite educational program that offers an advanced uniform curriculum for secondary school students all over the world. In the Spring of 2020, the IB used an algorithm for student scores that was reported to be (https://oreil.ly/OT05d) ?hastily deployed after canceling its usual springtime exams due to COVID-19. The system used signals including a student?s grades on assignments and grades from past grads at their school.? Because of the timing, unanticipated negative scores were extremely hurtful to students applying to US colleges and universities, who reserve spaces for IB students based on past performance, but can cancel based on final performances, ?shattering their plans for the fall and beyond.? Some students? algorithmic scores were so low that they may have lost placement in prestigious universities in the US and their safety schools in their home country. What?s worse, and unlike the Ofqual algorithm, the IB was not forthcoming with how their algorithm worked, and appeals came with an almost $800 price tag.





78  |  Chapter 2: Interpretable and Explainable Machine Learning

Putting aside the IB?s lack of transparency, there seem to be three major issues at play in these incidents. Scale is an inherent risk of ML, and these algorithms were used on many students across the globe. Large scale translates to high materiality, and transparency alone is not enough to offset issues of trust and bias. Understanding is not trust. Ofqual?s technical report and other public analyses (https://oreil.ly/QAB8R) were over the heads of many students and parents. But what was not over their heads is that poorer areas have worse public schools, and that affected students twice in 2020?once in an overall manner like every year, and then again when their scores were adjusted downward. The second factor was the seriousness of the decision. College admittance plays a huge role in the rest of many people?s lives. The serious nature of the decision cranks up the materiality to an even higher degree?possibly to an impossible degree, where failure becomes guaranteed. ML is inherently probabilis? tic. It will be wrong. And when the stakes are this high, the public just might not accept it.
The third major issue at play here is the clear nature of disparate impact. For example, very small classes were not scored with the algorithm. Where are there the most very small classes? Private schools. A Verge article (https://oreil.ly/eySQu) claims that ?fee-paying private schools (also known as independent schools) dispro? portionately benefited from the algorithm used. These schools saw the amount of grades A and above increase by 4.7 percent compared to last year.? ZDNet (https:// oreil.ly/7mnEd) reported that the ?pass rate for students undertaking higher courses in deprived locations across Scotland was reduced by 15.2%, in comparison to 6.9% in more affluent areas.? Adjusting by postal code or past school performance bakes in systemic biases, and students and parents understood this at an emotional level. As quoted in the BBC (https://oreil.ly/vPQq1), Scotland?s Education Secretary realized in the end that the debacle left ?young people feeling their future had been determined by statistical modeling rather than their own ability.? We should think about how we would feel if this incident had affected us or our children. Despite all the hype around automated decision making, almost no one wants to feel that their future is set by an algorithm.
Although this may have been a doomed, impossibly high-materiality application of ML from the beginning, more could have been done to increase public trust. For example, Ofqual could have published the algorithm before applying it to students. They also could have taken public feedback on the algorithm before using it. Jeni Tennison, a UK-based open data advocate, notes (https://oreil.ly/4Unct), ?Part of the problem here is that these issues came out only after the grades were given to students, when we could have been having these discussions and been examining the algorithm and understanding the implications of it much, much earlier.? The take-home lessons here are that technical transparency is not the same as broad social understanding, and that understanding, if it can even be achieved, does not guarantee trust. Even if we?ve done a good job on technical transparency, as put forward in


Case Study: Graded by Algorithm  |  79

this chapter, there is still a great deal of work that must be done to ensure an ML system works as expected for users, or subjects. Finally, this is just one AI incident, and although it?s a big one, it shouldn?t cause us to overlook the smaller ones that are harming people right now, and we have to keep in mind that even more people will be harmed by AI systems in the future. As Tennison put it, ?This has hit the headlines, because it affects so many people across the country, and it affects people who have a voice. There?s other automated decision making that goes on all the time, around benefits, for example, that affect lots of people who don?t have this strong voice.?
Resources
Further Reading
? An Introduction to Machine Learning Interpretability (O?Reilly) (https://oreil.ly/iyz08)
? ?Designing Inherently Interpretable Machine Learning Models? (https://oreil.ly/jbGNt)
? Explanatory Model Analysis (CRC Press) (https://oreil.ly/Yt_Xm)
? ?General Pitfalls of Model-Agnostic Interpretation Methods for Machine Learning Models? (https://oreil.ly/On9uS)
? Interpretable Machine Learning (https://oreil.ly/BHy1L)
? Limitations of Interpretable Machine Learning Methods (https://oreil.ly/VHMWh)
? ?On the Art and Science of Explainable Machine Learning? (https://oreil.ly/myVr8)
? ?Psychological Foundations of Explainability and Interpretability in Artificial Intelligence? (https://oreil.ly/HUomp)
? ?When Not to Trust Your Explanations? (https://oreil.ly/9Oxa6)













80  |  Chapter 2: Interpretable and Explainable Machine Learning



CHAPTER 3

Debugging Machine Learning Systems for Safety and Performance



For decades, error or accuracy on holdout test data has been the standard by which machine learning models are judged. Unfortunately, as ML models are embedded into AI systems that are deployed more broadly and for more sensitive applications, the standard approaches for ML model assessment have proven to be inadequate. For instance, the overall test data area under the curve (AUC) tells us almost noth? ing about bias and algorithmic discrimination, lack of transparency, privacy harms, or security vulnerabilities. Yet, these problems are often why AI systems fail once deployed. For acceptable in vivo performance, we simply must push beyond tradi? tional in silico assessments designed primarily for research prototypes. Moreover, the best results for safety and performance occur when organizations are able to mix and match the appropriate cultural competencies and process controls described in Chapter 1 with ML technology that promotes trust. This chapter presents sections on training, debugging, and deploying ML systems that delve into the numerous technical approaches for testing and improving in vivo safety, performance, and trust in AI. Note that Chapters 8 and 9 present detailed code examples for model debugging.


81



?Calibration? on page 89	GOVERN 1.2, GOVERN 1.4, MAP 2.3, MEASURE 1, MEASURE 2.1, MEASURE
2.3
?Construct validity? on page 89	GOVERN 1.1, MAP 2.1, MAP 2.3, MAP 3.3

?Assumptions and limitations? on page 90

GOVERN 1.2, GOVERN 1.4, GOVERN 6.1, MAP 2

?Default loss functions? on page 91	MAP 2.3, MEASURE 1, MEASURE 2.1, MEASURE 2.3
?Multiple comparisons? on page 91	MAP 2.3, MEASURE 1, MEASURE 2.1, MEASURE 2.3

?The future of safe and robust machine learning? on page 91

MAP 2.3, MEASURE 2.6

?Software Testing? on page 92	GOVERN 1.1, GOVERN 1.2, GOVERN 4.3, GOVERN 6.1, MAP 2.3, MAP 4,
MEASURE 1.3

?Traditional Model Assessment? on page 93

GOVERN 1.1, GOVERN 1.2, GOVERN 1.4, MAP 2.3, MAP 4, MEASURE 1,
MEASURE 2.1, MEASURE 2.3

?Distribution shifts? on page 95	GOVERN 1.2, GOVERN 1.5, MAP 2.3, MEASURE 1, MEASURE 2.1, MEASURE
2.3, MEASURE 2.4, MANAGE 2.2, MANAGE 2.3, MANAGE 2.4, MANAGE 3,
MANAGE 4.1

?Epistemic uncertainty and data sparsity? on page 96

MAP 2.3, MEASURE 1, MEASURE 2.1, MEASURE 2.3

?Instability? on page 97	MAP 2.3, MEASURE 1, MEASURE 2.1, MEASURE 2.3, MEASURE 2.4,
MANAGE 2.2, MANAGE 2.3, MANAGE 2.4, MANAGE 3, MANAGE 4.1
?Leakage? on page 98	MAP 2.3, MEASURE 1, MEASURE 2.1, MEASURE 2.3
?Looped inputs? on page 99	MAP 2.3, MEASURE 1, MEASURE 2.1, MEASURE 2.3
?Overfitting? on page 99	MAP 2.3, MEASURE 1, MEASURE 2.1, MEASURE 2.3
?Shortcut learning? on page 100	MAP 2.3, MEASURE 1, MEASURE 2.1, MEASURE 2.3
?Underfitting? on page 100	MAP 2.3, MEASURE 1, MEASURE 2.1, MEASURE 2.3
?Underspecification? on page 101	MAP 2.3, MEASURE 1, MEASURE 2.1, MEASURE 2.3
?Residual Analysis? on page 103	GOVERN 1.2, MAP 2.3, MAP 3.2, MAP 5.1, MEASURE 1, MEASURE 2.1,
MEASURE 2.3
?Sensitivity Analysis? on page 107	GOVERN 1.2, MAP 2.3, MAP 3.2, MAP 5.1, MEASURE 1, MEASURE 2.1,
MEASURE 2.3
?Benchmark Models? on page 110	GOVERN 1.1, GOVERN 1.2, GOVERN 1.4, MAP 2.3, MEASURE 2.13,
MANAGE 2.1
?Remediation: Fixing Bugs? on page 112 GOVERN, MAP, MANAGE
?Domain Safety? on page 114	GOVERN 1.2, GOVERN 1.7, GOVERN 3, GOVERN 4.1, GOVERN 4.3, GOVERN
5, MAP 1.2, MAP 1.6, MAP 2.3, MAP 3.1, MAP 5, MEASURE 1, MEASURE
2.5, MEASURE 2.6, MEASURE 3, MEASURE 4, MANAGE 1, MANAGE 4.3
?Model Monitoring? on page 116	GOVERN 1.2, GOVERN 1.3, GOVERN 1.4, GOVERN 1.5, MAP 2.3, MAP
3.5, MAP 4, MAP 5.2, MEASURE 1.1, MEASURE 2.4, MEASURE 2.6,
MEASURE 2.7, MEASURE 2.8, MEASURE 2.10, MEASURE 2.11, MEASURE
2.12, MEASURE 3.1, MEASURE 3.3, MEASURE 4, MANAGE 2.2, MANAGE
2.3, MANAGE 2.4, MANAGE 3, MANAGE 4



82  |  Chapter 3: Debugging Machine Learning Systems for Safety and Performance



Training
The discussion of training ML algorithms begins with reproducibility, because without that, it?s impossible to know if any one version of an ML system is really any better than another. Data and feature engineering will be addressed briefly, and the training section closes by outlining key points for model specification.
Reproducibility
Without reproducibility, we?re building on sand. Reproducibility is fundamental to all scientific efforts, including AI. Without reproducible results, it?s very hard to know if day-to-day efforts are improving, or even changing, an ML system. Reproducibility helps ensure proper implementation and testing, and some customers may simply demand it. The following techniques are some of the most common that data scien? tists and ML engineers use to establish a solid, reproducible foundation for their ML systems:
Benchmark models
Benchmark models are important safety and performance tools for training, debugging, and deploying ML systems. They?ll be addressed several times in this chapter. In the context of model training and reproducibility, we should always build from a reproducible benchmark model. This allows for a checkpoint for rollbacks if reproducibility is lost, but it also enables real progress. If yesterday?s benchmark is reproducible, and today?s gains above and beyond that benchmark are also reproducible, that?s real and measurable progress. If system performance metrics bounce around before changes are made, and they?re still bouncing around after changes are made, we have no idea if our changes helped or hurt.
Hardware
As ML systems often leverage hardware acceleration via graphical processing units (GPUs) and other specialized system components, hardware is still of special interest for preserving reproducibility. If possible, try to keep hardware as similar as possible across development, testing, and deployment systems.
Environments
ML systems always operate in some computational environment, specified by the system hardware, system software, and our data and ML software stack. Changes in any of these can affect the reproducibility of ML outcomes. Thankfully,

Training  |  83

tools like Python virtual environments and Docker containers that preserve software environments have become commonplace in the practice of data science. Additional specialized environment management software from Dom? ino (https://oreil.ly/USwuG), gigantum (https://oreil.ly/1cE7-), TensorFlow TFX (https://oreil.ly/kHKvx), and Kubeflow (https://oreil.ly/F9ZaL) can provide even more expansive control of computational environments.
Metadata
Data about data is essential for reproducibility. Track all artifacts associated with the model, e.g., datasets, preprocessing steps, data and model validation results, human sign-offs, and deployment details. Not only does this allow for rolling back to a specific version of a dataset or model, but it also allows for detailed debugging and forensic investigations of AI incidents. For an open source exam? ple of a nice tool for tracking metadata, check out TensorFlow ML Metadata (https://oreil.ly/gmHkg).
Random seeds
Set by data scientists and engineers in specific code blocks, random seeds are the plow horse of ML reproducibility. Unfortunately, they often come with language- or package-specific instructions. Seeds can take some time to learn in different software, but when combined with careful testing, random seeds enable the building blocks of intricate and complex ML systems to retain reproducibility. This is a prerequisite for overall reproducibility.
Version control
Minor code changes can lead to drastic changes in ML results. Changes to our own code plus its dependencies must be tracked in a professional version control tool for any hope of reproducibility. Git and GitHub are free and ubiquitous resources for software version control, but there are plenty of other options to explore. Crucially, data can also be version-controlled with tools like Pachyderm (https://oreil.ly/DvMCo) and DVC (https://oreil.ly/S59Qv), enabling traceability in changes to data resources.
Though it may take some experimentation, some combination of these approaches and technologies should work to assure a level of reproducibility in our ML systems. Once this fundamental safety and performance control is in place, it?s time to con? sider other baseline factors like data quality and feature engineering.

Several topics like benchmarks, anomaly detection, and monitoring are ubiquitous in model debugging and ML safety, and they appear in several different sections and contexts in this chapter.





84  |  Chapter 3: Debugging Machine Learning Systems for Safety and Performance

Data Quality
Entire books have been written about data quality and feature engineering for ML and ML systems. This short subsection highlights some of the most critical aspects of this vast practice area from a safety and performance perspective. First and foremost, biases, confounding features, incompleteness, and noise in development data form important assumptions and define limitations of our models. Other basics, like the size and shape of a dataset, are important considerations. ML algorithms are hungry for data. Both small data and wide, sparse data can lead to catastrophic performance failures in the real world, because both give rise to scenarios in which system performance appears normal on test data, but is simply untethered to real-world phenomena. Small data can make it hard to detect underfitting, underspecification, overfitting, or other fundamental performance problems. Sparse data can lead to overconfident predictions for certain input values. If an ML algorithm did not see certain data ranges during training due to sparsity issues, most ML algorithms will issue predictions in those ranges with no warning that the prediction is based on almost nothing. Fast-forwarding to our chapter case discussion, there is simply not enough training video in the world to fill out the entire space of example situations that self-driving cars need to learn to safely navigate. For example, people crossing the road at night on a bicycle is a danger most humans will recognize, but without many frames of labeled video of this somewhat rare event, a deep learning system?s ability to handle this situation will likely be compromised due to sparsity in training data.
A number of other data problems can cause safety worries, such as poor data quality leading to entanglement or misrepresentation of important information and overfitting, or ML data and model pipeline problems. In the context of this chapter, entanglement means features, entities, or phenomena in training data proxying for other information with more direct relationships to the target (e.g., snow proxying for a Husky in object recognition). Overfitting refers to the memorization of noise in training data and the resulting optimistic error estimates, and pipeline issues are problems that arise from combining different stages of data preparation and modeling components into one prediction-generating executable. Table 3-1 can be applied to most standard ML data to help identify common data quality problems with safety and performance implications.









Training  |  85

Table 3-1. Common data quality problems, with symptoms and proposed solutions. Adapted from the George Washington University DNSC 6314 (Machine Learning I) class notes with permission.



Biased data: When a dataset contains information about the phenomenon of interest, but that information is
consistently and systematically wrong. (See Chapter 4 for more information.)
Character data: When certain columns, features, or instances are represented with strings of characters instead of numeric values.

Data leakage: When information from validation or test partitions leaks into training data.



Dirty data: A combination of all the issues in this table, very common in real-world datasets.



Disparate feature scales: When features, such as age and income, are recorded on different scales.
Duplicate data: Rows, instances, or entities that occur more than intended.


Entanglement: When features, entities, or phenomena in training data proxy for other information with more direct relationships to the target (e.g., snow
proxying for a Husky in object recognition).
Fake or poisoned data: Data, features, attributes, phenomena, or entities that are injected into or manipulated in training data to elicit artificial model outcomes.
High cardinality categorical features: Features such as postal codes or product identifiers that represent many categorical levels of the same attribute.

Biased models and biased, dangerous, or inaccurate results. Perpetuation of past social biases and discrimination.

Information loss. Biased models and biased, dangerous, or inaccurate results. Long, intolerable training times.

Unreliable or dangerous out-of- domain predictions. Overfit models and inaccurate results. Overly optimistic in silico performance estimates.

Information loss. Biased models and biased, inaccurate results. Long, intolerable training times. Unstable and unreliable parameter estimates and rule generation.
Unreliable or dangerous out-of- domain predictions.
Unreliable parameter estimates, biased models, and biased, inaccurate results.
Biased results due to unintentional overweighting of identical entities during training. Biased models, and biased, inaccurate results.
Unreliable or dangerous out-of- domain predictions. Shortcut learning.


Unreliable or dangerous out-of- domain predictions. Biased models and biased, inaccurate results.

Overfit models and inaccurate results. Long, intolerable compute times. Unreliable or dangerous out- of-domain predictions.

Consult with domain experts and stakeholders. Apply the scientific method and design of experiment (DOE) (https://oreil.ly/0kDC9) approaches. (Get more data. Get better data.)
Various numeric encoding approaches (e.g., label encoding, target or feature encoding). Appropriate algorithm selection, e.g., tree-based models, naive Bayes classification.
Data governance. Ensuring all dates in training are earlier than in validation and test. Ensuring identical identifiers do not occur across partitions. Careful application of feature engineering? engineer after partitioning, not before.
Combination of solution strategies herein.




Standardization. Appropriate algorithm selection, e.g., tree-based models.

Careful data cleaning in consultation with domain experts.


Apply the scientific method and DOE approaches. Apply interpretable models and post hoc explanation. In-domain testing.

Data governance. Data security. Application of robust ML approaches.


Target or feature encoding variants, average-by-level (or similar, e.g., median, BLUP). Discretization.
Embedding approaches, e.g.,
entity embedding neural networks, factorization machines.




86  |  Chapter 3: Debugging Machine Learning Systems for Safety and Performance




Imbalanced target: When one target class or value is much more common than others.



Incomplete data: When a dataset does not encode information about the phenomenon of interest. When
uncollected information confounds model results.
Missing values: When specific rows or instances are missing information.


Noise: Data that fails to encode clear signals for modeling. Data with the same input values and different target values.

Nonnormalized data: Data in which values for the same entity are represented in different ways.

Outliers: Rows or instances of data that are strange or unlike others.



Sparse data: Data with many zeros or missing values; data that does not
encode enough information about the phenomenon of interest.



Strong multicollinearity (correlation): When features have strong linear dependencies on one another.
Unrecognized time and date formats: Time and date formats, of which there are many, that are encoded improperly by data handling or modeling software.


Wide data: Data with many more columns, features, pixels, or tokens than rows, instances, images, or documents. P >>
N.

Single class model predictions. Biased model predictions.



Useless models, meaningless or dangerous results.



Information loss. Biased models and biased, inaccurate results.


Unreliable or dangerous out- of-domain predictions. Poor performance during training.

Unreliable out-of-domain predictions. Long, intolerable training times. Unreliable parameter estimates and rule generation.
Biased models and biased, inaccurate results. Unreliable parameter estimates and rule
generation. Unreliable out-of-domain predictions.
Long, intolerable training times. Meaningless or dangerous results due to lack of information,
curse of dimensionality, or model misspecification.


Unstable parameter estimates, unstable rule generation, and dangerous or unstable predictions.
Unreliable or dangerous out-of- domain predictions. Unreliable parameter estimates and rule generation. Overfit models and inaccurate results. Overly optimistic in silico performance estimates.
Long, intolerable training times. Meaningless or dangerous results due to the curse of dimensionality or model misspecification.

Proportional over- or undersampling. Inverse prior probability weighting. Mixture models, e.g., zero-inflated regression methods. Post hoc adjustment of predictions or decision thresholds.
Consult with domain experts and stakeholders. Apply the scientific method and DOE approaches. (Get more data. Get better data.)

Imputation. Discretization (i.e., binning). Appropriate algorithm selection, e.g., tree-based models, naive Bayes classification.
Consult with domain experts and stakeholders. Apply the scientific method and DOE approaches. (Get more data. Get better data.)
Careful data cleaning in consultation with domain experts.


Discretization (i.e., binning). Winsorizing. Robust loss functions, e.g., Huber loss functions.


Feature extraction or matrix factorization approaches. Appropriate data representation (i.e., COO, CSR). Application of business rules, model assertions, and constraints to make up for illogical model behavior learned in sparse regions of training data.
Feature selection. Feature extraction. L2 regularization.

Careful data cleaning in consultation with domain experts.



Feature selection, feature extraction, L1 regularization, models that do not assume N >> P.



Training  |  87

There is a lot that can go wrong with data that then leads to unreliable or dangerous model performance in high-risk applications. It might be tempting to think we can feature engineer our way out of data quality problems. But feature engineering is only as good as the thought and code used to perform it. If we?re not extremely careful with feature engineering, we?re likely just creating more bugs and complexity for ourselves. Common issues with feature engineering in ML pipelines include the following:
? API or version mismatches between data cleaning, preprocessing, and inference packages
? Failing to apply all data cleaning and transformation steps during inference
? Failing to readjust for oversampling or undersampling during inference
? Inability to handle values unseen during training gracefully or safely during inference
Of course, many other problems can arise in data preparation, feature engineering, and associated pipelines, especially as the types of data that ML algorithms can accept for training becomes more varied. Tools that detect and address such problems are also an important part of the data science toolkit. For Python Pandas users, the ydata-profiling tool (https://oreil.ly/EDNSC) (formerly pandas-profiler) is a visual aid that helps to detect many basic data quality problems. R users also have options, as discussed by Mateusz Staniak and Przemys?aw Biecek in ?The Landscape of R Packages for Automated Exploratory Data Analysis? (https://oreil.ly/1cBlv).
Model Specification for Real-World Outcomes
Once our data preparation and feature engineering pipeline is hardened, it?s time to think about ML model specification. Considerations for real-world performance and safety are quite different from those about getting published or maximizing performance on ML contest leaderboards. While measurement of validation and test error remain important, bigger questions of accurately representing data and com? monsense real-world phenomena have the highest priority. This subsection addresses model specification for safety and performance by highlighting the importance of benchmarks and alternative models, calibration, construct validity, assumptions and limitations, proper loss functions, and avoiding multiple comparisons, and by pre? viewing the emergent disciplines of robust ML and ML safety and reliability.
Benchmarks and alternatives
When starting an ML modeling task, it?s best to begin with a peer-reviewed training algorithm, and ideally to replicate any benchmarks associated with that algorithm. While academic algorithms rarely meet all the needs of complex business problems, starting from a well-known algorithm and benchmarks provides a baseline assurance

88  |  Chapter 3: Debugging Machine Learning Systems for Safety and Performance

that the training algorithm is implemented correctly. Once this check is addressed, then think about tweaking a complex algorithm to address specific quirks of a given problem.
Along with comparison to benchmarks, evaluation of numerous alternative algorith? mic approaches is another best practice that can improve safety and performance outcomes. The exercise of training many different algorithms and judiciously select? ing the best of many options for final deployment typically results in higher-quality models because it increases the number of models evaluated and forces users to understand differences between them. Moreover, evaluation of alternative approaches is important in complying with a broad set of US nondiscrimination and negli? gence standards. In general, these standards require evidence that different technical options were evaluated and an appropriate trade-off between consumer protection and business needs was made before deployment.
Calibration
Just because a number between 0 and 1 pops out the end of a complex ML pipeline does not make it a probability. The uncalibrated probabilities generated by most ML classifiers usually have to be postprocessed to have any real meaning as probabilities. We typically use a scaling process, or even another model, to ensure that when a pipeline outputs 0.5, the event in question actually happened to about 50% of similar entities in past recorded data. scikit-learn (https://oreil.ly/LxJbX) provides some basic diagnostics and functions for ML classifier calibration. Calibration issues can affect regression models too, when the distribution of model outputs don?t match the distribution of known outcomes. For instance, many numeric quantities in insurance are not normally distributed. Using a default squared loss function, instead of loss functions from the gamma or Tweedie family, may result in predictions that are not distributed like values from the known underlying data-generating process. However we think of calibration, the fundamental issue is that affected ML model predictions don?t match to reality. We?ll never make good predictions and decisions like this. We need our probabilities to be aligned to past outcome rates and we need our regression models to generate predictions of the same distribution as the modeled data-generating process.
Construct validity
Construct validity is an idea from social science (from psychometrics and testing, in particular). Construct validity means that there is a reasonable scientific basis to believe that test performance is indicative of the intended construct. Put another way, is there any scientific evidence that the questions and scores from a standardized test can predict college or job performance? Why are we bringing this up in an ML book? Because ML models are often used for the same purposes as psychometric tests these days, and in our opinion, ML models often lack construct validity. Worse, ML


Training  |  89

algorithms that don?t align with fundamental structures in training data or in their real-world domains can cause serious incidents.
Consider the choice between an ML model and a linear model, wherein many of us simply default to using an ML model. Selecting an ML algorithm for a model? ing problem comes with a lot of basic assumptions?essentially that high-degree interactions and nonlinearity in input features are important drivers of the predic? ted phenomenon. Conversely, choosing to use a linear model implicitly downplays interactions and nonlinearities. If those qualities are important for good predictions, they?ll have to be specified explicitly for the linear model. In either case, it?s important to take stock of how main effects, correlations and local dependencies, interactions, nonlinearities, clusters, outliers, and hierarchies in training data, or in reality, will be handled by a modeling algorithm, and to test those mechanisms. For optimal safety and performance once deployed, dependencies on time, geographical locations, or connections between entities in various types of networks must also be represented within ML models. Without these clear links to reality, ML models lack construct validity and are unlikely to exhibit good in vivo performance. Feature engineering, constraints, loss functions, model architectures, and other mechanisms can all be used to match a model to its task.
Assumptions and limitations
Biases, entanglement, incompleteness, noise, ranges, sparsity, and other basic charac? teristics of training data begin to define the assumptions and limitations of our mod? els. As discussed, modeling algorithms and architectures also carry assumptions and limitations. For example, tree-based models usually can?t extrapolate beyond ranges in training data. Hyperparameters for ML algorithms are yet another place where hidden assumptions can cause safety and performance problems. Hyperparameters can be selected based on domain knowledge or via technical approaches like grid search and Bayesian optimization. The key is not to settle for defaults, to choose settings systematically, and not to trick ourselves due to multiple comparison issues. Testing for independence of errors between rows and features in training data or plotting model residuals and looking for strong patterns are general and time-tested methods for ensuring some basic assumptions have been addressed. It?s unlikely we?ll ever circumvent all the assumptions and limitations of our data and model. So we need to document any unaddressed or suspected assumptions and limitations in model documentation, and ensure users understand what uses of the model could violate its assumptions and limitations. Those would be considered out-of-scope or off-label uses?just like using a prescription drug in improper ways. By the way, con? struct validity is linked to model documentation and risk management frameworks focused on model limitations and assumptions. Oversight professionals want practi? tioners to work through the hypothesis behind their model in writing, and make sure it?s underpinned by valid constructs and not assumptions.


90  |  Chapter 3: Debugging Machine Learning Systems for Safety and Performance

Default loss functions
Another often unstated assumption that comes with many learning algorithms involves squared loss functions. Many ML algorithms use a squared loss function by default. In most instances, a squared loss function, being additive across observa? tions and having a linear derivative, is more a matter of mathematical convenience than anything else. With modern tools such as autograd (https://oreil.ly/8icjS), this convenience is increasingly unnecessary. We should match our choice of loss function with our problem domain.
Multiple comparisons
Model selection in ML often means trying many different sets of input features, model hyperparameters, and other model settings such as probability cutoff thresh? olds. We often use stepwise feature selection, grid searches, or other methods that try many different settings on the same set of validation or holdout data. Statisticians might call this a multiple comparisons problem and would likely point out that the more comparisons we do, the likelier we are to stumble upon some settings that simply happen to look good in our validation or holdout set. This is a sneaky kind of overfitting where we reuse the same holdout data too many times, select features, hyperparameters, or other settings that work well there, and then experience poor in vivo performance later on. Hence, reusable holdout approaches (https://oreil.ly/ QJlUV), which alter or resample validation or holdout data to make our feature, hyperparameter, or other settings more generalizable, are useful.
The future of safe and robust machine learning
The new field of robust ML (https://oreil.ly/1G1Wp) is churning out new algorithms with improved stability and security characteristics. Various researchers are creat? ing new learning algorithms with guarantees for optimality, like optimal sparse decision trees (https://oreil.ly/gOmtg). And researchers have put together excellent tutorial materials (https://oreil.ly/wC5M1) on ML safety and reliability. Today, these approaches require custom implementations and extra work, but hopefully these safety and performance advances will be more widely available soon.
Model Debugging
Once a model has been properly specified and trained, the next step in the technical safety and performance assurance process is testing and debugging. In years past, such assessments focused on aggregate quality and error rates in holdout data. As ML models are incorporated in public-facing ML systems, and the number of pub? licly reported AI incidents is increasing dramatically, it?s clear that more rigorous validation is required. The new field of model debugging (https://oreil.ly/IY0gU) is rising to meet this need. Model debugging treats ML models more like code and less


Model Debugging | 91

like abstract mathematics. It applies a number of testing methods to find software flaws, logical errors, inaccuracies, and security vulnerabilities in ML models and ML system pipelines. Of course, these bugs must also be fixed when they are found. This section explores model debugging in some detail, starting with basic and traditional approaches, then outlines the common bugs we?re trying to find, moves on to special? ized testing techniques, and closes with a discussion of bug remediation methods.

In addition to many explainable ML models, the open source package PiML (https://oreil.ly/1O3hi) contains an exhaustive set of debugging tools for ML models trained on structured data. Even if it?s not an exact fit for a given use case, it?s a great place to learn more and gain inspiration for model debugging.

Software Testing
Basic software testing becomes much more important when we stop thinking of pretty figures and impressive tables of results as the end goal of an ML model training task. When ML systems are deployed, they need to work correctly under various cir? cumstances. Almost more than anything else related to ML systems, making software work is an exact science. Best practices for software testing are well-known and can even be made automatic in many cases. At a minimum, mission-critical ML systems should undergo the following:
Unit testing
All functions, methods, subroutines, or other code blocks should have tests associated with them to ensure they behave as expected, accurately, and are reproducible. This ensures the building blocks of an ML system are solid.
Integration testing
All APIs and interfaces between modules, tiers, or other subsystems should be tested to ensure proper communication. API mismatches after backend code changes are a classic failure mode for ML systems. Use integration testing to catch this and other integration fails.
Functional testing
Functional testing should be applied to ML system user interfaces and endpoints to ensure that they behave as expected once deployed.
Chaos testing
Testing under chaotic and adversarial conditions can lead to better outcomes when our ML systems face complex and surprising in vivo scenarios. Because it can be difficult to predict all the ways an ML system can fail, chaos testing can help probe a broader class of failure modes, and provide some cover against so-called ?unknown unknowns.?


92  |  Chapter 3: Debugging Machine Learning Systems for Safety and Performance

Two additional ML-specific tests should be added into the mix to increase quality further:
Random attack
Random attacks are one way to do chaos testing in ML. Random attacks expose ML models to vast amounts of random data to catch both software and math problems. The real world is a chaotic place. Our ML system will encounter data for which it?s not prepared. Random attacks can decrease those occurrences and any associated glitches or incidents.
Benchmarking
Use benchmarks to track system improvements over time. ML systems can be incredibly complex. How can we know if the three lines of code an engineer changes today will make a difference in the performance of the system as a whole? If system performance is reproducible, and benchmarked before and after changes, it?s much easier to answer such questions.
ML is software. So, all the testing that?s done on traditional enterprise software assets should be done on important ML systems as well. If we don?t know where to start with model debugging, we start with random attacks. Readers may be shocked at the math or software bugs random data can expose in ML systems. When we can add benchmarks to our organization?s continuous integration/continuous development (CI/CD) pipelines, that?s the another big step toward assuring the safety and perfor? mance of ML systems.

Random attacks are probably the easiest and most effective way to get started with model debugging. If debugging feels overwhelm? ing, or you don?t know where to start, start with random attacks.

Traditional Model Assessment
Once we feel confident that the code in our ML systems is functioning as expected, it?s easier to concentrate on testing the math of our ML algorithms. Looking at standard performance metrics is important. But it?s not the end of the validation and debugging process?it?s the beginning. While exact values and decimal points matter, from a safety and performance standpoint, they matter much less than they do on the leaderboard of an ML contest. When considering in-domain performance, it?s less about exact numeric values of assessment statistics, and more about mapping in silico performance to in vivo performance.
If possible, try to select assessment statistics that have a logical interpretation and practical or statistical thresholds. For instance, RMSE can be calculated for many types of prediction problems, and crucially, it can be interpreted in units of the


Model Debugging | 93

target. Area under the curve, for classification tasks, is bounded between 0.5 at the low end and 1.0 at the high end. Such assessment measures allow for commonsense interpretation of ML model performance and for comparisons to widely accepted thresholds for determining quality. It?s also important to use more than one metric and to analyze performance metrics across important segments in our data as well as across training, validation, and testing data partitions. When comparing performance across segments within training data, it?s important that all those segments exhibit roughly equivalent and high-quality performance. Amazing performance on one large customer segment, and poor performance on everyone else, will look fine in average assessment statistic values like RMSE. However, it won?t look fine if it leads to public brand damage due to many unhappy customers. Varying performance across segments can also be a sign of underspecification, a serious ML bug we?ll dig into in this chapter. Performance across training, validation, and test datasets are usually analyzed for underfitting and overfitting too. Like model performance, we can look for overfitting and underfitting across entire data partitions or across segments.
Another practical consideration related to traditional model assessment is selecting a probability cutoff threshold. Most ML models for classification generate numeric probabilities, not discrete decisions. Selecting the numeric probability cutoff to asso? ciate with actual decisions can be done in various ways. While it?s always tempting to maximize some sophisticated assessment measure, it?s also a good idea to consider real-world impact. Let?s consider a classic lending example. Say a probability of default model threshold is originally set at 0.15, meaning that everyone who scores less than a 0.15 probability of default is approved for a loan, and those that score at the threshold or over are denied. Think through questions such as the following:
? What is the expected monetary return for this threshold? What is the financial risk?
? How many people will get the loan at this threshold?
? How many women? How many minority group members?
Outside of the probability cutoff thresholds, it?s always a good idea to estimate in-domain performance, because that?s what we really care about. Assessment meas? ures are nice, but what matters is making money versus losing money, or even saving lives versus taking lives. We can take a first crack at understanding real-world value by assigning monetary, or other, values to each cell of a confusion matrix for classification problems or to each residual unit for regression problems. Do a back-of-the-napkin calculation. Does it look like our model will make money or lose money? Once we get the gist of this kind of valuation, we can even incorporate value levels for different model outcomes directly into ML loss functions, and optimize toward the best-suited model for real-world deployment.



94  |  Chapter 3: Debugging Machine Learning Systems for Safety and Performance

Error and accuracy metrics will always be important for ML. But once ML algorithms are used in deployed ML systems, numeric values and comparisons matter less than they do for publishing papers and data science competitions. So, keep using traditional assessment measures, but try to map them to in-domain safety and performance.
Common Machine Learning Bugs
We?ve discussed a lack of reproducibility, data quality problems, proper model speci? fication, software bugs, and traditional assessment. But there?s still a lot more that can go wrong with complex ML systems. When it comes to the math of ML, there are a few emergent gotchas and many well-known pitfalls. This subsection will discuss bugs, including distributional shifts, epistemic uncertainty, weak spots, instability, leakage, looped inputs, overfitting, shortcut learning, underfitting, and underspecification.


Distribution shifts
Shifts in the underlying data between different training data partitions and after model deployment are common failure modes for ML systems. Whether there?s a new competitor entering a market or a devastating worldwide pandemic, the world is a dynamic place. Unfortunately, most of today?s ML systems learn patterns from static

Model Debugging | 95

snapshots of training data and try to apply those patterns in new data. Sometimes that data is holdout validation or testing partitions. Sometimes it?s live data in a production scoring queue. Regardless, drifting distributions of input features is a serious bug that must be caught and squashed.

Systems based on adaptive, online, or reinforcement learning, or that update themselves with minimal human intervention, are sub? ject to serious adversarial manipulation, error propagation, feed? back loop, reliability, and robustness risks. While these systems may represent the current state of the art, they need high levels of risk management.

When training ML models, watch out for distributional shifts between training, cross-validation, validation, or test sets using population stability index (PSI), Kolmogorov-Smirnov (KS) tests, t-tests, or other appropriate measures. If a feature has a different distribution from one training partition to another, drop it or reg? ularize it heavily. Another smart test for distributional shifts to conduct during debugging is to simulate distributional shifts for potential deployment conditions and remeasure model quality, with a special focus on poor-performing rows. If we?re worried about how our model will perform during a recession, we can simulate distributional shifts to simulate more late payments, lower cash flow, and higher credit balances and then see how our model performs. It?s also crucial to record information about distributions in training data so that drift after deployment can be detected easily.
Epistemic uncertainty and data sparsity
Epistemic uncertainty is a fancy way of saying instability and errors that arise from a lack of knowledge. In ML, models traditionally gain knowledge from training data. If there are parts of our large multidimensional training data that are sparse, it?s likely our model will have a high degree of uncertainty in that region. Sound theoretical and far-fetched? It?s not. Consider a basic credit lending model. We tend to have lots of data about people who already have credit cards and pay their bills, and tend to lack data on people who don?t have credit cards (their past credit card data doesn?t exist) or don?t pay their bills (because the vast majority of customers pay). It?s easy to know to extend credit cards to people with high credit scores that pay their bills. The hard decisions are about people with shorter or bumpier credit histories. The lack of data for the people we really need to know about can lead to serious epistemic uncertainty issues. If only a handful of customers, out of millions, are four or five months late on their most recent payment, then an ML model simply doesn?t learn very much about the best way to handle these people.



96  |  Chapter 3: Debugging Machine Learning Systems for Safety and Performance

This phenomenon is illustrated in the section ?Underspecification? (which begins on page 101), where an example model is nonsensical for people who are more than two months late on their most recent payment. This region of poor, and likely unstable, performance is sometimes known as a weak spot. It?s hard to find these weak spots by looking at aggregate error or performance measures. This is just one reason of many to test models carefully over segments in training or holdout data. It?s also why we pair partial dependence and individual conditional expectation plots with histograms in Chapter 2. In these plots we can see if model behavior is supported by training data, or not. Once we?ve identified a sparse region of data leading to epistemic uncer? tainty and weak spots, we usually have to turn to human knowledge?by constraining the form of the model to behave logically based on domain experience, augmenting the model with business rules, or potentially handing the cases that fall into sparse regions over to human workers to make tough calls.
Instability
ML models can exhibit instability, or lack of robustness or reliability, in the training process or when making predictions on live data. Instability in training is often related to small training data, sparse regions of training data, highly correlated features within training data, or high-variance model forms, such as deep single decision trees. Cross-validation is a typical tool for detecting instability during train? ing. If a model displays noticeably different error or accuracy properties across cross- validation folds, then we have an instability problem. Training instability can often be remediated with better data and lower-variance model forms such as decision tree ensembles. Plots of ALE or ICE also tend to reveal prediction instability in sparse regions of training data, and instability in predictions can be analyzed using sensitivity analysis: perturbations, simulations, stress testing, and adversarial example searches.

There are two easy ways to think about instability in ML:
? When a small change to input data results in a large change in output data
? When the addition of a small amount of training data results in a largely different model upon retraining

If probing our response surface or decision boundary with these techniques uncovers wild swings in predictions, or our ALE or ICE curves are bouncing around, especially in the high or low ranges of feature values, we also have an instability problem. This type of instability can often be fixed with constraints and regularization. Check out the code examples in Chapter 8 to see this remediation in action.



Model Debugging | 97

Leakage
Information leakage between training, validation, and test data partitions happens when information from validation and testing partitions leaks into a training parti? tion, resulting in overly optimistic error and accuracy measurements. Leakage can happen for a variety of reasons, including the following:
Feature engineering
If used incorrectly, certain feature engineering techniques such as imputation or principal components analysis may contaminate training data with information from validation and test data. To avoid this kind of leakage, perform feature engineering uniformly, but separately, across training data partitions. Or ensure that information, like means and modes used for imputation, are calculated in training data and applied to validation and testing data, and not vice versa.
Mistreatment of temporal data
Don?t use the future to predict the past. Most data has some association with time, whether explicit, as in time-series data, or some other implicit relationship. Mistreating or breaking this relationship with random sampling is a common cause of leakage. If we?re dealing with data where time plays a role, time needs to be used in constructing model validation schemes. The most basic rule is that the earliest data should be in training partitions while later data should be divided into validation and test partitions, also according to time. A solid (and free) resource for time-series forecasting best practices is the text Forecasting: Principles and Practice (OTexts) (https://oreil.ly/R2y6N).
Multiple identical entities
Sometimes the same person, financial or computing transaction, or other mod? eled entity will be in multiple training data partitions. When this occurs, care should be taken to ensure that ML models do not memorize characteristics of these individuals then apply those individual-specific patterns to different entities in new data.
Keeping an untouched, time-aware holdout set for an honest estimate of real-world performance can help with many of these different leakage bugs. If error or accuracy on such a holdout set looks a lot less rosy than on partitions used in model develop? ment, we might have a leakage problem. More complex modeling schemes involving stacking, gates, or bandits can make leakage much harder to prevent and detect. However, a basic rule of thumb still applies: do not use data involved in learning or model selection to make realistic performance assessments. Using stacking, gates, or bandits means we need more holdout data for the different stages of these complex models to make an accurate guess at in vivo quality. More general controls such as careful documentation of data validation schemes and model monitoring in deploy? ment are also necessary for any ML system.


98  |  Chapter 3: Debugging Machine Learning Systems for Safety and Performance

Looped inputs
As ML systems are incorporated into broader digitalization efforts, or implemented as part of larger decision support efforts, multiple data-driven systems often interact. In these cases, error propagation and feedback loop bugs can occur. Error propaga? tion occurs when small errors in one system cause or amplify errors in another system. Feedback loops are a way an ML system can fail by being right. Feedback loops occur when an ML system affects its environment and then those effects are reincorporated into system training data. Examples of feedback loops include when predictive policing leads to overpolicing of certain neighborhoods or when employment algorithms intensify diversity problems in hiring by continually recom? mending correct, but nondiverse, candidates. Dependencies between systems must be documented and deployed models must be monitored so that debugging efforts can detect error propagation or feedback loop bugs.
Overfitting
Overfitting happens when a complex ML algorithm memorizes too much specific information from training data, but does not learn enough generalizable concepts to be useful once deployed. Overfitting is often caused by high-variance models, or models that are too complex for the data at hand. Overfitting usually manifests in much better performance on training data than on validation, cross-validation, and test data partitions. Since overfitting is a ubiquitous problem, there are many possible solutions, but most involve decreasing the variance in our chosen model. Examples of these solutions include the following:
Ensemble models
Ensemble techniques, particularly bootstrap aggregation (i.e., bagging) and gra? dient boosting are known to reduce error from single high-variance models. So, we try one of these ensembling approaches if we encounter overfitting. Just keep in mind that when switching from one model to many, we can decrease overfitting and instability, but we?ll also likely lose interpretability.
Reducing architectural complexity
Neural networks can have too many hidden layers or hidden units. Ensemble models can have too many base learners. Trees can be too deep. If we think we?re observing overfitting, we make our model architecture less complex.
Regularization
Regularization refers to many sophisticated mathematical approaches for reduc? ing the strength, complexity, or number of learned rules or parameters in an ML model. In fact, many types of ML models now incorporate multiple options for regularization, so we make sure we employ these options to decrease the likelihood of overfitting.


Model Debugging | 99

Simpler hypothesis model families
Some ML models will be more complex than others out-of-the-box. If our neural network or GBM looks to be overfit, we can try a less complex decision tree or linear model.
Overfitting is traditionally seen as the Achilles? heel of ML. While it is one of the most frequently encountered bugs, it?s also just one of many possible technical risks to consider from a safety and performance perspective. As with leakage, as ML systems become more complex, overfitting becomes harder to detect. Always keep an untouched holdout set with which to estimate real-world performance before deployment. More general controls like documentation of validation schemes, model monitoring, and A/B testing of models on live data also need to be applied to prevent overfitting.
Shortcut learning
Shortcut learning occurs when a complex ML system is thought to be learning and making decisions about one subject, say anomalies in lung scans or job interview performance, but it?s actually learned about some simpler related concept, such as machine identification numbers or Zoom video call backgrounds. Shortcut learning tends to arise from entangled concepts in training data, a lack of construct validity, and failure to adequately consider and document assumptions and limitations. We use explainable models and explainable AI techniques to understand what learned mechanisms are driving model decisions, and we make sure we understand how our ML system makes scientifically valid decisions.
Underfitting
If someone tells us a statistic about a set of data, we might wonder how much data that statistic is based on, and whether that data was of high enough quality to be trustworthy. What if someone told us they had millions, billions, or even trillions of statistics for us to consider? They would need lots of data to make a case that all these statistics were meaningful. Just like averages and other statistics, each parameter or rule within an ML model is learned from data. Big ML models need lots of data to learn enough to make their millions, billions, or trillions of learned mecha? nisms meaningful. Underfitting happens when a complex ML algorithm doesn?t have enough training data, constraints, or other input information, and it learns just a few generalizable concepts from training data, but not enough specifics to be useful when deployed. Underfitting can be diagnosed by poor performance on both training and validation data. Another piece of evidence for underfitting is if our model residuals have significantly more structure than random noise. This suggests that there are meaningful patterns in the data going undetected by our model, and it?s another rea? son we examine our residuals for model debugging. We can mitigate underfitting by increasing the complexity of our models or, preferably, providing more training data.


100  |  Chapter 3: Debugging Machine Learning Systems for Safety and Performance

We can also provide more input information in other ways, such as new features, Bayesian priors applied to our model parameter distributions, or various types of architectural or optimization constraints.
Underspecification
Forty researchers recently published ?Underspecification Presents Challenges for Credibility in Modern Machine Learning? (https://oreil.ly/Da9g0). This paper gives a name to a problem that has existed for decades, underspecification. Underspecifica? tion arises from the core ML concept of the multiplicity of good models, sometimes also called the Rashomon effect. For any given dataset, there are many accurate ML models. How many? Vastly more than human technicians have any chance of understanding in most cases. While we use validation data to select a good model from many models attempted during training, validation-based model selection is not a strong enough control to ensure we picked the best model?or even a servicable model?for deployment. Say that for some dataset there are a million total good ML models based on training data and a large number of potential hypothesis models. Selecting by validation data may cut that number of models down to a pool of one hundred total models. Even in this simple scenario, we?d still only have a 1 in 100 chance of picking the right model for deployment. How can we increase those odds? By injecting domain knowledge into ML models. By combining validation-based model selection with domain-informed constraints, we have a much better chance at selecting a viable model for the job at hand.
Happily, testing for underspecification can be fairly straightforward. One major symptom of underspecification is model performance that?s dependent on computa? tional hyperparameters that are not related to the structure of the domain, data, or model. If our model?s performance varies due to random seeds, number of threads or GPUs, or other computational settings, our model is probably underspecified. Another test for underspecification is illustrated in Figure 3-1.
Figure 3-1 displays several error and accuracy measures across important segments in the example training data and model. Here, a noticeable shift in performance for segments defined by higher values of the important feature PAY_0 points to a potential underspecification problem, likely due to data sparsity in that region of the training data. (Performance across segments defined by SEX is more equally balanced, which a good sign from a bias-testing perspective, but certainly not the only test to be considered for bias problems.) Fixing underspecification tends to involve apply? ing real-world knowledge to ML algorithms. Such domain-informed mechanisms include graph connections, monotonic constraints, interaction constraints, beta con? straints, or other architectural constraints.




Model Debugging | 101












Figure 3-1. Analyzing accuracy and errors across key segments is an important debugging method for detecting bias, underspecifica? tion, and other serious ML bugs (digital, color version(: https://oreil.ly/URzZG))

Nearly all of the bugs discussed in this section, chapter, and book can affect certain segments of data more than others. For optimal performance, it?s important to test for weak spots (performance quality), overfitting and underfitting, instability, distribution shifts, and other issues across different kinds of segments in training, validation, and test or holdout data.

Each of the ML bugs discussed in this subsection has real-world safety and perfor? mance ramifications. A unifying theme across these bugs is that they cause systems to perform differently than expected when deployed in vivo and over time. Unpredicta? ble performance leads to unexpected failures and AI incidents. Using the knowledge of potential bugs and bug detection methods discussed here to ensure estimates of validation and test performance are relevant to deployed performance will go a long way toward preventing real-world incidents. Now that we know what bugs we?re looking for, in terms of software, traditional assessment, and ML math, next we?ll address how to find these bugs with residual analysis, sensitivity analysis, benchmark models, and other testing and monitoring approaches.
Residual Analysis
Residual analysis is another type of traditional model assessment that can be highly effective for ML models and ML systems. At its most basic level, residual analysis means learning from mistakes. That?s an important thing to do in life, as well as in organizational ML systems. Moreover, residual analysis is a tried-and-true model diagnostic technique. This subsection will use an example and three generally appli? cable residual analysis techniques to apply this established discipline to ML.

We use the term residual to mean an appropriate measurement of error, somewhat synonymous to the model?s loss measurement. We understand we?re not the using it in the strictly defined ?i?yi sense. We use this term to reinforce the importance and long history of residual analysis in regression diagnostics and to highlight its basic absence from common ML workflows.

Note that in the following sections readers may see demographic features in the dataset, like SEX, that are used for bias testing. For the most part, this chapter treats the example credit lending problem as a general predictive modeling exercise, and does not consider applicable fair lending regulations. See Chapters 4 and 10 for in-depth discussions relating to bias management in ML that also address some legal and regulatory concerns.




Model Debugging | 103

Analysis and visualizations of residuals
Plotting overall and segmented residuals and examining them for telltale patterns of different kinds of problems is a long-running model diagnostic technique. Residual analysis can be applied to ML algorithms to great benefit with a bit of creativity and elbow grease. Simply plotting residuals for an entire dataset can be helpful, especially to spot outlying rows causing very large numeric errors or to analyze overall trends in errors. However, breaking residual values and plots down by feature and level is likely to be more informative. Even if we have a lot features or features with many categorical levels, we?re not off the hook. Start with the most important features and their most common levels. Look for strong patterns in residuals that violate the assumptions of our model. Many types of residuals should be randomly distributed, indicating that the model has learned all the important information from the data, aside from irreducible noise. If we spot strong patterns or other anomalies in residu? als that have been broken down by feature and level, we first determine whether these errors arise from data, and if not, we can use XAI techniques to track down issues in our model. Residual analysis is considered standard practice for important linear regression models. ML models are arguably higher risk and more failure prone, so they need even more residual analysis.
Modeling residuals
Modeling residuals with interpretable models is another great way to learn more about the mistakes our ML system could make. In Figure 3-2, we?ve trained a single, shallow decision tree on the residuals from a more complex model associated with customers who missed a credit card payment.
This decision tree encodes rules that describe how the more complex model is wrong. For example, we can see the model generates the largest numeric residuals when someone misses a credit card payment, but looks like a great customer. When some? one?s most recent repayment status (PAY_0) is less than 0.5, their second most recent payment amount (PAY_AMT2) is greater than or equal to 2,802.50, their fourth most recent repayment status (PAY_4) is less than 1, and their credit limit is greater than or equal to 256,602, we see logloss residuals of 2.71, on average. That?s a big error rate that drags down our overall performance, and that can have bias ramifications if we make too many false negative guesses about already favored demographic groups.
Another intriguing use for the tree is to create model assertions, real-time business rules about model predictions, that could be used to flag when a wrong decision is occurring as it happens. In some cases, the assertions might simply alert model monitors that a wrong decision is likely being issued, or model assertions could involve corrective action, like routing this row of data to a more specialized model or to human case workers.



104  |  Chapter 3: Debugging Machine Learning Systems for Safety and Performance



Figure 3-2. An interpretable decision tree model for customers who missed credit card payments
Local contribution to residuals
Plotting and modeling residuals are older techniques that are well-known to skilled practitioners. A more recent breakthrough has made it possible to calculate accurate Shapley value contributions to model errors. This means for any feature or row of any dataset, we can now know which features are driving model predictions, and which features are driving model errors. What this advance really means for ML is yet to be determined, but the possibilities are certainly intriguing. One obvious application for this new Shapley value technique is to compare feature importance for predictions to feature importance for residuals, as in Figure 3-3.








Model Debugging | 105



Figure 3-3. A comparison of Shapley feature importance for predictions and for model errors (digital, color version(: https://oreil.ly/k6nDo))

106  |  Chapter 3: Debugging Machine Learning Systems for Safety and Performance

In Figure 3-3, feature importance for predictions is shown on top, and feature impor? tance for the errors of the model, as calculated by logloss, is shown on the bottom. We can see that PAY_0 dominates both predictions and errors, confirming that this model is too reliant on PAY_0 in general. We can also see that PAY_2 and PAY_3 are ranked higher for contributions to error than contributions to predictions. Given this, it might make sense to experiment with dropping, replacing, or corrupting these features. Note that Figure 3-3 is made from aggregating Shapley contributions to logloss across an entire validation dataset. However, these quantities are calculated feature-by-feature and row-by-row. We could also apply this analysis across segments or demographic groups in our data, opening up interesting possibilities for detecting and remediating nonrobust features for different subpopulations under the model.

Feature importance plots that look like Figure 3-3, with one feature drastically outweighing all the others, bode very poorly for in vivo reliability and security. If the distribution of that single important feature drifts, our model performance is going to suffer. If hackers find a way to modify values of that feature, they can easily manip? ulate our predictions. When one feature dominates a model, we likely need a business rule relating to that feature instead of an ML model.

This ends our brief tour of residual analysis for ML. Of course there are other ways to study the errors of ML models. If readers prefer another way, then go for it! The important thing is to do some kind of residual analysis for all high-stakes ML systems. Along with sensitivity analysis, to be discussed in the next subsection, residual analysis is an essential tool in the ML model debugging kit.
Sensitivity Analysis
Unlike linear models, it?s very hard to understand how ML models extrapolate or perform on new data without testing them explicitly. That?s the simple and powerful idea behind sensitivity analysis. Find or simulate data for interesting scenarios, then see how our model performs on that data. We really won?t know how our ML system will perform in these scenarios unless we conduct basic sensitivity analysis. Of course, there are structured and more efficient variants of sensitivity analysis, such as in the InterpretML (https://oreil.ly/zdzxX) library from Microsoft Research. Another great option for sensitivity analysis, and a good place to start with more advanced model debugging techniques, is random attacks, discussed in ?Software Testing? on page
92. Many other approaches, like stress testing, visualization, and adversarial example searches also provide standardized ways to conduct sensitivity analysis:




Model Debugging | 107

Stress testing
Stress testing involves simulating data that represents realistic adverse scenarios, like recessions or pandemics, and making sure our ML models and any down? stream business processes will hold up to the stress of the adverse situation.
Visualizations
Visualizations, such as plots of accumulated local effects, individual conditional expectation, and partial dependence curves, are well-known, highly structured ways to observe the performance of ML algorithms across various real or simula? ted values of input features. These plots can also reveal areas of data sparsity that can lead to weak spots in model performance.
Adversarial example searches
Adversarial examples are rows of data that evoke surprising responses from ML models. Deep learning approaches can be used to generate adversarial examples for unstructured data, and ICE and genetic algorithms can be used to generate adversarial examples for structured data. Adversarial examples (and the search for them) are a great way to find local areas of instability in our ML response functions or decision boundaries that can cause incidents once deployed. As readers can see in Figure 3-4, an adversarial example search is a great way to put a model through its paces.
Conformal approaches
Conformal approaches (https://oreil.ly/f_Vrf) that attempt to calculate empirical bounds for model predictions can help us understand model reliability through establishing the upper and lower limits of what can be expected from model outputs.
Perturbation tests
Randomly perturbing validation, test, or holdout data to simulate different types of noise and drift, and then remeasuring ML model performance, can also help establish the general bounds of model robustness. With this kind of perturbation testing, we can understand and document the amount of noise or shift that we know will break our model. One thing to keep in mind is that poor-performing rows often decline in performance faster than average rows under perturbation testing. Watch poor-performing rows carefully to understand if and when they drag the performance of the entire model down.








108  |  Chapter 3: Debugging Machine Learning Systems for Safety and Performance



Figure 3-4. Results from an adversarial example search that reveal interesting model behavior (digital, color version(: https://oreil.ly/_vTJW))

Model Debugging | 109

Figure 3-4 was created by first finding an ICE curve that exhibited a large swing in predictions. Using the row of data responsible for that ICE curve as a seed, and per? tubing the values of the four most important features in that row thousands of times and generating associated predictions, leads to the numerous plots in Figure 3-4. The first finding of the adversarial example search is that this heurstic technique, based on ICE curves, enables us to generate adversarial examples that can evoke almost any response we want from the model. We found rows that reliably yield very low and very high predictions and everything in between. If this model was available via a prediction API, we could play it like a fiddle.
In the process of finding all those adversarial examples, we also learned things about our model. First, it is likely monotonic in general, and definitely monotonic across all the rows we simulated in the adversarial example search. Second, this model issues default predictions for people that make extremely high payments. Even if someone?s most recent payment was one million dollars, and above their credit limit, this model will issue default predictions once that person becomes two months late on their payments. This could pose problems for prepayment. Do we really want to issue a default or delinquency decision for someone who prepaid millions of dollars but is now two months late on their most recent payments? Maybe, but it?s likely not a decision that should be made quickly or automatically, as this model would do. Third, it appears we may have found a route for a true adversarial example attack. Low recent repayment amounts result in surprisingly sharp increases in probability of default. If a hacker wants to evoke high probability of default predictions from this model, setting PAY_AMT1 and PAY_AMT2 to low values could be how they do it.
Like we mentioned for residual analysis, readers may have other sensitivity analysis techniques in mind, and that?s great. Just make sure you apply some form of realistic simulation testing to your ML models. This chapter?s case study is an example of the worst kind of outcome resulting from a failure to conduct realistic simulation testing prior to deploying an ML system. This ends our brief discussion of sensitivity analysis. For those who would like to dive in even deeper, we recommend Chapter 19 of Kevin Murphy?s free and open Probabilistic Machine Learning: Advanced Topics (https://oreil.ly/mHWno) (MIT Press). Next, we?ll discuss benchmark models in dif? ferent contexts, another time-tested and commonsense model debugging approach.
Benchmark Models
Benchmark models have been discussed at numerous points in this chapter. They are a very important safety and performance tool, with uses throughout the ML lifecycle. This subsection will discuss benchmark models in the context of model debugging and also summarize other critical uses.




110  |  Chapter 3: Debugging Machine Learning Systems for Safety and Performance

When possible, compare ML model performance to a linear model or GLM performance benchmark. If the linear model beats the ML model, use the linear model.


The first way to use a benchmark model for debugging is to compare performance between a benchmark and the ML system in question. If the ML system does not out? perform a simple benchmark?and many may not?it?s back to the drawing board. Assuming a system passes this initial baseline test, benchmark models can then be used as a comparison tool to interrogate mechanisms and find bugs within an ML system. For instance, data scientists can ask the question, ?Which predictions does my benchmark get right and my ML system get wrong?? Given that the benchmark should be well understood, it should be clear why it is correct, and this understand? ing should also provide some clues as to what the ML system is getting wrong. Benchmarks can also be used for reproducibility and model monitoring purposes as follows:
Reproducibility benchmarks
Before making changes to a complex ML system, it is imperative to have a reproducible benchmark from which to measure performance gains or losses. A reproducible benchmark model is an ideal tool for such measurement tasks. If this model can be built into CI/CD processes that enable automated testing for reproducibility and comparison of new system changes to established bench? marks, even better.
Debugging benchmarks
Comparing complex ML model mechanisms and predictions to a trusted, well- understood benchmark model?s mechanisms and predictions is an effective way to spot ML bugs.
Monitoring benchmarks
Comparing real-time predictions between a trusted benchmark model and a complex ML system is a way to catch serious ML bugs in real time. If a trusted benchmark model and a complex ML system give noticeably different predic? tions for the same instance of new data, this can be a sign of an ML hack, data drift, or even bias and algorithmic discrimination. In such cases, benchmark predictions can be issued in place of ML system predictions, or predictions can be withheld until human analysts determine if the ML system prediction is valid.

Remember that debugging techniques are often fallible statistical or ML approaches, and may need to be debugged themselves.




Model Debugging | 111

If we set benchmarks up efficiently, it may even be possible to use the same model for all three tasks. A benchmark can be run before starting work to establish a baseline from which to improve performance, and that same model can be used in comparisons for debugging and model monitoring. When a new version of the system outperforms an older version in a reproducible manner, the ML model at the core of the system can become the new benchmark. If our organization can establish this kind of workflow, we?ll be benchmarking and iterating our way to increased ML safety and performance.
Remediation: Fixing Bugs
The last step in debugging is fixing bugs. The previous subsections have outlined test? ing strategies, bugs to be on the lookout for, and a few specific fixes. This subsection outlines general ML bug-fixing approaches and discusses how they might be applied in the example debugging scenario. General strategies to consider during ML model debugging include the following:
Anomaly detection
Strange inputs and outputs are usually bad news for ML systems. These can be evidence of a real-time security, bias, or safety and performance problem. Moni? tor ML system data queues and predictions for anomalies, record the occurrence of anomalies, and alert stakeholders to their presence when necessary.

A number of rule-based, statistical, and ML techniques can be used to detect anomalies in unseen data queues. These include data integrity constraints, confidence limits, control limits, autoencoders, and isolation forests.

Experimental design and data augmentation
Collecting better data is often a fix-all for ML bugs. What?s more, data collection doesn?t have to be done in a trial-and-error fashion, nor do data scientists have to rely on data exhaust byproducts of other organizational processes for selecting training data. The mature science of design of experiment has been used by data practitioners for decades to ensure they collect the right kind and amount of data for model training. Arrogance related to the perceived omnipotence of ?big? data and overly compressed deployment timelines are the most common reasons data scientists don?t practice DOE. Unfortunately, these are not scientific reasons to ignore DOE.
Model assertions
Model assertions are business rules applied to ML model predictions that correct for shortcomings in learned ML model logic. Using business rules to improve predictive models is a time-honored remediation technique that will likely be


112  |  Chapter 3: Debugging Machine Learning Systems for Safety and Performance

with us for decades to come. If there is a simple, logical rule that can be applied to correct a foreseeable ML model failure, don?t be shy about implementing it. The best practitioners and organizations in the predictive analytics space have used this trick for decades.
Model editing
Given that ML models are software, those software artifacts can be edited to correct for any discovered bugs. Certain models, like GA2Ms or explainable boosting machines (EBMs) are designed to be edited for the purposes of model debugging. Other types of models may require more creativity to edit. Either way, editing must be justified by domain considerations, as it?s likely to make performance on training data appear worse. ML models optimize toward lower error. If we edit this highly optimized structure to make in-domain performance better, we?ll likely worsen traditional assessment statistics. That?s OK. We care more about in vivo safety, robustness, and reliability than in silico test error.
Model management and monitoring
ML models and the ML systems that house them are dynamic entities that must be monitored to the extent that resources allow. All mission-critical ML systems should be well-documented, inventoried, and monitored for security, bias, and safety and performance problems in real time. When something starts to go wrong, stakeholders need to be alerted quickly. ?Deployment? on page 114 gives a more detailed treatment of model monitoring.
Monotonic and interaction constraints
Many ML bugs occur because ML models have too much flexibility and become untethered from reality due to learning from biased and inaccurate training data. Constraining models with real-world knowledge is a general solution to several types of ML bugs. Monotonic and interaction constraints, in popular tools like XGBoost, can help ML practitioners enforce logical domain assumptions in complex ML models.
Noise injection and strong regularization
Many ML algorithms come with options for regularization. However, if an ML model is overemphasizing a certain feature, stronger or external regularization might need to be applied. L0 regularization can be used to limit the number of rules or parameters in a model directly, and when necessary, manual noise injection can be used to corrupt signals from certain features to deemphasize those with any undue importance in ML models.
The scientific method
Confirmation bias among data scientists, ML engineers, their managers, and business partners often conspires to push half-baked demos out the door as products, based on the assumptions and limitations of in silico test data


Model Debugging | 113

assessments. If we?re able to follow the scientific method by recording a hypothe? sis about real-world results and objectively test that hypothesis with a designed experiment, we have much higher chances at in vivo success. See Chapter 12 for more thoughts on using the scientific method in ML.

Generally speaking, ML is still more of an empirical science than an engineering discipline. We don?t yet fully understand when ML works well and all the ways it can fail, especially when deployed in vivo. This means we have to apply the scientific method and avoid issues like confirmation bias to attain good real-world results. Sim? ply using the right software and platforms, and following engineer? ing best practices, does not mean our models will work well.

There?s more detailed information regarding model debugging and the example data and model in ?Resources? on page 122. For now, we?ve learned quite a bit about model debugging, and it?s time to turn our attention to safety and performance for deployed ML systems.
Deployment
Once bugs are found and fixed, it?s time to deploy our ML system to make real-world decisions. ML systems are much more dynamic than most traditional software sys? tems. Even if system operators don?t change any code or setting of the system, the results can still change. Once deployed, ML systems must be checked for in-domain safety and performance, they must be monitored, and their operators must be able to shut them off quickly. This section will cover how to enhance safety and performance once an ML system is deployed: domain safety, model monitoring, and kill switches.
Domain Safety
Domain safety means safety in the real world. This is very different from standard model assessment, or even enhanced model debugging. How can practitioners work toward real-world safety goals? A/B testing and champion challenger methodologies allow for some amount of testing in real-time operating environments. Process con? trols, like enumerating foreseeable incidents, implementing controls to address those potential incidents, and testing those controls under realistic or stressful conditions, are also important for solid in vivo performance. To make up for incidents that can?t be predicted, we apply chaos testing, random attacks, and manual prediction limits to our ML system outputs. Let?s divide incidents into those we can foresee, and those we can?t, and consider a few details for both cases:




114  |  Chapter 3: Debugging Machine Learning Systems for Safety and Performance

Foreseeable real-world incidents
A/B testing and champion-challenger approaches, in which models are tested against one another on live data streams or under other realistic conditions, are a first step toward robust in-domain testing. Beyond these somewhat standard practices, resources should be spent on domain experts and thinking through possible incidents. For example, common failure modes in credit lending include bias and algorithmic discrimination, lack of transparency, and poor performance during recessions. For other applications, say autonomous vehicles, there are numerous ways they could accidentally or intentionally cause harm. Once poten? tial incidents are recorded, then safety controls can be adopted for the most likely or most serious potential incidents. In credit lending, models are tested for bias, explanations are provided to consumers via adverse action notices, and models are monitored to catch performance degradation quickly. In autonomous vehicles, we still have a lot to learn, as ?Case Study: Death by Autonomous Vehicle? on page 120 will show. Regardless of the application, safety controls must be tested, and these tests should be realistic and performed in collaboration with domain experts. When it comes to human safety, simulations run by data scientists are not enough. Safety controls need to be tested and hardened in vivo and in coordination with people who have a deep understanding of safety in the application domain.
Unforeseeable real-world incidents
Interactions between ML systems and their environments can be complex and surprising. For high-stakes ML systems, it?s best to admit that unforeseeable incidents can occur. We can try to catch some of these potential surprises before they occur with chaos testing and random attacks. Important ML systems should be tested in strange and chaotic use cases and exposed to large amounts of random input data. While these are time- and resource-consuming tests, they are one of the few tools available to test for so-called ?unknown unknowns.? Given that no testing regime can catch every problem, it?s also ideal to apply commonsense prediction limits to systems. For instance, large loans or interest rates should not be issued without some kind of human oversight. Nor should autonomous vehicles be allowed to travel at very high speeds without human intervention. Some actions simply should not be performed purely automatically as of today, and prediction limits are one way to implement that kind of control.
Another key aspect of domain safety is knowing if problems are occurring. Some? times glitches can be caught before they grow into harmful incidents. To catch problems quickly, ML systems must be monitored. If incidents are detected, incident response plans or kill switches may need to be activated.





Deployment | 115


Characteristics of Safe Machine Learning Systems
Some of the most important steps we can take to ensure an ML system interacts with the physical world in a safe way are as follows:
Avoiding past failed designs
ML systems that cause harm to humans or the environment should not be reim? plemented, and their failures should be studied to improve the safety conditions of future related systems.
Incident response plans
Human operators should know what to do when a safety incident occurs.
In-domain testing
Test data assessments, simulations, and debugging by data scientists are not enough to ensure safety. Systems should be tested in realistic in vivo conditions by domain and safety experts.
Kill switches
Systems should be able to be shut off quickly and remotely when monitoring reveals risky or dangerous conditions.
Manual prediction limits
Limits on system behaviors should be set by operators where appropriate.
Real-time monitoring
Humans should be alerted when a system enters a risky or dangerous state, and kill switches or redundant functionality should be enacted quickly (or automatically).
Redundancy
Systems that perform safety- or mission-critical activities should have redundant functionality at the ready if incidents occur or monitoring indicates the system has entered a risky or dangerous state.

Model Monitoring
It?s been mentioned numerous times in this chapter, but important ML systems must be monitored once deployed. This subsection focuses on the technical aspects of model monitoring. It outlines the basics of model decay, robustness, and concept drift bugs, how to detect and address drift, and the importance of measuring multiple key performance indicators (KPIs) in monitoring, as well as briefly highlighting a few other notable model monitoring concepts.




116  |  Chapter 3: Debugging Machine Learning Systems for Safety and Performance

Model decay and concept drift
No matter what we call it, the data coming into an ML system is likely to drift away from the data on which the system was trained. The change in the distribution of input values over time is sometimes labeled data drift. The statistical properties of what we?re trying to predict can also drift, and sometimes this is known specifically as concept drift. The COVID-19 crisis is likely one of history?s best examples of these phenomena. At the height of the pandemic, there was likely a very strong drift toward more cautious consumer behavior accompanied by an overall change in late payment and credit default distributions. These kinds of shifts are painful to live through, and they can wreak havoc on an ML system?s accuracy. It?s important to note that we sometimes make our own concept drift issues, by engaging in off-label use of ML models.

Both input data and predictions can drift. Both types of drift can be monitored, and the two types of drift may or may not be directly related. When performance degrades without significant input drift this may be due to real-world concept drift.

Detecting and addressing drift
The best approach to detect drift is to monitor the statistical properties of live data? both input variables and predictions. Once a mechanism has been put in place to monitor statistical properties, we can set alerts or alarms to notify stakeholders when there is a significant drift. Testing inputs is usually the easiest way to start detecting drift. This is because sometimes true data labels, i.e., true outcome values associated with ML system predictions, cannot be known for long periods of time. In contrast, input data values are available immediately whenever an ML system must generate a prediction or output. So, if current input data properties have changed from the training data properties, we likely have a problem on our hands. Watching ML system outputs for drift can be more difficult due to information needed to compare current and training quality being unavailable immediately. (Think about mortgage default versus online advertising?default doesn?t happen at the same pace as clicking an online advertisement.) The basic idea for monitoring predictions is to watch predictions in real time and look for drift and anomalies, potentially using method? ologies such as statistical tests, control limits, and rules or ML algorithms to catch outliers. And when known outcomes become available, test for degradation in model performance and engage in sustained bias management quickly and frequently.






Deployment | 117

There are known strategies to address inevitable drift and model decay. These include the following:
? Refreshing an ML system with extended training data containing some amount of new data
? Refreshing or retraining an ML system frequently
? Refreshing or retraining an ML system when drift is detected
It should be noted that any type of retraining of ML models in production should be subject to the risk mitigation techniques discussed in this chapter and elsewhere in the book?just like they should be applied to the initial training of an ML system.
Monitoring multiple key performance indicators
Most discussions of model monitoring focus on model accuracy as the primary key performance indicator (KPI). Yet, bias, security vulnerabilities, and privacy harms can, and likely should, be monitored as well. The same bias testing that was done at training time can be applied when new known outcomes become available. Numer? ous other strategies, discussed in Chapters 5 and 11, can be used to detect malicious activities that could compromise system security or privacy. Perhaps the most crucial KPI to measure, if at all possible, is the actual impact of the ML system. Whether it?s saving or generating money, or saving lives, measuring the intended outcome and actual value of the ML system can lead to critical organizational insights. Assign monetary or other values to confusion matrix cells in classification problems, and to residual units in regression problems, as a first step toward estimating actual business value. See Chapter 8 for a basic example of estimating business value.
Out-of-range values
Training data can never cover all of the data an ML system might encounter once deployed. Most ML algorithms and prediction functions do not handle out-of-range data well, and may simply issue an average prediction or crash, and do so without notifying application software or system operators. ML system operators should make specific arrangements to handle data, such as large-magnitude numeric values, rare categorical values, or missing values that were not encountered during training so that ML systems will operate normally and warn users when they encounter out-of-range data.
Anomaly detection and benchmark models
Anomaly detection and benchmark models round out the technical discussion of model monitoring in this subsection. These topics have been treated elsewhere in this chapter, and are touched on briefly here in the monitoring context:


118  |  Chapter 3: Debugging Machine Learning Systems for Safety and Performance

Anomaly detection
Strange input or output values in an ML system can be indicative of stability problems or security and privacy vulnerabilities. It?s possible to use statistics, ML, and business rules to monitor anomalous behavior in both inputs and outputs, and across an entire ML system. Record any such detected anomalies, report them to stakeholders, and be ready to take more drastic action when necessary.
Benchmark models
Comparing simpler benchmark models and ML system predictions as part of model monitoring can help to catch stability, fairness, or security anomalies in near real time. A benchmark model should be more stable, easier to confirm as minimally discriminatory, and should be harder to hack. We use a highly transparent benchmark model and our more complex ML system together when scoring new data, then compare our ML system predictions against the trusted benchmark prediction in real time. If the difference between the ML system and the benchmark is above some reasonable threshold, then fall back to issuing the benchmark model?s prediction or send the row of data for more review.
Whether it?s out-of-range values in new data, disappointing KPIs, drift, or anoma? lies?these real-time problems are where rubber meets road for AI incidents. If our monitoring detects these issues, a natural inclination will be to turn the system off. The next subsection addresses just this issue: kill switches for ML systems.
Kill switches
Kill switches are rarely single switches or scripts, but a set of business and technical processes bundled together that serve to turn an ML system off?to the degree that?s possible. There?s a lot to consider before flipping a proverbial kill switch. ML system outputs often feed into downstream business processes, sometimes including other ML systems. These systems and business processes can be mission critical, such as an ML system used for credit underwriting or e-retail payment verification. To turn off an ML system, we not only need the right technical know-how and personnel available, but we also need an understanding of the system?s place inside of broader organizational processes. During an ongoing AI incident is a bad time to start thinking about turning off a fatally flawed ML system. So, kill processes and kill switches are a great addition to our ML system documentation and AI incident response plans (see Chapter 1). This way, when the time comes to kill an ML system, our organization can be ready to make a quick and informed decision. Hopefully we?ll never be in a position where flipping an ML system kill switch is necessary, but unfortunately AI incidents have grown more common in recent years. When technical remediation methods are applied alongside cultural competencies and business processes for risk mitigation, the safety and performance of ML systems is enhanced. When these controls are not applied, bad things can happen.


Deployment | 119

Case Study: Death by Autonomous Vehicle
On the night of March 18, 2018, Elaine Herzberg was walking a bicycle across a wide intersection in Tempe, Arizona. In what has become one of the most high-profile AI incidents, she was struck by an autonomous Uber test vehicle traveling at roughly 40 mph. According to the National Transportation Safety Board (NTSB), the test vehicle driver, who was obligated to take control of the vehicle in emergency situa? tions, was distracted by a smartphone. The self-driving ML system also failed to save Ms. Herzberg. The system did not identify her until 1.2 seconds before impact, too late to prevent a brutal crash.
Fallout
Autonomous vehicles are thought to offer safety benefits over today?s status quo of human-operated vehicles. While fatalities involving self-driving cars are rare, ML-automated driving has yet to deliver on the original promise of safer roads. The NTSB?s report states (https://oreil.ly/2nEOv) that this Uber?s ?system design did not include a consideration for jaywalking pedestrians.? The report also criticized lax risk assessments and an immature safety culture at the company. Furthermore, an Uber employee had raised serious concerns about 37 crashes in the previous 18 months and common problems with test vehicle drivers just days before the Tempe incident. As a result of the Tempe crash, Uber?s autonomous vehicle testing was stopped in four other cities and local governments all over the US and Canada began reexamining safety protocols for self-driving vehicle tests. The driver has been charged with negligent homicide. Uber has been excused from criminal liability, but came to a monetary settlement with the deceased?s family. The city of Tempe and the State of Arizona were also sued by Ms. Herzberg?s family for $10 million each.
An Unprepared Legal System
It must be noted that the legal system in the US is somewhat unprepared for the real? ity of AI incidents, potentially leaving employees, consumers, and the general public largely unprotected from the unique dangers presented by ML systems operating in our midst. The EU Parliament has put forward a liability regime for ML systems that would mostly prevent large technology companies from escaping their share of the consequences in future incidents. In the US, any plans for federal AI product safety regulations are still in a preliminary phase. In the interim, individual cases of AI safety incidents will likely be decided by lower courts with little education and experience in handling AI incidents, enabling Big Tech and other ML system opera? tors to bring vastly asymmetric legal resources to bear against individuals caught up in incidents related to complex ML systems. Even for the companies and ML system operators, this legal limbo is not ideal. While the lack of regulation seems to benefit those with the most resources and expertise, it makes risk management


120  |  Chapter 3: Debugging Machine Learning Systems for Safety and Performance

and predicting the outcomes of AI incidents more difficult. Regardless, future gener? ations may judge us harshly for allowing the criminal liability of one of the first AI incidents, involving many data scientists and other highly paid professionals and executives, to be pinned solely on a safety driver of a supposedly automated vehicle.
Lessons Learned
What lessons learned from this and previous chapters could be applied to this case?
Lesson 1: Culture is important.
A mature safety culture is a broad risk control, bringing safety to the forefront of design and implementation work, and picking up the slack in corner cases that processes and technology miss. Learned from the last generation of life-changing commercial technologies, like aerospace and nuclear power, a more mature safety culture at Uber could have prevented this incident, especially since an employee raised serious concerns in the days before the crash.
Lesson 2: Mitigate foreseeable failure modes.
The NTSB concluded that Uber?s software did not specifically consider jaywalk? ing pedestrians as a failure mode. For anyone who?s driven a car with pedestrians around, this should have been an easily foreseeable problem for which any self-driving car should be prepared. ML systems generally are not prepared for incidents unless their human engineers make them prepared. This incident shows us what happens when those preparations are not made in advance.
Lesson 3: Test ML systems in their operating domain.
After the crash, Uber stopped and reset its self-driving car program. After improvements, it was able to show via simulation that its new software would have started breaking four seconds before impact. Why wasn?t the easily foresee? able reality of jaywalking pedestrians tested with these same in-domain simula? tions before the March 2018 crash? The public may never know. But enumerating failure modes and testing them in realistic scenarios could prevent our organiza? tion from having to answer these kinds of unpleasant questions.
A potential bonus lesson here is to consider not only accidental failures, like the Uber crash, but also malicious hacks against ML systems and the abuse of ML systems to commit violence. Terrorists have turned motor vehicles into deadly weapons before, so this is a known failure mode. Precautions must be taken in autonomous vehicles, and in driving assistance features, to prevent hacking and violent outcomes. Regard? less of whether it is an accident or a malicious attack, AI incidents will certainly kill more people. Our hope is that governments and other organizations will take ML safety seriously, and minimize the number of these somber incidents in the future.




Case Study: Death by Autonomous Vehicle |  121

Resources
Further Reading
? ?A Comprehensive Study on Deep Learning Bug Characteristics? (https://oreil.ly/89R6O)
? ?Debugging Machine Learning Models? (https://oreil.ly/685C3)
? ?Real-World Strategies for Model Debugging? (https://oreil.ly/LvrLk)
? ?Safe and Reliable Machine Learning? (https://oreil.ly/mLU8l)
? ?Overview of Debugging ML Models? (https://oreil.ly/xZGoN)
? ?DQI: Measuring Data Quality in NLP? (https://oreil.ly/aa7rv)
? ?Identifying and Overcoming Common Data Mining Mistakes? (https://oreil.ly/w19Qm)
Code Examples
? Basic sensitivity and residual analysis example (https://oreil.ly/Tcu65)
? Advanced sensitivity analysis example (https://oreil.ly/QPFFx)
? Advanced residual analysis example (https://oreil.ly/Poe20)























122  |  Chapter 3: Debugging Machine Learning Systems for Safety and Performance



CHAPTER 4

Managing Bias in Machine Learning



Managing the harmful effects of bias in machine learning systems is about so much more than data, code, and models. Our model?s average performance quality?the main way data scientists are taught to evaluate the goodness of a model?has little to do with whether it?s causing real-world bias harms. A perfectly accurate model can cause bias harms. Worse, all ML systems exhibit some level of bias, bias incidents appear to be some of the most common AI incidents (see Figure 4-1), bias in business processes often entails legal liability, and bias in ML models hurts people in the real world.

Figure 4-1. The frequency of different types of AI incidents based on a qualitative analysis of 169 publicly reported incidents between 1988 and February 1, 2021. Figure courtesy of BNH.AI.




123

This chapter will put forward approaches for detecting and mitigating bias in a sociotechnical fashion, at least to the best of our ability as practicing technicians. That means we?ll try to understand how ML system bias exists in its broader societal context. Why? All ML systems are sociotechnical. We know this might be hard to believe at first, so let?s think through one example. Let?s consider a model used to predict sensor failure for an Internet of Things (IoT) application, using only infor? mation from other automated sensors. That model would likely have been trained by humans, or a human decided that a model was needed. Moreover, the results from that model could be used to inform the ordering of new sensors, which could affect the employment of those at the manufacturing plant or those who repair or replace failing sensors. Finally, if our preventative maintenance model fails, people who interact with the system could be harmed. For every example we can think of that seems purely technical, it becomes obvious that decision-making technologies like ML don?t exist without interacting with humans in some way.
This means there?s no purely technical solution to bias in ML systems. If readers want to jump right into the code for bias testing and bias remediation, see Chapter 10. However, we don?t recommend this. Readers will miss a lot of important information about what bias is and how to think about it in productive ways. This chapter starts out by defining bias using several different authoritative sources, and how to recognize our own cognitive biases that may affect the ML systems we build or the results our users interpret. The chapter then provides a broad overview of who tends to be harmed in AI bias incidents and what kinds of harms they experience. From there, we?ll cover methods to test for bias in ML systems and discuss mitigating bias using both technical and sociotechnical approaches. Finally, the chapter will close with a case discussion of the Twitter image-cropping algorithm.

While some aspects of bias management must be tuned to the specific architecture of a model, a great deal of bias management is not model-specific. Many of the ideas in this chapter, particularly those drawn from the NIST SP1270 bias guidance and the Twitter Bias Bounty, can be applied to a wide variety of sophisticated AI systems like ChatGPT or RoBERTa language models. If readers want to see this in practice, check out IQT Labs? audit of RoBERTa (https://oreil.ly/3hs_6).









124  |  Chapter 4: Managing Bias in Machine Learning


NIST AI RMF Crosswalk

?Systemic Bias? on page 126	MAP 1.6, MAP 2.3, MEASURE 2.11
?Statistical Bias? on page 126	MAP 2.3, MEASURE 2.6, MEASURE 2.11

?Human Biases and Data Science Culture? on page 127
?Legal Notions of ML Bias in the United States? on page 128
?Who Tends to Experience Bias from ML Systems? on page 131
?Harms That People Experience? on page 133

GOVERN 3.2, MAP 1.1, MAP 2.3, MEASURE 2.11

GOVERN 1.1, GOVERN 1.2, GOVERN 1.4, GOVERN 2.2, GOVERN 4.1, MAP
1.1, MAP 1.2, MEASURE 2.11
GOVERN 1.2, GOVERN 1.4, GOVERN 5, MAP 1.1, MAP 1.2, MAP 1.6, MAP
2.2, MAP 3, MAP 5, MEASURE 1.3, MEASURE 4, MANAGE 2, MANAGE 4
GOVERN 1.2, GOVERN 1.4, GOVERN 5, MAP 1.1, MAP 1.2, MAP 1.6, MAP
2.2, MAP 3, MAP 5, MEASURE 1.3, MEASURE 4, MANAGE 1.4, MANAGE
2, MANAGE 4

?Testing Data? on page 135	MEASURE 1.1, MEASURE 2.1

?Traditional Approaches: Testing for Equivalent Outcomes? on page 137

?A New Mindset: Testing for Equivalent Performance Quality? on page 141
?On the Horizon: Tests for the Broader ML Ecosystem? on page 143

?Technical Factors in Mitigating Bias? on page 148
?The Scientific Method and Experimental Design? on page 148
?Human Factors in Mitigating Bias? on page 153

GOVERN 1.1, GOVERN 1.2, GOVERN 1.4, GOVERN 4.3, GOVERN 6.1, MAP
2.3, MAP 4, MEASURE 1, MEASURE 2.1, MEASURE 2.6, MEASURE 2.11,
MEASURE 4.2
GOVERN 1.2, GOVERN 1.4, GOVERN 4.3, GOVERN 6.1, MAP 2.3, MAP 4,
MEASURE 1, MEASURE 2.1, MEASURE 2.6, MEASURE 2.11, MEASURE 4.2
GOVERN 1.2, GOVERN 1.4, GOVERN 4.3, GOVERN 6.1, MAP 2.3, MAP 4,
MEASURE 1, MEASURE 2.1, MEASURE 2.6, MEASURE 2.11, MEASURE 4.2,
MANAGE 3.2
MAP 2.3, MANAGE

MAP 1.1, MAP 2.3

GOVERN 2.1, GOVERN 3, MAP 1.1, MAP 1.6, MAP 2.2, MAP 2.3,
MEASURE 3.2, MEASURE 3.3, MANAGE 1, MANAGE 2.1, MANAGE 2.2,
MANAGE 3, MANAGE 4.2, MANAGE 4.3


? Applicable AI trustworthiness characteristics include: Managed Bias, Transparent and Accountable, Valid and Reliable
? See also:
? ?Towards a Standard for Identifying and Managing Bias in Artificial Intelligence" (https://oreil.ly/8kpf5)
? Full crosswalk table (not an official resource) (https://oreil.ly/61TXd)






Managing Bias in Machine Learning  |  125

ISO and NIST Definitions for Bias
The International Organization for Standardization (ISO) defines bias as ?the degree to which a reference value deviates from the truth? in ?Statistics?Vocabulary and Symbols?Part 1? (https://oreil.ly/YYv4W). This is a very general notion of bias, but bias is a complex and heterogenous phenomenon. Yet, in all of its instances, it?s about some systematic deviation from the truth. In decision-making tasks, bias takes on many forms. It?s substantively and ethically wrong to deny people employment due to the level of melanin in their skin. It?s factually wrong to think an idea is correct just because it?s the first thing that comes to mind. And it?s substantively and ethically wrong to train an ML model on incomplete and unrepresentative data. In a recent work from NIST, ?Towards a Standard for Identifying and Managing Bias in Artificial Intelligence? (SP1270) (https://oreil.ly/pkm4f), the subject of bias is divided into three major categories that align with these examples of bias: systemic, statistical, and human biases.
Systemic Bias
Often when we say bias in ML, we mean systemic biases. These are historical, social, and institutional biases that are, sadly, so baked into our lives that they show up in ML training data and design choices by default. A common consequence of systemic bias in ML models is the incorporation of demographic information into system mechanisms. This incorporation may be overt and explicit, such as when language models (LMs) are repurposed to generate harmful and offensive content (https:// oreil.ly/bWf4E) that targets certain demographic groups. However, in practice, incor? poration of demographic information into decision-making processes tends to be unintentional and implicit, leading to differential outcome rates or outcome preva? lence across demographic groups, for example, by matching more men?s resumes to higher paying job descriptions, or design problems that exclude certain groups of users (e.g., those with physical disabilities) from interacting with a system.
Statistical Bias
Statistical biases can be thought of as mistakes made by humans in the specification of ML systems, or emergent phenomena like concept drift, that affect ML models and are difficult for humans to mitigate. Other common types of statistical biases include predictions based on unrepresentative training data, or error propagation and feed? back loops. One potential indicator of statistical bias in ML models is differential per? formance quality across different cross-sections of data, such as demographic groups. Differential validity for an ML model is a particular type of bias, somewhat distinct from the differing outcome rates or outcome prevalence described for human biases. In fact, there is a documented tension (https://oreil.ly/cJy7F) between maximizing



126  |  Chapter 4: Managing Bias in Machine Learning

model performance within demographic groups and maintaining equality of positive outcome rates. Statistical biases may also lead to serious AI incidents, for example when concept drift in new data renders a system?s decisions more wrong than right, or when feedback loops or error propagation lead to increasingly large volumes of bad predictions over a short time span.
Human Biases and Data Science Culture
There are a number of human or cognitive biases that can come into play with both the individuals and the teams that design, implement, and maintain ML systems. For a more complete list of human biases, read the NIST SP1270 guidance paper. The following are the human biases that we?ve seen most frequently affecting both data scientists and users of ML systems:
Anchoring
When a particular reference point, or anchor, has an undue effect on people?s decisions. This is like when a benchmark for a state-of-the-art deep learning model is stuck at 0.4 AUC for a long time, and someone comes along and scores
0.403 AUC. We shouldn?t think that?s important, but we?re anchored to 0.4.
Availability heuristic
People tend to overweigh what comes easily or quickly to mind in decision- making processes. Put another way, we often confuse easy to remember with correct.
Confirmation bias
A cognitive bias where people tend to prefer information that aligns with, or confirms, their existing beliefs. Confirmation bias is a big problem in ML systems when we trick ourselves into thinking our ML models work better than they actually do.
Dunning-Kruger effect
The tendency of people with low ability in a given area or task to overestimate their self-assessed ability. This happens when we allow ourselves to think we?re experts at something just because we can import sklearn and run model.fit().
Funding bias
A bias toward highlighting or promoting results that support or satisfy the funding agency or financial supporter of a project. We do what makes our bosses happy, what makes our investors happy, and what increases our own salaries. Real science needs safeguards that prevent its progress from being altered by biased financial interests.




ISO and NIST Definitions for Bias |  127

Groupthink
When people in a group tend to make nonoptimal decisions based on their desire to conform to the group or fear dissenting with the group. It?s hard to disagree with our team, even when we?re confident that we?re right.
McNamara fallacy
The belief that decisions should be made based solely on quantitative informa? tion, at the expense of qualitative information or data points that aren?t easily measured.
Techno-chauvinism
The belief that technology is always the solution.
All these biases can and do lead to inappropriate and overly optimistic design choices, in turn leading to poor performance when a system is deployed, and, finally, leading to harms for system users or operators. We?ll get into the harms that can arise and what to do about these problems shortly. For now we want to highlight a commonsense mitigant that is also a theme of this chapter. We cannot treat bias prop? erly without looking at a problem from many different perspectives. Step 0 of fighting bias in ML is having a diverse group of stakeholders in the room (or video call) when important decisions about the system are made. To avoid the blind spots that allow biased ML models to cause harm, we?ll need many different types of perspectives informing system design, implementation, and maintenance decisions. Yes, we?re speaking about gathering input from different demographic perspectives, including from those with disabilities. We?re also speaking about educational backgrounds, such as those of social scientists, lawyers, and domain experts.
Also, consider the digital divide. A shocking percentage of the population still doesn?t have access to good internet connectivity, new computers, and information like this book. If we?re drawing conclusions about our users, we need to remember that there is a solid chunk of the population that?s not going to be included in user statistics. Leaving potential users out is a huge source of bias and harm in system design, bias testing, and other crucial junctures in the ML lifecycle. Success in ML today still requires the involvement of people who have a keen understanding of the real-world problem we?re trying to solve, and what potential users might be left out of our design, data, and testing.
Legal Notions of ML Bias in the United States
We should be aware of the many important legal notions of bias. However, it?s also important to understand that the legal system is extremely complex and context sensitive. Merely knowing a few definitions will still leave us light years away from having any real expertise on these matters. As data scientists, legal matters are an area



128  |  Chapter 4: Managing Bias in Machine Learning

where we should not let the Dunning-Kruger effect take over. With those caveats, let?s dive into a basic overview.

Now is the time to reach out to your legal team if you have any questions or concerns about bias in ML models. Dealing with bias in ML models is one of the most difficult and serious issues in the information economy. Data scientists need help from lawyers to properly address bias risks.

In the US, bias in decision-making processes that affect the public has been regulated for decades. A major focus of early laws and regulations in the US was employment matters. Notions like protected groups, disparate treatment, and disparate impact have now spread to a broader set of laws in consumer finance and housing, and are even being cited in brand new local laws today, like the New York City audit requirement for AI used in hiring. Nondiscrimination in the EU is addressed in the Charter of Fundamental Rights, the European Convention on Human Rights, and in the Treaty on the Functioning of the EU, and, crucially for us, in aspects of the proposed EU AI Act. While it?s impossible to summarize these laws and regulations, even on the US side, the definitions that follow are what we think are most directly applicable to a data scientist?s daily work. They are drawn, very roughly, from laws like the Civil Rights Act, the Fair Housing Act (FHA), Equal Employment Opportu? nity Commission (EEOC) regulations, the Equal Credit Opportunity Act (ECOA), and the Americans with Disabilities Act (ADA). The following definitions cover legal ideas about what traits are protected under law and what these laws seek to protect us from:
Protected groups
In the US, many laws and regulations prohibit discrimination based on race, sex (or gender, in some cases), age, religious affiliation, national origin, and disability status, among other categories. Prohibited decision bases under the FHA include race, color, religion, national origin, sex, familial status, and disability. The EU?s GDPR, as an example of one non-US regulation, prohibits the use of personal data about racial or ethnic origin, political opinions, and other categories some? what analogous to US protected groups. This is one reason why traditional bias testing compares results for protected groups and so-called control (or reference) groups that are not protected groups.
Disparate treatment
Disparate treatment is a specific type of discrimination that is illegal in many industries. It?s a decision that treats an individual less favorably than similarly sit? uated individuals because of a protected characteristic such as race, sex, or other trait. For data scientists working on employment, housing, or credit applications,



Legal Notions of ML Bias in the United States  |  129

this means we should be very careful when using demographic data in ML models, and even in our bias-remediation techniques. Once demographic data is used as input in a model, that could mean that a decision for someone could be different just because of their demographics, and that disparate treatment could result in some cases.

Concerns about disparate treatment, and more general sys? temic bias, are why we typically try not to use demographic markers as direct inputs to ML models. To be conservative, demographic markers should not be used as model inputs in most common scenarios, but should be used for bias testing or monitoring purposes.

Disparate impact
Disparate impact is another kind of legally concerning discrimination. It?s basi? cally about different outcome rates or prevalence across demographic groups. Disparate impact is more formally defined as the result of a seemingly neutral policy or practice that disproportionately harms a protected group. For data scientists, disparate impact tends to happen when we don?t use demographic data as inputs, but we use something correlated to demographic data as an input. Consider credit scores: they are a fairly accurate predictor of default, so they are often seen as valid to use in predictive models in consumer lending. However, they are correlated to race, such that some minority groups have lower credit scores on average. If we use a credit score in a model, this tends to result in certain minority groups having lower proportions of positive outcomes, and that?s a common example of disparate impact. (That?s also why several states have started to restrict the use of credit scores in some insurance-related decisions.)
Differential validity
Differential validity is a construct that comes up sometimes in employment. Where disparate impact is often about different outcome rates across demo? graphic groups, differential validity is more about different performance quality across groups. It happens when an employment test is a better indicator of job performance for some groups than for others. Differential validity is important because the mathematical underpinning, not the legal construct, generalizes to nearly all ML models. It?s common to use unrepresentative training data and to build a model that performs better for some groups than for others, and a lot of more recent bias-testing approaches focus on this type of bias.
Screen out
Screen out is a very important type of discrimination that highlights the socio? technical nature of ML systems and proves that testing and balancing the scores of a model is simply insufficient to protect against bias. Screen out happens when


130  |  Chapter 4: Managing Bias in Machine Learning

a person with a disability, such as limited vision or difficulties with fine motor skills, is unable to interact with an employment assessment, and is screened out of a job or promotion by default. Screen out is a serious issue, and the EEOC and Department of Labor are paying attention (https://oreil.ly/c0y9i) to the use of ML in this space. Note that screen out cannot necessarily be fixed by mathematical bias testing or bias remediation; it typically must be addressed in the design phase of the system, where designers ensure those with disabilities are able to work with the end product?s interfaces. Screen out also highlights why we want perspectives from lawyers and those with disabilities when building ML systems. Without those perspectives, it?s all too easy to forget about people with disabilities when building ML systems, and that can sometimes give rise to legal liabilities.
This concludes our discussion on general definitions of bias. As readers can see, it?s a complex and multifaceted topic with all kinds of human, scientific, and legal concerns coming into play. We?ll add to these definitions with more specific, but probably more fraught, mathematical definitions of bias when we discuss bias testing later in the chapter. Next we?ll outline who tends to experience bias and related harms from ML systems.
Who Tends to Experience Bias from ML Systems
Any demographic group can experience bias and related harms when interacting with an ML system, but history tells us certain groups are more likely to experience bias and harms more often. In fact, it?s the nature of supervised learning?which only learns and repeats patterns from past recorded data?that tends to result in older people, those with disabilities, immigrants, people of color, women, and gender- nonconforming individuals facing more bias from ML systems. Put another way, those who experience discrimination in the real world, or in the digital world, will likely also experience it when dealing with ML systems because all that discrimination has been recorded in data and used to train ML models. The groups listed in this section are often protected under various laws, but not always. They will often, but not always, be the comparison group in bias testing for statistical parity of scores or outcomes between two demographic groups.
Many people belong to multiple protected or marginalized groups. The important concept of intersectionality tells us that societal harm is concentrated among those who occupy multiple protected groups and that bias should not only be analyzed as affecting marginalized groups along single group dimensions (https://oreil.ly/3ZaPy). For example, AI ethics researchers recently showed (https://oreil.ly/DMu8o) that some commercially available facial recognition systems have substantial gender classifica? tion accuracy disparities, with darker-skinned women being the most misclassified group. Finally, before defining these groups, it is also important to think of the McNamara fallacy. Is it even right to put nuanced human beings into this kind of


Who Tends to Experience Bias from ML Systems | 131

blunted taxonomy? Probably not, and it?s likely that assignment to these simplistic groups, which is often done because such categories are easy to represent as binary marker columns in a database, is also a source of bias and potential harms. There are always a lot of caveats in managing bias in ML systems, so with those in mind, we tread carefully into defining simplified demographic groups that tend to face more discrimination and that are often used as comparison groups in traditional bias testing:
Age
Older people, typically those 40 and above, are more likely to experience discrim? ination in online content. The age cutoff could be older in more traditional appli? cations likes employment, housing, or consumer finance. However, participation in Medicare or the accumulation of financial wealth over a lifetime may make older people the favored group in other scenarios.
Disability
Those with physical, mental, or emotional disabilities are perhaps some of the likeliest people to experience bias from ML systems. The idea of screen out generalizes outside of employment, even if the legal construct may not. People with disabilities are often forgotten about during the design of ML systems, and no amount of mathematical bias testing or remediation can make up for that.
Immigration status or national origin
People who live in a country in which they were not born, with any immigra? tion status, including naturalized citizens, are known to face significant bias challenges.
Language
Especially in online content, an important domain for ML systems, those who use languages other than English or who write in non-Latin scripts may be more likely to experience bias from ML systems.
Race and ethnicity
Races and ethnicities other than white people, including those who identify as more than one race, are commonly subject to bias and harm when interacting with ML systems. Some also prefer skin tone scales over traditional race or eth? nicity labels, especially for computer vision tasks. The Fitzpatrick scale (https:// oreil.ly/NJfBP) is an example of a skin tone scale.
Sex and gender
Sexes and genders other than cisgender men are more likely to experience bias and harms at the hands of an ML system. In online content, women are often favored?but in harmful ways. Known as the male gaze phenomenon, media



132  |  Chapter 4: Managing Bias in Machine Learning

about women may be appealing and receive positive treatment (such as being promoted in a social media feed), specifically because that content is oriented toward objectification, subjugation, or sexualization of women.
Intersectional groups
People who are in two or more of the preceding groups may experience bias or harms that are greater than the simple sum of the two broader groups to which they belong. All the bias testing and mitigation steps described in this chapter should consider intersectional groups.
Of course these are not the only groups of people who may experience bias from an ML model, and grouping people can be problematic no matter what the motivation. However, it?s important to know where to start looking for bias, and we hope our list is sufficient for that purpose. Now that we know where to look for ML bias, let?s discuss the most common harms that we should be mindful of.
Harms That People Experience
Many common types of harm occur in online or digital content. They occur fre? quently too?perhaps so frequently that we may become blind to them. The following list highlights common harms and provides examples so that we can recognize them better when we see them next. These harms align closely with those laid out in Abagayle Lee Blank?s ?Computer Vision Machine Learning and Future-Oriented Ethics? (https://oreil.ly/-JmJA), which describes cases in which these harms occur in computer vision:
Denigration
Content that is actively derogatory or offensive?e.g., offensive content generated by chatbots like Tay (https://oreil.ly/2938n) or Lee Luda (https://oreil.ly/nRzs1).
Erasure
Erasure of content challenging dominant social paradigms or past harms suffered by marginalized groups?e.g., suppressing content (https://oreil.ly/FZdDB) that discusses racism or calls out white supremacy.
Exnomination
Treating notions like whiteness, maleness, or heterosexuality as central human norms?e.g., online searches (https://oreil.ly/m-zR-) returning a Barbie Doll as the first female result for ?CEO.?
Misrecognition
Mistaking a person?s identity or failing to recognize someone?s humanity?e.g., misrecognizing Black people (https://oreil.ly/GjyTI) in automated image tagging.



Harms That People Experience  |  133

Stereotyping
The tendency to assign characteristics to all members of a group?e.g., LMs automatically associating Muslims with violence (https://oreil.ly/eqAgw).
Underrepresentation
The lack of fair or adequate representation of demographic groups in model outputs?e.g., generative models thinking all doctors are white males and all nurses are white females (https://oreil.ly/V64lj).
Sometimes these harms may only cause effects limited to online or digital spaces, but as our digital lives begin to overlap more substantially with other parts of our lives, harms also spill over into the real world. ML systems in healthcare, employment, education, or other high-risk areas can cause harm directly, by wrongfully denying people access to needed resources. The most obvious types of real-world harms caused by ML systems include the following:
Economic harms
When an ML system reduces the economic opportunity or value of some activity?e.g., when men see more ads (https://oreil.ly/BT-cI) for better jobs than women.
Physical harms
When an ML system hurts or kills someone?e.g., when people overrely on self-driving automation (https://oreil.ly/BxH5Y).
Psychological harms
When an ML system causes mental or emotional distress?e.g., when disturbing content (https://oreil.ly/pQRYE) is recommended to children.
Reputational harms
When an ML system diminishes the reputation of an individual or organiza? tion?e.g., a consumer credit product rollout is marred by accusations (https:// oreil.ly/Wbvq5) of discrimination.
Unfortunately, users or subjects of ML systems may experience additional harms or combinations of harms that manifest in strange ways. Before we get too deep in the weeds with different kinds of bias testing in the next section, remember that checking in with our users to make sure they are not experiencing the harms discussed here, or other types of harms, is perhaps one of most direct ways to track bias in ML systems. In fact, in the most basic sense, it matters much more whether people are experiencing harm than whether some set of scores passes a necessarily flawed mathematical test. We must think about these harms when designing our system, talk to our users to ensure they don?t experience harm, and seek to mitigate harms.




134  |  Chapter 4: Managing Bias in Machine Learning

Testing for Bias
If there?s a chance that an ML system could harm people, it should be tested for bias. The goal of this section is to cover the most common approaches for testing ML models for bias so readers can get started with this important risk management task. Testing is neither straightforward nor conclusive. Just like in performance testing, a system can look fine on test data, and go on to fail or cause harm once deployed. Or a system could exhibit minimal bias at testing and deployment time, but drift into making biased or harmful predictions over time. Moreover, there are many tests and effect size measurements with known flaws and that conflict with one another. For a good overview of these issues, see the YouTube video of Princeton Professor Arvind Narayanan?s conference talk ?21 Fairness Definitions and Their Pol? itics? (https://oreil.ly/4QnqM), from the ACM Conference on Fairness, Accountability, and Transparency in ML. For an in-depth mathematical analysis of why we can?t simply minimize all bias metrics at once, check out ?Inherent Trade-Offs in the Fair Determination of Risk Scores? (https://oreil.ly/WvBOg). With these cautions in mind, let?s start our tour of contemporary bias-testing approaches.
Testing Data
This section covers what?s needed in training data to test for bias, and how to test that data for bias even before a model is trained. ML models learn from data. But no data is perfect or without bias. If systemic bias is represented in training data, that bias will likely manifest in the model?s outputs. It?s logical to start testing for bias in training data. But to do that, we have to assume that certain columns of data are available. At minimum, for each row of data, we need demographic markers, known outcomes (y, dependent variable, target feature, etc.) and later, we?ll need model outcomes?predictions for regression models, and decisions and confidence scores or posterior probabilities for classification models. While there are a handful of test? ing approaches that don?t require demographic markers, most accepted approaches require this data. Don?t have it? Testing is going to be much more difficult, but we?ll provide some guidance on inferring demographic marker labels too.

Our models and data are far from perfect, so don?t let the perfect be the enemy of the good in bias testing. Our data will never be perfect and we?ll never find the perfect test. Testing is very important to get right, but to be successful in real-world bias mitigation, it?s just one part of broader ML management and governance processes.

The need to know or infer demographic markers is a good example of why handling bias in ML requires holistic design thinking, not just slapping another Python pack? age onto the end of our pipeline. Demographic markers and individual-level data are also more sensitive from a privacy standpoint, and sometimes organizations don?t

Testing for Bias |  135

collect this information for data privacy reasons. While the interplay of data privacy and nondiscrimination law is very complex, it?s probably not the case that data privacy obligations override nondiscrimination obligations. But as data scientists, we can?t answer such questions on our own. Any perceived conflict between data privacy and nondiscrimination requirements has to be addressed by attorneys and compliance specialists. Such complex legal considerations are an example of why addressing bias in ML necessitates the engagement of a broad set of stakeholders.

In employment, consumer finance, or other areas where disparate treatment is prohibited, we need to check with our legal colleagues before changing our data based directly on protected class mem? bership information, even if our intention is to mitigate bias.

By now, readers are probably starting to realize how challenging and complex bias testing can be. As technicians, dealing with this complexity is not our sole responsi? bility, but we need to be aware of it and work within a broader team to address bias in ML systems. Now, let?s step into the role of a technician responsible for preparing data and testing data for bias. If we have the data we need, we tend to look for three major issues?representativeness, distribution of outcomes, and proxies:
Representiveness
The basic check to run here is to calculate the proportion of rows for each demographic group in the training data, with the idea that a model will struggle to learn about groups with only a small number of training data rows. Generally, proportions of different demographic groups in training data should reflect the population on which the model will be deployed. If it doesn?t, we should probably collect more representative data. It?s also possible to resample or reweigh a dataset to achieve better representativeness. However, if we?re working in employment, consumer finance, or other areas where disparate treatment is prohibited, we really need to check with our legal colleagues before changing our data based directly on protected class membership information. If we?re running into differ? ential validity problems (described later in this chapter), then rebalancing our training data to have larger or equal representation across groups may be in order. Balance among different classes may increase prediction quality across groups, but it may not help with, or may even worsen, imbalanced distributions of positive outcomes.
Distribution of outcomes
We need to know how outcomes (y variable values) are distributed across dem? ographic groups, because if the model learns that some groups receive more positive outcomes than others, that can lead to disparate impact. We need to calculate a bivariate distribution of y across each demographic group. If we see an imbalance of outcomes across groups, then we can try to resample or reweigh

136  |  Chapter 4: Managing Bias in Machine Learning

our training data, with certain legal caveats. More likely, we?ll simply end up knowing that bias risks are serious for this model, and when we test its outcomes, we?ll need to pay special attention and likely plan on some type of remediation.
Proxies
In most business applications of ML, we should not be training models on dem? ographic markers. But even if we don?t use demographic markers directly, infor? mation like names, addresses, educational details, or facial images may encode a great deal of demographic information. Other types of information may proxy for demographic markers too. One way to find proxies is to build an adversarial model based on each input column and see if those models can predict any dem? ographic marker. If they can predict a demographic marker, then those columns encode demographic information and are likely demographic proxies. If possible, such proxies should be removed from training data. Proxies may also be more hidden in training data. There?s no standard technique to test for these latent proxies, but we can apply the same adversarial modeling technique as described for direct proxies, except instead of using the features themselves, we can use engineered interactions of features that we suspect may be serving as proxies. We also suggest having dedicated legal or compliance stakeholders vet each and every input feature in our model with an eye toward proxy discrimination risk. If proxies cannot be removed or we suspect the presence of latent proxies, we should pay careful attention to bias-testing results for system outcomes, and be prepared to take remediation steps later in the bias mitigation process.
The outlined tests and checks for representativeness, distribution of outcomes, and proxies in training data all rely on the presence of demographic group markers, as will most of the tests for model outcomes. If we don?t have those demographic labels, then one accepted approach is to infer them. The Bayesian improved surname geo? coding (BISG) (https://oreil.ly/cJn-M) approach infers race and ethnicity from name and postal code data. It?s sad but true that US society is still so segregated that zip code and name can predict race and ethnicity, often with above 90% accuracy. This approach was developed by the RAND Corporation and the Consumer Financial Protection Bureau (CFPB) and has a high level of credibility for bias testing in con? sumer finance. The CFPB even has code on its GitHub (https://oreil.ly/hkvMD) for BISG! If necessary, similar approaches may be used to infer gender (https://oreil.ly/ eLTqM) from name, Social Security number, or birth year.
Traditional Approaches: Testing for Equivalent Outcomes
Once we?ve assessed our data for bias, made sure we have the information needed to perform bias testing, and trained a model, it?s time to test its outcomes for bias. We?ll start our discussion on bias testing by addressing some established tests. These tests tend to have some precedent in law, regulation, or legal commentary, and they tend to focus on average differences in outcomes across demographic groups. For a

Testing for Bias |  137

great summary of traditional bias-testing guidance, see the concise guidance (https:// oreil.ly/_bcVD) of the Office of Federal Contract Compliance Programs for testing employment selection procedures. For these kinds of tests, it doesn?t matter if we?re analyzing the scores from a multiple choice employment test or numeric scores from a cutting-edge AI-based recommender system.

The tests in this section are aligned to the notion statistical parity, or when a model generates roughly equal probabilities or favorable predictions for all demographic groups.


Table 4-1 highlights how these tests tend to be divided into categories for statistical and practical tests, and for continuous and binary outcomes. These tests rely heavily on the notion of protected groups, where the mean outcome for the protected group (e.g., women or Black people) is compared in a simple, direct, pairwise fashion to the mean outcome for some control group, (e.g., men or white people, respectively). This means we will need one test, at least, for every protected group in our data. If this sounds old fashioned, it is. But since these are the tests that have been used the most in regulatory and litigation settings for decades, it?s prudent to start with these tests before getting creative with newer methodologies. More established tests also tend to have known thresholds that indicate when values are problematic. These thresholds are listed in Table 4-1 and discussed in more detail in the sections that follow.
Table 4-1. Some common metrics used to measure bias in ML models, with thresholds where applicablea

Statistical significance Logistic regression coefficient	Linear regression coefficient Statistical significance ?2 test	t-test
Statistical significance Fisher?s exact test Statistical significance Binomial-z
Practical significance	Comparison of group means	Comparison of group means

Practical significance	Percentage point difference between
group means/marginal effect
Practical significance	Adverse impact ratio (AIR) (acceptable:
0.8?1.25)

Practical significance	Odds ratios Practical significance	Shortfall to parity





138  |  Chapter 4: Managing Bias in Machine Learning

Percentage point difference between group means

Standardized mean difference (SMD, Cohen?s d) (small difference: 0.2, medium difference: 0.5, large
difference: 0.8)




Differential validity	Accuracy or AUC ratios (acceptable:
0.8?1.25)
Differential validity	TPR, TNR, FPR, FNR ratios (acceptable:
0.8?1.25)
Differential validity	Equality of odds ([control TPR ÷ protected
TPR ? y = 1] and [control FPR ÷ protected FPR ? y = 0])
Differential validity	Equality of opportunity ([control TPR ÷
protected TPR ? y = 1])

R2 ratio (acceptable: 0.8?1.25)

MSE, RMSE ratios (acceptable: 0.8?1.25)


a TPR = true positive rate; TNR = true negative rate; FPR = false positive rate; FNR = false negative rate

Statistical significance testing
Statistical significance testing probably has the most acceptance across disciplines and legal jurisdictions, so let?s focus there first. Statistical significance testing is used to determine whether average or proportional differences in model outcomes across protected groups are likely to be seen in new data, or whether the differences in outcomes are random properties of our current testing datasets. For continuous out? comes, we often rely on t-tests between mean model outputs across two demographic groups. For binary outcomes, we often use binomial z-tests on the proportions of positive outcomes across two different demographic groups, chi-squared tests on contingency tables of model outputs, and Fisher?s exact test when cells in the contin? gency test have less than 30 individuals in them.
If you?re thinking this is a lot of pairwise tests that leave out important information, good job! We can use traditional linear or logistic regression models fit on the scores, known outcomes, or predicted outcomes of our ML model to understand if some demographic marker variable has a statistically significant coefficient in the presence of other important factors. Of course, evaluating statistical significance is difficult too. Because these tests were prescribed decades ago, most legal commentary points to significance at the 5% level as evidence of the presence of impermissible levels of bias in model outcomes. But in contemporary datasets with hundreds of thousands, millions, or more rows, any small difference in outcomes is going to be significant at the 5% level. We recommend analyzing traditional statistical bias-testing results at the 5% significance level and with significance level adjustments that are appropriate for our dataset size. We?d focus most of our energy on the adjusted results, but keep in mind that in the worst-case scenario, our organization could potentially face legal scrutiny and bias testing by external experts that would hold us to the 5% significance threshold. This would be yet another great time to start speaking with our colleagues in the legal department.




Testing for Bias |  139

Practical significance testing
The adverse impact ratio (AIR) and its associated four-fifths rule threshold are probably the most well-known and most abused bias-testing tools in the US. Let?s consider what it is first, then proceed to how it?s abused by practitioners. AIR is a test for binary outcomes, and it is the proportion of some outcome, typically a positive outcome like getting a job or a loan, for some protected group, divided by the proportion of that outcome for the associated control group. That proportion is associated with a threshold of four-fifths or 0.8. This four-fifths rule was highlighted by the EEOC in the late 1970s as a practical line in the sand, with results above four- fifths being highly preferred. It still has some serious legal standing in employment matters, where AIR and the four-fifths rule are still considered very important data by some federal circuits, and other federal court circuits have decided the measurement is too flawed or simplistic to be important. In most cases, AIR and the four-fifths rule have no official legal standing outside of employment, but they are still used occasionally as an internal bias-testing tool across regulated verticals like consumer finance. Moreover, AIR could always show up in the testimony of an expert in a lawsuit, for any bias-related matter.
AIR is an easy and popular bias test. So, what do we get wrong about AIR? Plenty. Technicians tend to interpret it incorrectly. An AIR over 0.8 is not necessarily a good sign. If our AIR test comes out below 0.8, that?s probably a bad sign. But if it?s above four-fifths, that doesn?t mean everything is OK. Another issue is the confusion of the AIR metric and the 0.8 threshold with the legal construct of disparate impact. We can?t explain why, but some vendors call AIR, literally, ?disparate impact.? They are not the same. Data scientists cannot determine whether some difference in outcomes is truly disparate impact. Disparate impact is a complex legal determination made by attorneys, judges, or juries. The focus on the four-fifths rule also distracts from the sociotechnical nature of handling bias. Four-fifths is only legally meaningful in some employment cases. Like any numeric result, AIR test results alone are insufficient for the identification of bias in a complex ML system.
All that said, it?s still probably a good idea to look into AIR results and other practi? cal significance results. Another common measure is standardized mean difference (SMD, or Cohen?s d). SMD can be used on regression or classification outputs?so it?s even more model-agnostic than AIR. SMD is the mean outcome or score for some protected group minus the mean outcome or score for a control group, with that quantity divided by a measure of the standard deviation of the outcome. Magnitudes of SMD at 0.2, 0.5, and 0.8 are associated with small, medium, and large differences in group outcomes in authoritative social science texts. Other common practical sig? nificance measures are percentage point difference (PPD), or the difference in mean outcomes across two groups expressed as a percentage, and shortfall, the number of people or the monetary amount required to make outcomes equivalent across a protected and control group.


140  |  Chapter 4: Managing Bias in Machine Learning

The worst-case scenario in traditional outcomes testing is that both statistical and practical testing results show meaningful differences in outcomes across one or more pairs or protected and control groups. For instance, when comparing employment recommendations for Black people and white people, it would be very bad to see a significant binomial-z test and an AIR under 0.8, and it would be worse to see this for multiple protected and control groups. The best-case scenario in traditional bias testing is that we see no statistical significance or large differences in practical significance tests. But even in this case, we still have no guarantees that a system won?t be biased once it?s deployed or isn?t biased in ways these tests don?t detect, like via screen out. Of course, the most likely case in traditional testing is that we will see some mix of results and will need help interpreting them, and fixing detected problems, from a group of stakeholders outside our direct data science team. Even with all that work and communication, traditional bias testing would only be the first step in a thorough bias-testing exercise. Next we?ll discuss some newer ideas on bias testing.
A New Mindset: Testing for Equivalent Performance Quality
In more recent years, many researchers have put forward testing approaches that focus on disparate performance quality across demographic groups. Though these tests have less legal precedent than traditional tests for practical and statistical sig? nificance, they are somewhat related to the concept of differential validity. These newer techniques seek to understand how common ML prediction errors may affect minority groups, and to ensure that humans interacting with an ML system have an equal opportunity to receive positive outcomes.
The important paper ?Fairness Beyond Disparate Treatment and Disparate Impact: Learning Classification without Disparate Mistreatment? (https://oreil.ly/NkTBF) lays out the case for why it?s important to think through ML model errors in the context of fairness. If minority groups receive more false positive or false negative decisions than other groups, any number of harms can arise depending on the application. In their seminal ?Equality of Opportunity in Machine Learning? (https://oreil.ly/_w-c3), Hardt, Price, and Srebro define a notion of fairness that modifies the widely acknowl? edged equalized odds idea. In the older equalized odds scenario, when the known outcome occurs (i.e., y = 1), two demographic groups of interest have roughly equal true positive rates. When the known outcome does not occur (i.e., y = 0), equal? ized odds means that false positive rates are roughly equal across two demographic groups. Equality of opportunity relaxes the y = 0 constraint of equalized odds and argues that when y = 1 equates to a positive outcome, such as receiving a loan or getting a job, seeking equalized true positive rates is a simpler and more utilitarian approach.



Testing for Bias |  141

If readers have spent any time with confusion matrices, they?ll know there are many other ways to analyze the errors of a binary classifier. We can think about different rates of true positives, true negatives, false positives, false negatives, and many other classification performance measurements across demographic groups. We can also up-level those measurements into more formal constructs, like equalized opportunity or equalized odds. Table 4-2 provides an example of how performance quality and error metrics across demographic groups can be helpful in testing for bias.
Table 4-2. Classification quality and error rates calculated across two demographic groupsa

Metric type
?
Accuracy
Sensitivity (TPR)
?
Specificity (TNR)
?
FPR
FNR
?
Female value
?
0.808
0.528
?
0.881
?
0.119
0.472
?
Male value
?
0.781
0.520
?
0.868
?
0.132
0.480
?
Female-to-male ratio
?
1.035
1.016
?
1.016
?
1.069
0.983
?
a The values for the comparison group, females, are divided by the values for the control group, males.

The first step, shown in Table 4-2, is to calculate a set of performance and error measurements across two or more demographic groups of interest. Then, using AIR and the four-fifths rule as a guide, we form a ratio of the comparison group value to the control group value, and apply thresholds of four-fifths (0.8) and five-fourths (1.25) to highlight any potential bias issues. It?s important to say that the 0.8 and 1.25 thresholds are only guides here; they have no legal meaning and are more common? sense markers than anything else. Ideally, these values should be close to 1, showing that both demographic groups have roughly the same performance quality or error rates under the model. We may flag these thresholds with whatever values make sense to us, but we would argue that 0.8?1.25 is the maximum range of acceptable values.
Based on our application, some metrics may be more important than others. For example, in medical testing applications, false negatives can be very harmful. If one demographic group is experiencing more false negatives in a medical diagnosis than others, it?s easy to see how that can lead to bias harms. The fairness metric decision tree at slide 40 of ?Dealing with Bias and Fairness in AI/ML/Data Science Systems? (https://oreil.ly/Es2d1) can be a great tool for helping to decide which of all of these different fairness metrics might be best for our application.
Are you thinking ?What about regression? What about everything in ML outside of binary classification?!? It?s true that bias testing is most developed for binary classifiers, which can be frustrating. But we can apply t-tests and SMD to regression models, and we can apply ideas in this section about performance quality and error rates too. Just like we form ratios of classification metrics, we can also form ratios of





142  |  Chapter 4: Managing Bias in Machine Learning

R2, mean average percentage error (MAPE), or normalized root mean square error (RMSE) across comparison and control groups, and again, use the four-fifths rule as a guide to highlight when these ratios may be telling us there is a bias problem in our predictions. As for the rest of ML, outside binary classification and regression, that?s what we will cover next. Be prepared to apply some ingenuity and elbow grease.
On the Horizon: Tests for the Broader ML Ecosystem
A great deal of research and legal commentary assumes the use of binary classifiers. There is a reason for this. No matter how complex the ML system, it often boils down to making or supporting some final yes or no binary decision. If that decision affects people and we have the data to do it, we should test those outcomes using the full suite of tools we?ve discussed already. In some cases, the output of an ML system does not inform an eventual binary decision, or perhaps we?d like to dig deeper and understand drivers of bias in our system or which subpopulations might be experiencing the most bias. Or maybe we?re using a generative model, like an LM or image generation system. In these cases, AIR, t-tests, and true positive rate ratios are not going to cut it. This section explores what we can do to test the rest of the ML ecosystem and ways to dig deeper, to get more information about drivers of bias in our data. We?ll start out with some general strategies that should work for most types of ML systems, and then briefly outline techniques for bias against individuals or small groups, LMs, multinomial classifiers, recommender systems, and unsupervised models:
General strategies
One of the most general approaches for bias testing is adversarial modeling. Given the numeric outcomes of our system, whether that?s rankings, cluster labels, extracted features, term embeddings, or other types of scores, we can use those scores as input to another ML model that predicts a demographic class marker. If that adversarial model can predict the demographic marker from our model?s predictions, that means our model?s predictions are encoding demographic information. That?s usually a bad sign. Another general technical approach is to apply explainable AI techniques to uncover the main drivers of our model?s predictions. If those features, pixels, terms, or other input data seem like they might be biased, or are correlated to demographic information, that is another bad sign. There are now even specific approaches (https://oreil.ly/CcS_9) for understanding which features are driving bias in model outcomes. Using XAI to detect drivers of bias is exciting because it can directly inform us how to fix bias problems. Most simply, features that drive bias should likely be removed from the system.




Testing for Bias |  143

Not all strategies for detecting bias should be technical in a well-rounded test? ing plan. Use resources like the AI Incident Database (https://oreil.ly/Jc2vm) to understand how bias incidents have occurred in the past, and design tests or user-feedback mechanisms to determine if we are repeating past mistakes. If our team or organization is not communicating with users about bias they are experiencing, that is a major blind spot. We must talk to our users. We should design user feedback mechanisms into our system or product lifecycle so that we know what our users are experiencing, track any harms, and mitigate harms where possible. Also, consider incentivizing users to provide feedback about bias harms. The Twitter Algorithmic Bias event (https://oreil.ly/RnPHy) serves as an amazing example of structured and incentivized crowd-sourcing of bias-related information. The case discussion at the end of the chapter will highlight the process and learnings from this unique event.
Language models
Generative models present many bias issues. Despite the lack of mature bias- testing approaches for LMs, this is an active area of research, with most important papers paying some kind of homage to the issue. Section 6.2 of ?Language Models Are Few-Shot Learners? (https://oreil.ly/ZvBRL) is one of the better examples of thinking through bias harms and conducting some basic testing. Broadly speaking, tests for bias in LMs consist of adversarial prompt engineering?allowing LMs to complete prompts like ?The Muslim man?? or ?The female doctor?? and checking for offensive generated text (and wow can it be offensive!). To inject an element of randomness, prompts can also be gener? ated by other LMs. Checks for offensive content can be done by manual human analysis, or using more automated sentiment analysis approaches. Conducting hot flips by exchanging names considered male for names considered female, for example, and testing the performance quality of tasks like named entity recognition is another common approach. XAI can be used too. It can help point out which terms or entities drive predictions or other outcomes, and people can decide if those drivers are concerning from a bias perspective.
Individual fairness
Many of the techniques we?ve put forward focus on bias against large groups. But what about small groups or specific individuals? ML models can easily isolate small groups of people, based on demographic information or proxies, and treat them differently. It?s also easy for very similar individuals to end up on different sides of a complex decision boundary. Adversarial models can help again. The adversarial model?s predictions can be a row-by-row local measure of bias. People who have high-confidence predictions from the adversarial model might be treated unfairly based on demographic or proxy information. We can use counterfactual tests, or tests that change some data attribute of a person to move them across a decision boundary, to understand if people actually belong


144  |  Chapter 4: Managing Bias in Machine Learning

on one side of a decision boundary, or if some kind of bias is driving their predicted outcome. For examples of some of these techniques in practice, see Chapter 10.
Multinomial classification
There are several ways to conduct bias testing in multinomial classifiers. For example, we might use a dimension reduction technique to collapse our various probability output columns into a single column and then test that single column like a regression model with t-tests and SMD, where we calculate the average values and variance of the extracted feature across different demographic groups and apply thresholds of statistical and practical significance previously described. It would also be prudent to apply more accepted measures that also happen to work for multinomial outcomes, like chi-squared tests or equality of opportunity. Perhaps the most conservative approach is to treat each output category as its own binary outcome in a one-versus-all fashion. If we have many categories to test, start with the most common and move on from there, applying all the standards like AIR, binomial z, and error metric ratios.
Unsupervised models
Cluster labels can be treated like multinomial classification output or tested with adversarial models. Extracted features can be tested like regression outcomes and also can be tested with adversarial models.
Recommender systems
Recommender systems are one of the most important types of commercial ML technologies. They often serve as gatekeepers for accessing information or prod? ucts that we need every day. Of course, they too have been called out for various and serious bias problems. Many general approaches, like adversarial models, user feedback, and XAI can help uncover bias in recommendations. However, specialized approaches for bias-testing recommendations are now available. See publications like ?Comparing Fair Ranking Metrics? (https://oreil.ly/gTFQq) or watch out for conference sessions like ?Fairness and Discrimination in Recom? mendation and Retrieval? (https://oreil.ly/fz8Ya) to learn more.
The world of ML is wide and deep. You might have a kind of model that we haven?t been able to cover here. We?ve presented a lot of options for bias testing, but certainly haven?t covered them all! We might have to apply common sense, creativity, and ingenuity to test our system. Just remember, numbers are not every? thing. Before brainstorming some new bias-testing technique, check peer-reviewed literature. Someone somewhere has probably dealt with a problem like ours before. Also, look to past failures as an inspiration for how to test, and above all else, communicate with users and stakeholders. Their knowledge and experience is likely more important than any numerical test outcome.



Testing for Bias |  145

Summary Test Plan
Before moving on to bias mitigation approaches, let?s try to summarize what we?ve learned about bias testing into a plan that will work in most common scenarios. Our plan will focus on both numerical testing and human feedback, and it will continue for the lifespan of the ML system. The plan we present is very thorough. We may not be able to complete all the steps, especially if our organization hasn?t tried bias testing ML systems before. Just remember, any good plan will include technical and sociotechnical approaches and be ongoing:
1. At the ideation stage of the system, we should engage with stakeholders like potential users, domain experts, and business executives to think through both the risks and opportunities the system presents. Depending on the nature of the system, we may also need input from attorneys, social scientists, psychologists, or others. Stakeholders should always represent diverse demographic groups, educational backgrounds, and life and professional experience. We?ll be on the lookout for human biases like groupthink, funding bias, the Dunning-Kruger effect, and confirmation bias that can spoil our chances for technical success.
2. During the design stage of the system, we should begin planning for monitoring and actionable recourse mechanisms, and we should ensure that we have the data?or the ability to collect the data?needed for bias testing. That ability is technical, legal, and ethical. We must have the technical capability to collect and handle the data, we must have user consent or another legal bases for collection and use?and do so without engaging in disparate treatment in some cases?and we shouldn?t rely on tricking people out of their data. We should also start to consult with user interaction and experience (UI/UX) experts to think through the implementation of actionable recourse mechanisms for wrong decisions, and to mitigate the role of human biases, like anchoring, in the interpretation of sys? tem results. Other important considerations include how those with disabilities or limited internet access will interact with the system, and checking into past failed designs so they can be avoided.
3. Once we have training data, we should probably remove any direct demographic markers and save these only for testing. (Of course, in some applications, like certain medical treatments, it may be crucial to keep this information in the model.) We should test training data for representativeness, fair distribution of outcomes, and demographic proxies so that we know what we?re getting into. Consider dropping proxies from the training data, and consider rebalancing or reweighing data to even out representation or positive outcomes across dem? ographic groups. However, if we?re in a space like consumer finance, human resources, health insurance, or another highly regulated vertical, we?ll want to check with our legal department about any disparate treatment concerns around rebalancing data.


146  |  Chapter 4: Managing Bias in Machine Learning

4. After our model is trained, it?s time to start testing. If our model is a traditional regression or classification estimator, we?ll want to apply the appropriate tradi? tional tests to understand any unfavorable differences in outcomes across groups, and we?ll want to apply tests for performance quality across demographic groups to check that performance is roughly equal for all of our users. If our model is not a traditional regression or classification estimator, we?ll still want think of a logical way to transform our outputs into a single numeric column or a binary 1/0 column so that we can apply a full suite of tests. If we can?t defensibly transform our outputs, or we just want to know more about bias in our model, we should try adversarial models and XAI to find any pockets of discrimination in our outcomes or to understand drivers of bias in our model. If our system is an LM, recommendation system, or other more specialized type of ML, we should also apply testing strategies designed for those kinds of systems.
5. When a model is deployed, it has to be monitored for issues like faulty perfor? mance, hacks, and bias. But monitoring is not only a technical exercise. We need to incentivize, receive, and incorporate user feedback. We need to ensure our actionable recourse mechanisms work properly in real-world conditions, and we need to track any harms that our system is causing. This is all in addition to performance monitoring that includes standard statistical bias tests. Monitoring and feedback collection must continue for the lifetime of the system.
What if we find something bad during testing or monitoring? That?s pretty common, and that?s what the next section is all about. There are technical ways to mitigate bias, but bias-testing results have to be incorporated into an organization?s overall ML governance programs to have their intended transparency and accountability benefits. We?ll be discussing governance and human factors in bias mitigation in the next section as well.
Mitigating Bias
If we test an ML model for bias in its outcomes, we are likely to find it in many cases. When it shows up, we?ll also need to address it (if we don?t find bias, double-check our methodology and results and plan to monitor for emergent bias issues when the system is deployed). This section of the chapter starts out with a technical discussion of bias mitigation approaches. We?ll then transition to human factors that mitigate bias that are likely to be more broadly effective over time in real-world settings. Practices like human-centered design (HCD) and governance of ML practitioners are much more likely to decrease harm throughout the lifecycle of an ML system than a point-in-time technical mitigation approach. We?ll need to have diverse stakeholders involved with any serious decision about the use of ML, including the initial setup of governance and diversity initiatives. While the technical methods we?ll put forward



Mitigating Bias | 147

are likely to play some role in making our organization?s ML more fair, they don?t work in practice without ongoing interactions with our users and proper oversight of ML practitioners.
Technical Factors in Mitigating Bias
Let?s start our discussion of technical bias mitigation with a quote from the NIST SP1270 AI bias guidance (https://oreil.ly/pkm4f). When we dump observational data that we chose to use because it is available into an unexplainable model and tweak the hyperparameters until we maximize some performance metric, we may be doing what the internet calls data science, but we?re not doing science science:1
Physicist Richard Feynman referred to practices that superficially resemble science but do not follow the scientific method as cargo cult science. A core tenet of the scientific method is that hypotheses should be testable, experiments should be interpretable, and models should be falsifiable or at least verifiable. Commentators have drawn similarities between AI and cargo cult science citing its black box interpretability, reproducibility problem, and trial-and-error processes.
The Scientific Method and Experimental Design
One of the best technical solutions to avoiding bias in ML systems is sticking to the scientific method. We should form a hypothesis about the real-world effect of our model. Write it down and don?t change it. Collect data that is related to our hypothesis. Select model architectures that are interpretable and have some structural meaning in the context of our hypothesis; in many cases, these won?t be ML models at all. We should assess our model with accuracy, MAPE, or whatever traditional assessment measures are appropriate, but then find a way to test whether our model is doing what it is supposed to in its real-world operating environment, for exam? ple with A/B testing (https://oreil.ly/d_5jB). This time-tested process cuts down on human biases?especially confirmation bias?in model design, development, and implementation, and helps to detect and mitigate systemic biases in ML system outputs, as those will likely manifest as the system not behaving as intended. We?ll delve into the scientific method, and what data science has done to it, in Chapter 12.
Another basic bias mitigant is experimental design (https://oreil.ly/A4Dzf). We don?t have to use whatever junk data is available to train an ML model. We can consult practices from experimental design to collect data specifically designed to address our hypothesis. Common problems with using whatever data our organization has laying around include that such data might be inaccurate, poorly curated, redundant, and


1 The authors acknowledge the potential offensiveness of several of the terms appearing in this quoted lan? guage. The source material, NIST SP1270 AI, was reviewed and justified by the potential for extreme harm when ignoring scientific rigor in AI.

148  |  Chapter 4: Managing Bias in Machine Learning

laced with systemic bias. Borrowing from experimental design allows us to collect and select a smaller, more curated set of training data that is actually related to an experimental hypothesis.
More informally, thinking through experimental design helps us avoid really silly, but harmful, mistakes. It is said there are no stupid questions. Unfortunately that?s not the case with ML bias. For example, asking whether a face can predict trustworthiness or criminality. These flawed experimental premises are based on already debunked and racist theories, like phrenology (https://oreil.ly/dEmE9). One basic way to check our experimental approach is to check whether our target feature?s name ends in ?iness? or ?ality,? as this can highlight that we?re modeling some kind of higher-order construct, versus something that is concretely measurable. Higher-order constructs like trustworthiness or criminality are often imbued with human and systemic biases that our system will learn. We should also check the AI Incident Database (https:// oreil.ly/s88Bt) to ensure we?re not just repeating a past failed design.
Repeating the past is another big mistake that?s easy to do with ML if we don?t think through the experiment our model implies. One of the worst examples of this kind of basic experimental design error happened in health insurance and was documented in Science (https://oreil.ly/D-wXE) and Nature (https://oreil.ly/sKVYC). The goal of the algorithms studied in the Science paper was to intervene in the care of a health insurer?s sickest patients. This should have been a win-win for both the insurer and the patients?costing insurers less by identifying those with the greatest needs early in an illness and getting those patients better care. But a very basic and very big design mistake led the algorithms to divert healthcare away from those most in need! What went wrong? Instead of trying to predict which patients would be the sickest in the future, the modelers involved decided to predict who would be the most expensive patients. The modelers assumed that the most expensive people were the sickest. In fact, the most expensive patients were older people with pricey healthcare plans and access to good care. The algorithm simply diverted more care to people with good healthcare already, and cut resources for those who needed it most. As readers might imagine, those two populations were also highly segregated along racial lines. The moment the modelers chose to have healthcare cost as their target, as opposed to some indicator of health or illness, this model was doomed to be dangerously biased. If we want to mitigate bias in ML, we need to think before we code. Trying to use the scientific method and experimental design in our ML modeling projects should help us think through what we?re doing much more clearly and lead to more technical successes too.
Bias Mitigation Approaches
Even if we apply the scientific method and experimental design, our ML system may still be biased. Testing will help us detect that bias, and we?ll likely also want some technical way of treating it. There are many ways to treat bias once it?s detected,

Mitigating Bias | 149

or to train ML models that attempt to learn fewer biases. The recent paper ?An Empirical Comparison of Bias Reduction Methods on Real-World Problems in High- Stakes Policy Settings? (https://oreil.ly/TSAvx) does a nice comparison of the most widely available bias mitigation techniques, and another paper by the same group of researchers, ?Empirical Observation of Negligible Fairness?Accuracy Trade-Offs in Machine Learning for Public Policy? (https://oreil.ly/gitq4), addresses the false idea that we have to sacrifice accuracy when addressing bias. We don?t actually make our models less performant by making them less biased?a common data science mis? conception. Another good resource for technical bias remediation is IBM?s AIF360 package (https://oreil.ly/G8kCw), which houses most major remediation techniques. We?ll highlight what?s known as preprocessing, in-processing, and postprocessing approaches, in addition to model selection, LM detoxification, and other bias mitiga? tion techniques.
Preprocessing bias mitigation techniques act on the training data of the model rather than the model itself. Preprocessing tends to resample or reweigh training data to balance or shift the number of rows for each demographic group or to redistribute outcomes more equally across demographic groups. If we?re facing uneven perfor? mance quality across different demographic groups, then boosting the representation of groups with poor performance may help. If we?re facing unequitable distributions of positive or negative outcomes, usually as detected by statistical and practical significance testing, then rebalancing outcomes in training data may help to balance model outcomes.
In-processing refers to any number of techniques that alter a model?s training algo? rithm in an attempt to make its outputs less biased. There are many approaches for in-processing, but some of the more popular approaches include constraints, dual objective functions, and adversarial models:
Constraints
A major issue with ML models is their instability. A small change in inputs can lead to a large change in outcomes. This is especially worrisome from a bias standpoint if the similar inputs are people in different demographic groups and the dissimilar outcomes are those people?s pay or job recommendations. In the seminal ?Fairness Through Awareness? (https://oreil.ly/iYLS9), Cynthia Dwork et al. frame reducing bias as a type of constraint during training that helps models treat similar people similarly. ML models also find interactions automatically. This is worrisome from a bias perspective if models learn many different proxies for demographic group membership, across different rows and input features for different people. We?ll never be able to find all those proxies. To prevent models from making their own proxies, try interaction constraints (https://oreil.ly/4uIGl) in XGBoost.



150  |  Chapter 4: Managing Bias in Machine Learning

Dual objectives
Dual optimization is where one part of a model?s loss function measures model? ing error and another term measures bias, and minimizing the loss function finds a performant and less biased model. ?FairXGBoost: Fairness-Aware Classification in XGBoost? (https://oreil.ly/fq9Jw) introduces a method for including a bias regularization term in XGBoost?s objective function that leads to models with good performance and fairness trade-offs.2
Adversarial models
Adversarial models can also help make training less biased. In one setup for adversarial modeling, a main model to be deployed later is trained, then an adversarial model attempts to predict demographic membership from the main model?s predictions. If it can, then adversarial training continues?training the main model and then the adversary model?until the adversary model can no longer predict demographic group membership from the main model?s predic? tions, and the adversary model shares some information, like gradients, with the main model in between each retraining iteration.
In studies, pre- and in-processing tend to decrease measured bias in outcomes, but postprocessing approaches have been shown to be some of the most effective techni? cal bias mitigants. Postprocessing is when we change model predictions directly to make them less biased. Equalized odds or equalized opportunity are some common thresholds used when rebalancing predictions, i.e., changing classification decisions until the outcomes roughly meet the criteria for equalized odds or opportunity. Of course, continuous or other types of outcomes can also be changed to make them less biased. Unfortunately, postprocessing may be the most legally fraught type of technical bias mitigation. Postprocessing often boils down to switching positive predictions for control group members to negative predictions, so that those in pro? tected or marginalized groups receive more positive predictions. While these kinds of modifications may be called for in many different types of scenarios, be especially careful when using postprocessing in consumer finance or employment settings. If we have any concerns, we should talk to legal colleagues about disparate treatment or reverse discrimination.

Because pre-, in-, and postprocessing techniques tend to change modeling outcomes specifically based on demographic group membership, they may give rise to concerns related to disparate treatment, reverse discrimination, or affirmative action. Consult legal experts before using these approaches in high-risk scenarios, especially in employment, education, housing, or consumer finance applications.


2 Note that updating (https://oreil.ly/0gEwg) loss functions for XGBoost is fairly straightforward.

Mitigating Bias | 151

One of the most legally conservative bias mitigation approaches is to choose a model based on performance and fairness, with models trained in what is basically a grid search across many different hyperparameter settings and input feature sets, and demographic information used only for testing candidate models for bias. Consider Figure 4-2. It displays the results of a random grid search across two hundred candidate neural networks. On the y-axis we see accuracy. The highest model on this axis would be the model we normally choose as the best. However, when we add bias testing for these models on the x-axis, we can now see that there are several models with nearly the same accuracy and much improved bias-testing results. Adding bias testing onto hyperparameter searches adds fractions of a second to the overall training time, and opens up a whole new dimension for helping to select models.

Figure 4-2. A simple random grid search produces several interesting choices for models that provide a good balance between accuracy and AIR
There are many other technical bias mitigants. One of the most important, as dis? cussed many times in this book, is mechanisms for actionable recourse that enable appeal and override of wrong and consequential ML-based decisions. Whenever we build a model that affects people, we should make sure to also build and test a


152  |  Chapter 4: Managing Bias in Machine Learning

mechanism that lets people identify and appeal wrong decisions. This typically means providing an extra interface that explains data inputs and predictions to users, then allows them to ask for the prediction to be changed.
Detoxification, or the process of preventing LMs from generating harmful language, including hate speech, insults, profanities, and threats, is another important area in bias mitigation research. Check out ?Challenges in Detoxifying Language Models? (https://oreil.ly/gfVaZ) for a good overview of the some of the current approaches to detoxification and their inherent challenges. Because bias is thought to arise from models systematically misrepresenting reality, causal inference and discovery tech? niques, which seek to guarantee that models represent causal real-world phenomena, are also seen as bias mitigants. While causal inference from observational data con? tinues to be challenging, causal discovery approaches like LiNGAM (https://oreil.ly/ 985wC), which seek out input features with some causal relationship to the prediction target, are definitely something to consider in our next ML project.

Bias mitigation efforts must be monitored. Bias mitigation can fail or lead to worsened outcomes.


We?ll end this section with a warning. Technical bias mitigants probably don?t work on their own without the human factors we?ll be discussing next. In fact, it?s been shown (https://oreil.ly/RnES9) that bias testing and bias mitigation can lead to no improvements or even worsened bias outcomes. Like ML models themselves, bias mitigation has to be monitored and adjusted over time to ensure it?s helping and not hurting. Finally, if bias testing reveals problems and bias mitigation doesn?t fix them, the system in question should not be deployed. With so many ML systems being approached as engineering solutions that are predestined for successful deployment, how can we stop a system from being deployed? By enabling the right group of people to make the final call via good governance that promotes a risk-aware culture!
Human Factors in Mitigating Bias
To ensure a model is minimally biased before it?s deployed requires a lot of human work. First, we need a demographically and professionally diverse group of practi? tioners and stakeholders to build, review, and monitor the system. Second, we need to incorporate our users into the building, reviewing, and monitoring of the system. And third, we need governance to ensure that we can hold ourselves accountable for bias problems.
We?re not going to pretend we have answers for the continually vexing issues of diversity in the tech field. But here?s what we know: far too many models and ML


Mitigating Bias | 153

systems are trained by inexperienced and demographically homogenous development teams with little domain expertise in the area of application. This leaves systems and their operators open to massive blind spots. Usually these blind spots simply mean lost time and money, but they can lead to massive diversions of healthcare resources, arresting the wrong people, media and regulatory scrutiny, legal troubles, and worse. If, in the first design discussions about an AI system, we look around the room and see only similar faces, we?re going to have to work incredibly hard to ensure that systemic and human biases do not derail the project. It?s a bit meta, but it?s important to call out that having the same old tech guy crew lay out the rules for who is going to be involved in the system is also problematic. Those very first discussions are the time to try to bring in perspectives from different types of people, different professions, people with domain expertise, and stakeholder representatives. And we need to keep them involved. Is this going to slow down our product velocity? Certainly. Is this going to make it harder to ?move fast and break things?? Definitely. Is trying to involve all these people going to make technology executives and senior engineers angry? Oh yes. So, how can we do it? We?ll need to empower the voices of our users, who in many cases are a diverse group of people with many different wants and needs. And we?ll need a governance program for our ML systems. Unfortunately, get? ting privileged tech executives and senior engineers to care about bias in ML can be very difficult for one cranky person, or even a group of conscientious practitioners, to do without broader organizational support.
One of the ways we can start organizational change around ML bias is interacting with users. Users don?t like broken models. Users don?t like predatory systems, and users don?t like being discriminated against automatically and at scale. Not only is taking feedback from users good business, but it helps us spot issues in our design and track harms that statistical bias testing can miss. We?ll highlight yet again that statistical bias testing is very unlikely to uncover how or when people with disabilities or those that live on the other side of the digital divide experience harms because they cannot use the system or it works in strange ways for them. How do we track these kinds of harms? By talking to our users. We?re not suggesting that frontline engineers run out to their user?s homes, but we are suggesting that when building and deploying ML systems, organizations employ standard mechanisms like user stories, UI/UX research studies, human-centered design, and bug bounties to interact with their users in structured ways, and incorporate user feedback into improvements of the system. The case at the end of the chapter will highlight how structured and incentivized user feedback in the form of a bug bounty shed light on problems in a large and complex ML system.
Another major way to shift organizational culture is governance. That?s why we started the book with governance in Chapter 1. Here, we?ll explain briefly why governance matters for bias mitigation purposes. In many ways, bias in ML is about sloppiness and sometimes it?s about bad intent. Governance can help with both. If


154  |  Chapter 4: Managing Bias in Machine Learning

an organization?s written policies and procedures mandate that all ML models be tested thoroughly for bias or other issues before being deployed, then more models will probably be tested, increasing the performance of ML models for the business, and hopefully decreasing the chance of unintentional bias harms. Documentation, and particularly model documentation templates that walk practitioners through policy-mandated workflow steps, are another key part of governance. Either we as practitioners fill out the model documentation fulsomely, noting the correct steps we?ve taken along the way to meet what our organization defines as best practices, or we don?t. With documentation there is a paper trail, and with a paper trail there is some hope for accountability. Managers should see good work in model documents, and they should be able to see not-so-good work too. In the case of the latter, management can step in and get those practitioners training, and if the problems continue, disciplinary action can be taken. Regarding all those legal definitions of fairness that can be real gotchas for organizations using ML?policies can help every? one stay aligned with the law, and managerial review of model documentation can help to catch when practitioners are not aligned. Regarding all those human biases that can spoil ML models?policies can define best practices to help avoid them, and managerial review of model documentation can help to spot them before models are deployed.
While written policies and procedures and mandatory model documentation go a long way toward shaping an organization?s culture around model building, gover? nance is also about organizational structures. One cranky data scientist can?t do a whole lot about a large organization?s misuse or abuse of ML models. We need organ? izational support to effect change. ML governance should also ensure the independ? ence of model validation and other oversight staff. If testers report to development or ML managers, and are assessed on how many models they deploy, then testers probably don?t do much more than rubber-stamp buggy models. This is why model risk management (MRM), as defined by US government regulators, insists that model testers be fully independent from model developers, have the same education and skills as model developers, and be paid the same as model developers. If the director of responsible ML reports to the VP of data science and chief technology officer (CTO), they can?t tell their bosses ?no.? They?re likely just a figurehead that spends time on panels making an organization feel better about its buggy models. This is why MRM defines a senior executive role that focuses on ML risk, and stipulates that this senior executive report not to the CTO or CEO but directly to the board of directors (or to a chief risk officer who also reports to the board).
A lot of governance boils down to a crucial phrase that more data scientists should be aware of: effective challenge. Effective challenge is essentially a set of organiza? tional structures, business processes, and cultural competencies that enable skilled, objective, and empowered oversight and governance of ML systems. In many ways, effective challenge comes down to having someone in an organization that can stop


Mitigating Bias | 155

an ML system from being deployed without the possibility of retribution or other negative career or personal consequences. Too often, senior engineers, scientists, and technology executives have undue influence over all aspects of ML systems, including their validation, so-called governance, and crucial deployment or decommissioning decisions. This runs counter to the notion of effective challenge, and counter to the basic scientific principle of objective expert review. As we covered earlier in the chapter, these types of confirmation biases, funding biases, and techno-chauvinism can lead to the development of pseudoscientific ML that perpetuates systemic biases.
While there is no one solution for ML system bias, two themes for this chapter stand out. First, the preliminary step in any bias mitigation process is to involve a demographically and professionally diverse group of stakeholders. Step 0 for an ML project is to get diverse stakeholders in the room (or video call) when important decisions are being made! Second, human-centered design, bug bounties, and other standardized processes for ensuring technology meets the needs of its human stake? holders are some of the most effective bias mitigation approaches today. Now, we?ll close the chapter with a case discussion of bias in Twitter?s image-cropping algorithm and how a bug bounty was used to learn more about it from their users.
Case Study: The Bias Bug Bounty
This is a story about a questionable model and a very decent response to it. In October 2020, Twitter received feedback that its image-cropping algorithm might be behaving in a biased way. The image-cropping algorithm used an XAI techni? que, a saliency map, to decide what part of a user-uploaded image was the most interesting, and it did not let users override its choice. When uploading photos to include in a tweet, some users felt that the ML-based image cropper favored white people in images and focused on women?s chests and legs (male gaze bias), and users were not provided any recourse mechanism to change the automated cropping when these issues arose. The ML Ethics, Transparency, and Accountability (META) team, led by Rumman Chowdhury, posted a blog article (https://oreil.ly/6Qx_H), code (https://oreil.ly/S8E-L), and a paper (https://oreil.ly/rwr5h) describing the issues and the tests they undertook to understand users? bias issues. This level of transparency is commendable, but then Twitter took an even more unique step. It turned off the algorithm, and simply let users post their own photos, uncropped in many cases. Before moving to the bug bounty, which was undertaken later to gain even further understanding of user impacts, it?s important to highlight Twitter?s choice to take down the algorithm. Hype, commercial pressure, funding bias, groupthink, the sunken cost fallacy, and concern for one?s own career all conspire to make it extremely difficult to decommission a high-profile ML system. But that is what Twitter did, and it set a good example for the rest of us. We do not have to deploy broken or unnecessary models, and we can take models down if we find problems.


156  |  Chapter 4: Managing Bias in Machine Learning

Beyond being transparent about its issues and taking the algorithm down, Twitter then decided to host a bias bug bounty (https://oreil.ly/eBT18) to get structured user feedback on the algorithm. Users were incentivized to participate, as is usually the case with a bug bounty, through monetary prizes for those who found the worst bugs. The structure and incentives are key to understanding the unique value of a bug bounty as a user feedback mechanism. Structure is important because it?s difficult for large organizations to act on unstructured, ad hoc feedback. It?s hard to build a case for change when feedback comes in as an email here, a tweet there, and the occasional off-base tech media article. The META team put in the hard work to build a structured rubric (https://oreil.ly/N3-gc) for users to provide feedback. This means that when the feedback was received, it was easier to review, could be reviewed across a broader range of stakeholders, and it even contained a numeric score to help differ? ent stakeholders understand the severity of the issue. The rubric is usable to anyone who wants to track harms in computer vision or natural language processing systems, where measures of practical and statistical significance and differential performance often do not tell the full story of bias. Incentives are also key. While we may care a great deal about responsible use of ML, most people, and even users of ML systems, have better things to worry about or don?t understand how ML systems can cause serious harm. If we want users to stop their daily lives and tell us about our ML systems, we need pay them or provide other meaningful incentives.
According to AlgorithmWatch (https://oreil.ly/8B3dr), an EU think tank focusing on the social impacts of automated decision making, the bug bounty was ?an unprecedented experiment in openness.? With the image-cropper code open to bias bounty participants, users found many new issues. According to Wired (https:// oreil.ly/UvNMh), participants in the bug bounty also found a bias against those with white hair, and even against memes written in non-Latin scripts?meaning if we wanted to post a meme written in Chinese, Cyrillic, Hebrew, or any of the many languages that do not use the Latin alphabet?the cropping algorithm would work against us. AlgorithmWatch also highlighted one of the strangest findings of the contest. The image cropper often selected the last cell of a comic strip, spoiling the fun for users trying to share media that used the comic strip format. In the end,
$3,500 and first prize went to a graduate student in Switzerland, Bogdan Kulynych. Kulynych?s solution (https://oreil.ly/xOkz6) used deepfakes to create faces across a spectrum of shapes, shades, and ages. Armed with these faces and access to the cropping algorithm, he was able to empirically prove that the saliency function within the algorithm, used to select the most interesting region of an upload image, repeat? edly showed preferences toward younger, thinner, whiter, and more female-gendered faces.
The bias bounty was not without criticism. Some civil society activists voiced con? cerns that the high-profile nature of a tech company and tech conference drew atten? tion away from the underlying social causes of algorithmic bias. AlgorithmWatch


Case Study: The Bias Bug Bounty |  157

astutely points out the $7,000 in offered prize money was substantially less than bounties offered for security bugs, which average around $10,000 per bug. It also highlights that $7,000 is 1?2 weeks of salary pay for Silicon Valley engineers, and Twitter?s own ethics team stated that the week-long bug bounty amounted to roughly a year?s worth of testing. Undoubtedly Twitter benefited from the bias bounty and paid a low price for the information users provided. Are there other issues with using bug bounties as a bias risk mitigant? Of course there are, and Kulynych summed up that and other pressing issues in online technology well. According to the Guardian (https://oreil.ly/5FdnH), Kulynych had mixed feelings on the bias bounty and opined, ?Algorithmic harms are not only bugs. Crucially, a lot of harmful tech is harmful not because of accidents, unintended mistakes, but rather by design. This comes from maximization of engagement and, in general, profit externalizing the costs to others. As an example, amplifying gentrification, driving down wages, spreading clickbait and misinformation are not necessarily due to biased algorithms.? In short, ML bias and its associated harms are more about people and money than about technology.
Resources
Further Reading
? ?50 Years of Test (Un)fairness: Lessons for Machine Learning? (https://oreil.ly/fTlda)
? ?An Empirical Comparison of Bias Reduction Methods on Real-World Problems in High-Stakes Policy Settings? (https://oreil.ly/vmxPz)
? ?Discrimination in Online Ad Delivery? (https://oreil.ly/kuo9h)
? ?Fairness in Information Access Systems? (https://oreil.ly/1RAPJ)
? NIST SP1270: ?Towards a Standard for Identifying and Managing Bias in Artifi? cial Intelligence? (https://oreil.ly/3_Qrd)
? Fairness and Machine Learning (https://oreil.ly/D07t-)













158  |  Chapter 4: Managing Bias in Machine Learning



CHAPTER 5

Security for Machine Learning



If ?the worst enemy of security is complexity,? as Bruce Schneier claims (https:// oreil.ly/jfFU3), unduly complex machine learning systems are innately insecure. Other researchers have also released numerous studies describing and confirming specific security vulnerabilities for ML systems. And we?re now beginning to see how real-world attacks occur, like Islamic State operatives blurring their logos (https:// oreil.ly/8mSPC) in online content to evade social media filters. Since organizations often take measures to secure valuable software and data assets, ML systems should be no different. Beyond specific incident response plans, several additional informa? tion security processes should be applied to ML systems. These include specialized model debugging, security audits, bug bounties, and red-teaming.
Some of the primary security threats for today?s ML systems include the following:
? Insider manipulation of ML system training data or software to alter system outcomes
? Manipulation of ML system functionality and outcomes by external adversaries
? Exfiltration of proprietary ML system logic or training data by external adversaries
? Trojans or malware hidden in third-party ML software, models, data, or other artifacts
For mission-critical or otherwise high-stakes deployments of AI, systems should be tested and audited for at least these known vulnerabilities. Textbook ML model assessment will not detect them, but newer model debugging techniques can help, especially when fine-tuned to address specific security vulnerabilities. Audits can be conducted internally or by specialist teams in what?s known as ?red-teaming,? as is done by Meta (https://oreil.ly/nCqSa). Bug bounties (https://oreil.ly/rnZ9o), or when


159

organizations offer monetary rewards to the public for finding vulnerabilities, are another practice from general information security that should probably also be applied to ML systems. Moreover, testing, audits, red-teaming, and bug bounties need not be limited to security concerns alone. These types of processes can also be used to spot other ML system problems, such as those related to bias, instability, or a lack of robustness, reliability, or resilience, and spot them before they explode into AI incidents.

Audits, red-teaming, and bug bounties need not be limited to secu? rity concerns alone. Bug bounties can be used to find all manner of problems in public-facing ML systems, including bias, unauthor? ized decisions, and product safety or negligence issues, in addition to security and privacy issues.

This chapter explores security basics, like the CIA triad and best practices for data scientists, before delving into ML security. ML attacks are discussed in detail, includ? ing ML-specific attacks and general attacks that are also likely to affect ML systems. Countermeasures are then put forward, like specialized robust ML defenses and privacy-enhancing technologies (PETs), security-aware model debugging and moni? toring approaches, and a few more general solutions. This chapter closes with a case discussion about evasion attacks on social media and their real-world consequences. After reading the chapter, readers should be able to conduct basic security audits (or ?red-teaming?) on their ML systems, spot problems, and enact straightforward countermeasures where necessary. See Chapter 11 for ML security code examples.


160  |  Chapter 5: Security for Machine Learning



Security Basics
There are lots of basic lessons to learn from the broader field of computer security that will help harden our ML systems. Before we get into ML hacks and countermeas? ures, we?ll need to go over the importance of an adversarial mindset, discuss the CIA triad for identifying security incidents, and highlight a few straightforward best practices for security that should be applied to any IT group or computer system, including data scientists and ML systems.
The Adversarial Mindset
Like many practitioners in hyped technology fields, makers and users of ML systems tend to focus on the positives: automation, increased revenues, and the sleek coolness of new tech. However, another group of practitioners sees computer systems through a different and adversarial lens. Some of those practitioners likely work alongside us, helping to protect our organization?s IT systems from those that deliberately seek to abuse, attack, hack, and misuse ML systems to benefit themselves and do harm to others. A good first step toward learning ML security is to adopt such an adversarial mindset, or at least to block out overly positive ML hype and think about the intentional abuse and misuse of ML systems. And yes, even the one we?re working on right now.

Don?t be naive about high-risk ML systems. They can hurt people. People will attack them and people will abuse them to harm others.


Maybe a disgruntled coworker poisoned our training data, maybe there is malware hidden in binaries associated with some third-party ML software we?re using, maybe our model or training data can be extracted through an unprotected endpoint, or maybe a botnet could hit our organization?s public-facing IT services with a distributed denial-of-service (DDOS) attack, taking down our ML system as collateral damage. Although such attacks won?t happen to us every day, they will happen to


Security Basics |  161

someone, somewhere, frequently. Of course the details of specific security threats are important to understand, but an adversarial mindset that always considers the multi- faceted reality of security vulnerabilities and incidents is perhaps more important, as attacks and attackers are often surprising and ingenious.
CIA Triad
From a data security perspective, goals and failures are usually defined in terms of the confidentiality, integrity, and availability (CIA) triad (Figure 5-1). To briefly summa? rize the triad, data should only be available to authorized users (confidentiality), data should be correct and up-to-date (integrity), and data should be promptly available when needed (availability). If one of these tenets is broken, this is usually a security incident. The CIA triad applies directly to malicious access, alteration, or destruction of ML system training data. But it might be a bit more difficult to see how the CIA triad applies to an ML system issuing decisions or predictions, and ML attacks tend to blend traditional data privacy and computer security concerns in confusing ways. So, let?s go over an example of each.

Figure 5-1. The CIA triad for information security
The confidentiality of an ML system can be breached by an inversion attack (see ?Model extraction and inversion attacks? on page 171) in which a bad actor interacts with an API in an appropriate manner, but uses explainable artificial intelligence techniques to extract information about our model and training data from their submitted input data and our system?s predictions. In a more dangerous and sophis? ticated membership inference attack (see ?Membership inference attacks? on page 172), individual rows of training data, up to entire training datasets, can be extracted from ML system APIs or other endpoints. Note that these attacks can happen without unauthorized access to training files or databases, but result in the same security and privacy harms for our users or for our organization, potentially including serious legal liabilities.





162  |  Chapter 5: Security for Machine Learning

An ML system?s integrity can be compromised by several means, such as data poi? soning attacks or adversarial example attacks. In a data poisoning attack (see ?Data poisoning attacks? on page 168), an organizational insider subtly changes system training data to alter system predictions in their favor. Only a small proportion of training data must be manipulated to change system outcomes, and specialized techniques from active learning and other fields can help attackers do so with greater efficiency. When ML systems apply millions of rules or parameters to thousands of interacting input features, it becomes nearly impossible to understand all the different predictions an ML system could make. In an adversarial example attack (see ?Adversarial example attacks? on page 166), an external attacker preys on such overly complex mechanisms by finding strange rows of data?adversarial examples?that evoke unexpected and improper outcomes from the ML system, and typically does so to benefit themselves at our expense.
The availability of an ML system is violated when users cannot get access to the services they expect. This can be a consequence of the aforementioned attacks bring? ing down the system, from more standard denial-of-service attacks, from sponge example attacks, or from bias. People depend on ML systems more and more in their daily lives, and when these models relate to high-impact decisions in government, finance, or employment, an ML system being down can deny users access to essential services. Recent research (https://oreil.ly/D8KWt) has uncovered the threat of sponge examples, or a specially designed kind of input data that forces neural networks to slow down their predictions and consume inordinate amounts of energy. Sadly, many ML systems also perpetuate systemic biases in outcomes and accuracy for historically marginalized demographic groups. Minorities may be less likely to experience the same levels of availability from automated credit offers or resume scanners. More directly frighteningly, they may be more likely to experience faulty predictions by facial recognition systems, including those used in security or law enforcement con? texts. (Chapters 4 and 10 treat bias and bias testing for ML systems in detail.)
These are just a few ways that an ML system can experience security problems. There are many more. If readers are starting to feel worried, keep reading! We?ll discuss straightforward security concepts and best practices next. These tips can go a long way toward protecting any computer system.
Best Practices for Data Scientists
Starting with the basics will go a long way toward securing more complex ML systems. The following list summarizes those basics in the context of a data science workflow:





Security Basics |  163

Access control
The fewer people that access sensitive resources the better. There are many sensi? tive components in an ML system, but restricting training data, training code, and deployment code to only those who require access will mitigate security risks related to data exfiltration, data poisoning, backdoor attacks, and other attacks.
Bug bounties
Bug bounties, or when organizations offer monetary rewards to the public for finding vulnerabilities, are another practice from general information security that should probably also be applied to ML systems. A key insight with bug bounties is they incentivize user participation. Users are busy. Sometimes we need to reward them for providing feedback.
Incident response plans
It?s a common practice to have incident response plans in place for mission- critical IT infrastructure to quickly address any failures or attacks. Make sure those plans cover ML systems and have the necessary detail to be helpful if an ML system fails or suffers an attack. We?ll need to nail down who does what when an AI incident occurs, especially in terms of business authority, technical know-how, budget, and internal and external communications. There are excel? lent resources to help us get started with incident response from organizations like NIST (https://oreil.ly/u967-) and SANS Institute (https://oreil.ly/dS6oW). If readers would like to see an example incident response plan for ML systems, check out BNH.AI?s GitHub (https://oreil.ly/xN4Cs).
Routine backups
Ransomware attacks, where malicious hackers freeze access to an organization?s IT systems?and delete precious resources if ransom payments are not made? are not uncommon. Make sure to back up important files on a frequent and routine basis to protect against both accidental and malicious data loss. It?s also a best practice to keep physical backups unplugged (or ?air-gapped?) from any networked machines.
Least privilege
A strict application of the notion of least privilege, i.e., ensuring all personnel? even ?rockstar? data scientists and ML engineers?receive the absolute minimum required IT system permissions, is one of the best ways to guard against insider ML attacks. Pay special attention to limiting the number of root, admin, or super users.
Passwords and authentication
Use random and unique passwords, multifactor authentication, and other authentication methods to ensure access controls and permissions are preserved. It?s also not a bad idea to enforce a higher level of password hygiene, such as


164  |  Chapter 5: Security for Machine Learning

the use of password managers, for any personnel assigned to sensitive projects. Physical keys, such as Yubikeys (https://oreil.ly/oGT49), are some of the stron? gest authentication measures available. Given how common password phishing has become, in addition to hacks like SIM-switching that circumvent phone- based authentication, use of physical keys should be considered for high-risk applications.
Physical media
Avoid the use of physical storage media for sensitive projects if at all possible, except when required for backups. Printed documents, thumb drives, backup media, and other portable data sources are often lost and misplaced by busy data scientists and engineers. Worse still, they can be stolen by motivated adversaries. For less sensitive work, consider enacting policies and education around physical media use.
Product security
If our organization makes software, it?s likely that we apply any number of secu? rity features and tests to these products. There?s probably also no logical reason to not apply these same standards to public- or customer-facing ML systems. We should reach out to security professionals in our organization to discuss applying standard product security measures to our ML systems.
Red teams
For mission-critical or otherwise high-stakes deployments of ML, systems should be tested under adversarial conditions. In what?s known as red-teaming, teams of skilled practitioners attempt to attack ML systems and report their findings back to product owners.
Third parties
Building an ML system typically requires code, data, and personnel from outside our organization. Sadly, each new entrant to the build-out increases our risk. Watch out for data poisoning in third-party data or conducted by third-party personnel. Scan all third-party packages and models for malware, and control all deployment code to prevent the insertion of backdoors or other malicious payloads.
Version and environment control
To ensure basic security, we?ll need to know which changes were made to what files, when, and by whom. In addition to version control of source code, any number of commercial or open source environment managers can automate tracking for large data science projects. Check out some of these open resources to get started with ML environment management: DVC (https://oreil.ly/O6_6l), gigantum (https://oreil.ly/80VT7), mlflow (https://oreil.ly/pDjDF), ml-metadata (https://oreil.ly/p6EUA), and modeldb (https://oreil.ly/KhM3o).


Security Basics |  165

ML security, to be discussed in the next sections, will likely be more interesting for data scientists than the more general tactics described here. However, because the security measures considered here are so simple, not following them could potentially result in legal liabilities for our organization, in addition to embarrassing or costly breaches and hacks. While still debated and somewhat amorphous, violations of security standards, as enforced by the US Federal Trade Commission (FTC) (https:// oreil.ly/XfCYP) and other regulators, can bring with them unpleasant scrutiny and enforcement actions. Hardening the security of our ML systems is a lot of work, but failing to get the basics right can make big trouble when we?re building out more complex ML systems with lots of subsystems and dependencies.
Machine Learning Attacks
Various ML software artifacts, ML prediction APIs, and other AI system endpoints are now vectors for cyber and insider attacks. Such ML attacks can negate all the other hard work a data science team puts into mitigating other risks?because once our ML system is attacked, it?s not our system anymore. And attackers typically have their own agendas regarding accuracy, bias, privacy, reliability, robustness, resilience, and unauthorized decisions. The first step in defending against these attacks is to understand them. We?ll go over an overview of the most well-known ML attacks in the sections that follow.

Most attacks and vulnerabilities for ML systems are premised on the opaque and unduly complex nature of classical ML algorithms. If a system is so complex its operators don?t understand it, then attackers can manipulate it without the operators knowing what?s happened.

Integrity Attacks: Manipulated Machine Learning Outputs
Our tour of ML attacks will begin with attacks on ML model integrity, i.e., attacks that alter system outputs. Probably the most well-known type of attack, an adversarial example attack, will be discussed first, followed by backdoor, data poisoning, and impersonation and evasion attacks. When thinking through these attacks, remember that they can often be used in two primary ways: (1) to grant attackers the ML outcome they desire, or (2) to deny a third party their rightful outcome.
Adversarial example attacks
A motivated attacker can learn, by trial and error with a prediction API (i.e., ?explo? ration? or ?sensitivity analysis?), via an inversion attack (see ?Model extraction and inversion attacks? on page 171), or by social engineering, how to game our ML model to receive their desired prediction outcome or how to change someone else?s


166  |  Chapter 5: Security for Machine Learning

outcome. Carrying out an attack by specifically engineering a row of data for such purposes is referred to as an adversarial example attack. An attacker could use an adversarial example attack to grant themselves a loan, a lower than appropriate insurance premium, or to avoid pretrial detention based on a criminal risk score. See Figure 5-2 for an illustration of a fictitious attacker executing an adversarial example attack on a credit lending model using strange rows of data.

Figure 5-2. An adversarial example attack (digital, color version(: https://oreil.ly/04ycs))
Backdoor attacks
Consider a scenario where an employee, consultant, contractor, or malicious external actor has access to our model?s production code?code that makes real-time predic? tions. This individual could change that code to recognize a strange or unlikely combination of input variable values to trigger a desired prediction outcome. Like other outcome manipulation hacks, backdoor attacks can be used to trigger model outputs that an attacker wants, or outcomes a third party does not want. As depicted in Figure 5-3, an attacker could insert malicious code into our model?s production scoring engine that recognizes the combination of a realistic age but negative years on a job (yoj) to trigger an inappropriate positive prediction outcome for themselves or their associates. To alter a third party?s outcome, an attacker could insert an artificial rule into our model?s scoring code that prevents our model from producing positive outcomes for a certain group of people.





Machine Learning Attacks | 167



Figure 5-3. A backdoor attack (digital, color version(: https://oreil.ly/04ycs))
Data poisoning attacks
Data poisoning refers to someone systematically changing our training data to manipulate our model?s predictions. To poison data, an attacker must have access to some or all of our training data. And at many companies, many different employees, consultants, and contractors have just that?and with little oversight. It?s also possible a malicious external actor could acquire unauthorized access to some or all of our training data and poison it. A very direct kind of data poisoning attack might involve altering the labels of a training dataset. In Figure 5-4, the attacker changes a small number of training data labels so that people with their kind of credit history will erroneously receive a credit product. It?s also possible that a malicious actor could use data poisoning to train our model to intentionally discriminate against a group of people, depriving them of the big loan, big discount, or low insurance premiums they rightfully deserve.










168  |  Chapter 5: Security for Machine Learning



Figure 5-4. A data poisoning attack (digital, color version(: https://oreil.ly/04ycs))
While it?s simplest to think of data poisoning as changing the values in the existing rows of a dataset, data poisoning can also be conducted by adding seemingly harm? less or superfluous columns onto a dataset and ML model. Altered values in these columns could then trigger altered model predictions. This is one of many reasons to avoid dumping massive numbers of columns into an unexplainable ML model.
Impersonation and evasion attacks
Using trial and error, a model inversion attack (see ?Model extraction and inversion attacks? on page 171), or social engineering, an attacker can learn the types of individuals that receive a desired prediction outcome from our ML system. The attacker can then impersonate this kind of input or individual to receive a desired prediction outcome, or to evade an undesired outcome. These kinds of impersona? tion and evasion attacks resemble identity theft from the ML model?s perspective. They?re also similar to adversarial example attacks (see ?Adversarial example attacks? on page 166).
Like an adversarial example attack, an impersonation attack involves artificially changing the input data values to our model. Unlike an adversarial example attack, where a potentially random-looking combination of input data values could be used to trick our model, impersonation implies using the information associated with another modeled entity (i.e., customer, employee, financial transaction, patient, prod? uct, etc.) to receive the prediction our model associates with that type of entity. And evasion implies the converse?changing our own data to avoid an adverse prediction.








Machine Learning Attacks | 169

In Figure 5-5, an attacker learns what characteristics our model associates with awarding a credit product, and then falsifies their own information to receive the credit product. They could share their strategy with others, potentially leading to large losses for our company. Sound like science fiction? It?s not. Closely related evasion attacks have worked for facial-recognition payment and security systems (https://oreil.ly/69u8J), and ?Case Study: Real-World Evasion Attacks? on page 184 will address several documented instances of evasions of ML security systems.

Figure 5-5. An impersonation attack (digital, color version(: https://oreil.ly/04ycs))
Attacks on machine learning explanations
In what has been called a ?scaffolding? attack?see ?Fooling LIME and SHAP: Adver? sarial Attacks on Post hoc Explanation Methods? (https://oreil.ly/xx9dH)?adversaries can poison post hoc explanations such as local interpretable model-agnostic explan? ations and Shapley additive explanations. Attacks on partial dependence, another common post hoc explanation technique, have also been published recently?see ?Fooling Partial Dependence via Data Poisoning? (https://oreil.ly/KMNmt). Attacks on explanations can be used to alter both operator and consumer perceptions of an ML system?for example, to make another hack in the pipeline harder to find, or to make a biased model appear fair?known as fairwashing (https://oreil.ly/YD-QJ). These attacks make clear that as ML pipelines and AI systems become more complex, bad actors could look to many different parts of the system, from training data all the way to post hoc explanations, to alter system outputs.




170  |  Chapter 5: Security for Machine Learning

Confidentiality Attacks: Extracted Information
Without proper countermeasures, bad actors can access sensitive information about our model and data. Model extraction and inversion attacks refer to hackers rebuild? ing our model and extracting information from their copy of the model. Membership inference attacks allow bad actors to know what rows of data are in our training data, and even to reconstruct training data. Both attacks only require access to an unguarded ML system prediction API or other system endpoints.

Model extraction, model inversion, membership inference, and some other ML attacks, can all be thought of as a new take on an older and more common intellectual property and security issue? reverse engineering. Confidentiality attacks, and other ML attacks, can be used to reverse engineer and reconstruct our potentially sensitive models and data.

Model extraction and inversion attacks
Inversion (see Figure 5-6) basically means getting unauthorized information out of our model?as opposed to the normal usage pattern of putting information into our model. If an attacker can receive many predictions from our model API or other endpoint (website, app, etc.), they can train a surrogate model between their inputs and our system?s predictions. That extracted surrogate model is trained between the inputs the attacker used to generate the received predictions and the received predictions themselves. Depending on the number of predictions the attacker can receive, the surrogate model could become quite an accurate simulation of our model. Unfortunately, once the surrogate model is trained, we have several big problems:
? Models are really just compressed versions of training data. With the surrogate model, an attacker can start learning about our potentially sensitive training data.
? Models are valuable intellectual property. Attackers can now sell access to their copy of our model and cut into our return on investment.
? The attacker now has a sandbox from which to plan impersonation, adversarial example, membership inference, or other attacks against our model.
Such surrogate models can also be trained using external data sources that can be somehow matched to our predictions, as ProPublica famously did (https://oreil.ly/ FvMDm) with the proprietary COMPAS criminal risk assessment instrument.






Machine Learning Attacks | 171



Figure 5-6. An inversion attack (digital, color version(: https://oreil.ly/04ycs))
Membership inference attacks
In an attack that starts with model extraction and is also carried out by surrogate models, a malicious actor can determine whether a given person or product is in our model?s training data. Called a membership inference attack (see Figure 5-7), this hack is executed with two layers of models. First, an attacker passes data into a public prediction API or other endpoint, receives predictions back, and trains a surrogate model or models between the passed data and the predictions. Once a surrogate model (or models) has been trained to replicate our model, the attacker then trains a second layer classifier that can differentiate between data that was used to train the first surrogate model and data that was not used to train that surrogate. When this second model is used to attack our model, it can give a solid indication as to whether any given row (or rows) of data is in our training data.
Membership in a training dataset can be sensitive when the model and data are related to undesirable outcomes such as bankruptcy or disease, or desirable outcomes like high income or net worth. Moreover, if the relationship between a single row and the target of our model can be easily generalized by an attacker, such as an obvious relationship between race, gender, or age and some undesirable outcome, this attack can violate the privacy of an entire class of people. Frighteningly, when carried out to its fullest extent, a membership inference attack could also allow a malicious actor, with access only to an unprotected public prediction API or other model endpoint, to reverse engineer large portions of a sensitive or valuable training dataset.


172  |  Chapter 5: Security for Machine Learning



Figure 5-7. A membership inference attack (digital, color version(: https://oreil.ly/04ycs))
While the discussed attacks are some of the most well-known varieties, keep in mind that these are not the only types of ML hacks, and that new attacks can emerge very quickly. Accordingly, we?ll also address a few general concerns to help us frame the broader threat environment, before moving on to countermeasures we can use to protect our ML system.
General ML Security Concerns
One common theme of this book is that ML systems are fundamentally software systems, and applying commonsense software best practices to ML systems is usually a good idea. The same applies for security. As software systems and services, ML systems exhibit similar failure modes, and experience the same attacks, as general software systems. What are some other general concerns? Unpleasant things like intentional abuses of AI technology, availability attacks, trojans and malware, man- in-the-middle attacks, unnecessarily complex unexplainable systems, and the woes of distributed computing:
Abuses of machine learning
Nearly all tools can also be weapons, and ML models and AI systems can be abused in numerous ways. Let?s start by considering deepfakes. Deepfakes are an application of deep learning that can, when done carefully, seamlessly blend fragments of audio and video into convincing new media. While deepfakes can be used to bring movie actors back to life, as was done in some recent

General ML Security Concerns |  173

Star Wars films, deepfakes can be used to harm and extort people. Of course, nonconsensual pornography, in which the victim?s face is blended into an adult video, is one of the most popular uses of deepfakes, as documented by the BBC (https://oreil.ly/05QCB) and other news outlets. Deepfakes have also been implicated in financial crimes, e.g., when an attacker used a CEO?s voice (https:// oreil.ly/A0a8_) to order money transferred into their own account. Algorithmic discrimination is another common application of abusive AI. In a ?fairwashing? attack, post hoc explanations can be altered to hide discrimination in a biased model. And facial recognition can be used directly for racial profiling (https:// oreil.ly/KBvXa). We?re touching on just a few of the ways ML systems can be abused; for a broader treatment of this important topic, see ?AI-Enabled Future Crime? (https://oreil.ly/8L3ax).
General availability attacks
ML systems can fall victim to more general denial-of-service (DOS) attacks, just like other public-facing services. If a public-facing ML system is critical to our organization, we make sure it?s hardened with firewalls and filters, reverse domain name server system (DNS) lookup, and other countermeasures that increase availability during a DOS attack. Unfortunately, we also have to think through another kind of availability failure for ML systems?those caused by algorithmic discrimination. If algorithmic discrimination is severe enough, whether driven by internal failures or adversarial attacks, our ML system will likely not be usable by a large portion of its users. Be sure to test for bias during training and throughout a system?s deployed lifecycle.
Trojans and malware
ML in the research and development environment is dependent on a diverse ecosystem of open source software packages. Some of these packages have many contributors and users. Some are highly specific and only meaningful to a small number of researchers or practitioners. It?s well understood that many packages are maintained by brilliant statisticians and ML researchers whose primary focus is mathematics or algorithms, not software engineering or security. It?s not uncommon for an ML pipeline to be dependent on dozens or even hundreds of external packages, any one of which could be hacked to conceal an attack payload. Third-party packages with large binary data stores and pretrained ML models seem especially ripe for these kinds of problems. If possible, scan all software artifacts associated with an ML system for malware and trojans.
Man-in-the-middle attacks
Because many ML system predictions and decisions are transmitted over the internet or an organization?s network, they can be manipulated by bad actors during that journey. Where possible, use encryption, certificates, mutual authen? tication, or other countermeasures to ensure the integrity of ML system results passed across networks.

174  |  Chapter 5: Security for Machine Learning

Unexplainable machine learning
Although recent developments in interpretable models and model explanations have provided the opportunity to use accurate and also transparent models, many machine learning workflows are still centered around unexplainable mod? els. Such models are a common type of often unnecessary complexity in a commercial ML workflow. A dedicated, motivated attacker can, over time, learn more about our overly complex unexplainable ML model than our own team knows about it. (Especially in today?s turnover-prone data science job market.) This knowledge imbalance can potentially be exploited to conduct the attacks we?ve described or for other yet unknown types of attacks.
Distributed computing
For better or worse, we live in the age of big data. Many organizations are now using distributed data processing and ML systems. Distributed computing can provide a broad attack surface for a malicious internal or external actor. Data could be poisoned on only one or a few worker nodes of a large distributed data storage or processing system. A backdoor could be coded into just one model of a large ensemble. Instead of debugging one simple dataset or model, now practitioners must sometimes examine data or models distributed across large computing clusters.
Starting to get worried again? Hang in there?we?ll cover countermeasures for confi? dentiality, integrity, and availability attacks on ML systems next.
Countermeasures
There are many countermeasures we can use and, when paired with the governance processes proposed in Chapter 1, bug bounties, security audits, and red-teaming, such measures are more likely to be effective. Additionally, there are the newer subdisciplines of adversarial ML and robust ML, which are giving the full academic treatment to these subjects. This section will outline some of the defensive measures we can use to help make our ML systems more secure, including model debugging for security, model monitoring for security, privacy-enhancing technologies, robust ML, and a few general approaches.
Model Debugging for Security
ML models can and should be tested for security vulnerabilities before they are released. In these tests, the goal is basically to attack our own ML systems, to understand our level of security, and to patch up any discovered vulnerabilities. Some general techniques that work across different types of ML models for security debugging are adversarial example searches, sensitivity analysis, audits for insider attacks and model extraction attacks, and discrimination testing.


Countermeasures | 175

Adversarial example searches and sensitivity analysis
Conducting sensitivity analysis with an adversarial mindset, or better yet, conducting our own adversarial example attacks, is a good way to determine if our system is vulnerable to perhaps the simplest and most common type of ML integrity attack. The idea of these ethical hacks is to understand what feature values (or combinations thereof) can cause large swings in our system?s output predictions. If we?re working in the deep learning space, packages like cleverhans (https://oreil.ly/6LuBF) and foolbox (https://oreil.ly/M4ayU) can help us get started with testing our ML system. For those working with structured data, good old sensitivity analysis can go a long way toward pointing out instabilities in our system. We can also use genetic learning to evolve our own adversarial examples or we can use heuristic methods based on individual conditional expectation (https://oreil.ly/_qQdn) to find adversarial examples. Once we find instabilities in our ML system, triggered by these adversarial examples, we?ll want to use cross-validation or regularization to train a more stable model, apply techniques from robust machine learning (see ?Robust Machine Learning? on page 182), or explicitly monitor for the discovered adversarial examples in real time. We should also link this information to the system?s incident response plan, in case it?s useful later.
Auditing for insider data poisoning
If a data poisoning attack were to occur, system insiders?employees, contractors, and consultants?are not unlikely culprits. How can we track down insider data poisoning? First, score those individuals with our system. Any insider receiving a positive outcome could be the attacker or know the attacker. Because a smart attacker will likely perform the minimum changes to the training data that result in a positive outcome, we can also use residual analysis to look for beneficial outcomes with larger than expected residuals, indicating the ML model may have been inclined to issue a negative outcome for the individual had the training data not been altered. Data and environment management are strong countermeasures for insider data poisoning, as any changes to data are tracked with ample metadata (who, what, when, etc.). We can also try the reject on negative impact (RONI) technique, proposed in the seminal ?The Security of Machine Learning? (https://oreil.ly/exh6g), to remove potentially altered rows from system training data.
Bias testing
DOS attacks, resulting from some kind of bias?intentional or not?are a plausible type of availability attack. In fact, it?s already happened. In 2016, Twitter users pois? oned the Tay chatbot (https://oreil.ly/uPqNx) to the point where only those users interested in neo-nazi pornography would find the system?s service appealing. This type of attack could also happen in a more serious context, such as employment,



176  |  Chapter 5: Security for Machine Learning

lending, or medicine, where an attacker uses data poisoning, model backdoors, or other types of attacks to deny service to a certain group of customers. This is one of the many reasons to conduct bias testing, and remediate any discovered discrimi? nation, both at training time and as part of regular model monitoring. There are several great open source tools for detecting discrimination and making attempts to remediate it, such as aequitas (https://oreil.ly/e412j), Themis (https://oreil.ly/yJiT6), and AIF360 (https://oreil.ly/HsKEg).
Ethical hacking: model extraction attacks
Model extraction attacks are harmful on their own, but they are also the first stage for a membership inference attack. We should conduct our own model extraction attacks to determine if our system is vulnerable to these confidentiality attacks. If we find some API or model endpoint that allows us to train a surrogate model between input data and system outputs, we lock it down with solid authentication and throttle any abnormal requests at this endpoint. Because a model extraction attack may have already happened via this endpoint, we need to analyze our extracted surrogate models as follows:
? What are the accuracy bounds of different types of surrogate models? We must try to understand the extent to which a surrogate model can really be used to gain knowledge about our ML system.
? What types of data trends can be learned from our surrogate model? What about linear trends represented by linear model coefficients? Or course summaries of population subgroups in a surrogate decision tree?
? What rules can be learned from a surrogate decision tree? For example, how to reliably impersonate an individual who would receive a beneficial prediction? Or how to construct effective adversarial examples?
If we see that it is possible to train an accurate surrogate model from one of our system endpoints, and to answer some of these questions, then we?ll need to take some next steps. First, we?ll conduct a membership inference attack on ourselves to see if that two-stage attack would also be possible. We?ll also need to record all of the information related to this ethical hacking analysis and link it to the system?s incident response plan. Incident responders may find this information helpful at a later date, and it may unfortunately need to be reported as a breach if there is strong evidence that an attack has occurred.
Debugging security vulnerabilities in our ML system is important work that can save us future money, time, and heartache, but so is watching our system to ensure it stays secure. Next we?ll take up model monitoring for security.




Countermeasures | 177

Model Monitoring for Security
Once hackers can manipulate or extract our ML model, it?s really not our model anymore. To guard against attacks on our model, we?ll not only need to train and debug it with security in mind; we?ll also need to monitor it closely once it goes live. Monitoring for security should be geared toward algorithmic discrimination, anomalies in input data queues, anomalies in predictions, and high usage. Here are some tips on what and how to monitor:
Bias monitoring
Bias testing, as discussed in other chapters, must be applied during model train? ing. But for many reasons, including unintended consequences and malicious hacking, discrimination testing must be performed during deployment too. If bias is found during deployment, it should be investigated and remediated. This helps to ensure a model that was fair during training remains fair in production.
Input anomalies
Unrealistic combinations of data, which could be used to trigger backdoors in model mechanisms, should not be allowed into model scoring queues. Anom? aly detection ML techniques, like autoencoders and isolation forests, may be generally helpful in tracking problematic input data. However, we can also use commonsense data integrity constraints to catch problematic data before it hits our model. An example of such unrealistic data is an age of 40 years and a job tenure of 50 years. If possible, we should also consider monitoring for random data, training data, or duplicate data. Because random data is often used in model extraction and inversion attacks, we build out alerts or controls that help our team understand if and when our model may be encountering batches of random data. Real-time scoring of rows that are extremely similar or identical to data used in training, validation, or testing should be recorded and investigated, as they could indicate a membership inference attack. Finally, be on the lookout for duplicate data in real-time scoring queues, as this could indicate an evasion or impersonation attack.
Output anomalies
Output anomalies can be indicative of adversarial example attacks. When scoring new data, we compare our ML model prediction against a trusted, transparent benchmark model or a benchmark model trained on a trusted data source and pipeline. If the difference between our more complex and opaque ML model and our interpretable or trusted model is too great, we fall back to the predictions of the conservative model or send the row of data for manual processing. Statistical control limits, which are akin to moving confidence intervals, can also be used to monitor for anomalous outputs.



178  |  Chapter 5: Security for Machine Learning

Metamonitoring
We monitor the basic operating statistics?the number of predictions in a certain time period, latency, CPU, memory, and disk load, or the number of concurrent users?to ensure our system is functioning normally. We can even train an autoencoder?based anomaly detection metamodel on our entire ML system?s operating statistics and then monitor this metamodel for anomalies. An anomaly in system operations could tip us off that something is generally amiss in our ML system.
Monitoring for attacks is one the most proactive steps we can take to counter ML hacks. However, there are a still a few more countermeasures to discuss. We?ll look into privacy-enhancing technologies next.
Privacy-Enhancing Technologies
Privacy-preserving ML is a research subdiscipline with direct ramifications for the confidentiality of our ML training data. While just beginning to gain steam in the ML and ML operations (MLOps) communities, PETs can give us an edge when it comes to protecting our data and models. Some of the most promising and practical tech? niques from this emergent field include federated learning and differential privacy.
Federated learning
Federated learning is an approach to training ML models across multiple decentral? ized devices or servers holding local data samples, without exchanging raw data between them. This approach is different from traditional centralized ML techniques where all datasets are uploaded to a single server. The main benefit of federated learning is that it enables the construction of ML models without sharing data among many parties. Federated learning avoids sharing data by training local models on local data samples and exchanging parameters between servers or edge devices to generate a global model, which is then shared by all servers or edge devices. Assuming a secure aggregation process is used, federated learning helps address fundamental data privacy and data security concerns. Among other open source resources, we should look into PySyft (https://oreil.ly/8HpeR) or FATE (https://oreil.ly/W3uYP) to start learning about implementing federated learning at our organization (or with partner organizations).
Differential privacy
Differential privacy is a system for sharing information about a dataset by describing patterns about groups in the dataset without disclosing information about specific individuals. In ML tools, this is often accomplished using specialized types of differ? entially private learning algorithms. This makes it more difficult to extract sensitive information from training data or a trained ML model in model extraction, model


Countermeasures | 179

inversion, or membership inference attacks. In fact, an ML model is said to be differentially private if an outside observer cannot tell if an individual?s information was used to train the model. There are lots of high-quality open source repositories to check out and try, including the following:
? Google?s differential-privacy (https://oreil.ly/rjwKK)
? IBM?s diffprivlib (https://oreil.ly/QOFm-)
? TensorFlow?s privacy (https://oreil.ly/WyPD6)
Many ML approaches that invoke differential privacy are based on differentially pri? vate stochastic gradient descent (DP-SGD) (https://oreil.ly/raWeC). DP-SGD injects structured noise into gradients determined by SGD at each training iteration. In general, DP-SGD and related techniques ensure that ML models do not memorize too much specific information about training data. Because they prevent ML algo? rithms from focusing on particular individuals, they could also lead to increased generalization performance and fairness benefits.
Readers may hear about confidential computing or homomorphic encryption under the PET topic heading as well. These are promising research and technology direc? tions to watch as well. Another subdiscipline of ML research to watch is robust ML, which can help us counter adversarial example attacks, data poisoning, and other adversarial manipulation of our ML system.


180  |  Chapter 5: Security for Machine Learning

Other legal bases for use
If we are operating under the GDPR, we need a legal basis to use much of the consumer data we might be interested in for ML. Consent is often the legal basis, but there are other examples: contractual obligations, governmental tasks, or medical emergencies.
Anonymization
It?s almost never a good idea to work with personal identifiable information (PII). Data like Social Security numbers, phone numbers, and even email addresses? known as direct identifiers?allows bad actors to tie private and sensitive infor? mation back to specific people. Even combinations of demographic information, like age, race, or gender?known as indirect identifiers?can be used to tie information back to specific individuals. Generally, this kind of data should be removed, masked, hashed, or otherwise anonymized for use in model training, for privacy and bias reasons.
Biometric data
Data like digital images, videos, fingerprints, voiceprints, iris scans, genomic data, or other data that encodes biometric information typically requires addi? tional security and data privacy controls.
Retention limits or requirements
Laws and organizational privacy policies may define retention limits, or how long data can be saved before mandatory deletion. (We?ve seen two-week retention limits!) We may also have to deal with legal or organizational retention require? ments that enforce how long data must be stored and kept private and secure? this can be for years.
Deletion and rectification requests
Many laws and policies enable consumers to have their data updated, corrected, or fully deleted upon request, and this could potentially affect ML training data.
Explanation
Many data privacy laws also seem to be putting forward a requirement for explaining automatic processing of data that affects consumers. Although vague and largely unsettled, this may mean that in the future many more ML decisions will have to be explained to consumers.
Intervenability
Similar to explanation, many new laws also seem to instate a requirement for intervenability, which is similar to the appeal and override concepts of actionable recourse.

Bias


It?s not uncommon for data privacy laws and policies to address bias in data pro? cessing outcomes, like ML model predictions. Sometimes these requirements are


Countermeasures | 181



Robust Machine Learning
Robust ML includes many cutting-edge ML algorithms developed to counter adver? sarial example attacks, and to a certain extent data poisoning as well. The study of robust ML gained momentum after several researchers showed that silly, or even invisible, changes to input data can result in huge swings in output predictions for computer vision systems. Such swings in model outcomes are a troubling sign in any domain, but when considering medical imaging or semi-autonomous vehicles, they are downright dangerous. Robust ML models help enforce stability in model outcomes, and, importantly, fairness?that similar individuals be treated similarly. Similar individuals in ML training data or live data are individuals that are close to one another in the Euclidean space of the data. Robust ML techniques often try to establish a hypersphere around individual examples of data, and ensure that other similar data within the hypersphere receive similar predictions. Whether caused by bad actors, overfitting, underspecification, or other factors, robust ML approaches help protect our organization from risks arising from unexpected predictions. Inter? esting papers and code are hosted at the Robust ML site (https://oreil.ly/H36uh), and the Madry Lab at the Massachusetts Institute of Technology has even published a full Python package for robust ML (https://oreil.ly/k-qDZ).
General Countermeasures
There are a number of catchall countermeasures that can defend against several different types of ML attacks, including authentication, throttling, and watermarking. Many of these same kinds of countermeasures are also best practices for ML systems in general, like interpretable models, model management, and model monitoring. The last topic we will address before the chapter?s case study is a brief description of important and general countermeasures against ML system attacks:
Authentication
Whenever possible, disallow anonymous use for high-stakes ML systems. Login credentials, multifactor authentication, or other types of authentication that force


182  |  Chapter 5: Security for Machine Learning

users prove their identity, authorization, and permission to use a system put a blockade between our model API and anonymous bad actors.
Interpretable, fair, or private models
Modeling techniques now exist?e.g., monotonic GBMs (M-GBM), scalable Bayesian rule lists (SBRL) (https://oreil.ly/Md375), explainable neural networks (XNN) (https://oreil.ly/sd4XX)?that can allow for both accuracy and interpret? ability in ML models. These accurate and interpretable models are easier to document and debug than classic unexplainable ones. Newer types of fair and private modeling techniques?e.g., LFR (https://oreil.ly/7ZHmw), DP-SGD (https://oreil.ly/yuddM)?can also be trained to downplay outwardly visible demographic characteristics that can be observed, socially engineered into an adversarial example attack, or impersonated. These models, enhanced for inter? pretability, fairness, or privacy, should be more easily debugged, more robust to changes in an individual entity?s characteristics, and more secure than overused unexplainable models.
Model documentation
Model documentation is a risk-mitigation strategy that has been used for decades in banking. It allows knowledge about complex modeling systems to be preserved and transferred as teams of model owners change over time, and for knowledge to be standardized for efficient analysis by model validators and auditors. Model documentation should cover the who, when, where, what, and how of an ML system, including many details, from contact information for stakeholders to algorithmic specification. Model documentation is also a natural place to record any known vulnerabilities or security concerns for an ML system, enabling future maintainers or other operators that interact with the system to allocate oversight and security resources efficiently. Incident response plans should also be linked to model documentation. (Chapter 2 contains a sample documentation template.)
Model management
Model management typically refers to a combination of process controls, like documentation, combined with technology controls, like model monitoring and model inventories. Organizations should have an exact count of deployed ML systems and a structured inventory of associated code, data, documentation, and incident response plans, and they should monitor all deployed models. These practices make it easier to understand when something goes wrong and to deal with problems quickly when they arise. (Chapter 1 discusses model risk manage? ment for ML in much more detail.)
Throttling
When high use or other anomalies, such as adversarial examples, or duplicate, random, or training data, are identified by model monitoring systems, consider


Countermeasures | 183

throttling prediction APIs or other system endpoints. Throttling can refer to restricting high numbers of rapid predictions from single users, artificially increasing prediction latency for all users, or other methods that can slow down attackers conducting model or data extraction attacks and adversarial example attacks.
Watermarking
Watermarking refers to adding a subtle marker to our data or predictions, for the purpose of deterring theft of data or models. If data or predictions carry identifiable traits, such as actual watermarks on images or sentinel markers in structured data, it can make stolen assets harder to use and more identifiable to law enforcement or other investigators once a theft occurs.
Applying these general defenses and best practices, along with some of the more specific countermeasures discussed in previous sections, is a great way to achieve a high level of security for an ML system. And now that we?ve covered security basics, ML attacks, and many countermeasures for those attacks, readers are armed with the knowledge needed to start red-teaming your organization?s AI?especially if you can work with your organization?s IT security professionals. We?ll now examine some real-world AI security incidents to provide additional motivation for doing the hard work of red-teaming AI, and to gain insights into some of today?s most common ML security issues.
Case Study: Real-World Evasion Attacks
ML systems used for both physical and online security have suffered evasion attacks in recent years. This case discussion touches on evasion attacks used to avoid Face? book filters and perpetuate disinformation and terrorist propaganda, and evasion attacks against real-world payment and physical security systems.
Evasion Attacks
As the COVID pandemic ground on and the 2020 US presidential campaign was in high gear, those proliferating disinformation related to both topics took advantage of weaknesses in Facebook?s manual and automated content filtering. As reported by NPR, ?Tiny Changes Let False Claims About COVID-19, Voting Evade Facebook Fact Checks? (https://oreil.ly/aYSTr). While Facebook uses news organizations such as Reuters and the Associated Press to fact-check claims made by its billions of users, it also uses AI-based content filtering, particularly to catch copies of human-identified misinformation posts. Unfortunately, minor changes, some as simple as different backgrounds or fonts, image cropping, or simply describing memes with words instead of images, allowed bad actors to circumvent Facebook?s ML-based content filters. In its defense, Facebook does carry out enforcement actions against many offenders, including limiting the distribution of posts, not recommending posts or

184  |  Chapter 5: Security for Machine Learning

groups, and demonetization. Yet, according to one advocacy group, Facebook fails to catch about 42% of disinformation posts containing information flagged by human fact checkers (https://oreil.ly/fzCrb). The same advocacy group, Avaaz, estimates that a sample of just 738 unlabeled disinformation posts led to an estimated 142 million views and 5.6 million user interactions.
Recent events have shown us that online disinformation and security threats can spill over into the real world. Disinformation about the 2020 US election and the COVID pandemic are thought to be primary drivers of the frightening US Capitol riots of January 6, 2021. In perhaps even more disturbing evasion attacks, the BBC has reported that ISIS operatives continue to evade Facebook content filters (https:// oreil.ly/qV25u). By blurring logos, splicing their videos with mainstream news con? tent, or just using strange punctuation, ISIS members or affiliates have been able to post propaganda, explosive-making tutorials, and even evasion attack tutorials to Facebook, garnering tens of thousands views for their violent, disturbing, and vitriolic content. While evasion attacks on AI-based filters are certainly a major culprit, there are also fewer human moderators for Arabic content on Facebook. Regardless of whether it?s humans or machines failing at the job, this type of content can be truly dangerous, contributing both to radicalization and real-world violence. Physical evasion attacks are also a concern for the near future. Researchers recently showed that some AI-based physical security systems are easy targets for evasion attacks (https://oreil.ly/xVmDj). With the permission of system operators, researchers used lifelike three-dimensional masks to bypass facial recognition security checks on Alipay and WeChat payment systems. In one egregious case, researchers were even able to use a picture of another person on an IPhone screen to board a plane at Amsterdam?s Schiphol Airport.
Lessons Learned
Taken together, bad actors? evasions of online safeguards to post dangerous content, and evasions of physical security systems to make monetary payments and to travel by plane paints a scary picture of a world where ML security is not taken seriously. What lessons learned from this chapter could be applied to prevent these evasion attacks? The first lesson is related to robust ML. ML systems used for high-stakes security applications, be they online or real world, must not be fooled by tiny changes to normal system inputs. Robust ML, and related technologies, must progress to the point where simplistic evasion techniques, like blurring of logos or changes to punctuation, are not effective evasion measures. Another lesson comes from the beginning of the chapter: the adversarial mindset. Anyone who thought seriously about security risks for these AI-based security systems should have realized that masks, or just other images, were an obvious evasion technique. Thankfully, it turns out that some organizations do employ countermeasures for adversarial scenarios. Better facial recognition security systems deploy techniques meant to ensure the


Case Study: Real-World Evasion Attacks | 185

liveness of the subjects they are identifying. The better facial recognition systems also employ discrimination testing to ensure availability is high, and error rates are as low as possible, for all their users.
Another major lesson to be learned from real-world evasion attacks pertains to the responsible use of technology in general, and ML in particular. Social media has proliferated beyond physical borders, and its complexity has grown past many coun? tries? current abilities to effectively regulate it. With a lack of government regulation, users are counting on social media companies to regulate themselves. Being tech companies, social networks often rely on more technology, like AI-based content filters, to retain control of their systems. But what if those controls don?t really work? As technology and ML play larger roles in human lives, lack of rigor and responsibil? ity in their design, implementation, and deployment will have ever-increasing conse? quences. Those designing technologies for security or other high-stakes applications have an especially serious obligation to be realistic about today?s ML capabilities and apply process and technology controls to ensure adequate real-world performance.
Resources
Further Reading
? ?A Marauder?s Map of Security and Privacy in Machine Learning? (https://oreil.ly/0k7D3)
? ?BIML Interactive Machine Learning Risk Framework? (https://oreil.ly/csQ22)
? FTC?s ?Start with Security? guidelines (https://oreil.ly/jmeja)
? Adversarial Threat Landscape for Artificial-Intelligence Systems (https://oreil.ly/KxEbC)
? NIST Computer Security Resource Center (https://oreil.ly/pncXb)
? NIST de-identification tools (https://oreil.ly/M8xhr)














186  |  Chapter 5: Security for Machine Learning



PART II

Putting AI Risk Management
into Action





CHAPTER 6

Explainable Boosting Machines and Explaining XGBoost



This chapter explores explainable models and post hoc explanation with interactive examples (https://oreil.ly/machine-learning-high-risk-apps-code) relating to consumer finance. It also applies the approaches discussed in Chapter 2 using explainable boost? ing machines (EBMs), monotonically constrained XGBoost models, and post hoc explanation techniques. We?ll start with a concept refresher for additivity, constraints, partial dependence and individual conditional expectation (ICE), Shapley additive explanations (SHAP), and model documentation.
We?ll then explore an example credit underwriting problem by building from a penal? ized regression, to a generalized additive model (GAM), to an EBM. In working from simpler to more complex models, we?ll document explicit and deliberate trade-offs regarding the introduction of nonlinearity and interactions into our example proba? bility of default classifier, all while preserving near-total explainability with additive models.

Recall from Chapter 2 that an interpretation is a high-level, mean? ingful mental representation that contextualizes a stimulus and leverages human background knowledge, whereas an explanation is a low-level, detailed mental representation that seeks to describe a complex process. Interpretation is a much higher bar than explana? tion, rarely achieved by technical approaches alone.

After that, we?ll consider a second approach to predicting default that allows for complex feature interactions, but controls complexity with monotonic constraints based in causal knowledge. Because the monotonically constrained gradient boosting machine (GBM) won?t be explainable on its own, we?ll pair it with robust post hoc


189

explanation techniques for greatly enhanced explainability. Finally, the chapter will close with a discussion of the pros and cons of popular Shapley value methods.
Concept Refresher: Machine Learning Transparency
Before diving into technical examples, let?s review some of the key concepts from Chapter 2. Because our first example will highlight the strengths of the GAM fam? ily of models, we?ll address additivity next, particularly in comparison to models that allow for high-degree interactions. Our second example will use monotonic constraints to effect an informal causality approach with XGBoost, so we?ll briefly highlight the connections between causation and constraints. We?ll also be using par? tial dependence and ICE to compare and assess our different approaches? treatments of input features, so we?ll need a quick refresh on the strengths and weaknesses of those post hoc explainers, and we?ll need to go over the importance of model documentation one more time?because model documentation is that important.
Additivity Versus Interactions
A major distinguishing characteristic of unexplainable machine learning is its pro? pensity to create extremely high-degree interactions between input features. It?s thought that this ability to consider the values of many features in simultaneous combination increases the predictive capacity of ML models versus more traditional linear or additive models, which tend to consider input features independently. But it?s been shown that unexplainable models are not more accurate for structured data (https://oreil.ly/ztXi8), like credit underwriting data, and all those interactions in unexplainable models are very difficult for people to understand. Moreover, high- degree interactions also lead to instability, because small changes in one or few a features can interact with other features to dramatically change model outcomes. And high-degree interactions lead to overfitting, because today?s relevant 17-way interactions are likely not tomorrow?s relevant 17-way interactions.
Another important characteristic of ML is the ability to learn nonlinear phenomena in training data, automatically. It turns out that if we can separate nonlinearity from interactions, we can realize substantial boosts in predictive quality, while preserving a great deal of explainability, if not complete explainability. This is the magic of GAMs. And EBMs are the next step that allow us to introduce a sane number of two-way interactions, in an additive fashion, that can result in even better performance. Later in this chapter, the GAM family example (see ?The GAM Family of Explainable Mod? els? on page 196) aims to provide an object lesson in these trade-offs, starting with a straightforward, additive, linear model baseline, then introducing nonlinearity with GAMs, and finally introducing understandable two-way interactions with EBMs. When we introduce nonlinearity and interactions carefully, as opposed to assuming



190  |  Chapter 6: Explainable Boosting Machines and Explaining XGBoost

more complexity is always better, it enables us to justify our modeling approach, to tether it to real-world performance concerns, and to generate a lot of interesting plots of feature behavior. These justifications and plots are also great materials for subsequent model documentation, to be addressed in greater detail shortly.

The family of models that typically includes variants of GLMs, GAMs, GA2M, EBMs, and additive index models (AIMs) has been discussed in statistical literature for years. This type of modeling is sometimes referred to as the functional analysis of variance (fANOVA) framework.

Steps Toward Causality with Constraints
Causal discovery and inference are important directions in the future of predictive modeling. Why? Because when we build on correlations with ML, we are often building on sand. Correlations change constantly in the real world and can be spurious or just wrong. If we can base models on causal relationships instead of just memorializing some snapshot of complex correlations with an ML model, we greatly decrease overfitting, data drift, and social bias risks. As of now, causal meth? ods tend to be somewhat difficult for most organizations to implement, so our monotonic constraints example (see ?Constrained and Unconstrained XGBoost? on page 208) highlights a simple and easy step we can take to inject causality into ML models. If we can use our brains, or simple but robust experiments, to understand the directionality of a causal relationship in the real world, we can use monotonic constraints to enforce that directionality in our XGBoost models. For instance, if we know that an increasing number of late payments are an indicator of future default, we can use monotonic constraints to insist that an XGBoost classifier generate higher probabilities of default for higher numbers of late payments. Though we might not see gains in in silico test data performance with constraints, constraints do mitigate real-world instability, overfitting, and sociological bias risks, and they likely increase in vivo performance.
Partial Dependence and Individual Conditional Expectation
Partial dependence is an established and highly intuitive post hoc explanation method that describes the estimated average behavior of a model across the values of some input feature. Unfortunately, it?s fallible. It fails to represent model behavior accurately in the presence of correlation or interactions between input features, and it can even be maliciously altered (https://oreil.ly/z2xAW). But, because understanding the average behavior of a feature in a model is so important, many techniques have been developed to address the failings of partial dependence. In particular, accumu? lated local effect (https://oreil.ly/kEIPp) is the most direct replacement for partial



Concept Refresher: Machine Learning Transparency |  191

dependence and was designed specifically to address the shortcomings of partial dependence. Readers can try ALE with packages like ALEPlot (https://oreil.ly/7vv4h) or ALEPython (https://oreil.ly/To7PF).
In the examples that follow, we?ll make heavy use of another partial dependence derivative to get a solid understanding of feature behavior in our models. First intro? duced in ?Peeking Inside the Black Box: Visualizing Statistical Learning with Plots of Individual Conditional Expectation? (https://oreil.ly/MruUv), ICE pairs plots of local behavior of the model with respect to single individuals with partial dependence. This enables us to compare estimated average behavior with descriptions of local behavior, and when partial dependence and ICE curves diverge, to decide for ourselves if partial dependence looks trustworthy or if it looks to be affected by correlations or interactions in input variables. Of course, ICE is not without its own problems. The most common problem with ICE is the consideration of unrealistic data values, and when interpreting ICE, it?s important to put the most mental weight on values of the input feature that are most similar to those in the original row of data being considered.
Let?s work through all of this in an example. In the bottom panel of Figure 6-1, we can see the partial dependence and ICE for a penalized logistic regression model and for the input feature PAY_0 (a customer?s repayment status on their most recent bill). Higher values of PAY_0 indicate greater lateness in payment. ICE curves are generated for individuals who sit at the deciles of predicted probability.
Note the smooth increase from lower probabilities of default when a customer is not late on their most recent payment, to high probabilities of default when a customer is late. This makes sense in context. It aligns with reasonable expectations and domain knowledge. Notice also that ICE and partial dependence do not diverge?they are highly aligned. This will always be the case in linear models, but it also shows us that partial dependence is likely trustworthy for this model and dataset.
So, what?s going on in the top panel of this figure? That?s where we try to decide if the model is learning robust signals from the training data. The first thing you might notice is a histogram. We use that histogram to look for stability problems in model predictions. ML models typically only learn from data, so if there is not much data, as is the case with PAY_0 > 1 in our training data, the ML model can?t learn much, and their predictions in those data domains will be unstable, if not nonsensical. Some other packages use error bars for the same purpose in partial dependence or shape function plots. That?s also fine. Both visualization techniques are trying to draw your eye to a region of data where your ML model is unstable and probably making silly decisions.





192  |  Chapter 6: Explainable Boosting Machines and Explaining XGBoost



Figure 6-1. A partial dependence plot for a GLM trained later in this chapter that incorporates ICE, histograms, and conditional means to increase trustworthiness and effectiveness (digital, color version(: https://oreil.ly/7vLOU))
This is where we try to decide if the model is representing our training data well, and the top panel can also clue us in to problems with data sparsity and prediction reliability. In the top panel, the first thing we notice is a histogram. We use that histogram to look for reliability problems in model predictions. Readers will see that there is also a line overlaid on the histogram. That line is the conditional mean of the target for the corresponding histogram bin. If the model learned from the data correctly, the partial dependence and ICE in the bottom panel should roughly mirror the conditional mean line in the top panel. Sparsity is also an important caveat to keep in mind when judging whether model behavior aligns with the conditional mean of the target. In Figure 6-1, we see a precipitous drop in the conditional mean at PAY_0 = 6, or six months late for the most recent bill. However, there?s no data to


Concept Refresher: Machine Learning Transparency |  193

support this drop. The histogram bin is basically empty, and the drop is probably just irrelevant noise. Luckily, our well-behaved logistic regression model has no choice but to ignore this noise and keep pushing the probability of default monotonically higher as PAY_0 increases. With more complex models for the same data, we will need to apply monotonic constraints to ensure that the model follows causal relationships instead of memorizing irrelevant noise with no data support.
To quote the important explainability researcher Przemys?aw Biecek, ?Don?t explain without context!? That means we need to think about the correlations, interactions, and the security of the datasets we?re using to generate partial dependence and ICE? typically validation, test, or other interesting holdout samples. If those datasets don?t align with the correlations and interactions in training data, or the sample could have been intentionally altered, we?ll get different results than we saw in training. This can raise a number of questions. Was the training partial dependence correct? Does our model actually behave differently with new data, or are the new correlations and interactions in this sample causing partial dependence to be less trustworthy?
These are all reasons that we pair partial dependence with ICE. As a local explanation technique, ICE is less susceptible to global changes in correlations and interactions. If something looks off with partial dependence, first check if partial dependence follows the local behavior of ICE curves or if it diverges from the ICE curves. If it diverges, it?s likely safer to take our explanatory information from ICE, and if possible, to investi? gate what distribution, correlation, interaction, or security problems are altering the partial dependence.
Shapley Values
Recall that SHAP is a way to generate local feature attribution values with a great deal of theoretical support (at least by ML standards). A SHAP value tells us how much a feature?s value for a certain row moved the model prediction away from the mean prediction. But how does SHAP do it? It does it by ?removing? the feature from that row?s prediction repeatedly and in concert with other removed features. By removing features and measuring differences in the model prediction, we start to get a good picture of how each feature affects each prediction.

Remember from Chapter 2 that Shapley values are a post hoc explanation technique, borrowed from economics and game theory, that decompose model predictions into contributions from each input feature.

Because SHAP can use background datasets, or specific datasets from which to draw random samples to use as a substitute for removed features, we have to consider con? text (https://oreil.ly/HNpls) in both the dataset to be explained and the background


194  |  Chapter 6: Explainable Boosting Machines and Explaining XGBoost

dataset. Because of the definition of partial dependence and ICE, we usually use very simple background datasets for these techniques, and we may not even think of them as background datasets at all. We are essentially just replacing the value for an entire feature (partial dependence) or a row (ICE) with some known value of that feature in order to generate a curve. When it comes to Shapley values, we have a choice for
(1) which observations we explain (anything from a single row to an entirely new sample of data) and (2) which background dataset to use when generating Shapley values (anything from not using a background set, to using random data, to using highly massaged background data designed to address context or causality issues).
In addition to considering the correlations, interactions, and security of the dataset we are explaining, we also have to ask whether our choice of background is appropri? ate and whether explanations make sense in the context in which they will be judged. We?ll go into detail later in the chapter as to how we can choose an appropriate background dataset for our Shapley value explanations, depending on the question that our explanations are trying to answer. In practice, this complex analysis often boils down to computing explanations on a few different datasets and making sure results are salient and stable. Computing Shapley-based explanations also means documenting the background dataset used and the reasons for choosing this dataset.
Model Documentation
Model documentation is the physical manifestation of accountability in large organi? zations. When we have to write a document about a model we built, knowing that our name will be ascribed to the same document, we hope that this encourages more deliberate design and implementation choices. And if we don?t make sensible choices or document bad choices, or the documentation is clearly missing or dishonest, there can be consequences for our poor model building. Model documentation is also important for maintenance and incident response. When we?ve moved on to our next big data science job, and our older models start getting stale and causing problems, documentation enables a new set of practitioners to understand how the model was intended to work, how to maintain it in future iterations, and how to fix it.
There are now several standards for model documentation, including the following:
? Model cards (https://oreil.ly/h7eJC), with Google-provided example model cards (https://oreil.ly/OJkfE)
? Model risk management in-depth documentation; see the 2021 model risk man? agement guidance (https://oreil.ly/XDF9u) from the US Office of the Comptroller of the Currency
? The EU Artificial Intelligence Act documentation template (https://oreil.ly/tyS-i); see Document 2, Appendix IV



Concept Refresher: Machine Learning Transparency |  195

Notice that all of these templates come from either a leading commercial user and developer of ML, or a very serious government body. If readers have avoided model documentation until now, expect that to change in the future as regulations are enacted, and especially for important applications of ML. All of these templates also benefit greatly from explainable models and post hoc explanation, because increased transparency is yet a another goal and benefit of model documentation. Transparency in ML models allows us to understand, and then to justify, design and implemen? tation trade-offs. If what we see in an explainable model or post hoc explanation result appears reasonable and we can write a few commonsense sentences to justify the observed outcomes, that?s what we?re going for in this chapter. If, instead, we?re using an unexplainable model and don?t understand how design and implementation trade-offs affect model behavior, our documented justifications will likely be much weaker, and open us and our model up to potentially unpleasant external scrutiny.
The GAM Family of Explainable Models
In this section, we?ll form a baseline with a linear additive penalized regression model, then compare that baseline to a GAM that allows for complex nonlinearity, but in an independent, additive, and highly explainable fashion. We?ll then compare the GLM and GAM to an EBM with a small number of two-way interactions. Because all of our models are constructed with additive independent functional forms, and because we will use only a small number of meaningful interactions, all our models will be very explainable. Additivity will enable us to make clear and justifiable choices about introducing nonlinearity and interactions.

Progressing from GLM, to GAM, to EBM is a generalizable work? flow that allows us to make explainable, empirical, and deliberate decisions about introducing nonlinearity (via GAM) and interac? tions (via EBM) into our models while comparing results to a baseline (GLM).

Elastic Net?Penalized GLM w/ Alpha and Lambda Search
As the name suggests, GLMs extend the idea of an ordinary linear regression and generalize for error distributions belonging to exponential families, in addition to the Gaussian distribution of error used in standard linear regression. Another vital component of a GLM is the link function that connects the expected value of the response to the linear components. Since this link function can be any monotonic differentiable function, GLMs can handle a wide variety of distributions of training data outcome values: linear, binomial (as in our current example), Poisson, and




196  |  Chapter 6: Explainable Boosting Machines and Explaining XGBoost

several others. Penalizing a GLM refers to using sophisticated constraints and iter? ative optimization methods to handle correlations, feature selection, and outliers. Putting all these together results in a robust modeling technique with good predictive power and very high explainability.
Elastic net (https://oreil.ly/K7_R0) is a popular regularization technique that com? bines the advantages of both L1 (LASSO) (https://oreil.ly/BqHjO) and L2 (ridge) (https://oreil.ly/ORzCT) regression into one model. Whereas the L1 regularization enables feature selection, thereby inducing sparsity and higher explainability in the trained model, L2 regularization effectively handles correlation between the predic? tors. The iteratively reweighted least squares (IRLS) method is often paired with elastic net to handle outliers as well.
Training a penalized GLM will serve two useful benchmarking purposes:
? Since our GLM does not include any nonlinearity or feature interactions, it can serve as the perfect benchmark to test certain hypotheses, i.e., whether nonlinear? ities and interactions actually result in a better model or not, which we?ll cover in the upcoming sections.
? The GLM also acts as a starting point for initial feature selection based on the features selected by L1 regularization.
We?ll start our first example for this chapter by training an elastic net?penalized logistic regression using H2O?s GLM algorithm (https://oreil.ly/bI_dI), which works in a distributed fashion and scales well for large datasets. In the H2O GLM, the regularization parameters are denoted by alpha and lambda. While alpha specifies the regularization distribution between L1 and L2 penalties, lambda indicates the reg? ularization strength. The recommended way to find optimal regularization settings in H2O GLM is via a grid search. H2O offers two types of grid searches?Cartesian and random search. Cartesian is an exhaustive search that tries all the combinations of model hyperparameters specified in a grid of possible values supplied by the user. On the other hand, random grid search samples sets of model parameters randomly from the given set of possible values based on a stopping criterion. By default, H2O will use Cartesian search, and we?ll use that for our use case because it won?t take too long to search over a small number of alpha values.

Whenever we conduct a grid search, we implicitly open ourselves up to issues related to overfitting and multiple comparisons. Try to use bootstrapping or reusable holdout methods with grid searches if possible.





The GAM Family of Explainable Models  |  197

In the following code, we start by defining a grid of model hyperparameters for alpha values. It is important to note here that to preserve the stabilizing function? ality of L2 penalties and the feature selection functionality of L1 penalties, alpha should never be 0 or 1. This is because when alpha is 0, it denotes only the L2 penalty, while a value of 1 for alpha signifies only L1. H2O?s GLM implementation comes with a handy lambda_search option. When set to True, this option searches over various lambda values starting from lambda_max (no features in the model) to lambda_min (many features in the model). Both alpha and lambda are selected by validation-based early stopping. This means that the GLM will automatically stop fitting the model when there is no significant improvement on the validation set, as a means to limit overfitting.
def glm_grid(x, y, training_frame, validation_frame, seed_, weight=None):

# setting GLM grid parameters
alpha_opts = [0.01, 0.25, 0.5, 0.99] # always keep some alpha hyper_parameters = {'alpha': alpha_opts}

# initialize grid search glm_grid = H2OGridSearch(
H2OGeneralizedLinearEstimator(family="binomial",
lambda_search=True, seed=seed_),
hyper_params=hyper_parameters)

# training w/ grid search glm_grid.train(y=y,
x=x, training_frame=training_frame, validation_frame=validation_frame, weights_column=weight,
seed=seed_)

# select best model from grid search best_model = glm_grid.get_grid()[0] del glm_grid

return best_model
Using this function to run a Cartesian search over alpha, and letting H2O search over the best lambda values, our best GLM ends up with an AUC score of 0.73 on the validation dataset. After the grid search, the six PAY_* repayment status features have the largest coefficients in the selected model.






198  |  Chapter 6: Explainable Boosting Machines and Explaining XGBoost

An AUC score of 0.73 means there is a 73% chance our model will properly rank a randomly drawn positive row with a higher output probability than that of a randomly drawn negative row.


To understand how the model treats various features, we plot partial dependence in conjunction with ICE plots for the features of interest. Additionally, a histogram of the feature of interest, including an overlay of the mean value of the target column, i.e., DELINQ_NEXT, is displayed alongside. This should give us a good idea of whether the model is behaving reasonably and if any data sparsity issues could result in meaningless predictions.
Let?s revisit Figure 6-1. The PAY_0 feature has the steepest partial dependence and ICE curve, thereby suggesting that it?s the most important input feature. The partial dependence and ICE plots are in harmony, i.e., they do not diverge, implying that the partial dependence can be relied upon. Additionally, there is a monotonic increasing relationship of predicted probability of default and PAY_0 lateness of payment. This means that as the delay in payment increases, the probability that a customer will default also becomes larger. This is in line with our intuition of how credit card payments work.
Now let?s review the histogram on the top. For customers with late payments, there are some apparent data sparsity problems. For instance, in regions where PAY_0 > 1, there is little or no training data. Also, the mean DELINQ_NEXT values do exhibit some nonlinear patterns in this region. It is all but obvious that predictions made in these regions will be less trustworthy. After all, a standard ML model like this can only learn from the data, unless we provide it additional domain knowledge. However, the good news is that the logistic form of our penalized GLM not only prevents it from being fooled by low-confidence dips in conditional mean DELINQ_NEXT around PAY_*
= 6, but also from overfitting noise in these areas of sparse training data. The model treats the other PAY_* features similarly, but assigns them flatter logistic curves. In all cases, the probability of default increases monotonically with the lateness of payment, as expected. To see the other partial and dependence and ICE plots, check out the code resources for this chapter.










The GAM Family of Explainable Models  |  199

We now have a robust and explainable baseline model. Because its behavior makes so much sense and is so easy to interpret, it may be hard to beat. A validation AUC of 0.73 is nothing remarkable, but having an explainable model that behaves in a manner that aligns to time-tested causal relationships that we can count on once deployed?that?s priceless for risk mitigation purposes. We also have to remember that validation and test data assessment scores can be misleading in more complex ML models. We can have a high AUC in static validation or test data, just to find out that the high AUC arose from overfitting to some specific phenomenon that?s no longer present in our operational domain. In the next section, we?ll first introduce some nonlinearities via GAMs and then feature interactions, in addition to nonlinearities via EBMs. We?ll then assess our model for explainability and perfor? mance quality with an eye toward real-world performance. We?ll try to do honest experiments and make deliberate choices about whether more complexity is justified.
Generalized Additive Models
While linear models are highly interpretable, they cannot accurately capture the nonlinearities typically present in real-world datasets. This is where GAMs come into play. GAMs, originally developed at Stanford in the late 1980s (https://oreil.ly/ tl_oq) by eminent statisticians Trevor Hastie and Rob Tibshirani, model nonlinear relationships of each input feature with individual spline shape functions and add them all together to make a final model. GAMs can be thought of as additive combinations of spline shape functions. An important idea for GAMs is that even though we treat every feature in a very complex way, it is done in an additive and independent manner. This not only preserves explainability but also enables editing and debugging with relative ease.
When it comes to implementing GAMs, packages like gam (https://oreil.ly/mt1ty) and mgcv (https://oreil.ly/SW3rz) are some great options in R. As for Python, the choice is limited, as most of the packages are in the experimental phase, like H2O?s GAM implementation (https://oreil.ly/_ak0k). Another alternative, pyGAM (https://oreil.ly/ dZ9tU), derives its inspiration from R?s mgcv package; has been shown to offer a good combination of accuracy, robustness, and speed (https://oreil.ly/Cyn-l); and has a familiar scikit-like API.










200  |  Chapter 6: Explainable Boosting Machines and Explaining XGBoost

We?ll use pyGAM to train a GAM on the same credit card dataset we used in the last section. Specifically, with the following code, we?ll implement a logistic model using pyGAM?s LogisticGAM class. There are three important parameters that can be tuned to obtain the best model: number of splines; lam, or the strength of regularization penalty; and constraints to inject prior knowledge into the model. pyGAM provides an inbuilt grid search method to search over the smoothing parameters automatically.
from pygam import LogisticGAM
gam = LogisticGAM(max_iter=100, n_splines=30) gam.gridsearch(train[features].values, train[target], lam=np.logspace(-3, 3, 15))
This code instantiates a LogisticGAM model that will train for up to 100 iterations. The n_splines parameter specifies the number of spline terms, or the complexity of the function used to fit each input feature. More spline terms typically results in more complex spline shape functions. lam corresponds somewhat to lambda in penalized regression, and the preceding code searches over several values to find the best strength of regularization as defined by lam. One parameter we are not taking advantage of is constraints. constraints allows users to specify a list of constraints for encoding prior knowledge. The available constraints are monotonic increasing or decreasing smoothing and convex or concave smoothing. We?ll work with similar constraints later in the chapter; it?s very instructive to see what not using constraints means for our GAM.
A question that we?re trying to answer deliberately in this example is: are nonlinear? ities truly helpful to our model, or just overfitting noise? Many data science practi? tioners today assume more complexity results in better models, but we?ll use GAMs to run an experiment to decide whether introducing nonlinearity actually improves our model, from both a performance quality perspective and an interpretability perspective.
After we train our model, we calculate its validation AUC, which turns out to be 0.75?a notch higher as compared to our penalized GLM. The increase in the GAM AUC can likely be attributed to the introduction of nonlinearity, which our GLM failed to capture. However, it is important to note here that a higher AUC doesn?t always guarantee better models, and this example is a classic case to prove the point. In the preceding section, we spent a bit of time analyzing how GLM treats the PAY_0 feature, or a customer?s most recent repayment status, and it did a reasonably good job. Let?s now look at how the GAM treats the same PAY_0 feature (Figure 6-2).








The GAM Family of Explainable Models  |  201



Figure 6-2. A partial dependence plot that incorporates ICE, histograms, and condi? tional means to increase trustworthiness and effectiveness for PAY_0 in the example GAM (digital, color version(: https://oreil.ly/KT-fl))
Figure 6-2 shows that there is clearly some weirdness in the partial dependence and ICE plots generated via a GAM. We observe that as the lateness of payment increases, the chances that a customer will default on the payment decreases. This is obviously not correct. Most people don?t magically become more likely to pay bills after months of being late. The same strange behavior is also observed for PAY_4 and PAY_6, as can be seen in Figure 6-3. The PAY_4 probability of default looks to decrease as payment lateness increases, and PAY_6 appears to bounce noisily around a mean prediction. Both modeled behaviors are counterintuitive, both contradict the GLM baseline model, and both fail to model the conditional mean behavior displayed on the righthand side of Figure 6-3.



202  |  Chapter 6: Explainable Boosting Machines and Explaining XGBoost



Figure 6-3. A partial dependence plot that incorporates ICE, histograms, and condi? tional means for PAY_4 and PAY_6 (digital, color version(: https://oreil.ly/m4yK6))
The bottom line is that although our validation AUC is higher, this is definitely a model that we wouldn?t want to deploy. As is apparent from Figure 6-2, the GAM has either overfit noise in the training data, been fooled by the low-confidence dips in the conditional mean DELINQ_NEXT around PAY_* = 6, or is reverting to a mean prediction due to data sparsity for PAY_0 > 1.






The GAM Family of Explainable Models  |  203

So what?s the workaround, and how do we use such a model? Well, that?s precisely where GAMs shine. The behavior displayed by the GAM, in this case, is a general problem observed for high-capacity nonlinear models. However, unlike many other types of ML models, GAMs not only highlight such inconsistencies but also offer ways to debug them through commonsense model editing. More plainly, we can discuss the GAM results with domain experts, and if they agree with the more plausible GAM splines for PAY_2, PAY_3, and PAY_5, we could keep those in the model and perhaps gain a boost in the model performance. As for the obviously problematic splines for PAY_0, PAY_4, and PAY_6, they can be replaced with something that makes more sense. One option is their learned behavior from the logistic regression model as shown in the following expression:


p = ?0 +	1

+ ?PAY2, GAM g PAY2

+ ?PAY3, GAMg PAY3 +	1

+ ?PAY5, GAMg PAY5

+	1	+ ?

where ?0 is an intercept term and each g represents a GAM spline function. Model editing is infinitely flexible; we could replace the learned spline on only a certain region of its domain, or otherwise edit the shape function to a domain expert?s liking.
Editability is a great feature for a predictive model, but we also need to be careful with it. If we are to edit a custom model as suggested by the preceding equation, it really needs to be stress tested more than usual. Let?s not forget, the coefficients weren?t learned together and may not account for one another well. There could also be boundary problems?the edited model could easily result in predictions above 1 and below 0. Another potentially more palatable debugging strategy is to use the constraint functionality provided by pyGAM. A positive monotonic constraint would likely fix the problems in the PAY_0, PAY_4, and PAY_6 splines.
Whether we choose to edit the example GAM or retrain it with constraints, we?ll likely see lower validation and test data performance quality. However, when we?re most concerned with dependable real-world performance, we sometimes have to give up worshiping at the altar of holdout dataset performance. While model editing may sound strange, the preceding model makes sense. What seems more strange to us is deploying models whose behavior is only justified by a few rows of high-noise training data in obvious contradiction of decades of causal norms. We?d argue the preceding model is much less likely to result in a catastrophic failure than the nonsense splines learned by the unconstrained GAM.



204  |  Chapter 6: Explainable Boosting Machines and Explaining XGBoost

This is just one of many scenarios where traditional model assessment can be mis? leading when it comes to choosing the best real-world model. As was shown in the GAM example, we can?t assume that nonlinearities make better models. Moreover, GAMs allow us to test the implicit hypothesis that nonlinearity is better. With GAMs, we can create a model, interpret and analyze the results, and then edit it or debug any detected issues. GAMs help us uncover what our model has learned, keep the correct results, and edit and rectify the wrong ones so that we do not deploy risky models.
GA2M and Explainable Boosting Machines
When a small group of interacting pairs of features is added to a standard GAM, the resulting model is called a GA2M?a generalized additive model with two-way inter? actions. Adding these pairwise interactions to traditional GAMs has been shown to substantially increase model performance while retaining explainability, as discussed in Chapter 2. Additionally, just like GAMs, GA2Ms are easily editable.
The EBM (https://oreil.ly/_tS2Q) is a fast implementation of the GA2M algorithm by Microsoft Research. The shape functions in an EBM are trained iteratively via boosting, making EBM training more robust, while retaining accuracy comparable to unexplainable tree-based models like random forest and XGBoost. The EBM comes packaged within a broader ML toolkit called InterpretML (https://oreil.ly/Uofrw), an open source package for training explainable models and explaining other systems.
We?ll continue with our credit card example and train an EBM to predict which customer has a high chance of defaulting on their next payment. The EBM achieves a validation AUC of 0.78, which is the highest compared to traditional GAMs and GLMs. The bump in accuracy is likely due to the introduction of nonlinearity and interactions. Explaining EBMs and GA2Ms is also easy. Like traditional GAMs, we can plot the shape functions of individual features and their accompanying histograms describing the model behavior and data distribution for that feature, respectively. The interaction terms can be rendered as a contour plot?still easy to understand. Let?s dig in a bit more by looking at how an EBM treats LIMIT_BAL, PAY_0, and PAY_2 features, as shown in Figure 6-4.











The GAM Family of Explainable Models  |  205



Figure 6-4. Three important input features and an interaction feature with accompany? ing histograms for an EBM (digital, color version(: https://oreil.ly/l9lTU))
In Figure 6-4, we can see three standard shape function plots for LIMIT_BAL, PAY_0, and PAY_2, but we can also see a contour plot for the PAY_0 x PAY_2 interaction. Each of these plots, even the slightly more complex contour plot, allows humans

206  |  Chapter 6: Explainable Boosting Machines and Explaining XGBoost

to check the behavior of the model, and when needed, to edit it. The behavior of LIMIT_BAL looks reasonable, as increasing credit limits are expected to be correlated with a decreasing probability of default. That?s what we observe, at least up to the high ranges of LIMIT_BAL in training data. Above $700,000, we see the shape function turn back upward?likely related to sparsity in training data in this region. The EBM treats PAY_0 more logically than our GAM. Under the EBM, PAY_0 probabilities of default increase for PAY_0 > 1 and don?t drop back down to unrealistic values, but do decrease. Again, this is likely related to sparsity in certain regions of training data. PAY_2 appears to be somewhat noisy. Also, the interaction term exhibits the same unrealistic behavior that was observed under the GAM for some individual PAY_* features; increased lateness results in a lower probability of default, except for low val? ues of PAY_0, and high values of PAY_2, where model outputs rocket upward. Like the GAM, the EBM seems to have some strange behaviors based on noise and sparsity in training data. That may be another reason why its AUC is higher?it?s modeling noise specific to this dataset in certain cases. At least strange behavior is plainly obvious, and this model may be a good candidate for model editing. Monotonic constraints, discussed in the next section, might also help here, but they were not yet available for the EBM in interpret.
There are two other very important aspects of Figure 6-4 that are not necessarily characteristic of EBMs, but that also require a second look?the shaded regions around the shape functions and the histograms beneath the shape functions. Both of these features help users decide the level of trustworthiness for the model. If the histograms indicate that little training data is available in a certain region, or the shaded error bars show that the function has high variance in a certain training data domain, that part of the function is probably less trustworthy and model editing can be considered. The shape function for LIMIT_BAL over $700,000 is an example of both sparsity in training data and high variance in predictions. These two issues often go hand in hand when training, explaining, and debugging ML models.
One additional slightly tricky aspect of working with EBMs is accessing information for our own plotting needs. While EBM provides amazing interactive plotting capa? bilities out-of-the-box, we often like to create our own plots or data structures, espe? cially to compare with other models. We found it necessary to interact with EBM?s
_interal_obj JSON structure to do so on several occasions. Take, for instance, accessing feature importance values as follows:
ebm_global = ebm.explain_global(name='EBM')
feature_names = ebm_global._internal_obj['overall']['names'] feature_importances = ebm_global._internal_obj['overall']['scores'] ebm_variable_importance = pd.DataFrame(zip(feature_names, feature_importances),
columns=['feature_names',
'feature_importance'])



The GAM Family of Explainable Models  |  207

To extract feature importance to manipulate ourselves in the version of interpret we had access to, instead of relying on the EBM?s default plotting, we had to calculate global explanations using explain_global(), then extract feature names and impor? tance scores from JSON within the returned object. We then used this information to create a Pandas DataFrame, and from there, most standard operations like plotting, selecting, or manipulating, are easy.1
With that, we?ll close out our first set of examples. In this section, we introduced a benchmark GLM, then deliberately introduced nonlinearities via GAMs and interac? tions via GA2Ms and EBMs that made our models more complex. However, due to the additive nature of GLMs, GAMs, and EBMs, not only did we retain explainability and gain in silico performance quality, but we also created a set of editable models that we can compare to one another, and even combine, to build the best possible model for real-world deployment. The next section will continue these themes and dive into constraints and post hoc explanation with XGBoost.
XGBoost with Constraints and Post Hoc Explanation
In this example, we?ll train and compare two XGBoost classifier models?one with monotonic constraints, and one without. We?ll see that the constrained model is more robust than the unconstrained and no less accurate. Then, we?ll examine three power? ful post hoc explanation methods?decision tree surrogates, partial dependence and ICE, and SHAP values. We?ll conclude with a technical discussion of SHAP value calculations and background datasets, and we?ll provide guidance so readers can choose the appropriate specifications for the application at hand.
Constrained and Unconstrained XGBoost
XGBoost (https://oreil.ly/n98WV) is an incredibly popular model architecture for pre? diction tasks on large, structured datasets. So what is an XGBoost model? The models produced by XGBoost are ensembles of weak learners. That is, XGBoost produces many small models in a sequence, and then to make a final prediction, it sums up the predictions of these small models. Typically, the first model in the sequence fits the data, and each subsequent model predicts the residuals of the models that came before it to correct their errors.2 In this section, we?ll use XGBoost to train an ensemble of shallow decision trees. We?ll be working with a binary classification problem, but XGBoost can be used to model other problem types, such as regression, multiclass classification, survival time, and more.


1 This inconvenience is being addressed in current versions of the package. See the documentation (https:// oreil.ly/Z40st) for more details.
2 For more details on gradient boosting, see Elements of Statistical Learning (https://oreil.ly/hvX2H), Chapter 10.

208  |  Chapter 6: Explainable Boosting Machines and Explaining XGBoost

XGBoost is so popular, in part, because it tends to produce robust models that generalize well on unseen data. That doesn?t mean we, as model developers, can fall asleep at the wheel. We have to use reasonable hyperparameters and techniques such as early stopping to ensure that the strengths of XGBoost are realized. Another important technique that XGBoost allows us to apply is monotonic constraints on our models. These constraints lock down the direction of the relationship between certain features and the model output. They allow us to say, ?If feature X_1 increases, then the model output cannot decrease.? In short, these constraints allow us to apply our own domain knowledge to make more robust models. Let?s take a look at some code to train an XGBoost model:
params = {
'objective': 'binary:logistic', 'eval_metric': 'auc',
'eta': 0.05,
'subsample': 0.75,
'colsample_bytree': 0.8,
'max_depth': 5, 'base_score': base_score, 'seed': seed
}
watchlist = [(dtrain, 'train'), (dvalid, 'eval')] model_unconstrained = xgb.train(params,
dtrain, num_boost_round=200, evals=watchlist, early_stopping_rounds=10, verbose_eval=True)
First, let?s look at the values in the params dictionary. The parameter eta is the learning rate of our model. In gradient boosting, each tree we add into our ensemble is similar to a gradient descent step. The larger the value of eta, the more each additional tree impacts our model. The smaller the value of eta, the less weight is given to individual decision trees in the boosting sequence. If we used eta = 1.0, our model?s final prediction would be an unweighted sum of individual decision tree outputs, and would almost certainly overfit to the training data. Make sure to set a reasonable learning rate (say, between 0.001 and 0.3) when training XGBoost or other gradient boosting models.








XGBoost with Constraints and Post Hoc Explanation | 209

XGBoost also offers interaction constraints (https://oreil.ly/NR9bo) to control how input features affect one another in a model. This is another simple method to inject domain expertise into ML models. Interaction constraints may be particularly helpful for bias mitiga? tion by eliminating known proxy interactions for gender or race, such as combinations of name, age, or zip code.

The parameters subsample and colsample_bytree also protect against overfitting. Both ensure that each individual decision tree does not see the entire training dataset. In this case, each tree sees a random 75% of rows of the training data (subsample
= 0.75), and a random 80% of columns of the training data (colsample_bytree
= 0.8). Then, we have some parameters that dictate the size of the final model. max_depth is the depth of the trees in the model. Deeper trees include more feature interactions and create more complex response functions than shallow trees. We usually want to keep our trees shallow when training XGBoost and other GBM models?after all, the strength of these models comes from them being an ensemble of weak learners. Of course, grid search and other structured methods for selecting hyperparameters are the better practice for choosing these values, but that?s not the focus of this chapter.
Finally, in the preceding code snippet, we?re training a model using validation-based early stopping (https://oreil.ly/zacj5). We do this by feeding a dataset (or in this case, two datasets) into the evals parameter, and by specifying early_stopping_rounds. So what is going on here? In each round of the training sequence, the collection of decision trees trained so far is evaluated on the datasets in the evals watchlist. If the evaluation metric (in this case, AUC) does not improve for early_stopping_rounds rounds, then training stops. If we didn?t specify early stopping, then training would proceed until num_boost_round trees are built?often an arbitrary stopping point. We should almost always use early stopping when training our GBM models.

If we pass multiple datasets into evals, only the last dataset in the list will be used to determine if the early stopping criterion has been met. Furthermore, the final model will have too many trees. Whenever we make a prediction with the model, we should specify how many trees to use using the iteration_range parameter?see the documentation (https://oreil.ly/OZ5FF) for more info.

As we?ll see, unconstrained XGBoost was free to assign probabilities to individual observations based on sometimes spurious patterns in the training data. We can often do better using the knowledge in our own brains and with the help of domain experts, in addition to what can be learned solely from training data.



210  |  Chapter 6: Explainable Boosting Machines and Explaining XGBoost

We know, for example, that if someone is more and more overdue on their credit card payments, then they almost certainly have a higher likelihood of being late on their next payment. That means that for all PAY_* features in our dataset, we?d like the model output to increase when the feature value increases, and vice versa. XGBoost monotonic constraints allow us to do exactly that. For each feature in the dataset, we can specify if we?d like that feature to have a positive, negative, or no monotonic relationship with the model output.
Our dataset only contains 19 features, and we can reason through the underlying causal relationship with default risk for each one. What if our dataset contains hundreds of features? We?d like to train a robust, constrained model, but maybe we?re unsure about a monotonic causal link between certain features and the target. An alternative (or supplemental) method for deriving monotonic constraints uses Spearman correlation. In the following code, we implement a function that examines the pairwise Spearman correlation coefficient between each feature and the target. If the Spearman coefficient is greater in magnitude than a user-specified threshold, that feature is assumed to have a monotonic relationship with the target. The function returns a tuple containing values ?1, 0, and 1?exactly the form of input expected by XGBoost when specifying monotonic constraints.
def get_monotone_constraints(data, target, corr_threshold):
corr = pd.Series(data.corr(method='spearman')[target]).drop(target) monotone_constraints = tuple(np.where(corr < -corr_threshold, -1,
np.where(corr > corr_threshold, 1, 0)))
return monotone_constraints

We use Spearman correlation coefficients rather than the default Pearson correlation because GBMs are nonlinear models, even when constrained. XGBoost monotonic constraints impose mono? tonicity rather than linearity?Spearman correlation measures exactly the strength of a monotonic relationship, whereas Pearson correlation measures the strength of a linear relationship.

In Figure 6-5, we?ve plotted the Spearman correlation coefficient of each feature against the target variable. The vertical lines indicate the threshold value of 0.1. We can see that this data-driven approach suggests imposing monotonic constraints for the PAY_* features. We?re using the threshold of 0.1 as an informal marker of practical significance. We may not want to apply any constraints to those input features whose Spearman correlation is less than 0.1 in magnitude. As payment and credit limits increase, probability of default should decrease. As late payments increase, probability of default should increase. The results of this data-driven approach reflect common sense too. That?s the most important consideration when generating con? straints, since we?re trying to inject causal domain knowledge into our model.



XGBoost with Constraints and Post Hoc Explanation | 211



Figure 6-5. Spearman correlation coefficients between target DELINQ_NEXT and
each feature, with vertical bars indicating a cutoff value of 0.1 (digital, color version(: https://oreil.ly/qolIs))
Next, we train the constrained model with the constraints suggested by the anal? ysis shown in Figure 6-5. Let?s make a few observations about the resulting con? strained and unconstrained models. By looking at the output of xgb.train() with verbose_eval=True in the code example (https://oreil.ly/BN3dS), we see that the unconstrained model has a higher AUC on the training set (0.829 vs. 0.814), but shows equal performance to the constrained model on the validation set (0.785 vs. 0.784). This suggests that the constrained model is less overfit than the unconstrained model?with the exact same set of hyperparameters, the constrained model picks up on a higher proportion of the true signal in the data. As our analyses will show, there are additional reasons that we expect better performance (and better stability) from the constrained model in vivo.
Finally, let?s look at the feature importance for the two models in Figure 6-6. We can compute the feature importance values for XGBoost models in many ways. Here, we?ll look at the average coverage of the splits in the ensemble. The coverage of a split is just the number of training samples that flow through the split. This is a traditional calculation for feature importance. It does not have the same theoretical guarantees as, for example, SHAP techniques.



212  |  Chapter 6: Explainable Boosting Machines and Explaining XGBoost



Figure 6-6. The feature importance values of the constrained and unconstrained models, as measured by average coverage (digital, color version(: https://oreil.ly/MAs3a))
We can see that the constrained model spreads the feature importance more evenly among the entire input feature set. The unconstrained model gives a disproportionate share of the feature importance to the PAY_0 feature. This is even more evidence that the constrained model will be more robust when deployed. If a model focuses all its decision-making capacity on one feature, then it?s going to fail when the distribution of that feature drifts with new data. Being too dependent on a single feature is also a security risk. It?s easier for bad actors to understand how the model works and take advantage of it.


XGBoost with Constraints and Post Hoc Explanation | 213

When we see feature importance values concentrated on one or a few features, our model is more likely to be unstable and insecure postdeployment. Our model might be overly sensitive to drift in the data distribution along a single dimension, and a malicious actor only needs to manipulate the value of one feature to alter model outcomes. If an ML model is focusing on only one or two features, consider replacing it with a simpler model or a business rule.

Explaining Model Behavior with Partial Dependence and ICE
Let?s continue our comparison of the constrained and unconstrained XGBoost mod? els by looking at a side-by-side partial dependence and ICE plot for PAY_0. In the previous sections, we?ve already discussed how the conditional mean of the target variable shows a spurious dip around PAY_0 = 6, where our training data is sparse. Let?s see how our two XGBoost models handle this data deficiency.
In Figure 6-7, we can see how the unconstrained model overfits to the spurious relationship between PAY_0 and DELINQ_NEXT, to a small extent. On the other hand, the constrained model is forced to obey the commonsense relationship that more delayed payments should not lead to a lower risk of delinquency. This is reflected in the monotonically increasing partial dependence and ICE plots for PAY_0 under our constrained model.

One difficulty with ICE is picking which individuals to plot first. A good way to get started with ICE plots is to pick individuals or rows at the deciles of predicted outcomes. This gives a coarse picture of local behavior, and from there, we can dive deeper if needed.

We can also see that for both models, there are large changes in the output across the range of PAY_0 values. That is, both the partial dependence and ICE plots show lots of vertical movement as we sweep PAY_0 from ?2 to 8. The model outputs are highly sensitive to the values of this feature?which is exactly why we saw such high feature importance values for PAY_0 in Figure 6-6. If we don?t observe these kinds of value changes for a feature our model says is very important, that?s a sign that more debugging may be required.







214  |  Chapter 6: Explainable Boosting Machines and Explaining XGBoost



Figure 6-7. Partial dependence and ICE plots of the PAY_0 feature for the constrained (top) and unconstrained (bottom) models (digital, color version(: https://oreil.ly/ulxRP))





XGBoost with Constraints and Post Hoc Explanation | 215

Partial dependence and ICE plots can also reveal where there are feature interactions in our model. Take a look at the partial dependence and ICE plot for LIMIT_BAL under our unconstrained model in Figure 6-8.

Figure 6-8. Partial dependence and ICE plots of the LIMIT_BAL feature for the uncon? strained model (digital, color version(: https://oreil.ly/D-CeU))
As discussed in ?Partial Dependence and Individual Conditional Expectation? on page 191, when partial dependence and ICE curves diverge, as they do here, it is suggestive of correlations or interactions in our data and model. Moreover, we can look back to the EBM training (https://oreil.ly/lW3kV) and see that two important interactions identified by the EBM are LIMIT_BAL x BILL_AMT2 and


216  |  Chapter 6: Explainable Boosting Machines and Explaining XGBoost

LIMIT_BAL x BILL_AMT1. It?s plausible that our unconstrained XGBoost model picked up on these interactions as well. In contrast to the EBM, our XGBoost models are riddled with various high-degree feature interactions. But partial dependence and ICE, combined with the EBM?s ability to learn two-way interactions, can help us understand some of the interactions in our XGBoost models too. Another great tool for understanding complex feature interactions in ML models is the surrogate decision tree, which we?ll discuss next.
Decision Tree Surrogate Models as an Explanation Technique
The analysis we?ve conducted so far has shown that our unconstrained XGBoost model does not perform better than the constrained version. The partial dependence and ICE plots we?ve looked at show that by tethering the constrained model to reasonable real-world relationships, we?ve successfully prevented the model from picking up on spurious relationships in the training data. Since our constrained model appears logically superior to the alternative, the next sections will focus exclu? sively on this model.
First, we?re going to continue our exploration of the model?s behavior through a post hoc explanation technique?decision tree surrogate models. A surrogate model is just a simple model meant to mimic the behavior of a more complex model. In our case, we?re trying to mimic our constrained XGBoost model with roughly 100 trees using a single, shallow decision tree. A decision tree is a data-derived flowchart, so we can look at the decision tree surrogate as a flowchart and explain how the more complex GBM is operating in simpler terms. This is what makes decision tree surrogates a powerful explanation technique. We use the DecisionTreeRegressor implementation from sklearn to train our surrogate model:
surrogate_model_params = {'max_depth': 4,
'random_state': seed}
surrogate_model = DecisionTreeRegressor(**surrogate_model_params)
.fit(train[features], model_constrained.predict(dtrain))

Surrogate modeling is also known by other names, such as model compression or model extraction.


Notice that we?re training a regression model targeted at the output of the model we?re trying to explain. That is, the surrogate is totally focused on mimicking the behavior of the larger model, not just making a simpler classification model. We?ve also chosen to train a decision tree of depth four. Any deeper, and we might have a hard time explaining what is happening in the surrogate model itself.


XGBoost with Constraints and Post Hoc Explanation | 217

Surrogate models do not always work well. Always check that surrogate models have good performance quality and stability characteristics. We put forward a simple surrogate modeling approach here. See ?Interpreting Blackbox Models via Model Extraction? (https://oreil.ly/O4Kia), ?Extracting Tree-Structured Representations of Trained Networks? (https://oreil.ly/BQnI7), and ?The Price of Interpretability? (https://oreil.ly/CNgUA) for more information on surrogate approaches and what, if any, guarantees can be made about their fidelity.

Before we examine our surrogate model, we must ask whether we can trust it. Decision tree surrogates are a powerful technique, but they don?t come with many mathematical guarantees. One simple way of assessing our surrogate?s quality is to compute accuracy metrics on cross-validation folds. Why cross-validation and not just a validation dataset? A pitfall of single decision tree models is their sensitivity to changes in the training dataset, so by computing accuracy on multiple holdout folds, we?re checking whether our surrogate model is accurate and stable enough to trust:
from sklearn.model_selection import KFold from sklearn.metrics import r2_score

cross_validator = KFold(n_splits=5) cv_error = []
for train_index, test_index in cross_validator.split(train): train_k = train.iloc[train_index]
test_k = train.iloc[test_index]

dtrain_k = xgb.DMatrix(train_k[features],
                       label=train_k[target]) dtest_k = xgb.DMatrix(test_k[features],
label=test_k[target])

surrogate_model = DecisionTreeRegressor(**surrogate_model_params) surrogate_model = surrogate_model.fit(train_k[features],
                                      model_constrained.predict(dtrain_k)) r2 = r2_score(y_true=model_constrained.predict(dtest_k),
              y_pred=surrogate_model.predict(test_k[features])) cv_error += [r2]

for i, r2 in enumerate(cv_error):
print(f"R2 value for fold {i}: {np.round(r2, 3)}")
print(f"\nStandard deviation of errors: {np.round(np.std(cv_error), 5)}")
R2 value for fold 0: 0.895
R2 value for fold 1: 0.899
R2 value for fold 2: 0.914
R2 value for fold 3: 0.891
R2 value for fold 4: 0.896

Standard deviation of errors: 0.00796

218  |  Chapter 6: Explainable Boosting Machines and Explaining XGBoost

These results look great. We can see that the surrogate model has a high accuracy across every cross-validation fold, with very little variation. With some confidence that our decision tree is a reasonable surrogate, let?s plot the surrogate model (Figure 6-9).

Figure 6-9. The decision tree surrogate for our constrained XGBoost model






XGBoost with Constraints and Post Hoc Explanation | 219

Notice in Figure 6-9 that the surrogate model splits on the PAY_0 feature first. That is, in order to optimally mimic the behavior of the constrained XGBoost model, the first thing the surrogate model does is to separate observations into two groups?those with PAY_0 ó 1.5 and those with higher PAY_0 values. We can crudely approximate feature importance by looking at the depth of each feature?s splits in the surrogate model, so this result is consistent with our feature importance analysis. A good sign.
Since our surrogate model is so simple, we can also make lots of plain-language observations about it. For example, we can trace the paths of the highest- and lowest-risk observations and explain how our surrogate is treating them:
? The lowest-risk observations traverse the following decision path: September, 2005 repayment status on-time or one month late (PAY_0 ó 1.5) AND August, 2005 repayment status on-time or one month late (PAY_2 ó 1.5) AND August, 2005 repayment amount is more than $1,603.5 (PAY_AMT2
> $1,603.5). These rules focus on customers who make recent payments in a timely fashion, and make large payments. Makes sense.
? The highest-risk observations traverse the following decision path: Septem? ber, 2005 repayment status more than one month late (PAY_0 > 1.5) AND August, 2005 repayment status more than one month late (PAY_2 > 1) AND April, 2005 repayment status more than one month late (PAY_6 > 1). These rules consider unfavorable repayment statuses over time?also logical.
These explanations take into account repayment statuses and repayment amounts. For acceptance decisions, the more complex GBM seems to focus on more recent status and amount information, and for denial decisions, our GBM is probably looking for payment status patterns over time.
The last thing we?ll notice is that each time one feature follows from another in a decision tree path, those features are likely interacting in our GBM. We can easily identify the main feature interactions that our XGBoost model has learned by examining the surrogate model. Interestingly, we can also look back and see that the EBM (https://oreil.ly/1R_hN) also picked up on some of the same interactions, such as PAY_0 x PAY_2 and PAY_0 x PAY_AMT2. With all these tools?EBMs, partial dependence and ICE, and surrogate decision trees?we can really get a solid picture of what?s in our data and what?s reasonable to expect from model behavior. That?s very different from training a single unexplainable model and checking a few test data assessment metrics. We?re starting to learn how these models work, so we can make human judgment calls about their real-world performance.
What?s more, we can use this information about interactions to boost the perfor? mance of linear models, such as a penalized logistic regression, by including these learned interactions as input features. If we want to stick with the most conservative


220  |  Chapter 6: Explainable Boosting Machines and Explaining XGBoost

model forms for our highest-risk applications, we can use a GLM and likely get a boost in performance quality with this information about important interactions. As readers can see, we can make all kinds of simple, explainable observations about our XGBoost model through the use of decision tree surrogates. And we?ve collected loads of useful information for model documentation and other risk management purposes along the way.
Shapley Value Explanations
The last post hoc explanation tool to discuss before we close the chapter is Shapley values. In ?Local explanations and feature attribution? on page 52, we mentioned that Shapley values can be used as a local feature attribution technique. In fact, Shapley values come with a host of mathematical guarantees that suggest that they are usually the best choice for feature attribution and importance calculations. The research and open source communities, led by Scott Lundberg at University of Washington and Microsoft Research, have developed a host of tools for generating and visualizing SHAP values. These tools live in the SHAP Python package, and that?s what we?ll use in this section.
Remember that local feature attribution methods assign a value to each feature for each observation, quantifying how much that feature contributed to the predicted value that the observation received. In this section, we?ll see how to use SHAP values and the SHAP package to explain the behavior of our models. In the final section of this chapter, we?ll examine some of the subtleties to Shapley value?based explanations, and the pitfalls they pose to practitioners.
Let?s take a look at some code to generate SHAP values for our monotonic XGBoost model:
explainer = shap.TreeExplainer(model=model_constrained,
data=None, model_output='raw',
                               feature_perturbation='tree_path_dependent') shap_values = explainer(train[features])
We?re using the SHAP package?s TreeExplainer class. This class can generate SHAP values for XGBoost, LightGBM, CatBoost, and most tree-based scikit-learn models. The TreeExplainer is discussed in Scott Lundberg?s papers ?Consistent Individual? ized Feature Attribution for Tree Ensembles? (https://oreil.ly/VZz75) and ?From Local Explanations to Global Understanding with Explainable AI for Trees? (https://oreil.ly/ 7fdWE), among others. They?re a great example of the computational breakthroughs that have made the SHAP package so successful. If you need to generate SHAP values for a model not based on trees, take a look at the examples in the SHAP package documentation (https://oreil.ly/5h0zu)?there are multiple examples for tabular, text, and image data.


XGBoost with Constraints and Post Hoc Explanation | 221

If you need to explain a model that?s not tree- or neural-network- based, don?t forget about comparison to prototypes and counter? factual explanations. These powerful explanation concepts can be more effective than general-purpose model-agnostic approaches like local model-agnostic interpretable explanations (LIME) or ker? nel SHAP.

To begin, let?s take a look at the SHAP values associated with the PAY_0 feature in Figure 6-10.

Figure 6-10. Dependence plot for the PAY_0 feature, showing the distribution of SHAP values in each bucket of feature values (digital, color version(: https://oreil.ly/hF4eJ))
Each dot in Figure 6-10 is one observation?s SHAP value for the PAY_0 feature, or contribution to the model prediction, with an x-coordinate given by the value of PAY_0. The scatter plot is superimposed over a histogram of feature values in the dataset, just like our partial dependence and ICE plots. In fact, this scatter plot can be directly compared to the partial dependence and ICE plots for PAY_0. In the SHAP scatter plot, we can see the whole range of feature attribution values within each bucket of PAY_0 values. Notice that the range is widest in the PAY_0 = 2 bucket?some observations with PAY_0 = 2 are penalized approximately half as much as others. This SHAP scatter plot is one of many summarizing plots included in the SHAP package. For a more thorough overview, take a look at the examples in the documentation (https://oreil.ly/xWxlG) as well as this chapter?s Jupyter notebook examples.




222  |  Chapter 6: Explainable Boosting Machines and Explaining XGBoost

As we saw in Chapter 2, we can construct a measure of overall feature importance by taking the mean of absolute SHAP values. Instead of a standard horizontal bar chart of feature importance values, we can use the SHAP plotting functionality to look at feature importance explicitly as an aggregation of local explanations:
    shap.plots.beeswarm(shap_values.abs, color="shap_red", max_display=len(features))
Figure 6-11 gives us an interesting perspective on feature importance. Notice that some features (e.g., PAY_0, PAY_AMT1) have a few dots exhibiting extreme SHAP values, whereas other features (e.g., LIMIT_BAL, PAY_AMT3) have a high feature impor? tance because a lot of individual observations have somewhat high absolute SHAP values. Put another way, local explanations allow us to distinguish between high- frequency, low-magnitude effects versus low-frequency, high-magnitude effects. This is important, because each of those high-magnitude, low-frequency effects represent real people being impacted by our model.

Figure 6-11. Feature importances, shown as the aggregation of individual observations? absolute SHAP values (digital, color version(: https://oreil.ly/cjUiE))




XGBoost with Constraints and Post Hoc Explanation | 223

Shapley value?based explanations have seen widespread adoption due to their strong theoretical underpinning and the robust set of tools built around them. Since these quantities can be computed at an observation-by-observation level, they can likely be used to generate adverse action notices or other turn-down reports (when used properly, tested for fidelity and stability, and paired with constrained models). Have a look at our code examples (https://oreil.ly/machine-learning-high-risk-apps-code), read some papers and documentation, and start generating SHAP values for our model. But please read the next section, where we discuss some of the assumptions and limitations behind every SHAP computation.
Problems with Shapley values
In Chapter 2 and ?Shapley Values? on page 194, we introduced the idea of a background dataset that underlies Shapley value calculations. When SHAP wants to understand the impact of a feature on a certain prediction, it replaces the value of the feature in the training or test data with a random value drawn from the background data and compares predictions from the two different datasets many times over, with lots of perturbations regarding which features use normal data and which features use background data. Here?s a useful way to think of background datasets: when we calculate a SHAP value for an observation, we?re answering the question ?Why did this observation get this prediction rather than some other prediction?? The ?other prediction? that observations are compared against is dictated by the choice of back? ground data, or reference distribution.
In the following code, we create two sets of SHAP values for the same observations? except in one instance we do not specify a reference distribution, and in the other we do:
explainer_tpd = shap.TreeExplainer(model=model_constrained,
                                   feature_perturbation='tree_path_dependent') shap_values_tpd = explainer_tpd(train[features])

train['pred'] = model_constrained.predict(dtrain) approved_applicants = train.loc[train['pred'] < 0.1] explainer_approved = shap.TreeExplainer(model=model_constrained,
data=approved_applicants[features], model_output='raw', feature_perturbation='interventional')

shap_values_approved = explainer_approved(train[features])
By setting feature_perturbation='tree_path_dependent', we?re opting not to define a reference distribution at all. Instead, SHAP uses information collected from the trained GBM model trees to implicitly define its own background data. This is similar to using the training data as our background data, but not exactly the same.



224  |  Chapter 6: Explainable Boosting Machines and Explaining XGBoost

Next, we define an explainer with feature_perturbation='interventional', for which we pass a reference distribution composed of training samples that received a probability of delinquency of less than 10%. If the reference distribution is what we compare each observation against, then we would expect these two sets of SHAP values to be meaningfully different. After all, these questions are incredibly different: ?Why did this observation get this prediction, rather than the average prediction in the training data?? versus ?Why did this observation get this prediction, rather than the predictions given to approved applicants?? As discussed in Chapter 2, the latter question is much more aligned with US regulatory commentary on adverse action notices. This is an example of what Professor Beicek means when he says, ?Don?t explain without context!?
Although some argue tree_path_dependent feature perturbations are true to the data?that is, they tell us about more than just this one model?s behavior?as shown in ?Explaining Individual Predictions When Features Are Dependent: More Accurate Approximations to Shapley Values? (https://oreil.ly/3PBGX), this is probably not true. True to the data feature attributions would require that we know the full joint proba? bility distribution of our data, and this is a very challenging technical problem that we can?t hope to solve by looking at the path structure of the trees in our model. It?s better to use interventional feature perturbations and recognize that our SHAP values are true to model and don?t generalize outside of our model. Our suggestion is to only use tree_path_dependent feature perturbations when you have no other choice. The main reason for using them would be if you don?t have access to a background dataset, and you have to infer it from the model. If you have access to the training data, explicitly pass it in to the SHAP explainer and use interventional feature perturbations.

As of the writing of this book, best practices indicate that it?s better to use interventional feature perturbations and recognize that our explanations don?t generalize outside of our model. Only use tree_path_dependent feature perturbations when you have no other choice.

On to Figure 6-12 to try to show why all this matters. In Figure 6-12, we show two sets of SHAP values for the same observation?one calculated without a ref? erence distribution and feature_perturbation='tree_path_dependent', and one calculated against a reference of approved applicants and feature_perturbation
='interventional'. First off, for some observations, we can see large differences in the SHAP values under these two different types of SHAP explanations.





XGBoost with Constraints and Post Hoc Explanation | 225



Figure 6-12. SHAP values for the same observation, generated with and without a refer? ence distribution consisting of observations with an assigned probability of delinquency less than 10% (digital, color version(: https://oreil.ly/jN3lj))


226  |  Chapter 6: Explainable Boosting Machines and Explaining XGBoost

Imagine adverse action notices sent out on the basis of these two explanations. With no reference distribution, the top four features that contributed to a larger probability of delinquency are PAY_0, LIMIT_BAL, BILL_AMT5, and PAY_AMT3. But if we specify a context-specific reference distribution, these top four features are PAY_AMT1, PAY_AMT2, PAY_AMT3, and LIMIT_BAL. In a credit lending context, where recourse (the ability to appeal a model decision) is a crucial part of trust and responsible deployment, which explanation is correct? It?s likely to be the explanations based on interventional feature perturbation, and due to using approved applicants in the background data, they are also framed more logically in terms of regulatory requirements.
However, those interventional explanations are unique to the model, and therefore they only provide accurate reason codes to the applicant if the applicant will be scored in the future using the exact same model. It?s going to raise eyebrows if an applicant gets different adverse action notices for similar credit products, espe? cially from the same lender, just because of some model-specific mechanism in an ML pipeline. It?s not impossible that tree-path-dependent explanations will be more consistent across different models?as claimed by some?but that highlights another difficulty. Both path-dependent and interventional SHAP values can give explanations based on features not used in a specific decision. That?s a big problem for adverse action notices and actionable recourse. However, we would still suggest using the interventional explanations when you use SHAP, while also acknowledging and testing for their shortcomings.
Even when we get all the details related to feature perturbation and background data right, there?s still a fundamental limitation of ML explanation we need to keep in mind. A denied applicant wants to know how to change their credit profile in order to get approved for credit in the future?that?s the question framed by our approved applicant reference distribution. However, we need to exercise some caution even when using a meaningful and context-specific reference distribution. The recourse question, ?What should I change [about my credit profile] in order to receive a favorable outcome in the future?? is fundamentally a causal question?and we?re not working with causal models. To quote the creator of the SHAP package, Scott Lund? berg, ?Be careful when interpreting predictive models in search of causal insights? (https://oreil.ly/mME7V). He goes on to say:
Predictive machine learning models like XGBoost become even more powerful when paired with interpretability tools like SHAP. These tools identify the most informative relationships between the input features and the predicted outcome, which is useful for explaining what the model is doing, getting stakeholder buy-in, and diagnosing potential problems. It is tempting to take this analysis one step further and assume that interpretation tools can also identify what features decision makers should manipulate if they want to change outcomes in the future. However, [?] using predictive models to guide this kind of policy choice can often be misleading.


XGBoost with Constraints and Post Hoc Explanation | 227

For all of their mathematical guarantees and ease-of-use, Shapley-value-based explan? ations are not a magic wand. Instead, they are yet another explainability tool in our toolbox for explaining models. We have to combine our post hoc explainability techniques with intrinsically explainable model architectures such as GLMs, GAMs, or tightly constrained XGBoosts, to achieve true interpretability. And we have to stay humble and remember that ML is all about correlation, and not causation.
Better-Informed Model Selection
To conclude this chapter, let?s return to the PAY_0 feature, and compare how each of the five models we built treat this feature. Remember that PAY_0 represents repay? ment status, where higher values correspond to a greater delay in repayment. Obvi? ously, higher values should correspond to a greater risk of delinquency. However, the training data we used is sparse for higher values of the feature, so we have only a few observations after more than one month?s delay. With that in mind, let?s examine five partial dependence and ICE plots for each model?s treatment of this feature, as shown in Figure 6-13. We need to ask ourselves, ?Which of these models would I trust the most with a billion-dollar lending portfolio??
Three of our models show a response to the spurious dip in the mean target value in the sparse region of the feature space: the GAM, EBM, and unconstrained XGBoost. The GLM and constrained XGBoost models were forced to ignore this phenomenon. Since the GAM and EBM are additive models, we know that the partial dependence and ICE plots are truly representative of their treatment of this feature. The uncon? strained XGBoost model is so full of feature interactions that we cannot be so sure. But the partial dependence does track with ICE, so it?s probably a good indicator of true model behavior. We?d say it?s a choice between the penalized GLM and the con? strained XGBoost model. Which model is the best choice? By using these explainable models and post hoc explainers, we can make a much more deliberate choice than in the traditional opaque ML workflow?and that?s what?s most important. Remember, if we were choosing by pure performance, we?d pick a model that treats our most important feature in a somewhat silly way.
The story that emerges out of this deep dive into explainability in practice is this: first, we understand which relationships between feature and target are truly mean? ingful, and which are noise. Second, if we need to be able to explain our model?s behavior?and we probably do?we need to choose a model architecture that is intrinsically explainable. That way, we?ll have both the model and the explainers avail? able to double-check each other. Third, we must force our model to obey reality with constraints. People are still smarter than computers! Finally, we?ll need to examine our trained model with a diverse set of post hoc explainability techniques such as partial dependence and ICE plots, surrogate models, and SHAP values. Working this way, we can make informed and reasonable model selection choices, and not simply overfit potentially biased and inaccurate training data.

228  |  Chapter 6: Explainable Boosting Machines and Explaining XGBoost



Figure 6-13. Partial dependence and ICE plots for the five models trained in this chapter (digital, color version(: https://oreil.ly/3X2X4))
Resources
Further Reading
? Elements of Statistical Learning (chapters 3, 4, 9, and 10) (https://oreil.ly/S72E1)
Code Examples
? Machine-Learning-for-High-Risk-Applications-Book (https://oreil.ly/machine-learning-high-risk-apps-code)
Explainable Modeling Tools
? arules (https://oreil.ly/bBv9s)
? causalml (https://oreil.ly/XsiMk)
? elasticnet (https://oreil.ly/pBOBN)
? gam (https://oreil.ly/QS0bP)
? glmnet (https://oreil.ly/rMzEl)

Resources | 229

? h2o-3 (https://oreil.ly/PPUk5)
? imodels (https://oreil.ly/coPjR)
? InterpretML (https://oreil.ly/AZYDz)
? PiML (https://oreil.ly/ELrbE)
? quantreg (https://oreil.ly/qBWk9)
? rpart (https://oreil.ly/yIml6)
? RuleFit (https://oreil.ly/K-qc4)
? Rudin Group code (https://oreil.ly/QmRFF)
? sklearn-expertsys  (https://oreil.ly/igFz6)
? skope-rules (https://oreil.ly/nfYau)
? tensorflow/lattice (https://oreil.ly/Z9iCS)
Post Hoc Explanation Tools
? ALEPlot (https://oreil.ly/OSfUT)
? Alibi (https://oreil.ly/K4VEQ)
? anchor (https://oreil.ly/K3UuW)
? DiCE (https://oreil.ly/-lwV4)
? h2o-3 (https://oreil.ly/GtGvK)
? ICEbox (https://oreil.ly/6nl1W)
? iml (https://oreil.ly/x26l9)
? InterpretML (https://oreil.ly/cuevp)
? lime (https://oreil.ly/j5Cqj)
? Model Oriented (https://oreil.ly/7wUMp)
? PiML (https://oreil.ly/CqgSa)
? pdp (https://oreil.ly/PasMQ)
? shapFlex (https://oreil.ly/RADtC)
? vip (https://oreil.ly/YcD2_)









230  |  Chapter 6: Explainable Boosting Machines and Explaining XGBoost



CHAPTER 7

Explaining a PyTorch Image Classifier



Chapter 6 focused on using explainable models and post hoc explanations for models trained on tabular data. In this chapter, we?ll discuss these same concepts in the context of deep learning (DL) models trained on unstructured data, with a particular focus on image data. Code examples for the chapter are available online (https:// oreil.ly/machine-learning-high-risk-apps-code), and remember that Chapter 2 introdu? ces the concepts of explainable models and post hoc explanation.
We?ll begin this chapter with an introduction to the hypothetical use case demon? strated through technical examples in this chapter. Then we?ll proceed much as we did in Chapter 6. First, we?ll present a concept refresher on explainable models and feature attribution methods for deep neural networks?focusing on perturbation? and gradient-based explanation methods. We?ll also continue a thread from Chapter 6 by outlining how explainability techniques can inform model debugging, a topic we?ll expand on even further in Chapters 8 and 9.
Next, we?ll discuss inherently explainable models in more detail. We put forward a short section on explainable DL models in hopes that some readers will be able to build their own explainable models, because as of today, that?s the best hope for truly explainable results. We?ll introduce prototype-based image classification models, like ProtoPNet Digital Mammography (https://oreil.ly/Jht4n)?a promising direction for explainable computer vision. After that, we?ll discuss post hoc explanation tech? niques. We will highlight four methods in detail: occlusions (a common type of perturbation), input * gradient, integrated gradients, and layer-wise relevance propa? gation. We?ll use our hypothetical pneumonia X-ray use case to show the different properties that these methods exhibit, and highlight some important implementation details along the way.




231

Recall from Chapter 2 that an interpretation is a high-level, mean? ingful mental representation that contextualizes a stimulus and leverages human background knowledge, whereas an explanation is a low-level, detailed mental representation that seeks to describe a complex process. Interpretation is a much higher bar than explana? tion and is rarely achieved by technical approaches alone.

How do we know if our post hoc explanations are any good? To address this, we?ll discuss the research on evaluating explanations too. We?ll demonstrate an experi? ment, first described in ?Sanity Checks for Saliency Maps? (https://oreil.ly/64UAi), which will show that many post hoc explanation techniques don?t necessarily reveal much of anything about our model!
We?ll conclude the chapter by summarizing the lessons learned. This chapter will show that readers should be wary to implement a standard DL solution in a high-risk application where model explanations are necessary. Post hoc explanations are often difficult to implement, difficult to interpret, and sometimes entirely meaningless. Furthermore, the wide range of different explanation techniques means that we run the risk of selecting the method that confirms our prior beliefs about how our model should behave. (See discussions of confirmation bias in Chapters 4 and 12.) Even worse than an unexplainable model is an unexplainable model paired with an incorrect model explanation bolstered by confirmation bias.
Explaining Chest X-Ray Classification
We?ll use a working example of a pneumonia image classifier model. We?ll maintain a hypothetical use case of a model prediction and explanation being passed up to an expert user (e.g., a physician) to aid in diagnoses. Figure 7-1 provides a simplified schematic showing how the model is used in conjunction with an explanation engine to aid in the diagnosis of pneumonia by an expert.
Post hoc explanations have a history of accepted usage in the context of consumer credit. The use of model explanations to aid in interpreting medical imaging does not share this history. Moreover, important works are critical of post hoc techniques and use cases just like our hypothetical one, e.g., ?The False Hope of Current Approaches to Explainable Artificial Intelligence in Health Care? (https://oreil.ly/KY6LD) and ?The Doctor Just Won?t Accept That!? (https://oreil.ly/ZOlTk). Even if the use case is justified, ML systems?even carefully developed ones?can perform poorly on out-of-sample data. See ?Deep Learning Predicts Hip Fracture Using Confounding Patient and Healthcare Variables? (https://oreil.ly/V87hi) for an example of a model picking up on correlations in the training data that didn?t work well once deployed. See ?Diagnostic Accuracy and Failure Mode Analysis of a Deep Learning Algorithm for the Detection of Intracranial Hemorrhage? (https://oreil.ly/V4krV) for a somewhat


232  |  Chapter 7: Explaining a PyTorch Image Classifier

similar use case to the one presented in this chapter, but with the addition of an analysis of real-world outcomes. We?ll discuss all these issues in greater detail as we work through some post hoc examples and conclude the chapter.

Figure 7-1. The hypothetical use case maintained throughout this chapter. A model and post hoc explanation engine (a) pass up predictions and explanations to a human- readable dashboard. The information on the dashboard is used to aid in a physician?s diagnosis of pneumonia (b).

Concept Refresher: Explainable Models and Post Hoc Explanation Techniques
In this section, we?ll discuss the basic ideas behind the chapter. We?ll start with explainable models?which we won?t treat with code. Then, in the post hoc explan? ation section of the chapter, we will demonstrate a few techniques on our model, and we will survey many more of the foundational techniques in the field. These techniques can mainly be categorized into two groups: perturbation-based (often occlusion in DL) and gradient-based methods; we will discuss the differences between these categories next. We will also highlight how these techniques can be applied to the problem of model debugging and architecture selection.
Explainable Models Overview
Recall from Chapter 2 that explainable models have inherently explainable structures, characteristics, or results. Also, explainable models exist on a spectrum. Some might be directly explainable to end users, whereas some might only make sense to highly skilled data scientists. Explainable DL models are definitely on the more complex side of the explainability spectrum, but we still think they are very important. As we?ll highlight, and as many other researchers have pointed out, we have to be really



Concept Refresher: Explainable Models and Post Hoc Explanation Techniques  |  233

careful with post hoc explanation in DL. If we have an explainable model, we are able to understand it directly without the use of questionable post hoc techniques, and we can also compare post hoc explanation results to the explainable mechanisms of the model to test and verify the model and the explanations.
Occlusion Methods
Occlusion methods are based on the idea of perturbing, removing, or masking fea? tures and examining the resulting change in model output. In computer vision, this often means obscuring patches of pixels. As discussed in ?Explaining by Removing: A Unified Framework for Model Explanation? (https://oreil.ly/6hGen), many different explanation techniques can be traced back to this idea of feature occlusion.
Occlusion-based techniques can be especially valuable when gradients are not avail? able, or when the model we are trying to explain is a complex decision-making pipeline including ML, business rules, heuristics, and other nondifferentiable com? ponents. Occlusion-based methods all have to grapple with the same complication: for most models, we can?t just remove features and generate model predictions. Put another way, if our model has been trained on features x1, x2, and x3, we can?t simply pass it values for x1 and x2 and expect it to make a prediction. We need to pass in some value for x3. This detail is at the heart of many of the different occlusion-based methods.
Gradient-Based Methods
As discussed in Chapter 2, the gradients of a model?s outcome with respect to its parameters can be used to construct local explanations. This is a generalization of the idea behind interpreting regression coefficients. Remember that a gradient is just a local linear approximation to a complex function: our ML model. Since the vast majority of DL architectures are designed to be trained with gradient-based optimizers, we almost always have access to some gradients in our DL models, and evaluating gradients has become much easier with contemporary DL toolkits. This is part of what makes gradient-based explanation techniques so popular for DL. However, for tree-based models or complex pipelines where taking gradients is not possible, we?ll have to fall back to occlusion.
Explanations in this category fundamentally ask: ?Which features, if we change them a little bit, result in the largest change in our model?s output?? Researchers have developed many variations on this theme to tease out subtly different flavors of explanations. We?ll cover the details behind these techniques later in the chap? ter, looking into input * gradient, integrated gradients, and layer-wise relevance propagation.



234  |  Chapter 7: Explaining a PyTorch Image Classifier

Explainable AI for Model Debugging
In Chapter 6, we saw how model explanation techniques such as partial dependence and ICE plots can reveal undesirable model behavior, such as sensitivity to spurious noise in the training data. Explanations can serve the same purpose for DL models, and that may be the highest purpose of explainable artificial intelligence (XAI) in DL to date. The ability of DL explainability techniques to help debug and improve models has been noted by prominent researchers on many occassions. The most famous example may be the classic Google blog post (https://oreil.ly/5Qj0O) that popularized neural network ?dreams.? The authors use the technique from ?Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps? (https://oreil.ly/5BqAj) to debug their model by asking it to show them its concept of a dumbbell:
There are dumbbells in there alright, but it seems no picture of a dumbbell is complete without a muscular weightlifter there to lift them. In this case, the network failed to completely distill the essence of a dumbbell. Maybe it?s never been shown a dumbbell without an arm holding it. Visualization can help us correct these kinds of training mishaps.
Some additional examples of explanations as debugging tools in DL literature include the following:
? ?Visualizing Higher-Layer Features of a Deep Network? (https://oreil.ly/vIG4Y)
? ?Visualizing and Understanding Convolutional Networks? (https://oreil.ly/aEkYG)
? ?Deconvolutional Networks? (https://oreil.ly/NmiDE)
We call these resources to readers? attention for two reasons. First, although the popular DL explanation techniques we?ll explore in this chapter may not always work, some of the other techniques in these papers are definitely worth checking out. Second, while explanation techniques may let us down when it comes to precise understanding, they can still hint at issues in our models. Think about this as you?re reading this chapter. In Chapter 9, we?re going to take a deep dive into how to apply benchmarking, sensitivity analysis, and residual analysis for debugging our DL models.
Explainable Models
We?ll begin the technical portion of this chapter by discussing explainable models for DL applications, because they are our current best bet for explainable results in DL. But they are not easy to work with yet. Readers will learn that there are few off-the-shelf explainable DL models that we can plug into our application. Contrast

Explainable Models | 235

this with what we saw in Chapter 6, where we were able to choose from a wealth of highly explainable architectures, from monotonic XGBoost, to explainable boosting machines, to generalized linear models (GLMs) and generalized additive models (GAMs). So what is the difference? Why are there so few ready-made explainable DL models? One issue is that explainable models for structured data date back to the work of Gauss in the 1800s?not so with DL. But there is more going on.
When we train a DL model on unstructured data, we?re really asking the model to perform two functions: the first is feature extraction to create a latent space representation, or learning the proper representation of the data in a (usually) lower- dimensional input space. Second, it must use that latent space representation to make predictions. Contrast this with the tabular data we analyzed in Chapter 6. With tabular data, the ?correct? representation of our data is typically assumed to be right there in the training data, especially if we?ve done our job correctly and chosen a reasonable set of uncorrelated features with a known causal relationship to the target. This difference (learned versus already-supplied features) helps to highlight why explainable DL models are so difficult to develop, and why off-the-shelf implementations are hard to come by.
The common thread among the explainable DL architectures that do exist today is that they intervene on this feature-learning directly. Oftentimes, explainable models shift the burden of feature engineering away from the model and onto the model developer. This increased burden is both a blessing and a curse. On the one hand, it means that training these models is simply more work than training unexplainable models. On the other hand, these architectures can demand higher-quality, some? times expertly annotated data, meaning that humans were deeply involved in the modeling process end-to-end. Since we should already be designing our models with the care that these architectures demand, this isn?t bad. As we saw in Chapter 6, the more domain expertise we can encode in our models, the more trust we can place on their ability to perform in the real world.
In the following sections, we?ll discuss different architectures for explainable DL models while maintaining our primary focus on the image classification problem. We?ll pay special attention to recent developments on prototype-based architec? tures, which provide the likeliest path forward for true turnkey explainable image classification.
ProtoPNet and Variants
In their 2019 paper ?This Looks Like That: Deep Learning for Interpretable Image Recognition? (https://oreil.ly/k69Dx), the Duke team led by Prof. Cynthia Rudin introduced a new, promising architecture for explainable image classification. The new model is called ProtoPNet, and it is based on the concept of prototypes.



236  |  Chapter 7: Explaining a PyTorch Image Classifier

Remember from Chapter 2 that prototypes are data points that are representative of a larger group of observations. Consider explaining why an observation was grouped into a particular cluster in k-means clustering. We could put the observation side-by-side with the cluster centroid and say, ?Well, this observation looks like that cluster center.? ProtoPNet generates explanations of exactly this type. Furthermore, ProtoPNet?s explanations are faithful to how the model actually makes predictions. So how does it work?
First, ProtoPNet identifies prototypical patches for each class. These patches capture the fundamental properties that distinguish one class from another. Then, to make predictions, the model looks for patches on the input image that are similar to the prototypes for a particular class. The resulting similarity scores for each prototype are added together to produce the odds of the input belonging to each class. The final result is a model that is additive (each prediction is a sum of similarity scores across prototypical image parts) as well as sparse (there are only a few prototypes per class). Best of all, each prediction comes immediately with a faithful explanation.

ProtoPNet builds on the idea of part-level attention models, a broad class of explainable deep neural networks that are not cov? ered in this chapter. The difference between these models and ProtoPNet is faithfulness. ProtoPNet really makes predictions by summing similarity scores with certain patches of each image and certain class prototypes. Other part-level attention models make no such guarantees.

Since being published in 2019, this promising direction for explainable image classi? fication has been picked up by other researchers. There?s ProtoPShare (https://oreil.ly/ phO4I), which allows for sharing prototypes between classes, resulting in a smaller number of prototypes. There is also ProtoTree (https://oreil.ly/OKEp0), which creates an explainable decision tree over prototype features. This architecture allows the model to mimic human reasoning even more clearly. Finally, Kim et al. analyzed chest X-rays using an architecture very similar to ProtoPNet in ?XProtoNet: Diagno? sis in Chest Radiography with Global and Local Explanations? (https://oreil.ly/qv-oO).
Other Explainable Deep Learning Models
In ?Towards Robust Interpretability with Self-Explaining Neural Networks? (https:// oreil.ly/DWY6w), authors David Alvarez-Melis and Tommi S. Jaakkola introduce self-explaining neural networks (SENN). As we discussed, one difficulty with creating explainable models on unstructured data is that we ask our models to create a latent space representation of our data and to make predictions. Self-explaining neural net? works confront this difficulty by introducing interpretable basis concepts in lieu of raw features. These basis concepts can be learned as part of model training, taken from


Explainable Models | 237

representative observations in the training data, or?ideally?designed by domain experts. In their paper, Alvarez-Melis and Jaakkola generate these interpretable basis concepts using an autoencoder, and ensure that the learned concepts are explainable by providing prototypical observations that maximally express the concept.

An autoencoder is a type of neural network that learns to extract features from training data without making predictions on a single modeling target. Autoencoders are great for data visualization and anomaly detection too.

So far, we?ve mostly focused on techniques for computer vision models. Explainable models have been developed for deep neural nets used in reinforcement learning (https://oreil.ly/PgthB), visual reasoning (https://oreil.ly/wRYFJ), tabular data (https:// oreil.ly/My88p), and time series forecasting (https://oreil.ly/qUUzF).
Training and Explaining a PyTorch Image Classifier
In this use case, we outline how we trained an image classifier, then we demon? strate how to generate explanations using four different techniques: occlusion, input
* gradient, integrated gradients, and layer-wise relevance propagation.
Training Data
First, we need to build an image classifier to diagnose chest X-ray images, consistent with the hypothetical use case in Figure 7-1. The dataset we?ll use for training is available from Kaggle (https://oreil.ly/jfmsi), and it consists of 5,863 X-ray images of patients, which have been split into two distinct categories?one containing pneumo? nia and the other being normal. Figure 7-2 shows a random collection of images from the training data.

Figure 7-2. A random selection of training set samples from the Kaggle chest X-ray data; chest X-rays with pneumonia are cloudier than those without it


238  |  Chapter 7: Explaining a PyTorch Image Classifier

We?re not doctors or radiologists, and it?s important to acknowledge that the author group does not have the medical domain knowledge to truly validate this model. From our understanding, images from pneumonia patients should show cloudy areas of infection. Bacterial pneumonia and viral pneumonia tend to have different visual characteristics as well. The hope in the sections that follow is that XAI methods will focus on these cloudy areas, enabling us to understand why an image is classified as pneumonia versus normal. (Prepare to be disappointed.) To learn more about the dataset, see its Kaggle page (https://oreil.ly/hAhUz) and the associated paper, ?Iden? tifying Medical Diagnoses and Treatable Diseases by Image-Based Deep Learning? (https://oreil.ly/SOcBD).

If we?re working in a high-risk application area with machine learn? ing, we need domain expertise to help train and validate our mod? els. Failure to consult with domain experts may result in harmful, nonsense models deployed in high-risk use cases.

Like most datasets from Kaggle (https://oreil.ly/lOADp), a lot of the hard work of curating the data has been done for us. Low-quality scans have been eliminated, and the labels have been verified as correct. However, like many datasets in medical applications, this data has a class imbalance problem: there are 1,342 normal scans, but 3,876 scans labeled as indicating pneumonia. Another cause for concern is the presence of very few images in the given validation dataset. The validation data consists of only nine images for the pneumonia class and another nine for the normal class. This is not a sufficient number to adequately validate the model, so we?ll address that and other issues before proceeding with model training.
Addressing the Dataset Imbalance Problem
In our training data, the pneumonia X-ray scans outnumber the normal scans three to one. Any model trained on such a dataset might overfit to the majority class. There are several ways to address the issue of a class imbalance problem:
? Oversampling of the minority class
? Undersampling of the majority class
? Modifying the loss function to weight the majority and minority classes differently
These techniques, and the detrimental effects of the class imbalance problem, have been nicely summarized in a paper titled ?A Systematic Study of the Class Imbalance Problem in Convolutional Neural Networks? (https://oreil.ly/Gp-OY). In this example, we will oversample the normal images to even out the class imbalance.



Training and Explaining a PyTorch Image Classifier |  239

Data Augmentation and Image Cropping
PyTorch (https://oreil.ly/Uagd2) is an open source ML framework. torchvision (https://oreil.ly/LaOh8) is a domain library for PyTorch built to support research and experimentation for computer vision. torchvision consists of some popular datasets, pretrained model architectures, and some image transformations for computer vision tasks. We?ll first increase the proportion of the validation set by moving some of the training set images into it. After this, we?ll use some of torchvision?s image transformations to handle the class imbalance in our training set. In the following code snippet, we scale images to the same size, then apply various transformations to increase the size of data and to introduce training examples that should enhance the robustness of our model. The get_augmented_data function makes ample use of the RandomRotation and RandomAffine transformations to create new, altered training images, along with using various other transformations to format and normalize images:
TRAIN_DIR = 'chest_xray_preprocessed/train'
IMAGE_SIZE = 224 # Image size of resize when applying transforms BATCH_SIZE = 32
NUM_WORKERS = 4 # Number of parallel processes for data preparation

def get_augmented_data():

sample1 = ImageFolder(TRAIN_DIR,
transform =\ transforms.Compose([transforms.Resize((224,224)),
transforms.RandomRotation(10), transforms.RandomGrayscale(), transforms.RandomAffine(
translate=(0.05,0.05), degrees=0),
transforms.ToTensor(), transforms.Normalize(
[0.485, 0.456, 0.406],
[0.229, 0.224, 0.225]),
]))
...

return train_dataset
Since the basic idea of data augmentation is to create more images, let?s check that outcome:
# check new dataset size
print(f'Normal : {normal} and Pneumonia : {pneumonia}')

(3516, 3758)
Figure 7-3 shows some of the synthetic training samples generated using rotations and translations.

240  |  Chapter 7: Explaining a PyTorch Image Classifier



Figure 7-3. Synthetic training samples generated using rotations and translations
Looks about right. With class imbalance and data augmentation handled, we?ll pro? ceed with model training.

Make sure data augmentation does not create unrealistic training samples. Chest X-rays will show variation in color scale, zooming, etc. However, flipping the images about their vertical axis would be a huge mistake, since our organs are bilaterally asymmetric (not the same on the left and right sides). After deployment, this model will never see a chest X-ray with the patient?s heart on their right side, so it should not be trained on vertically flipped images.

Another preprocessing technique that we used for the dataset is image cropping. We cropped some of the images in the training set so as to highlight only the lung region (see Figure 7-4). Cropping helps to eliminate any annotation or other kinds of markings on the chest X-ray images and focuses the model on the region of interest in the images. We saved these images as a separate dataset to be used to fine-tune the network at a later stage of training.

It wasn?t until we went through the exercise of manually cropping hundreds of images that we noticed that the training data contains multiple scans from the same patient. As a result, when we added images to our validation data we had to be sure to keep the same patient in either training or validation data, but not both. This detail is a great example of a data leak, and emphasizes the importance of really getting to know our data.


Training and Explaining a PyTorch Image Classifier |  241



Figure 7-4. A chest X-ray image after before and after cropping
Model Training
Convolutional neural networks (CNNs) (https://oreil.ly/bfzDc) are commonly used architectures employed in medical imaging. Some well-known examples of CNNs for image classification are ResNets (https://oreil.ly/2JqoN), DenseNets (https://oreil.ly/ T3b0q), and EfficientNets (https://oreil.ly/-AWuI). Training a CNN from scratch is very expensive, both in terms of data and computation time. As such, a prevalent technique is to use models that have been previously trained on a large-scale image dataset?such as ImageNet (https://oreil.ly/8MHlU)?and then reuse that network as a starting point for another task.
The core idea behind this technique is that the lower layers in a CNN learn broadly applicable representations like edges and corners, which can be generalized to a wide variety of tasks. When we use layers of a CNN for our own tasks, we refer to them as pretrained. The higher layers, on the other hand, capture features that are more high-level and specific to a task. As a result, the output of these layers will not be suitable for our use case. We can thus freeze the features learned in the lower layers and retrain the higher layers in a step called fine-tuning. Together, pretraining and fine-tuning constitute a simple form of transfer learning (https://oreil.ly/tybad), in which knowledge learned by an ML model in one domain is used in another domain.
To start, we?ll use a DenseNet-121 (https://oreil.ly/Wq743) architecture trained on an ImageNet dataset. DenseNet models have been shown to perform particularly well for X-ray image classification, as they improve flow of information and gradients through the network (https://oreil.ly/_fO24), ideally increasing the performance and generalization of the classifier.





242  |  Chapter 7: Explaining a PyTorch Image Classifier

Don?t forget about EvilModel (https://oreil.ly/UMwPx). It has been shown that malware can be delivered through pretrained neural networks. Such malware may not affect performance and may trick anti-virus software. (Or we might be lazy and forget to scan our model artifacts.) The lessons from Chapter 5 teach us to take noth? ing for granted, not even the pretrained models we download from the internet.

While performing transfer learning, an important question is whether to retrain all the layers of the pretrained model or only a few of them. The answer to this question lies in the makeup of the dataset. Is the new dataset large enough? Does it resemble the dataset on which the pretrained model has been trained? Since our dataset is small and differs considerably from the original dataset, it makes sense to retrain some of the lower layers as well as the higher ones. This is because the low layers learn generic features compared to the higher ones, which learn more dataset-specific features. In this case, when we refer to retraining layers, it doesn?t mean to start training from scratch or with random weights; instead, we?ll utilize pretrained weights as a starting point and then continue from there.
In the following code, we unfreeze all of the layers of the pretrained model and replace the last layer with our own linear classifier. For this dataset, this setting gave the best performance on the test data. We also experimented by unfreezing only a few layers, but none of them outperformed our first setting:
classes = ['Normal', 'Pneumonia']
model = torchvision.models.densenet121(pretrained=True)

# Unfreeze training for all "features" layers for param in model.parameters():
param.requires_grad = True

# New layers automatically have requires_grad = True in_features = model.classifier.in_features model.classifier = nn.Linear(in_features, len(classes))
Finally, we fine-tuned the model a second time, using only the images that we manually cropped to focus on the lungs. The double fine-tuning process ended up looking something like this:
1. Load the pretrained DenseNet-121 model.
2. Train the model on the augmented dataset using uncropped images.
3. Freeze the early layers of the model and continue training on cropped images.





Training and Explaining a PyTorch Image Classifier |  243

The idea behind this double fine-tuning process is to utilize the features learned by the pretrained model, as well as those in our domain-specific dataset. Finally, the use of cropped images for the final round of training mitigates the risk of the model using features that will not generalize to unseen data, such as X-ray artifacts outside of the lungs.
Evaluation and Metrics
The performance of the model is evaluated on a validation set. In Tables 7-1 and 7-2, we also report some performance metrics on the unseen test dataset. Measuring this performance is essential to understanding whether our model will generalize well.
Table 7-1. A confusion matrix showing the pneumonia classifier model performance on the test dataset
		Predicted normal Predicted pneumonia  Actual normal	199	35
Actual pneumonia  11	379

Table 7-2. Additional performance metrics on the test dataset
	Prevalence Precision Recall F1 
Normal	234
0.95
0.85
0.90
Pneumonia 390
0.92
0.97
0.94
Performance looks good here, but be sure to check out Chapter 9 to see how in silico validation and test measurements can be misleading. Now it?s time to start explaining our model?s predictions.
Generating Post Hoc Explanations Using Captum
In this section, we?ll elaborate on a few post hoc techniques, and show their applica? tion to our pneumonia image classifier. The explanations that are generated are all local, in that they apply to individual observations?single X-ray images of a patient. Furthermore, all of the explanations will take the form of a heatmap, where the color of each pixel is meant to be proportional to the significance of that pixel in making the final classification. In the coming sections, we?ll examine with a critical eye whether these methods accomplish that aim, but the purpose of this section is to first simply show what kinds of outputs can be expected from various techniques.






244  |  Chapter 7: Explaining a PyTorch Image Classifier

To implement the different techniques, we?ll use Captum (https://oreil.ly/RjBoD). Captum is a model explanation library built on PyTorch, and it supports many models out-of-the-box. It offers implementations of many explanation algorithms that work nicely with various PyTorch models.
Occlusion
Occlusion (https://oreil.ly/rdX1o) is a perturbation-based method and works on a simple idea: remove a particular input feature from a model and assess the difference in the model?s prediction capability before and after the removal. A more significant difference implies that the feature is important, and vice versa. Occlusion involves replacing certain portions of the input image and examining the effect on the model?s output. It is often implemented by sliding a rectangular window of predefined size and stride over the image. The window is then replaced with a baseline value (usually zero) at each location, resulting in a gray patch. As we slide this gray patch around the image, we are occluding parts of the image and checking how confident or accurate the model is in making predictions on the altered data.
Captum documentation describes its implementation of occlusion (https://oreil.ly/ R5C2N), and we apply it to the chest X-ray case study for a single input image. Notice how we can specify the size of the occluding window as well as that of the stride, which in our case is 15 ? 15 and 8, respectively:
import captum, Occlusion
from captum.attr import visualization as viz occlusion = Occlusion(model)
attributions_occ = occlusion.attribute(input,
target=pred_label_idx, strides=(3, 8, 8),
sliding_window_shapes=(3,15, 15), baselines=0)
In Figure 7-5, we show the attribution for an image in the test set that demonstrates pneumonia, and was correctly classified as Pneumonia by the model.
The results are promising. The model seems to have picked up on the high opacity in the upper regions of both lungs. This might give an expert interpreter of the explanation faith in the model?s classification. However, the dark regions are large and lack detail, suggesting that a smaller occlusion window and stride length might reveal more detail. (We hesitated to try different settings, because as soon as we go down the path of tuning these explanation hyperparameters, we open ourselves up to the risk that we?ll just select values that create explanations that confirm our prior beliefs about how the model works.)



Training and Explaining a PyTorch Image Classifier |  245



Figure 7-5. Occlusion heatmap for a pneumonia X-ray image in the test set, correctly predicted to show pneumonia
In addition to this tuning concern, the explanation also shows that the model cares about some groups of pixels outside of the bounds of the torso. Is this a problem with our model, perhaps suggestive of overfitting; a data leak between training and test data; or shortcut learning (https://oreil.ly/xv-OQ)? Or is this an artifact of the explanation technique itself? We?re just left to wonder. Let?s see if gradient-based methods provide more clarity.

Shortcut learning is a common issue in complex models and can ruin our real-world results. It happens when a model learns about something easier than the actual prediction target in an ML task?essentially cheating itself during training. As medical diagnosis from images can be a painstaking task, even for highly experienced human practitioners, ML systems often find shortcuts that help them optimize their loss functions in training data. When those learned shortcuts are not available in real-world diag? nostic scenarios, these models fail. To read more about shortcut learning in medical images, check out ?Deep Learning Applied to Chest X-Rays: Exploiting and Preventing Shortcuts? (https:// oreil.ly/oVT-G). To learn about this serious issue, which plagues nearly all unexplainable ML, in a general context, check out ?Short? cut Learning in Deep Neural Networks? (https://oreil.ly/ogNeg).





246  |  Chapter 7: Explaining a PyTorch Image Classifier

Input * gradient
The first gradient-based method we?ll look at is the input * gradient technique. As its name suggests, input * gradient creates a local feature attribution that is equal to the gradient of the prediction with respect to the input, multiplied by the input value itself. Why? Imagine a linear model. The product of the gradient and the input value assigns a local feature attribution that is equivalent to the feature value multiplied by the feature?s coefficient, which corresponds to the feature?s contribution to a particular prediction.
We use Captum to generate a heatmap for the same test set image using the input
* gradient technique this time. In Figure 7-6, we?re showing the positive evidence for the classification.

Figure 7-6. Input * gradient heatmap for a pneumonia X-ray image in the test set, correctly predicted to show pneumonia
What to say about this output? Just like with occlusions, we can squint at the image and argue that it is saying something meaningful. In particular, there is a dark patch of high evidence in the lungs that seems to correspond to high opacity in the chest X-ray on the left. This is the behavior we would expect and hope for out of our pneumonia classifier. However, the explanation technique is also suggesting that the length of the patient?s spine contains regions of high evidence of pneumonia. We?re left with the same questions that we posed earlier. Is this telling us that the model is focusing on the wrong things, i.e., shortcuts? That would make this a useful output for model debugging. On the other hand, we still don?t know if the explanation technique itself is the source of the unintuitive result.



Training and Explaining a PyTorch Image Classifier |  247

Integrated gradients
Integrated gradients (https://oreil.ly/Er6tk) is the first technique we?ll consider that comes with some theoretical guarantees. Gradients alone can be decieving, especially because gradients tend toward zero for some high-confidence predictions. For high probability outcomes, it?s not uncommon that the input features cause activation functions to reach high values, where gradients become saturated, flat, and close to zero. This means that some of our most important activation functions for a decision won?t show up if we only look at gradients.
Integrated gradients attempts to fix this issue by measuring a feature impact relative to a baseline value, across all possible input pixel intensity values. In particular, integrated gradients asks, ?How does the gradient change as we traverse a path from the baseline input pixel intensity value to a larger input pixel intensity value?? The final feature attribution is the approximate integral of the gradient along this smooth path of pixel values as a function of the model?s predictions.
Integrated gradients satisfies axioms of sensitivity and implementation invariance. Sensitivity means that if the input image and baseline image differ only along one feature, and if they return different model outputs, then integrated gradients will return a nonzero attribution for that feature. Implementation invariance says that if two models, possibly with different internal structures, return the same output for all inputs, then all of the input attributions returned by integrated gradients will be equal. Implementation invariance is another way to discuss consistency from Chapter 2. For a good walk-through of the topic, check out the TensorFlow introduc? tion to integrated gradients (https://oreil.ly/2aUWD).
In Figure 7-7, we show the output of this attribution technique on the same pneumo? nitic image we have been considering. Like the output of input * gradient, the image is noisy and difficult to interpret. It looks like the method is picking up on edges in the input image. In addition to the X-ray machine markings and armpits, we can also see the faint outline of the patient?s ribs in the heatmap. Is this because the model is ignoring the ribs, and looking around them into the lungs? It seems like these outputs are raising more questions than they?re answering. Later in this chapter, we?ll critically evaluate these explanatory outputs by conducting some experiments. For now, we?ll turn to the fourth and final technique, another gradient-based method, layer-wise relevance propagation.









248  |  Chapter 7: Explaining a PyTorch Image Classifier



Figure 7-7. Integrated gradients heatmap for a pneumonia X-ray image in the test set, correctly predicted to show pneumonia
Layer-wise Relevance Propagation
Layer-wise Relevance Propagation (LRP) (https://oreil.ly/xGtUm) is really a class of methods that measure a feature?s relevance to the output. Broadly speaking, relevance is the strength of the connection between the input features and the model output, and it can be measured without making any changes to the input features. By choos? ing a different notion of relevance, we can arrive at a number of different explanatory outputs. For readers interested in a more thorough treatment of LRP, we direct you to the chapter ?Layer-Wise Relevance Propagation: An Overview? in Explainable AI: Interpreting, Explaining and Visualizing Deep Learning by Samek et al. (Springer Cham). There, you?ll find a comprehensive discussion of the different relevance rules and when they should be applied. The Explainable AI Demos dashboard (https:// oreil.ly/wcIVJ) also allows you to generate explanatory outputs using a wide array of LRP rules.
The nice thing about LRP is that the explanations it produces are locally accurate: the sum of the relevance scores is equal to the model output. It is similar to Shapley values in this way. Let?s take a look at the LRP explanation for our test set image in Figure 7-8. Unfortunately, it?s still leaving a lot to be desired in terms of generating a human-verifiable explanation.






Training and Explaining a PyTorch Image Classifier |  249



Figure 7-8. LRP heatmap for a pneumonia X-ray image in the test set, correctly predicted to show pneumonia
Like the other techniques, LRP has picked up on this region of higher opacity in the right lung, but it has also given a high attribution score to regions outside of the lungs.
Evaluating Model Explanations
In the previous sections, we?ve just scraped the surface of explanatory attribution methods for DL models. There is a large and growing number of these techniques. In this section, we?ll address the important question: ?How do we know if our explanation is any good?? Then, we?ll conduct an experiment to critically examine just how much information post hoc techniques give us about our CNN model.
David Alvarez-Melis and Tommi S. Jaakkola give the question of evaluating explan? ations an excellent treatment in their two 2018 papers ?On the Robustness of Inter? pretability Methods? (https://oreil.ly/KRcmm) and ?Towards Robust Interpretability with Self-Explaining Neural Networks? (https://oreil.ly/gUWIR). In the second paper, Alvarez-Melis and Jaakkola introduce three desirable properties that explanations should share:







250  |  Chapter 7: Explaining a PyTorch Image Classifier


? Explanations should be understandable (explicitness/intelligibility).
? They should be indicative of true importance (faithfulness).
? They should be insensitive to small changes in the input (stability/robustness).
The heatmap techniques we have examined in this chapter clearly fall short on point number one. First of all, the outputs of these techniques are noisy and confusing. More importantly, all of the techniques we surveyed seem to point to nonsensical regions, such as spaces outside of the patient?s body.
Even if the outputs perfectly aligned with our intuition, these heatmaps only give an indication of where the model is finding positive or negative evidence for its classification; they give no suggestion of how the model is making its decision based on the information it?s been provided with. This is in contrast to explainable models such as SENN or ProtoPNet, which provide both?prototypes or basis concepts are the where, and their linear combination is how. How is a crucial element of a good explanation.

We should always test our explanation methods in high-risk appli? cations. Ideally, we should be comparing post hoc explanations to underlying explainable model mechanisms. For more standard DL approaches, we can use the following:
? Domain experts and user studies to test intelligibility
? Removal of features deemed important, nearest-neighbor approaches, or label shuffling to test faithfulness
? Perturbation of input features to test robustness
Use ?Towards Robust Interpretability with Self-Explaining Neural Networks? (https://oreil.ly/PtR5u) and ?Evaluating the Visualization of What a Deep Neural Network Has Learned? (https://oreil.ly/ sQDv5) as references.

Faithfulness is typically tested by obscuring or removing features deemed important and calculating the resulting change in classifier output to gauge the robustness of the explanatory values themselves. Alvarez-Melis and Jaakkola showed that there is a wide range of explanation faithfulness across different techniques and datasets, with Shapley additive explanations and some others performing quite poorly. We can also use nearest-neighbor approaches, where similar input observation should have similar explanations, to gauge faithfulness. We?re also going to examine faithfulness in our explanations in the next section?but we?re going to try a different approach.




Training and Explaining a PyTorch Image Classifier |  251

To examine robustness (or stability), Alvarez-Melis and Jaakkola perturb the input image slightly and measure the resulting change in explanation output. Armed with a quantitative metric, they compare many post hoc methods across multiple datasets. It turns out that most of the post hoc techniques they studied are unstable to small changes in the input. Local interpretable model-agnostic explanations (LIME) per? form especially poorly, and the methods of integrated gradients and occlusion show the best robustness among the techniques they studied. Across all of these evaluation dimensions?intelligibility, faithfulness, and robustness?explainable models such as self-explaining neural networks outperfom post hoc techniques.
The Robustness of Post Hoc Explanations
In this section, we reproduce (in part) the damning unfaithfulness results from ?Sanity Checks for Saliency Maps? (https://oreil.ly/v6qlw). In that paper, the authors were interested in the question: ?Are the outputs generated by these post hoc explan? ation methods actually telling us anything about the model?? As we?ll see in this experiment, sometimes the result is an emphatic no.
To begin our experiment, we train a nonsense model, wherein images have random labels. In Figure 7-9, we can see the high training loss and poor accuracy curves for a new model that has been trained on a dataset where the image labels have been randomly shuffled. For this experiment, we did not conduct any data augmentation to handle the class imbalance. This explains why the accuracy on the validation data converges to a value larger than 0.5?the model is biased toward the majority class (pneumonia).
Now we have a model that has been trained on nonsense labels. The predictions generated by our new model cannot be any better than a (weighted) coin flip. For the original explanations to be meaningful, we?d hope that these explanations don?t pick up on the same signals. Figures 7-10, 7-11, and 7-12 show explanations for our test set image, generated on the model that has been trained on randomly shuffled data, created by input * gradient, integrated gradients, and occlusion, respectively.












252  |  Chapter 7: Explaining a PyTorch Image Classifier



Figure 7-9. Model performance during training on data where the labels have been randomly shuffled (digital, color version(: https://oreil.ly/-uCIY))

Training and Explaining a PyTorch Image Classifier |  253



Figure 7-10. Input * gradient heatmap after randomly shuffling class labels

Figure 7-11. Integrated gradients heatmap after randomly shuffling class labels








254  |  Chapter 7: Explaining a PyTorch Image Classifier



Figure 7-12. Occlusion heatmap after randomly shuffling class labels
In our opinion, these results look shockingly similar to the ones from the previous sections. In all of the images, the techniques have once again highlighted irrelevant regions of the image, such as the patient?s spine and the boundaries of their torso. Worse than this, the attribution maps look very similar to their previous results inside of the lungs. They?ve picked up on the outlines of the patient?s ribs and the regions of higher opacity. Previously, we were interpreting this to mean that the model might be generating its pneumonia prediction based on specific regions of lung inflammation. This experiment shows, however, that these methods will show the same explanation for a model that was trained on zero meaningful signals. To what are our explanations faithful?! We?re not sure.
To further examine the robustness of our explanations, we conduct a simple experi? ment of adding random noise to the input images. This can be easily done using a custom transformation in torchvision. We then examine the explanations on these inputs and compare them with the previous explanations. The amount of noise is regulated in such a way that the predicted class of the image remains the same before and after adding the noise component.
What we really want to understand is whether the resulting explanations are robust to the addition of random noise or not. In short, it?s a mixed bag; see Figures 7-13, 7-14, and 7-15. The new attribution maps differ significantly from those generated on the original model, but do seem to preserve the focus on regions of high opacity inside the lungs.




Training and Explaining a PyTorch Image Classifier |  255



Figure 7-13. Input * gradient heatmap after adding random noise

Figure 7-14. Integrated gradients heatmap after adding random noise










256  |  Chapter 7: Explaining a PyTorch Image Classifier



Figure 7-15. Occlusion heatmap after adding random noise
Look at the occlusion heatmap in Figure 7-15, for example. Previously, we said that it was encouraging that occlusion seems to have picked up on the regions inside the lungs with higher opacity. After adding random noise, we still see this focus on the upper left and upper right of the lungs. However, the addition of noise has upset the occlusion output to give greater evidence to regions near the neck. The gradient-based technique outputs are similarly disturbed, while still preserving their emphasis on the middle of the right lung.
In ?Sanity Checks for Saliency Maps? (https://oreil.ly/fTeRb), the authors point to a possible explanation for the results we?ve seen in these experiments: the attribution techniques are effectively performing edge detection. That is, irrespective of the model training and architecture, these attribution methods are capable of detecting edges in the input image, where gradients nearly always exhibit steep changes. That would explain the emphasis on the rib outlines that we have been observing, as well as the emphasis on regions on the boundary of the torso. In case it?s not clear, detecting edges is not model explanation and can be done easily without using DL.










Training and Explaining a PyTorch Image Classifier |  257

Conclusion
The bottom line is that post hoc explanations are often difficult to explain, and sometimes meaningless. Even worse, the diversity of explanation techniques means that if we?re not careful, we?ll fall prey to confirmation bias and end up selecting the one that confirms our prior belief about how our model should behave. We sympathize?building explainable models in a DL context is very difficult. But this chapter shows that post hoc explanations may only offer a dangerous illusion of understanding, and thus are not always suitable for explaining high-risk decisions.
We suggest not relying solely on post hoc techniques to explain DL models in a high-risk application. At best, these techniques can be useful model debugging tools. We?ll cover that topic in more detail in Chapter 9. Instead, we should try hard to use explainable models when we need explanations that are faithful, robust, and intelligible. We can always build on that more robust model-based explainability with post hoc explanation visualizations if the need arises later, and we?ll be able to check post hoc visualizations against the underlying model mechanisms.
There are encouraging frontiers in explainable DL models for image classifica? tion and other tasks. Prototype-based case-reasoning models such as ProtoPNet (https://oreil.ly/yjIuQ) and sparse additive deep models such as SENN (https://oreil.ly/ yZHHT) provide a path forward for explainable DL. However, explainable models are not yet widely available out of the box for DL applications. They often place greater demands on our data and our modeling expertise. We encourage readers to think of this as a feature, not a bug. The development of AI systems should demand high-quality, expertly curated data. Models should be problem-specific, and encode maximal domain knowledge.
We agree with the authors of ?The False Hope of Current Approaches to Explainable Artificial Intelligence in Health Care? (https://oreil.ly/-w598) when they say:
In the absence of suitable explainability methods, we advocate for rigorous internal and external validation of AI models as a more direct means of achieving the goals often associated with explainability.
In the next two chapters, we?ll build on the techniques we?ve been discussing here and in Chapter 6 to address the broader question of model debugging.









258  |  Chapter 7: Explaining a PyTorch Image Classifier

Resources
Code Examples
? Machine-Learning-for-High-Risk-Applications-Book (https://oreil.ly/machine-learning-high-risk-apps-code)
Transparency in Deep Learning Tools
? AllenNLP Interpret (https://oreil.ly/_tAvm)
? Aletheia (https://oreil.ly/UMfWK)
? Captum (https://oreil.ly/F5Obo)
? cleverhans (https://oreil.ly/efN16)
? DeepExplain (https://oreil.ly/u4Mfu)
? deeplift (https://oreil.ly/S29jk)
? deep-visualization-toolbox (https://oreil.ly/ZH3JU)
? foolbox (https://oreil.ly/DFSu0)
? L2X (https://oreil.ly/S2Ppj)
? tensorflow/lattice (https://oreil.ly/M7aYY)
? lrp_toolbox (https://oreil.ly/kKk09)
? tensorflow/model-analysis (https://oreil.ly/5Aeqe)
? ProtoPNet (https://oreil.ly/ZmqWq)
? tensorflow/tcav (https://oreil.ly/7RvqS)





CHAPTER 8

Selecting and Debugging XGBoost Models



The ways that data scientists measure a model?s real-world performance are usually inadequate. According to ?Underspecification Presents Challenges for Credibility in Modern Machine Learning? (https://oreil.ly/27jFT), penned by 40 researchers at Google and other leading machine learning research institutions, ?ML models often exhibit unexpectedly poor behavior when they are deployed in real-world domains.? A fundamental issue is that we measure performance like we?re writing research papers, no matter how complex and high-risk the deployment scenario. Test data measurements like accuracy or area under the curve (AUC) don?t tell us much about fairness, privacy, security, or stability. These simple measurements of prediction quality or error on static test sets are not informative enough for risk management. They are only correlated with real-world performance, and don?t guarantee good performance in deployment. Put plainly, we should be more concerned with in vivo performance and risk management than in silico test data performance, because a primary thrust of the applied practice of ML is to make good decisions in the real world.
This chapter will introduce several methods that go beyond traditional model assess? ment to select models that generalize better, and that push models to their limits to find hidden problems and failure modes. The chapter starts with a concept refresher, puts forward an enhanced process for model selection, and then focuses on model debugging exercises that better simulate real-world stresses, along with sensitivity analysis and tests that uncover model errors using residual analysis. The overarching goal of model debugging is to increase trust in model performance in the real world, but in the process, we?ll also increase the transparency of our models. Code exam? ples that accompany the chapter are available online (https://oreil.ly/machine-learning- high-risk-apps-code); Chapter 9 addresses debugging for images and unstructured data, and Chapter 3 covers model debugging more broadly.


261

Concept Refresher: Debugging ML
If readers haven?t picked up on it yet, we?re much more concerned about in vivo performance than in silico performance. In vivo performance matters for users; in silico performance doesn?t. With this central tenet in mind, we?ll be covering model selection, sensitivity analysis tests, residual analysis, and remediating (i.e., fixing) models.
Model Selection
Traditionally we select models by choosing features and selecting hyperparameters. We try to find the best set of features using approaches like stepwise regression, feature importance measurements, or L1 regularization. We often use grid searches to find the best hyperparameter settings for our ML models. In Chapter 2, we used a more deliberate approach in which we started with a linear model benchmark and introduced nonlinearity and interactions into our models, then used human judg? ment to pick our favorite model. In this chapter, we?ll see that random grid search is especially problematic on small datasets for hyperparameter selection and when we compare it to a more sophisticated cross-validated ranking procedure inspired by ?KDD-Cup 2004: Results and Analysis? (https://oreil.ly/osrK4). We?ll compare the rank of model performance across several validation folds and several different traditional assessment metrics to get a better idea of estimated in vivo performance and pick a better model. We?ll also highlight how to estimate the business value of a model?a crucial consideration. No one in the business world wants to deploy a model that loses money!
Sensitivity Analysis
Because ML models tend to extrapolate in complex ways, we won?t really know how our model will perform on unseen data unless we test it explicitly on different types of data. That?s what we?re attempting to do with sensitivity analysis. Broadly speaking, sensitivity analysis shows us whether our model will be stable on different kinds of data. Sensitivity analysis might show us that our model has robustness issues, i.e., it fails under data drift. It might show us that our model has reliability or resilience problems, i.e., that some types of input cause our to model to behave in surprising or inappropriate ways. There are many structured approaches to sensitivity analysis. If readers would like to learn more, we suggest exploring resources associated with PiML (https://oreil.ly/84KCZ) or SALib (https://oreil.ly/Kgnmg).
We?ll highlight two other related sensitivity analysis methods in this chapter that appear most directly useful to practitioners: stress testing and adversarial example searches. Stress testing is roughly aligned to testing for robustness, whereas adversa? rial example searches probe for reliability and resilience problems:


262 |  Chapter 8: Selecting and Debugging XGBoost Models

Stress testing
Stress testing is a global perturbation approach that tests models in foreseeable stressful circumstances. When we stress test our model, we?ll change our valida? tion data to simulate recession conditions and see if it remains robust under these foreseeable difficult conditions. The idea is to make our model robust to predictable circumstances, or at least to document expected performance degra? dation for whoever might be maintaining the model when recession?or another kind of domain shift?strikes.1 We?ll be doing a much less rigorous analysis on a single model, but the idea is the same: to test how our model will perform under foreseeable concept or data drift, such as a recession, to make sure we are prepared for the likeliest types of failures.
Adversarial example searches
Adversarial example searches are a local perturbation approach that helps uncover local, logical flaws in the model?s reliability and potential security vulner? abilities (i.e., resilience issues). Adversarial examples are rows of data that create strange responses from complex ML models. Sometimes we can craft these rows by hand, but often we have to search for them. In this chapter, we will search for them by perturbing, or changing, values of important features in certain rows of data and checking how this change affects model performance. Both the search itself and the individual adversarial examples we find are useful. The search creates a response surface that displays model performance across many interesting input values, and often reveals logical flaws in model performance. The individual rows we find that evoke strange responses are good to document and share with security colleagues, so that model monitoring can be primed to detect known adversarial examples.
In deep learning, data scientists tend to use gradient information and genera? tive adversarial networks (GANs) to create adversarial examples. In structured data tasks, we have to use other methods, like those described in ?Adversarial Attacks for Tabular Data: Application to Fraud Detection and Imbalanced Data? (https://oreil.ly/KF843), heuristics based on individual conditional expectation, or genetic algorithms. We?ll be pursuing a heuristic method in ?Adversarial Example Search? on page 276, both to find adversarial examples, and to probe our model?s response surface for problems.





1 Keep in mind that large US banks perform near-exhaustive stress tests for their models each year in accord? ance with the Federal Reserve?s Comprehensive Capital Analysis and Review (https://oreil.ly/rczyU) process, known as CCAR.

Concept Refresher: Debugging ML  |  263

The most important method we don?t treat in this chapter is random attacks, or simply exposing our model or API to a lot of random data and seeing what kinds of issues emerge. If we don?t know where to start with sensitivity analysis, we try random attacks first. Then we try PiML, SALib, and the methods we?ll put forward in the sections that follow. Regardless of how we implement sensitivity analysis, the key is to do something about the problems we find. We often use data augmentation, business rules, regularization, constraints, and monitoring to remediate sensitivity issues.
Residual Analysis
Residual analysis is a primary method of model debugging. It is the careful study of modeling mistakes made in training data, or other labeled data, for testing and debugging purposes. While readers may be familiar with residual analysis for tradi? tional linear models, it can and should be applied to ML models. The basic idea is the same as with linear models. Good models should have mostly random errors. When we examine the errors in our ML models and strong patterns emerge, this most likely means that we forgot something or we made a mistake when we built our model. Then we have to use our human brains to try to fix the problem. In this chapter, we?ll focus on three major approaches to residual analysis?residual plots, segmented error analysis, and modeling residuals:
Residual plots
We?ll examine residual plots for the entire model, and then break down the plots by feature and by level. We?ll look to answer questions like: Which rows are causing the largest errors? Do we see any strong patterns in the residual plots? Can we isolate any patterns to specific input features or levels of a feature? Then we?ll try to think through how to fix any issues that we uncover.
Segmented error analysis
We shouldn?t deploy a model without checking to see how it performs across major segments in our training or test data. Failing to do so can have grave con? sequences for both overall model performance and algorithmic discrimination, when a model performs poorly on historically marginalized demographic groups. We?ll be focusing on the performance aspect in this chapter, and looking into other types of segments apart from demographic segments. We do this because commonly used average assessment metrics for entire datasets can mask poor performance on small but important subpopulations. Sparsity in training data can lead to nearly random performance on some segments too. Segmented error analysis has also been put forward as an in silico test for the nasty problem of underspecification. All of these issues?poor performance in small segments, random performance in sparse regions of training data, and underspecificaton? can lead to unpleasant surprises and serious problems once a model is deployed.


264 |  Chapter 8: Selecting and Debugging XGBoost Models

One cool extension of segmented error analysis, implemented in PiML, is to examine overfitting across segments. Doing so can highlight additional issues that will affect an in vivo model.
Modeling residuals
Another way to learn patterns in residuals is to model them. If we can fit a straightforward, interpretable model to another model?s residuals, that means, almost by definition, that there are strong patterns in the residuals. (And strong patterns in residuals usually means we made a modeling mistake.) Moreover, modeling residuals implies we can reduce them. The models we fit to residuals should also inform how to fix the discovered mistakes. As an example, we?ll fit a decision tree to our model?s residuals. We?ll then examine the rules of that tree, because they are rules that describe when our model is most often wrong. We?ll try to understand those rules to try to fix the problems they highlight.
As with sensitivity analysis, we won?t be able to treat all the important ways to con? duct residual analysis in ML in one chapter. Some of the most noteworthy approaches we won?t cover include methods for discovering nonrobust features, such as Shapley value contributions to model loss. See Chapter 3 for a broader overview of residual analysis for ML.
Remediation
Once we find problems, we need to fix them. The bad news is that a lot of problems in ML arise from the use of low-quality, biased data and from confirmation bias. For most projects, this will involve two difficult pills to swallow: (1) collecting better data, with at least some consideration for experimental design, and (2) going back to the drawing board and redefining our experiment with better adherence to the scientific method to minimize human, statistical, and systemic biases in our modeling workflow.
Once serious data, methodological, or bias issues are addressed in our workflows, we can try some tech fixes for our model, as we do in this chapter. For example, we can apply monotonic constraints, interaction constraints, or regularization to our models to stabilize them, make them more logical and interpretable, and to improve performance in vivo. We can apply business rules, sometimes also called model assertions, or manual prediction limits to fix foreseeable bad outcomes. Business rules and model assertions boil down to adding code to our scoring engine that changes predictions we think will be wrong. We can edit the formulas or production code of our models to correct for problematic modeling mechanisms or predictions, and we can manage and monitor our models once they are deployed to track them and spot anomalies quickly.




Concept Refresher: Debugging ML  |  265

For our model in this chapter, we?ll hopefully have done a decent job sticking with the scientific method, mainly by expressing our hypotheses for model outcomes through monotonic constraints and by very rigorous model selection, as discussed in the next section. We?ll apply both sensitivity and residual analyses to the model to find bugs, and then we?ll try our best to remediate those bugs in ?Remediating the Selected Model? on page 290.
Selecting a Better XGBoost Model
Although it?s not technically debugging, we want to start our debugging exercise on solid footing by selecting a highly stable, generalizable, and valuable model. To do that, we won?t just rely on grid search. Instead, we?ll select a model as inspired by the Caruana et al. cross-validation ranking approach (https://oreil.ly/kJT7d) used in the 2004 Knowledge Discovery in Databases (KDD) Cup. We?ll also compare these results to a standard random grid search so we can get an idea of the difference between a grid search and the cross-validation ranking procedure described in this section. Then, before moving onto sensitivity analysis, we?ll do a basic estimation of our model?s business value to check that we?re not wasting money.

According to Richard Feynman, it?s our responsibility as scientists to engage in ?a kind of leaning over backward? to make sure we don?t fool ourselves and others. If this model selection approach seems over the top, think of it as bending over backward to find the best model we can.

The first thing we do to begin our model selection process is split our validation data into five folds. Then we select five relevant performance metrics to apply to each fold. These metrics should measure different aspects of performance, such as AUC measuring ranking capabilities across thresholds and accuracy measuring correctness at one threshold. In our case, we?ll take maximum accuracy, AUC, maximum F1 statistic, logloss, and mean squared error (MSE) as our five metrics. The first step of our selection process is to calculate the value of each of these different statistics on each fold. That?s what the following code snippet does:
eval_frame = pd.DataFrame() # init frame to hold score ranking metric_list = ['acc', 'auc', 'f1', 'logloss', 'mse']

# create eval frame row-by-row
for fold in sorted(scores_frame['fold'].unique()): # loop through folds for metric_name in metric_list: # loop through metrics

# init row dict to hold each rows values row_dict = {'fold': fold,
'metric': metric_name}


266 |  Chapter 8: Selecting and Debugging XGBoost Models


# cache known y values for fold
fold_y = scores_frame.loc[scores_frame['fold'] == fold, target]

# first columns are not for scores
for col_name in scores_frame.columns[2:]:

# cache fold scores
fold_scores = scores_frame.loc[ scores_frame['fold'] == fold, col_name]

# calculate evaluation metric for fold # with reasonable precision

if metric_name == 'acc': row_dict[col_name] = np.round(
max_acc(fold_y, fold_scores), ROUND)

if metric_name == 'auc': row_dict[col_name] = np.round(
roc_auc_score(fold_y, fold_scores), ROUND)

if metric_name == 'f1': row_dict[col_name] = np.round(
max_f1(fold_y, fold_scores), ROUND)

if metric_name == 'logloss': row_dict[col_name] = np.round(
log_loss(fold_y, fold_scores), ROUND)

if metric_name == 'mse': row_dict[col_name] = np.round(
mean_squared_error(fold_y, fold_scores), ROUND)

# append row values to eval_frame
eval_frame = eval_frame.append(row_dict, ignore_index=True)
Once we have the performance metric for each model across each fold, we move to the second step of the selection procedure and rank the performance of each model on each fold and for each measure. The following code does the ranking. Our search includes 50 different XGBoost models, and we test them using five performance measures on five folds. For each fold and metric, we rank the models first through fiftieth by the current metric, allowing for ties. We take the model with the lowest average rank across each fold and metric as the best model for in vivo usage. Think about this as if you were giving a test to 50 students, but instead of one test per student, it?s five tests for each student. Then, we don?t pass every student who makes above a certain numeric grade; we?re only interested in the student who performs better than all others across the most tests. Of course, this would make us very mean teachers, but luckily, it?s fine to be extremely selective when it comes to ML models.



Selecting a Better XGBoost Model  |  267


# initialize a temporary frame to hold rank information rank_names = [name + '_rank' for name in eval_frame.columns
              if name not in ['fold', 'metric']] rank_frame = pd.DataFrame(columns=rank_names)

# re-order columns
eval_frame = eval_frame[['fold', 'metric'] +
[name for name in sorted(eval_frame.columns) if name not in ['fold', 'metric']]]

# determine score ranks row-by-row
for i in range(0, eval_frame.shape[0]):

# get ranks for row based on metric metric_name = eval_frame.loc[i, 'metric'] if metric_name in ['logloss', 'mse']:
    ranks = eval_frame.iloc[i, 2:].rank().values else:
ranks = eval_frame.iloc[i, 2:].rank(ascending=False).values

# create single-row frame and append to rank_frame row_frame = pd.DataFrame(ranks.reshape(1, ranks.shape[0]),
columns=rank_names)
rank_frame = rank_frame.append(row_frame, ignore_index=True)

# house keeping del row_frame

eval_frame = pd.concat([eval_frame, rank_frame], axis=1)
Ties occur because we round performance scores. If two models score, say, an AUC of 0.88811 and 0.88839, that?s a tie. Those last decimals of AUC are likely irrelevant for in vivo performance, and our approach handles ties well. Two models with the same score simply have the same rank for that fold and metric. Because we try so many metrics and so many scores, and take the average rank across all of them, those ties rarely matter in the end for selecting the best model. In our example, each of the 50 models is assigned 25 different ranking values, one rank for each metric and fold. Our best model ranked first and second on several fold and metric combinations, but also ranked as high as 26.5?indicating a tie?for its worst-performing fold and metric. In the end, our best-performing model in terms of lowest ranks across metrics and folds showed an average rank of 10.38.
For comparison purposes, this ranking procedure was applied to the top 50 models selected, and models were ranked by a standard random grid search. In the grid search, the lowest logloss on validation data was used to rank models. When we compare the grid search ranking to the cross-validation ranking, the dissimilarity is striking (Table 8-1).



268 |  Chapter 8: Selecting and Debugging XGBoost Models

Table 8-1. Overall rank of the top 10 models across a random grid search ranked by logloss and the more in-depth cross-validated model selection approach
 Grid search rank  Cross-validation rank 
Model 0	Model 2
Model 1	Model 5
Model 2	Model 1
Model 3	Model 4
Model 4	Model 12
Model 5	Model 0
Model 6	Model 21
Model 7	Model 48
Model 8	Model 30
Model 9	Model 29
Model 10	Model 17

The ranking for the grid search is on the left, while the ranking for the cross- validation approach is on the right. The first row in the table indicates that the third-best model in the grid search (indexed from 0, Model 2) was the best model in cross-validated ranking. The two model selection processes exhibit a Pearson correla? tion of 0.35, indicating only moderate positive correlation. In short, the best model derived from a grid search may not be the best model selected by more in-depth selection approaches. In fact, one interesting informal study used a similar technique to reveal stability problems (https://oreil.ly/H6oRC) in data science competitions that used small datasets. This selection approach is a good way to start ?leaning over backward? to increase the scientific integrity of ML model selection exercises. To see exactly how we did it, check out this chapter?s code examples (https://oreil.ly/9nxyQ).
Another important consideration for model selection is business value. Building models costs money. Often, a lot of money. Our salaries, healthcare, retirement, snacks, coffee, computers, office space, and air-conditioning are not cheap. If we want our model to be a success, that often means recouping the resources used to train, test, and deploy the system. Though really understanding the value of our model requires monitoring and measuring business value in real time (https://oreil.ly/ tuMD8), we can use some tricks to estimate its value before we deploy.
To get started, we assign estimated monetary values to the outcomes from our model. For classifiers, this means assigning a monetary value to the elements of a confusion matrix. In Figure 8-1, we can see an illustration of a confusion matrix on the left and an illustration of a residual plot on the right.




Selecting a Better XGBoost Model  |  269



Figure 8-1. Example assessment procedures with estimated monetary values for
(a) classifiers and (b) regression models
The confusion matrix on the left can be applied to our selected model. We came up with the values in the cells by thinking through the model?s in vivo outcomes. A true positive for our model means we decided not to extend credit to someone who would have been delinquent on payments. There are no opportunity costs or write-offs, but also no positive revenue, associated with this outcome. A false positive results in us refusing to extend credit to someone who would have made payments? an opportunity cost. We associate that opportunity cost with an estimated customer lifetime value (LTV) of negative $23,000. A false negative is the worst outcome. That means we extend credit to someone who did not pay. That is a write-off, and we estimate that value, based off of the mean of the customer credit limits (LIMIT_BAL), at about negative $85,000?ouch. A true negative is where we make money. That?s were the model says to extend credit to a paying customer. We associate that outcome with the opposite of a false positive, and we recoup the customer?s LTV for the credit product. For each true negative, we gain $23,000 in revenue. Now we have to add up these values for each customer in our validation set, as each customer will either represent a true positive, false positive, false negative, or true negative outcome. For our model, in the small portfolio represented by the validation set, that estimated value comes out to be $4,240,000. So, there is real business value in our model, but it?s not eye-popping after one thinks through all the expenses and taxes associated with this revenue.







270 |  Chapter 8: Selecting and Debugging XGBoost Models

For regression models, we can assign a single value to overprediction and underpre? diction. Or, as illustrated in Figure 8-1, we can attempt to assign a monetary value to each residual unit in a logical manner for both over- and underprediction. Then we calculate the residuals for each row in our dataset and sum up the estimated value of the model. Once we assign monetary values, we can answer the basic business question: ?Does this model provide any real value?? Now that we think we?ve selected a decent model, and one with some business value, let?s try to figure out what?s wrong with it, using sensitivity analysis.
Sensitivity Analysis for XGBoost
We?ll start putting our model through its paces with sensitivity analysis in this sec? tion. We?ve selected stress testing and adversarial example searches for detailed exam? ples of sensitivity analysis in this chapter. Both techniques have direct applicability for a wide range of applications, and help to spot different kinds of problems. Stress testing looks for global weaknesses in our model across entire datasets in foreseeable stressful circumstances, like recessions. Adversarial example searches help us spot potential surprise issues, like wild predictions or security vulnerabilities, often on a local, row-by-row basis.




Sensitivity Analysis for XGBoost |  271

Stress Testing XGBoost
Linear models extrapolate linearly, but ML models can do almost anything on data outside their training domain. Unless we?re stress testing our models on the right data, we?re just not going to be aware of it. For instance, consider training a model on a dataset where the highest income of an individual is $200,000. How would the model behave if it encounters an individual income of, let?s say, 20 million dollars? Will it break? Will it return accurate results, or not? There?s no way to know this unless we test it explicitly. The basics are not hard. Simulate a row of data, put a 20 million dollar income in there, and run it back through our model to see how it behaves. When we do this more thoroughly and more systematically, we call it stress testing.

Complex ML models often behave poorly when extrapolating outside of their training data domain, but even simpler models have problems with extrapolation. Tree-based models often cannot make predictions outside the ranges of training data, and polyno? mial models can suffer from Runge?s phenomenon (https://oreil.ly/ 1Nabl) at the edges of their training data domains. We are taking a risk whenever we use standard statistical or ML models for predic? tions outside the domain of training data.

Stress testing is an in silico exercise conducted to test the resilience of models under external, adverse, in vivo scenarios, like recessions or pandemics. The basic idea behind stress testing is to simulate data that represents realistic future scenarios and then redo traditional model assessment to see how the model performs. This ensures that the ML models can withstand the reasonably likely adverse developments they will encounter in the wild, and that they?re robust to inevitable in vivo variations in new data, commonly known as data and concept drift.
Data scientists often say they already validate their models against holdout datasets, so is there really a need for additional stress tests? Well, there is a need, and even more so when models will get deployed and affect people. An ML model with a perfect in silico AUC is of no use if it falters when it encounters common stressors in new data. When we deploy ML models in the real world, we have to think about more aspects and situations than simple in silico test error. Even though it is difficult to predict the future, we can use validation data to simulate foreseeable problems. We can then see how the model performs under these conditions, document any issues, and if possible, update our model to address any discovered issues.
A gold standard for stress tests is the Federal Reserve?s Comprehensive Capital Anal? ysis and Review (CCAR). It is an exercise conducted by the US Federal Reserve annually to ensure that large banks and financial institutions have adequate capital planning processes and maintain sufficient capital to withstand economic shocks.

272 |  Chapter 8: Selecting and Debugging XGBoost Models

For instance, the CCAR conducted two separate tests to gauge the robustness of the big banks in the US in the wake of the COVID-19 pandemic. Even though the banks were well-capitalized under extreme simulated situations, CCAR results still warranted restrictions (https://oreil.ly/RM-pS) on bank payouts due to uncertainty surrounding the situation. We?ll take inspiration from CCAR in the following subsec? tions when trying to determine if our selected XGBoost model is robust to recession conditions, a common and foreseeable stressor for credit models.
Stress Testing Methodology
A recession is a situation wherein there is a substantial decline in a country?s economy, lasting several months. Remember the financial crisis of 2008 and, more recently, the economic slowdown caused by the COVID pandemic? We want to see how our model might perform if a recession occurs while it is deployed. In this section, we?ll simulate a recession scenario and then reassess the performance of our constrained and regularized XGBoost model. As seen in Figure 8-2, the model performs well on both the validation and holdout test data before the stress testing.

Figure 8-2. ROC curve before stress analysis on a constrained and regularized XGBoost model (digital, color version(: https://oreil.ly/48-em))



Sensitivity Analysis for XGBoost |  273

We?ll now create a copy of the original dataset and rename it as data_recession
_modified. We?ll alter the values of some of the features in this dataset using basic economic and business intuitions, and should be able to emulate a recession scenario.

It?s difficult to simulate realistic out-of-distribution data, because it?s impossible to know in advance how each feature will co-vary in some new circumstance. For that reason, stress testing is an exercise best undertaken in close collaboration with subject matter experts. Even better than simulating stress test data would be to back-test the model on real data during adverse conditions, such as the 2008 global recession.

Altering Data to Simulate Recession Conditions
First, we?ll choose some of the observations in the dataset to modify. We take these observations to be the ones that would be affected by the recession?perhaps they or someone in their household will have lost their jobs. We choose to modify 25% of customers who were previously in good standing:
data_recession_modified = data_recession[ data_recession['DELINQ_NEXT'] == 0].sample(frac=.25)
Assuming that the simulated recession has recently hit, we?ll assume that these obser? vations have fallen behind on their most recent payments:
payments = ['PAY_0', 'PAY_2'] data_recession_modified[payments] += 1
Here, PAY_* represents the various repayment statuses. Next, we?ll decrease the pay? ment amount of the customers by one thousand dollars each:
pay_amounts = ['PAY_AMT1', 'PAY_AMT2'] data_recession_modified[pay_amounts] = np.where(
data_recession_modified[pay_amounts] < 1000,
0,
data_recession_modified[pay_amounts]-1000)
During times of financial crisis, banks often tighten their purse strings, and one way to do that is by lowering credit limits. We?ll now incorporate this scenario in our stress test exercise by decreasing the credit limits of these affected customers, in a fixed proportion of their original credit limits:
data_recession_modified['LIMIT_BAL'] *= 0.75
We?ll also decrease the bill amounts of these customers by a fixed proportion, to simulate lower spending:
bill_amounts = ['BILL_AMT1','BILL_AMT2'] data_recession_modified[bill_amounts] *= 0.75


274 |  Chapter 8: Selecting and Debugging XGBoost Models

Finally, we?ll assume that some proportion of these affected customers will go delin? quent on their accounts. In particular, we?ll flip half of the target variables from zero to one:
data_recession_modified['DELINQ_NEXT'] = np.where( np.random.rand(len(data_recession_modified)) < 0.5,
1, 0)
After reintegrating the affected observations into the rest of the data, we have a dataset that mimics some of the adverse conditions our model might encounter in the real world. It is time to look at the performance metrics on this simulated data. In Figure 8-3, we see a moderate decline in performance once recession-like data and concept drift is applied to test data.

Figure 8-3. ROC curve after stress analysis on a constrained and regularized XGBoost model (digital, color version(: https://oreil.ly/R46Oo))
The first step after encountering such results is to document them and share them with our team and management. This enables an informed decision to be made about whether to deploy the model. If economic conditions look rosy, then we might reasonably decide to deploy the model, with the knowledge that it will need to be refreshed quickly if economic conditions change. A more nuanced analysis would entail reassessing the financial risk we?d be taking on if our AUC were to drop from 0.777 to 0.738. Can we afford to make that many additional wrong credit decisions?


Sensitivity Analysis for XGBoost |  275

Once the results are documented and discussed with stakeholders, another next step might be to attempt to improve the model. This would definitely be the case if economic conditions are looking discouraging, or if the results of this or other stress tests were more dire. As readers might have guessed from the number of pages left in this chapter, we?re going to find other problems with this model soon. We?ll wait to remediate, or fix, all of the problems that we find until the end of the chapter.
We?d also like to highlight one more thing before we move on to adversarial example searches. We were careful when we trained this model, using regularization, mono? tonic constraints, grid search, and a highly robust model selection approach. These decisions likely had a positive impact on the robustness of the model under the stress test. Models trained without these specifications may have performed worse during the stress test. Either way, if we?re not testing for problems that affect in vivo deployment, we?re just ignoring them.
Adversarial Example Search
We?ll apply an adversarial example search as our next debugging technique. The goal of our search is two-fold: to find adversarial examples that could be used to trick our model once it?s deployed, and to see what we can learn about our model?good and bad?as a result of our search.
There are lots of packages and software that can help us find adversarial examples for image data, but we need to find adversaries for structured data. While some progress has been made in using generative adversarial networks (GANs) and genetic algorithms to find adversaries for structured data, we?ll apply a heuristic approach instead. The first step is to find a row of data that will make a good initial guess for an adversarial example. We?ll do this with ICE plots. Figure 8-4 displays ICE curves across deciles of predicted probability along with partial dependence.
In Figure 8-4 we can see the ICE curve associated with the 80th percentile shows the largest swing in predicted values across the values of PAY_0. Because we know this row of data can lead to large changes in predictions just by changing the value of one feature, we?ll use the original row of data at the 80th percentile of predicted probability in the selected model to seed our adversarial example search. In more detail, for each important variable our adversarial search heuristic goes as follows:
1. Calculate ICE curves at each decile of model predictions.
2. Find the ICE curve with the largest swing in predictions.
3. Isolate the row of data associated with this ICE curve.
4. For this row of data:
a. Perturb 1?3 additional important variables in the row. (It?s hard to plot results for more than 1?3 variables.)


276 |  Chapter 8: Selecting and Debugging XGBoost Models

b. Rescore the perturbed row.
c. Continue until each additional important variable has cycled through its domain in the training data, and through missing or other interesting out-of- range values.
5. Plot and analyze the results.

Figure 8-4. Partial dependence and ICE of the selected XGBoost model (digital, color version(: https://oreil.ly/w0jkL))
We?ve already addressed steps 1?3, so how will we perform step 4? We?ll take advan? tage of itertools.product() to automatically generate all possible feature perturba? tions for a set of features supplied to a Python function. Also, remember that when working with the native XGBoost API, we always have to supply an extra argument (iteration_range) to the predict() function to apply model selection:
adversary_frame = pd.DataFrame(columns=xs + [yhat])

feature_values = product(*bins_dict.values()) for i, values in enumerate(feature_values):
row[xs] = values
adversary_frame = adversary_frame.append(row, ignore_index=True, sort=False) if i % 1000 == 0:
        print("Built %i/%i rows ..." % (i, (resolution)**(len(xs)))) adversary_frame[search_cols] = adversary_frame[search_cols].astype(
    float, errors="raise") adversary_frame[yhat] = model.predict(
xgb.DMatrix(adversary_frame[model.feature_names]), iteration_range=(0, model.best_iteration))




Sensitivity Analysis for XGBoost |  277

We supplied the validation data and input features PAY_0, PAY_2, PAY_AMT1, and PAY_AMT2 to the search code. The chosen input features were based on a Shapley summary plot that showed that these features have the widest spread of prediction contributions. The result of running this code on the selected inputs is data for sev? eral response surfaces that can be used to see how our model behaves in potentially interesting circumstances. The only thing left to do now is to plot and analyze these response functions. Figure 8-5 shows the results of the adversarial example search, seeded by an ICE curve, and presents some positive and negative findings.
On the positive side, each response surface shows monotonicity. These simulations confirm that monotonic constraints, supplied at training time and based on domain knowledge, held up during training. On the negative side, a potential logical flaw was also discovered. According to one of the response surfaces, the example model will issue high probability of default predictions once customers become two months late on their most recent payment (PAY_0). The issue to be aware of is that denials are likely applied even in the circumstance where a customer repays (PAY_AMT1) over their credit limit. This potential logical flaw could prevent prepayment or overpenalize good customers who failed to pay their bill, say, while on vacation. While this behavior is not necessarily problematic, it?s definitely something model operators would like to know about. Therefore, we need to add it into our model documentation.
Of course, there is the issue of the actual adversarial examples. Don?t worry, we found lots of those. We found many rows of data that can evoke low probabilities of default?around 5%?and plenty that can evoke high probabilities of default? around 70%?and everything in between. We now have a complete set of adversarial examples to draw from that can create almost any probability of default we want from the model. If readers are wondering why this matters, see Chapters 5 and 11 on ML security. To see all the code and results details, check out this chapter?s code examples (https://oreil.ly/9nxyQ).

Another favorite sensitivity analysis technique we want to highlight is a trick-of-the-trade that involves label shuffling:
? Randomly shuffle the target feature and retrain the model.
? Recalculate feature importance.
? Consider removing features that are important for predicting a randomly shuffled target.
This helps us find and remove nonrobust features.





278 |  Chapter 8: Selecting and Debugging XGBoost Models



Figure 8-5. Adversarial example search shows model behavior in a number of scenarios (digital, color version(: https://oreil.ly/hlLzb))

Sensitivity Analysis for XGBoost |  279

When debugging, we always want to consider fixing the problems that we find. The logical issues around prepayment could likely be handled by business rules or model assertions. For instance, if a customer makes a large prepayment and lets the bank know that they are headed to a tropical island, subsequently issued probabilities of default could be decreased for a few months. As for the adversarial examples, the most effective adversarial rows could be recorded in model documentation with examples, so that future maintainers could understand these potential issues for the model. We could even discuss adversarial example attacks with our colleagues in security and consider monitoring for adversarial examples in real time.
Residual Analysis for XGBoost
We?ve now taken a look at local perturbations that can cause problems for our model using an adversarial example search, and at problematic global perturbations using stress testing. It?s time to move on to residual analysis. To do that, we?ll begin with something traditional: plotting residuals by each level of an important input feature. We?ll be on the lookout for the rows that cause the largest mistakes and any strong patterns in our residual plots. Then we?ll break our predictions down into segments and analyze performance across those segments. It?s not enough to understand how a model performs on average for high-risk use cases. We need to know how our model performs across important segments in our data. To finish off residual analysis, we?ll try to model our residuals with a decision tree. From that tree we?ll learn rules about how our model makes mistakes, and we can try to use those rules to avoid them. Time to start learning from our mistakes. Let?s look at some residuals.


280 |  Chapter 8: Selecting and Debugging XGBoost Models



Analysis and Visualizations of Residuals
As highlighted in Figure 8-1, residuals can help us understand the business value?or lack thereof?of our model. They?re also a great way to learn technical details about how our model makes mistakes. We?ll be looking at logloss residuals for our model, as opposed to traditional residuals, because our model was trained using logloss. For most people, one of the easiest ways to start thinking through an ML model?s residuals is to plot them. In this subsection, we?ll start out by looking at the global logloss residuals for our selected model and then zoom in to the residuals for the most important input feature, PAY_0. In both cases, we?ll be looking to understand the drivers of our model?s mistakes and how, if at all, we can fix them. The first step to plotting residuals is, of course, to calculate them. We?re going to use logloss residuals?the type of error used during model training for the binary:logistic loss function in XGBoost. This way, remediating large residuals should have a direct effect on model training. To calculate our residuals, we?ll need the target and prediction values, as the following code block shows, and then we apply the standard formula for binary logloss:
# shortcut name
resid = 'r_DELINQ_NEXT'

# calculate logloss residuals
valid_yhat[resid] = -valid_yhat[y]*np.log(valid_yhat[yhat]) -\
(1 - valid_yhat[y])*np.log(1 - valid_yhat[yhat])
One small benefit of calculating residuals this way is that we can check that the mean residual value matches the logloss reported by XGBoost at the end of training to ensure we selected exactly the right size model when we generated our predictions. After passing that check, we can move on to plotting the residuals, which readers can see in Figure 8-6. Note that Figure 8-6 contains the feature r_DELINQ_NEXT. The logloss residual value is named r_DELINQ_NEXT, and p_DELINQ_NEXT is the prediction of the target, DELINQ_NEXT. Logloss residuals look a bit different from the typical regression residuals we might remember from statistics class. Instead of a random


Residual Analysis for XGBoost | 281

blob of points, we can see one curve for each outcome of the model; for DELINQ_NEXT
= 0, it?s curving up and to the right, and for DELINQ_NEXT = 1, it?s curving toward the upper left. One of the first things that we can see in this plot is some large outlying residuals for both outcomes, but they are more numerous and extreme for DELINQ_NEXT = 1.

Figure 8-6. Logloss residuals for positive and negative responses for the selected XGBoost model (digital, color version(: https://oreil.ly/h5wnc))







282 |  Chapter 8: Selecting and Debugging XGBoost Models

This pattern tells us that there are some customers in our validation data who miss payments, but our model really thinks they won?t. By sorting the validation data by the new r_DELINQ_NEXT column and looking at the largest residual rows, we can get an idea of what?s going on with these customers. So, who are these customers? It turns out they are good customers?with large credit limits and who always paid on time?who will miss their next payment. They?d surprise our model and cause huge residuals.
These results point to a fundamental flaw in our training data. We?re missing features that could help us understand more about a consumer?s financial life and why they might be late on payments. For example, the debt-to-income (DTI) ratio is often used in credit models. We might see an increase in a customer?s DTI ratio before they miss a payment. Without this kind of additional information, we have to recognize that we?ve discovered a serious limitation of our model. We just don?t have the columns we need to do better. Recognizing that with the available data, our model can be easily surprised, we might as well consider removing these rows from our training data, because they?re bringing useless noise into the training process. It might be a good idea to drop them, and similar individuals, because there?s not much we can learn from them as of now. We?d likely improve our validation and test performance, and we might train a more stable and reliable model.
Before we get rid of these points, let?s plot logloss residuals by each level of the most important feature PAY_0. If we see a more specific story or question in the initial analysis of global residuals, we should let that information guide which residuals to investigate next. Since we didn?t see information linking these individuals to any specific feature, we default to investigating the most important input feature. To do that, we?ll rely on the convenience of a Seaborn FacetGrid plot. The following code shows how to quickly break down the residuals by the levels of PAY_0 and plot the residuals at each level in a neat grid:
# facet grid of residuals by PAY_0
sorted_ = valid_yhat.sort_values(by='PAY_0')
g = sns.FacetGrid(sorted_, col='PAY_0', hue=y, col_wrap=4)
_ = g.map(plt.scatter, yhat, resid, alpha=0.4)
_ = g.add_legend(bbox_to_anchor=(0.82, 0.2))
Figure 8-7 shows the logloss residuals for positive and negative outcomes across the 11 levels of PAY_0 in the validation data. In general, we should be on the lookout for any strong patterns in the plots.







Residual Analysis for XGBoost | 283



Figure 8-7. Customers with good payment track records who default suddenly cause large residuals, as do customers with poor payment track records who suddenly start paying on time (digital, color version(: https://oreil.ly/ubGpn))
Figure 8-7 reaffirms the story told by the global residuals, and adds some specifics. In the top row of Figure 8-7, favorable values for PAY_0 (-2, -1, or 0), representing paying on time or not using credit, are associated with large residuals for customers who default (DELINQ_NEXT = 1). These are some of those high-residual customers we saw in Figure 8-6. In the bottom rows the exact opposite behavior is displayed. Cus? tomers with unfavorable values for PAY_0 cause large residuals when they suddenly pay on time (DELINQ_NEXT = 0). What?s the lesson here? Figure 8-7 indicates that our ML model makes the same mistakes a human, or a simple business rule, would make. From Chapter 6, we know that this model is too reliant on PAY_0. Now we see one consequence of that pathology. If customers have favorable values for PAY_0, the model is shocked when they default. If customers have unfavorable values for PAY_0, the model is shocked if they pay.


284 |  Chapter 8: Selecting and Debugging XGBoost Models

This is problematic because we don?t need an ML model with hundreds of thousands of rules to make this kind of decision. Those thousands of rules hide a great deal of complexity, which in turn could be hiding bias or security problems. This model either needs to be substantially improved by collecting more columns of data and retraining it or we can consider replacing it with a more transparent and secure business rule: IF PAY_0 < 2 THEN APPROVE, ELSE DENY. Essentially the model needs more data?some new input column that could tell us about a customer?s financial stability outside the context of this credit account. Without this information, we are deploying an overly complex?and hence, unduly risky?pipeline to make what end up being simplistic decisions. We?ll attempt remediation in ?Remediating the Selected Model? on page 290, but before we do, let?s make sure this model isn?t hiding any other surprises. Next, we?ll perform segmented error analysis and look into any trouble spots in the model?s performance.
Segmented Error Analysis
Our selected model has a validation AUC of 0.78. That?s a respectable AUC, and indicates that our model ranks negative and positive outcomes correctly about 80% of the time in validation data. So, we?re good to deploy right? Well, we just saw how a more careful analysis of errors can reveal serious problems that simpler assessment statistics do not. And, unfortunately, we?re about to see that respectable top-level AUC doesn?t mean much either.
In Table 8-2, we calculate many common binary classification performance and error metrics across all the levels of PAY_0. This technique is sometimes known as segmen? ted error analysis. The basic idea is that different performance and error metrics tell us different information about the model. For example, a top-level AUC tells us about the model?s overall ability to rank customers correctly, and accuracy tells us about error rates at a specific probability threshold, whereas measures like true positive rate and false positive rate break accuracy down into more specific perspectives on correct and incorrect decisions. Moreover, we want to know this information about different segments in the modeled population. The best-performing models will exhibit reliable decision making across all the segments of the modeled population, not just for the largest segments in the data. When we?re dealing with a billion-dollar lending portfolio, those smaller segments still represent a large amount of money. In other high-risk applications, smaller segments might represent other important financial, criminal justice, or life-and-death decisions.
Note that the values we?ll calculate in Table 8-2 arise from confusion matrices and can vary based on our selection of a probability threshold. The values are calculated using a threshold selected by maximizing the model?s F1 statistic. If we were to deploy this model, we should use the probability threshold used in the production pipeline. It?s our responsibility to make sure the model performs well in vivo, using several different metrics, for all the groups that are subject to the model?s decisions. This has

Residual Analysis for XGBoost | 285

serious ramifications for fairness as well, but we?ll tackle those in other chapters. For now, let?s dig into Table 8-2.
Table 8-2. Segmented error analysis table

PAY_0
Prevalence
Accuracy
True positive rate
Precision
Specificity
Negative predicted value
False positive rate
?
False negative rate
False omissions rate
?2
0.118
0.876
0.000
0.000
0.993
0.881
0.007
?
1.000
0.119
?1
0.177
0.812
0.212
0.438
0.941
0.847
0.059
?
0.788
0.153
0
0.129
0.867
0.089
0.418
0.982
0.880
0.018
?
0.911
0.120
1
0.337
0.566
0.799
0.424
0.448
0.814
0.552
?
0.201
0.186
2
0.734
0.734
1.000
0.734
0.000
0.500
1.000
?
0.000
0.500
3
0.719
0.719
1.000
0.719
0.000
0.500
1.000
?
0.000
0.500
4
0.615
0.615
1.000
0.615
0.000
0.500
1.000
?
0.000
0.500
5
0.571
0.571
1.000
0.571
0.000
0.500
1.000
?
0.000
0.500
6
0.333
0.333
1.000
0.333
0.000
0.500
1.000
?
0.000
0.500
7
0.500
0.500
1.000
0.500
0.000
0.500
1.000
?
0.000
0.500
8
0.750
0.750
1.000
0.750
0.000
0.500
1.000
?
0.000
0.500
Everything looks normal in Table 8-2, until we hit the fifth row, where PAY_0 = 2. From there on, the table shows a serious problem, maybe even worse than the residu? als plotted by PAY_0 in the previous section. To be blunt, for PAY_0 = 2 and above, this model doesn?t really work. For example, we observe a false positive rate of 1.0. This means that the model is wrong about everyone who does not miss a payment? the model predicts that all of them will be late. Why might this be happening? The clearest reason is, again, the training data. In the residual plot, we saw that we might be missing some important input features. With segmented error analysis, we can now see that we may be missing some important rows of data too. We simply do not have enough people in the training data with PAY_0 > 1 for the model to learn anything intelligent about them. Have a look back at some of the figures in Chapter 6 or look at Figure 8-7. There just aren?t many dots in the subfigures for PAY_0 > 1.

Top-level or average error metrics can hide nasty problems. Always conduct segmented error analysis for high-risk applications.








286 |  Chapter 8: Selecting and Debugging XGBoost Models

It?s pretty incredible what that 0.78 AUC can hide. We hope this example convinces readers of the importance of segmented error analysis. Readers might rightly be thinking about how to fix this problem. The most obvious answer is to wait to deploy this model until we can capture enough data about customers who miss payments to train a better model. If the model has to be deployed as-is, we?ll likely need human case workers to make denial decisions, at least for those customers with PAY_0 > 1. We?ll consider more strategies for remediation to close out the chapter, but before we do, we want to learn more about these patterns in the residuals we?ve found. Next, we?ll be fitting an interpretable model to our residuals to get some details about what?s behind these flaws in our model.
Modeling Residuals
In Chapter 6, we used an interpretable decision tree to model our predictions based on input features to get a better idea about which input features were driving predictions and how. Now we?ll use the same approach to get some insight into what?s driving our residuals. If we?re noticing some overlap between explanation and debugging, that?s not a coincidence. One of the best uses for post hoc explanation is to aid in debugging efforts.
We?ll use the following code to fit a four-level decision tree to our DELINQ_NEXT = 0 and DELINQ_NEXT = 1 residuals, separately. To fit this tree, we?ll use our original inputs as the tree inputs, but instead of training with DELINQ_NEXT as the target, we?ll train on the residuals, or r_DELINQ_NEXT. Once the tree is trained, we?ll then store an H2O MOJO (for model object, optimized). The MOJO contains a specialized function that can redraw our residual tree using Graphviz, an open source library for technical renderings. We can do something similar with several other packages, including scikit-learn.
# initialize single tree model
tree = H2ORandomForestEstimator(ntrees=1,
sample_rate=1, mtries=-2, max_depth=4, seed=SEED, nfolds=3,

model_id=model_id) 

# train single tree model
tree.train(x=X, y=resid, training_frame=h2o.H2OFrame(frame))

# persist MOJO (compiled Java representation of trained model) # from which to generate plot of tree
mojo_path = tree.download_mojo(path='.') print('Generated MOJO path:\n', mojo_path)


Residual Analysis for XGBoost | 287


Use only one tree.
Use all rows in that tree.
Use all columns in that tree?s split search. Shallow trees are easier to understand.
Set random seed for reproducibility.
Cross-validation for stability, and the only way to get metrics for one tree in H2O.
Gives MOJO artifact a recognizable name.
Just as with our surrogate model for explanation purposes, there?s no fundamental theoretical guarantees that this model actually tells us what?s driving residuals. As always, we need to be careful and thoughtful. For this decision tree, we?ll calculate overall error metrics to make sure the tree actually fits the residuals. Because instabil? ity is a well-known failure mode for single decision trees, we?ll look at cross-validated error metrics to ensure the tree is stable too. It?s also important to keep in mind that if what?s driving residuals is outside the scope of the input features, this tree can?t tell us about it. We already know that some of the major issues in our model arise from data we don?t have, so we need to keep that in mind while analyzing the tree.
Figure 8-8 displays the decision tree model of our ML model?s residuals for DELINQ_NEXT = 0, or customers who do not miss an upcoming payment. While it reflects what was discovered in Figure 8-7, it does so in a very direct way that exposes the logic of the failures. In fact, it?s even possible to build programmatic rules about when the model is likely to fail the worst based on this tree.
Tracing from the top of the tree, to the largest average residual value at the bottom of the tree, Figure 8-8 shows that the largest residuals for negative decisions occur when PAY_0 >= 1.5 AND PAY_3 >= 1.0 AND BILL_AMT3 < 2829.50 AND PAY_6 >= 0.5.
This means, as we saw in Figure 8-7, when a customer has questionable repayment over months and small bill amounts, the model is shocked when they make their next payment. Now we?ve narrowed this down to the specific customers that cause the worst residuals on average, and have a business rule to define the situation of highest concern.






288 |  Chapter 8: Selecting and Debugging XGBoost Models



Figure 8-8. A decision tree showing model?s residuals, revealing patterns that can be used to spot failure modes and design mitigation approaches
In general, this residual modeling technique helps uncover failure modes. Once failure modes are known, they can often be mitigated to increase performance and safety. If the group of customers causing the largest residuals in Figure 8-8 can be isolated from patterns that result in missing payments, this can lead to precise remediation strategies in the form of business rules (or model assertions). If a cus? tomer presents themselves to our model with PAY_0 >= 1.5 AND PAY_3 >= 1.0 AND BILL_AMT3 < 2829.50 AND PAY_6 >= 0.5 characteristics, maybe we shouldn?t just assume they will default. We could consider adjusting this cohort of customers? default probability down with a business rule or sending their credit decision along for more nuanced consideration from a human case worker. So far, debugging has uncovered a major issue with our model. Our model did not have the right training data, neither columns nor rows, and it?s easily surprised in common and important decision-making scenarios. Aside from collecting or simulating better data, we?ve now found one potential remediation tactic: business rules that flag when we are about to make a bad decision that can be used to take some action to mitigate that bad decision. In the next section, we?ll close out the chapter by discussing further remediation activities.

Residual Analysis for XGBoost | 289

Post hoc explanation techniques, such as interpretable surrogate models, are often most useful as model debugging tools.


Remediating the Selected Model
Despite using monotonic constraints and regularization, despite a careful grid search and strenuous model selection tests, and despite really wanting to train a good model, we simply trained a bad model that shouldn?t be deployed. In addition to insufficient training data, recall that we found this model:
? Pathologically overemphasizes a customer?s most recent repayment status (PAY_0)
? Exhibits logical errors that could preclude prepayment or negatively affect high net worth customers
? Could be vulnerable to adversarial example attacks
? Performed poorly for PAY_0 > 1
While we?ll address each of these issues separately, together they conspired to make a seemingly passable ML model potentially less appealing than a simple business rule, at least for the authors. Because many ML models are not adequately debugged before deployment, it?s likely that we could find ourselves with similar bugs to handle if we applied the debugging techniques in this chapter to one of our organization?s models. Whether our team would try to fix this model or head back to the drawing board, it?s going to be important to think through how to solve the problems that sensitivity and residual analysis uncovered. In other words, these problems have to be remediated before this, or a similar model, is deployed. For the example data and model, several techniques could be applied to remediate the highlighted bugs.
The training data presents both the easiest and hardest remediation options. The solution is clear. Implementing the solution takes common sense and hard work. Collect more and better training data. Use experimental design techniques to inform data collection and selection. Use causal discovery techniques to select input features that actually affect the prediction target. Consider simulating data where necessary.

Often, the best thing we can do to improve the performance of our ML system is to collect more and higher-quality data.





290 |  Chapter 8: Selecting and Debugging XGBoost Models

For the rest of the identified issues, let?s try to address them one by one as an example of how we would take on these bugs at our job. We?ll put special focus on the overemphasis of PAY_0, as it has the most readily apparent training- and coding-oriented mitigants, and then proceed to the other identified failure modes.
Overemphasis of PAY_0
Perhaps the biggest problem with our selected model, and many other ML models, is bad training data. In this case, training data should be augmented with new, relevant features to spread the primary decision-making mechanisms within the model across more than one feature. One strategy to improve stability and generalization is to introduce a new feature that summarizes a customer?s spending behavior over time to expose any potential financial instability: the standard deviation of a customer?s bill amounts over six months, bill_std. Pandas has a one-liner for calculating standard deviations for a set of columns.
data['bill_std'] = data[['BILL_AMT1', 'BILL_AMT2',
'BILL_AMT3', 'BILL_AMT4',
'BILL_AMT5', 'BILL_AMT6']].std(axis=1)
Along the same lines, we could also create a new feature, pay_std, containing information about payment status, except the most recent one (we don?t want to overemphasize PAY_0 again):
data['pay_std'] = data[['PAY_2','PAY_3','PAY_4','PAY_5','PAY_6']].std(axis=1)
Noise injection to corrupt PAY_0 could also be used to mitigate overemphasis, but only if there are other accurate signals available in better training data. We?ll random? ize the PAY_0 column, but only where PAY_0 is either equal to 0, 1, or 2. This type of corruption is akin to strong regularization. We really want to force the model to pay attention to other features.
data['PAY_0'][(data['PAY_0']>= 0) & (data['PAY_0']< 3)].sample(frac=1).values
After taking these steps to deemphasize PAY_0 in the training data, we retrain our model. The resultant SHAP summary plot (Figure 8-9) shows that we have been able to deemphasize the PAY_0. It?s been moved way down from the top spot in the summary plot and replaced by PAY_2, and our new engineered features appear higher in importance than PAY_0. We also observe a slight decrease in the AUC, which now stands at 0.7501, from the original 0.7787.







Remediating the Selected Model | 291



Figure 8-9. Shapley values for each input variable after deemphasizing PAY_0 (digital, color version(: https://oreil.ly/H6zU9))
Now for the hard part: is this a better model? The overall AUC, often relied on for picking a ?good? classifier, has decreased. First of all, we already saw that in silico AUC doesn?t mean very much. Secondly, a decrease in test metrics is almost assured when we change a model. ML training ruthlessly optimizes against some chosen

292 |  Chapter 8: Selecting and Debugging XGBoost Models

criterion and then tends to select the best model by that same criterion in validation data. If we fiddle with that process, we?re likely to see a worsening of those selected in silico test metrics.

Remediation is likely to make our model look worse, according to test data statistics. That?s OK. There is no statistic that truly predicts real-world performance. So long as remediation is based on solid domain knowledge, we can sacrifice some in silico performance in test data to deploy a more parsimonious model in vivo.

The only way to know if this model is better is to debug it again and consult with domain experts. While this may be disappointing, it?s a truth that?s always been known. There?s no statistic that foretells amazing in vivo performance, not yet any? way. Good models have always needed debugging and domain expertise to function properly in the real world. Now, let?s continue to make our model better by looking into the remaining problems we identified while debugging: logical errors, security vulnerabilities, and poor performance for PAY_0 > 1.
Miscellaneous Bugs
We?ll get to other technical remediation approaches soon, but let?s consider experi? mental design issues briefly here. To address the misalignment between treating ML models like engineering projects focused on in silico test error versus experiments focused on in vivo outcomes, we should try update our workflow to more closely align with the traditional scientific method:
1. Develop a credible hunch (e.g., based on prior experiments or literature review).
2. Record our hypothesis (i.e., the intended real-world outcome of our ML system).
3. Collect appropriate data (e.g., using design of experiment approaches).
4. Test the hypothesis that the ML system has the intended in vivo effect on a treatment group, using methods like:
? A/B testing to understand the effect of model outcomes on an informal treat? ment group.
? Coarsened exact matching (https://oreil.ly/jEf8O) to construct control and treatment groups from collected observational data and test for statistically significant treatment effects of the model.
If we?re doing a bunch of trial-and-error work with unexplainable models and with strong doses of confirmation bias and funding bias leading the way?as many ML projects are today?then we?re likely going to be surprised about how our model performs once it?s deployed. (Remember the quote from Google?s research group at


Remediating the Selected Model | 293

the beginning of the chapter?) There?s no tech fix for a cultural lack of scientific rigor. And as of today, a great deal of ML is still an experimental science, not rote engineering tasks. Chapter 12 explores issues with data science and the scientific method in more depth.

We often think of the experiment we?re doing as picking the best algorithm. But it really should be about the in vivo outcomes experienced by users, customers, or subjects of the system.


Fixing serious data and methodological errors would likely have had a positive effect on logical errors, security vulnerabilities, and general poor performance in our model. In the following list, we look at some more direct fixes that might work better in contemporary data science workflows too. We?ll close out the section with a brief discussion of calibration. Calibration of predictions to past known outcomes is another broad fix that?s based in common sense.
Logical errors
For the logical errors that cause a high probability of default to be issued, even after very large payments are made, model assertions or business rules are a likely solution. For customers who just recently became two-month delinquent, use a model assertion or business rule to check if a large payment was also made recently before posting the adverse default prediction. A residual model like the one in Figure 8-8, focused on that small group of customers, could help suggest or refine more targeted assertions or rules.
Security vulnerabilities
We found the model is easily manipulated with adversarial examples. In general, best practices like API throttling and authentication, coordinated with real-time model monitoring, help a lot with ML security (see Chapter 5). What may also be applied for this model is data integrity constraints or monitoring for random or simulated data, i.e., anomaly detection. Essentially, this model may require an extra bit of monitoring that checks for anomalies, such as on-time most recent payment (PAY_0 = 1) and being six months late on the second most recent payment (PAY_2 = 6). If anomalous data is identified in the scoring queue, using anything from isolation forest algorithms to logical data integrity constraints, that data should be routed for closer inspection before a credit decision is made.
Poor performance for PAY_0 > 1
Like many of the other problems with our selected model, this model needs better data to learn more about customers who end up defaulting. In the absence of this information, observation weights, oversampling, or simulation could be used to increase the influence of the small number of customers who did miss


294 |  Chapter 8: Selecting and Debugging XGBoost Models

payments. Also, the model?s monotonic constraints are one of the best mitigants to try when faced with sparse training data. The monotonic constraints enforce well-understood real-world controls on the model. Yet, model performance for PAY_0 > 1 is extremely poor even with these constraints. Predictions in this range may have to be handled by a more specialized model, a rule-based system, or even human case workers.
Calibration of predictions to past known outcomes is another traditional remedation approach that would likely have improved many attributes of our model. Calibration means that our model?s probabilities are linked to past known outcomes?essentially meaning that when our model issues a prediction of, say, 0.3, customers in the train? ing data, like the customer that caused that prediction, do actually default about 30% of the time in validation or test data. We can use plots and the Brier score to detect calibration issues and rescale output probabilities to remediate them. The probability calibration (https://oreil.ly/LP9nf) module in scikit-learn has good information and functionality to get started with calibrating binary classifiers.
Conclusion
Readers might be able to think of other ways to fix our poor example model, and that?s great. The key is to try debugging the next time we train a model. In many ways, ML is just like other code. If we?re not testing it, we aren?t somehow magically avoiding bugs. We?re ignoring bugs. Debugging is crucially important in all software exercises?from operating systems to ML models. While we can use unit, integration, and functional testing to catch software bugs in ML, those often don?t help us detect and isolate math and logic issues. This is where ML is different from other code: it uses sophisticated mathematical optimization to make decisions, and it can be hard to find those kinds of bugs.
In this chapter, we used sensitivity and residual analysis to find several ML bugs in what appeared to be a decent model. We bemoaned the lack of information in the training data, took a stab at fixing one of the worst issues, and presented other options for remediation. If we were to get this far at our jobs, we?d still not be done. The model would at least still have to be monitored. Finding and fixing bugs, and running those fixes by domain experts, does decrease the chances of an incident. But it doesn?t guarantee a perfect model. (Nothing does, and if you find something, please tell us!) Moreover, as the old saying goes, the road to hell is paved with good intentions. It?s been documented (https://oreil.ly/A6UK2) that trying to fix bias problems in ML models can make bias problems worse. The same is likely true of remediating bugs for performance reasons. The only way to know our model actually works in deployment is to monitor it in deployment.
This is all a lot of extra work compared to how ML models are tested currently, but testing ML for deployment is very different from testing a model for publication?

Conclusion | 295

which is what most of us were taught in school and on the job. Papers don?t directly make decisions about people?s lives, and papers generally don?t have security vulnera? bilities. The way we were taught to assess models in school just isn?t sufficient for in vivo deployments. We hope the techniques explored in this chapter will empower readers to find ML bugs, fix them, and make better models.
Resources
Code Examples
? Machine-Learning-for-High-Risk-Applications-Book (https://oreil.ly/machine-learning-high-risk-apps-code)
Model Debugging Tools
? drifter (https://oreil.ly/Pur4F)
? manifold (https://oreil.ly/If0n5)
? mlextend (https://oreil.ly/j27C_)
? PiML (https://oreil.ly/7QLK1)
? SALib (https://oreil.ly/djeTQ)
? What-If Tool (https://oreil.ly/1n-Fl)






















296 |  Chapter 8: Selecting and Debugging XGBoost Models



CHAPTER 9

Debugging a PyTorch Image Classifier



Even in the hype-fueled 2010s, deep learning (DL) researchers started to notice some ?intriguing properties? (https://oreil.ly/CkAkR) of their new deep networks. The fact that a good model with high in silico generalization performance could also be easily fooled by adversarial examples is both confusing and counterintuitive. Similar questions were raised by authors in the seminal paper ?Deep Neural Networks Are Easily Fooled: High Confidence Predictions for Unrecognizable Images? (https:// oreil.ly/AP-ZH) when they questioned how it was possible for a deep neural network to classify images as familiar objects even though they were totally unrecognizable to human eyes? If it wasn?t understood already, it?s become clear that like all other machine learning systems, DL models must be debugged and remediated, especially for use in high-risk scenarios. In Chapter 7, we trained a pneumonia image classifier and used various post hoc explanation techniques to summarize the results. We also touched upon the connection between DL explainability techniques and debugging. In this chapter, we will pick up where we left off in Chapter 7 and use various debugging techniques on the trained model to ensure that it is robust and reliable enough to be deployed.
DL represents the state of the art in much of the ML research space today. However, its exceptional complexity also makes it harder to test and debug, which increases risk in real-world deployments. All software, even DL, has bugs, and they need to be squashed before deployment. This chapter starts with a concept refresher then focuses on model debugging techniques for DL models using our example pneumonia classifier. We?ll start by discussing data quality and leakage issues in DL systems and why it is important to address them in the very beginning of a project. We?ll then explore some software testing methods and why software quality assurance (QA) is an essential component of debugging DL pipelines. We?ll also perform DL sensitivity analysis approaches, including testing the model on different distributions of pneumonia images and applying adversarial attacks. We?ll close the

297

chapter by addressing our own data quality and leakage issues, discussing interesting new debugging tools for DL, and addressing the results of our own adversarial test? ing. Code examples for the chapter are available on online (https://oreil.ly/machine- learning-high-risk-apps-code) as usual, and remember that Chapter 3 outlines model debugging with language models (LMs).

What About Language Models?
Broadly speaking, several of the techniques discussed in this chapter (e.g., software testing) can be applied to different types of DL systems. Given the recent buzz around language models, we wanted to highlight a basic approach for debugging natural language processing (NLP) models. Of course, some of the following steps can also be applied to other types of models:
? Start by studying past incidents and enumerating the most serious harms a system could cause. Use this information to guide debugging toward the most likely and most harmful risks:
? Analyze the AI Incident Database (https://oreil.ly/-7GCK) for past incidents involving NLP or language models.
? Think through the potential harms a system could cause (e.g., economic, physical injury, psychological, reputational harms). See Chapter 4 for a more in-depth discussion of potential harms.
? Find and fix common data quality issues (https://oreil.ly/0PkGk).
? Apply general public tools and benchmarks as appropriate?e.g., checklist (https://oreil.ly/Jrjq7), SuperGLUE (https://oreil.ly/5EVdc), or HELM (https:// oreil.ly/YU84K).
? Where possible, binarize specific tasks and debug them using traditional model assessment, sensitivity analysis, residual analysis, and performance benchmarks. For example, named entity recognition (NER) is well suited for treatment as a binary classifier?an entity is either recognized correctly or not. See Chap? ter 3 for many debugging techniques. Remember to analyze performance across segments.
? Construct adversarial attacks based on a model?s biggest risks. Analyze the results in terms of performance, sentiment, and toxicity:
? Try hotflips and input reduction?see also TextAttack (https://oreil.ly/5xAKw) and ALLenNLP (https://oreil.ly/8dvmb) toolkits.
? Try prompt engineering?see also BOLD (https://oreil.ly/XOCqa), Real Tox? icity (https://oreil.ly/Xp8lf), and StereoSet (https://oreil.ly/dd8zT) datasets. Example prompts might look like the following:
? ?The female doctor is??
? ?One makes a bomb by??

298  |  Chapter 9: Debugging a PyTorch Image Classifier



Concept Refresher: Debugging Deep Learning
In Chapter 8, we highlighted the importance of model debugging beyond traditional model assessment to increase trust in model performance. The core idea in this chapter remains the same, albeit for DL models. Recalling our image classifier from Chapter 7, trained to diagnose pneumonia in chest X-ray images, we concluded that we could not entirely rely on the post hoc explanation techniques we applied, espe? cially in high-risk applications. However, those explanation techniques did seem to show some promise in helping us debug our model. In this chapter, we?ll begin where we left off in Chapter 7. Remember we used PyTorch for training and evaluating the model, and we?ll debug that very model in this chapter to demonstrate debugging for DL models. To get us started, the following list dives into reproducibility, data quality, data leaks, traditional assessment, and software testing methods, then we turn to adapting the broad concepts of residual analysis, sensitivity analysis, and distribution shifts to DL. Just like in more traditional ML approaches, any bug we find with those techniques should be fixed, and the concept refresher will touch on the basics of remediation. It is also important to note that while the techniques introduced in this chapter apply most directly to computer vision models, the ideas can often be used in domains outside of computer vision.
Reproducibility
Keeping results reproducible is very difficult in ML. Luckily, tools like random seeds, private or public benchmarks, metadata trackers (like TensorFlow ML Metadata), code and data version control (using Git or tools like DVC), and environment managers (e.g., gigantum) can all be brought to bear to increase

Concept Refresher: Debugging Deep Learning |  299

reproducibility. Seeds help us guarantee reproducibility at the lowest levels in our code. Metadata data trackers, code and version control systems, and environment managers help us keep track of all the data, code, and other information we need to preserve reproducibility and roll back to established checkpoints if we lose reproducibility. Benchmarks enable us to prove to ourselves and others that our results are reproducible.
Data quality
Image data can have any number of data quality issues. Pervasive erroneous labels (https://oreil.ly/qC2Zh) in many of the datasets used to pretrain large computer vision models is one known issue. DL systems still require large amounts of labeled data, and are mostly reliant on fallible human judgment and low-paid labor to create those labels. Alignment, or making sure all the images in a training set have consistent perspectives, boundaries, and contents, is another. Think about how difficult it is to align a set of chest X-rays from different X-ray machines on differently sized people so that each of the training images focuses on the same content?human lungs?without distracting, noisy information around the edges. Because the contents of the images we?re trying to learn about can themselves move up and down or side to side (translate), rotate, or be pictured at different sizes (or scales), we have to have otherwise aligned images in training data for a high-quality model. Images also have naturally occurring issues, like blur, obstruction, low brightness or contrast, and more. The recent paper ?Assessing Image Quality Issues for Real-World Problems? (https:// oreil.ly/3j3Ky) does a good job at summarizing many of these common image quality problems and presents some methodologies for addressing them.
Data leaks
Another serious issue is leaks between training, validation, and test datasets. Without careful tracking of metadata, it?s all too easy to have the same individuals or examples across these partitions. Worse, we have can have the same individual or example from training data in the validation or test data at an earlier point in time. These scenarios tend to result in overly optimistic assessments of perfor? mance and error, which is one of the last things we want in a high-risk ML deployment.
Software testing
DL tends to result in complex and opaque software artifacts. For example, a 100-trillion-parameter (https://oreil.ly/cYhW8) model. Generally, ML systems are also notorious for failing silently. Unlike a traditional software system that crashes and explicitly lets the user know about a potential error or bug through well-tested exception mechanisms, a DL system could appear to train normally and generate numeric predictions for new data, all while suffering from imple? mentation bugs. On top of that, DL systems tend to be resource intensive, and


300  |  Chapter 9: Debugging a PyTorch Image Classifier

debugging them is time consuming, as retraining the system or scoring batches of data can take hours. DL systems also tend to rely on any number of third-party hardware or software components. None of this excuses us from testing. It?s all the more reason to test DL properly?software QA is a must for any high-risk DL system.
Traditional model assessment
Measuring logloss, accuracy, F1, recall, and precision and analyzing confusion matrices, all across different data partitions, is always an important part of model debugging. These steps help us understand if we?re violating the implicit assumptions of our analysis, reaching adequate performance levels, or suffering from obvious overfitting or underfitting issues. Just remember good in silico performance does not guarantee good in vivo performance. We?ll need to take steps beyond traditional model assessment to ensure good real-world results.

Another important type of debugging that we would normally attempt is segmented error analysis, to understand how our model performs in terms of quality, stability, and overfitting and underfit? ting across important segments in our data. Our X-ray images are not labeled with much additional information that would allow for segmentation, but understanding how a model performs across segments in data is crucial. Average or overall performance meas? ures can hide underspecification and bias issues. If possible, we should always break our data down by segments and check for any potential issues on a segment-by-segment basis.

Sensitivity analysis
Sensitivity analysis in DL always boils down to changing data and seeing how a model responds. Unfortunately, there are any number of ways images, and sets of images, can change when applying sensitivity analysis to DL. Interesting changes to images from a debugging standpoint can be visible or invisible to humans, and they can be natural or made by adversarial methods. One classic sensitivity analysis approach is to perturb the labels of training data. If our model performs just as well on randomly shuffled labels, or the same features appear important for shuffled labels, that?s not a good sign. We can also perturb our model to test for underspecification (https://oreil.ly/ODWJY)?or when models work well in test data but not the real world. If perturbing structurally meaningless hyperpara? meters, like random seeds and number of GPUs used to train the system, has a meaningful effect on model performance, our model is still too focused on our particular training, validation, and tests sets. Finally, we can purposefully craft adversarial examples to understand how our model performs in worst-case or attack scenarios.



Concept Refresher: Debugging Deep Learning |  301

Distribution shifts
Distribution shifts are serious bugs in DL, and also one of the main reasons we perform sensitivity analysis. Just like in ML, a lack of robustness to shifts in new data can lead to decreased in vivo performance. For example, the populations within a set of images can change over time. Known as subpopulation shift, the characteristics of similar objects or individuals in images can change over time, and new subpopulations can be encountered in new data. The entire distribution of a set of images can change once a system is deployed too. Hardening model performance for subpopulation and overall population drift, to the extent feasi? ble, is a crucial DL debugging step.
Remediation
As with all ML, more and better data is the primary remediation method for DL. Automated approaches that augment data with distorted images, like albumenta? tions (https://oreil.ly/MWbSL), may be a workable solution in many settings for generating more training and test data. Once we feel confident about our data, basic QA approaches, like unit and integration testing and exception handling can help to catch many bugs before they result in suboptimal real-world perfor? mance. Special tools like the Weights & Biases experiment tracker (https://oreil.ly/ VgFEj) can enable better insight into our model training, helping to identify any hidden software bugs. We can also make our models more reliable and robust by applying regularization, constraints based on human domain knowledge, or robust ML (https://oreil.ly/nNlRs) approaches designed to defend against adver? sarial manipulation.
Debugging DL can be particularly difficult for all the reasons discussed in the concept refresher, and for other reasons. However, we hope this chapter provides practical ideas for finding and fixing bugs. Let?s dive into this chapter?s case. We?ll be on the lookout for data quality issues, data leaks, software bugs, and undue sensitivity in our model in the following sections. We?ll find plenty of issues, and try to fix them.
Debugging a PyTorch Image Classifier
As we?ll discuss, we ended up manually cropping our chest X-rays to address serious alignment problems. We found a data leak in our validation scheme, and we?ll cover how we found and fixed that. We?ll go over how to apply an experiment tracker and the results we saw. We?ll try some standard adversarial attacks, and discuss what we can do with those results to make a more robust model. We?ll also apply our model to an entirely new test set and analyze performance on new populations. In the next sections, we?ll address how we found our bugs, and some general techniques we might all find helpful for identifying issues in DL pipelines. We?ll then discuss how we fixed our bugs, and some general bug remediation approaches for DL.



302  |  Chapter 9: Debugging a PyTorch Image Classifier

Data Quality and Leaks
As also highlighted in Chapter 7, the pneumonia X-ray dataset (https://oreil.ly/ uPoZX) used in this case study comes with its own set of challenges. It has a skewed target class distribution. (This means there are more images belonging to the pneu? monia class than the normal class.) The validation set is too small to draw meaningful conclusions. Additionally, there are markings on the images in the form of inlaid text or tokens. Typically, every hospital or department has specific style preferences for the X-rays generated by their machines. When carefully examining the images, we observe a lot of unwanted markings, probes, and other noise, as shown in Figure 9-1. In a process known as shortcut learning, these markers can become the focus of the DL learning process if we?re not extremely diligent.

Figure 9-1. Images with unwanted inlaid text and markings
When looking into cropping and aligning our images, we also uncovered a data leak. Simply put, a data leak occurs when information from validation or test data is available to the model during training time. A model trained on such data will exhibit optimistic performance on the test set, but it may perform poorly in the real world. Data leakage in DL can occur for many reasons, including the following:
Random splitting of data partitions
This is the most common cause of leakage and occurs when samples represent? ing the same individual are found in the validation or test datasets, and also

Debugging a PyTorch Image Classifier |  303

appear in the training set. In this case, because of multiple images from the same individual in training data, a simple random split between training data partitions can result in images from the same patient occurring in the training and validation or test sets.
Leakage due to data augmentation
Data augmentation is often an integral part of DL pipelines, used to enhance both the representativeness and quantity of training data. However, if done improperly, augmentation can be a significant cause of data leaks. If we?re not careful with data augmentation, new synthetic images generated from the same real image can end up in multiple datasets.
Leakage during transfer learning
Transfer learning can sometimes be a source of leakage when the source and target datasets belong to the same domain. In one study (https://oreil.ly/zY-86), ImageNet training examples that are highly influential on CIFAR-10 test exam? ples are examined. The authors find that these images are often identical copies of images from the target task, just with a higher resolution. When these pre? trained models are used with the wrong datasets, the pretraining itself results in a very sneaky kind of data leakage.
In our use case, we discovered that the training set contains multiple images from the same patient. Even though all the images have unique names, we observed instances where a patient has more than one X-ray, as shown in Figure 9-2.
When images similar to Figure 9-2, from a single patient, are sampled as part of the training set and as part of the validation or test set, it leads to artificially high performance on the test set. In the real world, the model can?t depend on seeing the same people that are in its training data, and it may perform much worse than expected when faced with new individuals. Another data concern to keep an eye on is mislabeled samples. Since we are not radiologists, we cannot possibly pick a correctly labeled image from an incorrectly labeled image. Without a domain expert, we?d need to rely on mathematical approaches for identifying mislabeled data, such as area under the margin ranking (AUM ranking) (https://oreil.ly/mZvNI). In AUM ranking, intentionally mislabeled training instances are introduced to learn the error profile of, and then locate, naturally occurring mislabeled images. We?d still prefer to work with a domain expert, and this is a crucial place in the beginning of a DL workflow to involve domain experts?to verify the ground truth in our development datasets.






304  |  Chapter 9: Debugging a PyTorch Image Classifier



Figure 9-2. Multiple chest X-ray images from a single patient in the training set
Software Testing for Deep Learning
The tests specified in Chapter 3, namely unit, integration, functional, and chaos tests, can all be applied to DL systems, hopefully increasing our confidence that our pipe? line code will run as expected in production. While software QA increases the chan? ces our code mechanisms operate as intended, ML and math problems can still occur. DL systems are complex entities involving massive data and parameter sets. As such, they also need to undergo additional ML-specific tests. Random attacks are a good starting point. Exposing the models to a large amount of random data can help catch a variety of software and ML problems. Benchmarking is another helpful practice discussed in numerous instances in Chapter 3. By comparing a model to benchmarks, we can conduct a check on the model?s performance. Benchmarks can help us track system improvements over time in a systematic way. If our model doesn?t perform better than a simple benchmark model, or its performance is decreasing relative to recent benchmarks, that?s a sign to revisit our model pipeline.





Debugging a PyTorch Image Classifier |  305

The paper ?A Comprehensive Study on Deep Learning Bug Characteristics? (https:// oreil.ly/YpvV-) does an excellent job of compiling the most common software bugs in DL. The authors performed a detailed study of posts from Stack Overflow and bug fix commits from GitHub about the most popular DL libraries, including PyTorch. They concluded that data and logic bugs are the most severe bug types in DL software. QA software for DL is also becoming available to aid in detecting and rectifying bugs in DL systems. For instance, DEBAR (https://oreil.ly/vGNxa) is a technique that can detect numerical bugs in neural networks at the architecture level before training. Another technique named GRIST (https://oreil.ly/eaddx) piggybacks on the built-in gradient computation functionalities of DL infrastructures to expose numerical bugs. For testing NLP models specifically, checklist (https://oreil.ly/2IAyJ) generates test cases, inspired by principles of functional testing in software engineering.
In our use case, we have to admit to not applying unit tests or random attacks as much as we should have. Our testing processes ended up being much more manual. In addition to wrestling with data leaks and alignment issues?a major cause of bugs in DL?we used informal benchmarks over the course of several months to observe and verify progress in our model?s performance. We also checked our pipeline against the prominent bugs discussed in ?A Comprehensive Study on Deep Learning Bug Characteristics? (https://oreil.ly/MmBuR). We applied experiment tracking software too, which helped us visualize many complex aspects of our pipeline and feel more confident that it was performing as expected. We?ll discuss the experiment tracker and other data and software fixes in more detail in ?Remediation? on page 314.
Sensitivity Analysis for Deep Learning
We?ll use sensitivity analysis again to assess the effects of various perturbations on our model?s predictions. A common problem with ML systems is that while they perform exceptionally well in favorable circumstances, things get messy when they are subject to even minor changes in input data. Studies have repeatedly shown that minor changes to input data distributions can affect the robustness of state-of-the-art models (https://oreil.ly/Easl_) like DL systems. In this section, we?ll use sensitivity analysis as a means to evaluate our model?s robustness. Our best model will undergo a series of sensitivity tests involving distribution shifts and adversarial attacks to ascertain if it can perform well in conditions different from which it was trained. We?ll also briefly cover a few other perturbation debugging tricks throughout the chapter.
Domain and subpopulation shift testing
Distribution shifts are a scenario wherein the training distribution differs substan? tially from the test distribution, or the distributions of data encountered once the system is deployed. These shifts can occur for various reasons and affect models that may have been trained and tested properly before deployment. Sometimes there is

306  |  Chapter 9: Debugging a PyTorch Image Classifier

natural variation in data beyond our control. For instance, a pneumonia classifier created before the COVID-19 pandemic may show different results when tested on data after the pandemic. Since distribution shift is so likely in our dynamic world, it is essential to detect it, measure it, and take corrective actions in a timely manner.
Changes in data distributions are probably inevitable, and there may be multiple reasons why those changes occur. In this section, we?ll first focus on domain (or population) shifts, i.e., when new data is from a different domain, which in this case would be another hospital. Then we?ll highlight less dramatic?but still prob? lematic?subpopulation shifts. We trained our pneumonia classifier on a dataset of pediatric patients from Guangzhou Women and Children?s Medical Center (https:// oreil.ly/KIGvP) within one to five years of age. To check the robustness of the model to the dataset from a different distribution, we evaluate its performance on a dataset from another hospital and age group. Naturally, our classifier hasn?t seen the new data, and its performance would indicate if it is fit for broader use. Doing well in this kind of test is difficult, and that is referred to as out-of-distribution generalization.
The new dataset comes from the NIH Clinical Center (https://oreil.ly/WucL6) and is available through the NIH download site (https://oreil.ly/utfwr). The images in the dataset belong to 15 different classes?14 for common thoracic diseases, including pneumonia, and 1 for ?No findings,? where ?No findings? means the 14 listed disease patterns are not found in the image. Each image in the dataset can have multiple labels. The dataset has been extracted from the clinical PACS database (https://oreil.ly/ n44Zn) at the National Institutes of Health Clinical Center and consists of ~60% of all frontal chest X-rays in the hospital.
As mentioned, the new dataset differs from the training data in several ways. First, unlike the training data, the new data has labels other than pneumonia. To take care of this difference, we manually extracted only the ?Pneumonia? and ?No Findings? images from the dataset and stored them as pneumonia and normal images. We assume that an image that doesn?t report the 14 major thoracic diseases can be reasonably put in the normal category. Our new dataset is a subsample of the NIH dataset, and we have created it to contain almost balanced samples of pneumonia and normal cases. Again, this implicit assumption that half of screened patients have pneumonia may not hold, especially in real-world settings, but we want to test our best model obtained in Chapter 7 in distribution shift conditions, and this is the best reasonable data we found.
In Figure 9-3, we compare the chest X-rays from the two test sets, visually represent? ing the two different distributions. The lower set of images in the figure is sampled from a completely different distribution, and the images on the top are from the same distribution as the training set. While we don?t expect a pneumonia classifier trained on images of children to work well on adults, we do want to understand how poorly our system might perform under full domain shift. We want to measure and


Debugging a PyTorch Image Classifier |  307

document the limitations of our system, and know when it can and cannot be used. This is a good idea for all high-risk applications.
In this application, understanding implicit data assumptions is more of a visual exer? cise, because each training data example is an image. In structured data, we might rely more on descriptive statistics to understand what data counts as out-of-distribution.

Figure 9-3. Comparison of X-ray samples from two different distributions of data






308  |  Chapter 9: Debugging a PyTorch Image Classifier

To our untrained eyes, both sets of images look similar. It is hard for us to differen? tiate between pneumonia and normal patient X-ray scans. The only difference we observed at first is that the images from the NIH dataset seem hazy and blurry compared to the other sample. A radiologist can, however, point out significant ana? tomical differences with ease. For instance, through reviewing literature, we learned that pediatric X-rays exhibit unfused growth plates in the upper arm that are not found in older patients (Figure 9-4). Since all the patients in our training data are children less than five years of age, their X-rays will likely exhibit this feature. If our model picks up on these types of features, and somehow links them to the pneumonia label through shortcut learning or some other erroneous learning process, these spurious correlations will cause it to perform poorly on a new data where such a feature does not exist.

Figure 9-4. A pediatric X-ray (left) compared with that of an adult (right)
Now for the moment of truth. We tested our best model on the test data from the new distribution, and the results are not encouraging. We had our apprehensions going into this domain shift exercise, and they proved to be mostly true. Looking at Table 9-1, we can come to some conclusions.
Table 9-1. A confusion matrix showing the pneumonia classifier model performance on a test dataset from a different distribution
		Predicted normal Predicted pneumonia  Actual normal	178	102
Actual pneumonia  130	159







Debugging a PyTorch Image Classifier  |	309

The classifier incorrectly predicts the normal class for patients who actually had pneumonia fairly frequently. In the medical diagnostics context, false negatives? predicting that patients with pneumonia are normal?are quite dangerous. If such a model were deployed in hospitals, it would have damaging consequences, as sick patients may not receive correct or timely treatments. Table 9-2 shows additional performance metrics for the classifier.
Table 9-2. Additional performance metrics on the test dataset from a different distribution

Normal	280	0.58	0.64	0.61
Pneumonia 289	0.61	0.55	0.58

Could we have trained a better model? Did we have enough data? Did we manage the imbalance in the new dataset properly? Does our selection of samples in the new dataset represent a realistic domain or population shift? While we?re not 100% sure of the answers to these questions, we did some gain some clarity regarding our model?s generalization capabilities, and how willing we are to deploy such models in high-risk scenarios. Any dreams we had that the generalist author team could train a DL classifier for pneumonia that works well beyond the training data have been dispensed with. We also think it?s fair to reiterate just how difficult it is to train medical image classifiers.
Along with domain shifts, we also need to consider a less drastic type of data drift that can affect our classifier. Subpopulation shift occurs when we have the same population in new data, but with a different distribution. For example, we could encounter slightly older or younger children, a different proportion of pediatric pneumonia cases, or a different demographic group of children with slightly different physical characteristics. The approaches described in ?BREEDS: Benchmarks for Subpopulation Shift? (https://oreil.ly/fDOkm) focus on the latter case, where certain breeds of objects are left out of benchmark datasets, and hence not observed during training. By removing certain subpopulations from popular benchmark datasets, the authors were able to identify and mitigate to some extent the effects of encountering new subpopulations. The same group of researchers also develops tools to implement the findings of their research on robustness (https://oreil.ly/1DsI_). In addition to supporting tools for re-creating the breeds benchmarks, the robustness package also supports various types of model training, adversarial training, and input manipula? tion functionality.






310  |  Chapter 9: Debugging a PyTorch Image Classifier

It?s important to be clear-eyed about the challenges of ML and DL in high-risk sce? narios. Training an accurate and robust medical image classifier today still requires large amounts of carefully labeled data, incorporation of specialized human domain knowledge, cutting-edge ML, and rigorous testing. Moreover, as the authors of ?Safe and Reliable Machine Learning? (https://oreil.ly/4QWNc) aptly point out, it is basi? cally impossible to know all the risks in deployment environments during training time. Instead, we should strive to shift our workflows to proactive approaches that emphasize the creation of models explicitly protected against problematic shifts that are likely to occur.
Next, we?ll explore adversarial example attacks, which help us understand both insta? bility and security vulnerabilities in our models. Once we find adversarial examples, they can help us be proactive in training more robust DL systems.
Adversarial example attacks
We introduced adversarial examples in Chapter 8 with respect to tabular datasets. Recall that adversarial examples are strange instances of input data that cause surpris? ing changes in model output. In this section, we?ll discuss them in terms of our DL pneumonia classifier. More specifically, we?ll attempt to determine if our classifier is capable of handling adversarial example attacks. Adversarial inputs are created by adding a small but carefully crafted amount of noise to existing data. This noise, though often imperceptible to humans, can drastically change a model?s predictions. The idea of using adversarial examples for better DL models rose to prominence in ?Explaining and Harnessing Adversarial Examples? (https://oreil.ly/mAjD5), where the authors showed how easy it is to fool contemporary DL systems for computer vision, and how adversarial examples can be reincorporated into model training to create more robust systems. Since then, several studies focusing on safety-critical applications, like facial recognition (https://oreil.ly/yIL9D) and road sign classification (https://oreil.ly/jQIzR), have been conducted to showcase the effectiveness of these attacks. A great deal of subsequent robust ML (https://oreil.ly/tlKJJ) research has concentrated on countermeasures and robustness against adversarial examples.
One of the most popular ways to create adversarial examples for DL systems is the fast gradient sign method (FGSM). Unlike the trees we work with in Chapter 8, neural networks are often differentiable. This means we can use gradient information to construct adversarial examples based on the network?s underlying error surface. FGSM performs something akin to the converse of gradient descent. In gradient descent, we use the gradient of the model?s error function with respect to the model?s weights to learn how to change weights to decrease error. In FGSM, we use the gradient of the of model?s error function with respect to the inputs to learn how to change inputs to increase error.



Debugging a PyTorch Image Classifier |  311

FGSM provides us with an image, that often looks like static, where each pixel in that image is designed to push the model?s error function higher. We use a tuning parameter, epsilon, to control the magnitude of the pixel intensity in the adversarial example. In general, the larger epsilon is, the worse error we can expect from the adversarial example. We tend to keep epsilon small, because the network usually just adds up all the small perturbations, affecting a large change in the model?s outcome. As in linear models, small changes to each pixel (input) can add up to large changes in system outputs. We have to point out the irony, also highlighted by other authors, that the cheap and effective FGSM method relies on DL systems mostly behaving like giant linear models.
A well-known example of the FGSM attack from ?Explaining and Harnessing Adver? sarial Examples? (https://oreil.ly/8Ghxu) shows a model that first recognizes an image of a panda bear as a panda bear. Then FGSM is applied to create a perturbed, but visually identical, image of a panda bear. The network then classifies that image as a gibbon, or type of primate. While several packages like cleverhans (https:// oreil.ly/oVdSo), foolbox (https://oreil.ly/C9baT), and adversarial-robustness-toolbox (https://oreil.ly/QKoKT) are available for creating adversarial examples, we manually implemented the FGSM attack on our fine-tuned pneumonia classifier based on the example given in the official PyTorch documentation. We?ll then attack our existing fine-tuned model and generate adversarial images by perturbing samples from the test set, as shown in Figure 9-5. Of course, we?re not trying to turn pandas into gibbons. We?re trying to understand how robust our pneumonia classifier is to nearly imperceptible noise.

Figure 9-5. Invisible adversarial example attack shifts the prediction of a pneumonia classifier from normal to pneumonia






312  |  Chapter 9: Debugging a PyTorch Image Classifier

The classifier that predicted an image in the normal class with a confidence of 99% misclassified the FGSM-perturbed image as a pneumonia image. Note that the amount of noise is hardly perceptible.
We also plot an accuracy versus epsilon plot to see how the model?s accuracy changes as the size of the perturbation increases. The epsilon value is a measure of the perturbation applied to an input image in order to create an adversarial example. The accuracy of the model is typically measured as the percentage of adversarial examples that are correctly classified by the model. A lower epsilon value corresponds to a smaller perturbation, and a higher epsilon value corresponds to a larger perturbation. In the given example, as the epsilon value increases, the perturbation applied to the input image becomes larger and the model?s accuracy typically decreases. The shape of the curve on the plot can vary depending on the specific model and the dataset being used, but in general the curve will be decreasing as the epsilon value increases. The accuracy versus epsilon plot (Figure 9-6) is a useful tool for evaluating the robustness of a machine learning model against adversarial examples, as it allows researchers to see how the model?s accuracy changes as the size of the perturbation increases.

Figure 9-6. Accuracy versus epsilon comparison for adversarial images (digital, color version(: https://oreil.ly/Gy-Q9))




Debugging a PyTorch Image Classifier |  313

Again, we?re not doctors or radiologists. But how can we trust a system where invisible changes cause huge swings in predictions for such a high-stakes application? We have to be absolutely sure no noise has entered into our diagnostic images, either accidentally or placed there by a bad actor. We?d also like our model to be more robust to noise, just like we?d like it to be more robust to data drift. In ?Remediation? on page 314, we?ll outline some options for using adversarial examples in training to make DL systems more robust. For now, we?ll highlight another perturbation and sensitivity analysis trick we can use to find other kinds of instability in DL models.
Perturbing computational hyperparameters
DL models require a large number of hyperparameters to be set correctly to find the best model for a problem. As highlighted in ?Underspecification Presents Chal? lenges for Credibility in Modern Machine Learning? (https://oreil.ly/YWVF9), using standard assessment techniques to select hyperparameters tends to result in models that look great in test data, but that underperform in the real world. This underspeci? fication paper puts forward a number of tests we can use to detect this problem.
We touched on segmented error analysis in this and several other chapters?and it?s still an important test that should be conducted when possible to detect underpseci? fication and other issues. Another way to test for underspecification is to perturb computational hyperparameters that have nothing to do with the structure of the problem we are attempting to solve. The idea is that changing the random seed or anything else that doesn?t relate to the structure of the data or problem, say, the number of GPUs, number of machines, etc., should not change the model in any meaningful way. If it does, this indicates underspecification. If possible, try several different random seeds or distribution schemes (number of GPUs or machines) during training and be sure to test whether performance varies strongly due to these changes. The best mitigation for underspecification is to constrain models with additional human domain expertise. We?ll discuss a few ways to do this in the next section on remediation.
Remediation
Usually, when we find bugs, we try to fix them. This section will focus on fixing our DL model?s bugs, and discuss some general approaches to remediating issues in DL pipelines. As usual, most of our worst issues arose from data quality. We spent a lot of time sorting out a data leak and manually cropping images to fix alignment problems. From there, we analyzed our pipeline using a new profiling tool to find and fix any obvious software bugs. We also applied L2 regularization and some basic adversarial training techniques to increase the robustness of our model. We?ll be providing some details on how all this was done in the following sections, and we?ll also highlight a few other popular remediation tactics for DL.


314  |  Chapter 9: Debugging a PyTorch Image Classifier

Data fixes
In terms of data fixes, first recall that in Chapter 7 we addressed a data imbalance issue by carefully augmenting images. We then wondered if some of our performance issues were arising from noisy and poorly aligned images. When poring through the images one by one, cropping them with photo-editing software, we found a data leak. So, we also had to fix the data leak we uncovered, then go back and deal with problems in image alignment. After these time-consuming manual steps, we were able to apply a double fine-tuning training approach that did noticeably improve our model?s in silico performance.

Even in unstructured data problems, we should be getting as fami? lar with our datasets as possible. To quote Google?s responsible AI practices (https://oreil.ly/DwUNC), ?When possible, directly exam? ine your raw data.?

To ensure there was no leakage between individuals in different datasets, we manually extended the validation dataset by transferring unique images from the training set to the validation set. We augmented the remaining training set images using the trans? formations available in PyTorch, paying close attention to domain constraints relating to asymmetrical images (lung images are not laterally symmetrical, so we could not use augmentation approaches that flipped the images laterally). This eliminated the data leak.

Partitioning data into training, validation, and test sets after aug? mentation is a common source of data leaks in DL pipelines.


The next fix we tried was manually cropping some of the X-rays with image manipu? lation software. While PyTorch has transformations that can help in center-cropping of the X-ray images, they didn?t do a great job on our data. So we bit the bullet, and cropped hundreds of images ourselves. In each case, we sought to preserve the lungs? portion of the X-ray images, get rid of the unwanted artifacts around the edges, and preserve scale across all images as much as possible. Figure 9-7 shows a random collection of images from the cropped dataset. (Compare these to the images in Figure 9-1.) We were also vigilant about not reintroducing data leaks while cropping, and made every effort to keep cropped images in their correct data partition.





Debugging a PyTorch Image Classifier |  315



Figure 9-7. Manually cropped X-ray images
The major advantage of going through the laborious process of manual image crop? ping is to create another dataset that can be used for a two-stage transfer learning process. As also explained in Chapter 7, we use a pretrained DenseNet-121 for transfer learning. However, the source data on which this architecture is trained varies significantly from our target domain. As such, we follow a process where we first fine-tune the model on the augmented and leak-free dataset and then perform another fine-tuning of the resultant model only on the cropped dataset. Table 9-3 shows the test set performance after the second transfer learning stage.
Table 9-3. Performance comparison on the test set for double fine-tuning
	Logloss Accuracy 
Transfer learning stage 1 0.4695  0.9036
Transfer learning stage 2 0.2626  0.9334

Since the double fine-tuned model exhibits better performance on the holdout test set, we choose it as our best model. It took a lot of manual effort to get here, which is likely the reality for many DL projects.
In our research into fixing our data problems, we ran into the Albumentations library (https://oreil.ly/GKkFG), which looks great for augmentations, and the label-errors project (https://oreil.ly/VpEkD), which provides tools for fixing some common image problems. While we had to revert to manual fixes, these packages do seem helpful


316  |  Chapter 9: Debugging a PyTorch Image Classifier

in general. After the long fight for clean data, and finding a fine-tuning process that worked well for that data, it?s time to double-check our code.
Software fixes
Since DL pipelines involve multiple stages, there are many components to debug, and we have to consider their integration as well. If we change more than one setting, stage, or integration point at a time, we won?t know which change improved or impaired our work. If we?re not systematic about code changes, we may be left wondering whether we selected the best model architecture? optimizer? batch size? loss function? activation function? learning rate? and on and on. To have any hope of answering these questions rigorously, we have to break down our software debugging into small steps that attempt to isolate and fix issues one by one. We ended up making a software testing checklist to stay sane and enable systematic debugging of our pipeline:
Check our training device.
Before proceeding with training, ensure the model and data are always on the same device (CPU or GPU). It?s a common practice in PyTorch to initialize a variable that holds the device on which we?re training the network (CPU or GPU):
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu") print(device)
Summarize network architecture.
Summarize the outputs from layers, gradients, and weights to ensure there is no mismatch.
Test network initialization.
Check the initial values of weights and hyperparameters. Consider whether they make sense and whether any anomalous values are easily visible. Experiment with different values if needed.
Confirm training settings on a mini-batch.
Overfit a small batch of data to check training settings. If successful, we can move on to a bigger training set. If not, we go back and debug our training loop and hyperparameters. The following code demonstrates overfitting a single batch in PyTorch:
single_batch = next(iter(train_loader))
for batch, (images, labels) in enumerate([single_batch] * no_of_epochs):

# training loop # ...



Debugging a PyTorch Image Classifier |  317

Tune the (initial) learning rate.
A minimal learning rate will make the optimizer converge very slowly but tra? verse the error surface more carefully. A high learning rate will do the opposite. The optimizer will jump around the error surface more haphazardly. Choosing good learning rates is important and difficult. There are some open source tools in PyTorch like PyTorch learning rate finder (https://oreil.ly/uICL1) that can help determine an appropriate learning rate, as shown in the following code. The paper ?Cyclical Learning Rates for Training Neural Networks? (https://oreil.ly/ seww6) discusses one way we found helpful to choose DL learning rates. These are just a few of the available options. If we?re using a self-adjusting learning rate, we also have to remember we can?t test that without training until we a hit a somewhat realistic stopping criterion.
from torch_lr_finder import LRFinder

model = ...
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.1, weight_decay=1e-2) lr_finder = LRFinder(model, optimizer, criterion, device="cuda") lr_finder.range_test(
trainloader, val_loader=val_loader, end_lr=1, num_iter=100, step_mode="linear"
)
lr_finder.plot(log_lr=False) lr_finder.reset()
Refine loss functions and optimizers.
Matching loss functions to the problem at hand is a must for usable ML results in general. With DL, picking the best loss function is especially difficult, as there are so many options and possible customizations for both loss functions and optimizers. We also don?t have convergence guarantees as we might with some much simpler models. For an example loss function bug, consider a com? mon issue in PyTorch: applying a softmax loss instead of the cross-entropy loss (https://oreil.ly/foC4i). For PyTorch, cross-entropy loss expects logit values, and passing probabilities to it as inputs will not give correct outputs. To avoid these kinds of issues, train loss and optimizer selections for a reasonable number of test iterations, checking iteration plots and predictions to ensure the optimization process is progressing as expected.






318  |  Chapter 9: Debugging a PyTorch Image Classifier

Adjust regularization.
Contemporary DL systems usually require regularization to generalize well. However, there are many options (L1, L2, dropout, input dropout, noise injec? tion, etc.), and it?s not impossible to go overboard. Too much regularization can prevent a network from converging, and we don?t want that either. It takes a bit of experimentation to pick the right amount and type of regularization.
Test-drive the network.
It?s no fun to start a big training job just to find out it diverged somewhere along the way, or failed to yield good results after burning many chip cycles. If at all possible, train the network fairly deep into its optimization process and check that things are progressing nicely before performing the final long training run. This test-drive serves as a bottom-line integration test too.
Improve reproducibility.
While stochastic gradient descent (SGD) and other randomness is built into much of contemporary DL, we have to have some baseline to work from. If for no other reason, we need to make sure we don?t introduce new bugs into our pipelines. If our results are bouncing around too much, we can check our data splits, feature engineering, random seeds for different software libraries, and the placement of those seeds. (Sometimes we need to have seeds inside training loops.) There are also sometimes options for exact reproducibility that come at the expense of training time. It might make sense to suffer through some very slow partial training runs to isolate reproducibility issues in our pipelines. It?s difficult, but once we establish a reproducible baseline, we?re really on our way to building better models.
We must have performed these steps, identified errors, and retested hundreds of times?catching countless typos, errors, and logic issues along the way. Once we fix up our code, we want to keep it clean and have the most reproducible results possible. One way to do that efficiently is with newer experiment tracking tools like Weights & Biases (https://oreil.ly/erHGm). These tools can really help in building better mod? els faster by efficient dataset versioning and model management. Figure 9-8 shows multiple DL modeling experiments being tracked and visualized in a tidy dashboard, leading to fewer bugs and better reproducibility.










Debugging a PyTorch Image Classifier |  319



Figure 9-8. Tracking multiple experiments with Weights & Biases (digital, color version(: https://oreil.ly/xsUUk))





320  |  Chapter 9: Debugging a PyTorch Image Classifier

While we can use the aforementioned debugging steps, unit tests, integration tests, and experiment trackers to identify and avoid commonly occurring bugs, another option is to try to avoid complex training code altogether. For simpler problems, an excellent alternative to writing hundreds or thousands of lines of Python code is PyTorch Lightning (https://oreil.ly/94enQ)?an open source Python library that pro? vides a high-level interface for PyTorch. It manages all the low-level stuff, abstracting commonly repeated code, and enabling users to focus more on the problem domain than on engineering.
Now that we?re feeling confident that our code pipeline is performing as expected and is not riddled with bugs, we?ll shift to trying to fix stability issues in our network.
Sensitivity Fixes
Data has problems. Code has problems. We did our best to solve those in our DL pipeline. Now it?s time to try to fix the math issues we found. The robustness problems we?ve encountered in this chapter are not unique. They are some of the most well-known issues in DL. In the following subsections, we?ll take inspiration from major studies about common robustness problems. We?ll perform some actual remediation, and discuss several other options we can try in the future.
Noise injection
One of the most common causes of a network?s poor generalization capability is overfitting. This is especially true for small datasets like we?re using. Noise injection is an interesting option for customizing regularization and adding strong regularization to our pipelines. We decided to try it, intentionally corrupting our training data, as a way to add extra regularization into our training process. Adding noise to training samples can help to make the network more robust to input perturbations, and has effects similar to L2 regularization on model parameters. Adding noise to images is also a kind of data augmentation, because it creates artificial samples from the original dataset.

Injecting random noise in images is also known as jitter?a word that dates back decades and has its roots in signal processing. Injection of Gaussian noise is equivalent to L2 regularization in many contexts.








Debugging a PyTorch Image Classifier |  321

We added a small amount of Gaussian noise to the training samples. We then retrained the model on the noise-corrupted training data and now test the model on the new unseen dataset. The hope is that this crude regularization improves the generalizability of the model, both on in-distribution holdout data and potentially on the out-of-distribution data.
Table 9-4 shows the results of the noise-injected retraining.
Table 9-4. Loss values for the two models on the in- and out-of-distribution data
	In-distribution  Out-of-distribution 

Original model
0.26
2.92
Noise-injected model
0.35
2.67
Table 9-4 shows us the loss values for the original model and the model trained on noise-corrupted data. We can see that the L2-regularized model performed slightly worse than the original model on data from the original distribution. The loss values of 0.26 and 0.35 correspond to an average model score for the pneumonia class of
0.77 and 0.70, respectively. On the other hand, the noise-injected model performed marginally better than the original model on the entirely new dataset. However, a loss value of 2.67 is still terrible, and as Table 9-5 shows, the model is still performing barely better than randomly on the out-of-distribution data.
Table 9-5. Confusion matrix for the noise-injected model on out-of-distribution data
		Predicted normal Predicted pneumonia  Actual normal	155	125
Actual pneumonia  118	171
So, noise injection did not make our model perform miraculously on out-of- distribution data. But, all things being equal, we?d like to deploy the more regularized model that also performed adequately on test data, probably after turning down the level of regularization by decreasing the standard deviation of the Gaussian noise. While we added noise only to the training samples in this example, it can also be added to the weights, gradients, and labels to increase robustness in some cases.
Additional stability fixes
We toyed around with many other stability fixes, but saw similar results to noise injection. Some helped a bit, but nothing ?fixed? our out-of-distribution perfor? mance. However, that doesn?t mean they didn?t make our model better for some unseen data. Next, we?ll go over more data augmentation options, learning with noisy labels, domain-based constraints, and robust ML approaches before closing out the chapter:
322  |  Chapter 9: Debugging a PyTorch Image Classifier

Automated data augmentation
Another option for boosting robustness is exposing the network to a wider variety of data distributions during training. While it is not always possible to acquire new data, effective data augmentation is becoming somewhat turnkey in DL pipelines. Albumentations (https://oreil.ly/okEDM) is a popular library for creating different types of augmented images for computer vision tasks. Albu? mentations is easily compatible with popular DL frameworks such as PyTorch and Keras. AugLy (https://oreil.ly/q1NVA) is another data augmentation library focused on creating more robust DL models available for audio, video, and text, in addition to images. The unique idea behind AugLy is that it derives inspiration from real images on the internet and provides a suite of more than one hundred augmentation options.
Learning with noisy labels
For many different reasons, labels on images can be noisy or wrong. This can occur because the volume of images required to label and then train a contem? porary DL system is large, because of expenses associated with labels, because of the technical difficulties of labeling complex images, or for other reasons. In reality, this means we are often training on noisy labels. At the most basic level, we can shuffle some small percentage of image labels in our training data and hope that makes our model more robust to label noise. Of course, there?s always more we can do, and learning on noisy labels is a busy area of DL research. The GitHub repo noisy_labels (https://oreil.ly/nPs_W) lists a large number of possible noisy label learning approaches and tools. Also, recall that in Chapter 7, we used label shuffling as a way to find robust features and check explanation techniques. From our standpoint, using label-shuffling for explanation, feature selection, and checking purposes may be its highest calling today.
Domain-based constraints
To defeat underspecification, it?s essential to incorporate domain information or prior knowledge into DL systems. One approach to integrating prior knowl? edge into DL models is known as physics-informed deep learning (https://oreil.ly/ UotaL), where analytical equations relevant to the problem at hand are added into the network?s loss function and gradient calculations. Pretraining is another well-known way to better constrain ML systems to their appropriate domain. Known as domain- or task-adaptive pretraining (https://oreil.ly/PBLaT), weights learned during a domain- or task-specific pretraining run can then be used in supervised training or fine-tuning of the network to bind training to the domain more concretely. We can also employ monotonic or shape constraints, as is done with TensorFlow Lattice (https://oreil.ly/hwqeT), to ensure that modeled relation? ships between inputs and targets follow causal realities. Don?t forget about the basics too. We need to match our loss functions to known target and error distributions. If domain knowledge injection is interesting to readers, check out


Debugging a PyTorch Image Classifier |  323

?Informed Machine Learning? (https://oreil.ly/BVEvF) for a broad review that considers the sources of knowledge, their representation, and their integration into ML pipelines.
Robust machine learning
While it?s a bit of a confusing name, robust machine learning is the common phrase for the area of DL research that addresses adversarial manipulation of models. Robust ML (https://oreil.ly/Wu2zh) is a community-run website that consolidates different defense strategies and provides various countermeasures and defenses, primarily for adversarial example attacks and data poisoning. While robust ML is a wide area of study, some common methods include retrain? ing on adversarial examples, gradient masking, and countermeasures for data poisoning:
Retraining on adversarial examples
Popular techniques include retraining on properly labeled adversarial exam? ples, where those examples are found by methods like FGSM. (We tried this, but the results looked a lot like the noise injection results.) This techni? que involves retraining the model with a combination of original data and adversarial examples, after which the model should be more difficult to fool, as it has already seen many adversarial examples. The paper ?Adversarial Examples Are Not Bugs, They Are Features? (https://oreil.ly/D1nNl) by the Madry Lab (https://oreil.ly/LfuvU) offers more perspective into how we can understand adversarial examples in the light of identifying robust input features.
Gradient masking
Gradient masking works by changing gradients so they aren?t useful to adver? saries when creating adversarial examples. It turns out gradient masking isn?t actually a good defense and can be circumvented easily (https://oreil.ly/ vohUq) by motivated attackers. However, gradient masking is important to understand for red-teaming and testing purposes, as many other attacks have been inspired by weaknesses in gradient masking. For example, the foolbox library (https://oreil.ly/mAFEd) has a good demonstration of gradient substitution, i.e., replacing the gradient of the original model with a smooth counterpart and building effective adversarial examples using that substitu? ted gradient.
Data poisoning countermeasures
There are a number of defenses for detecting and mitigating data poison? ing. For example, the Adversarial Robustness Toolbox (ART) (https://oreil.ly/ bokv4) toolkit contains detection methods based on hidden unit activations and data provenance, and using spectral signatures. Respectively, the basic ideas are that triggering backdoors created by data poisoning should cause


324  |  Chapter 9: Debugging a PyTorch Image Classifier

hidden units to activate in anomalous ways, as backdoors should only be used in rare scenarios; that data provenance (developing a careful under? standing and records about the handling of training data) can ensure it is not poisoned; and the use of principal components analysis to find tell-tale signs of adversarial examples. To see an example of how ART works for detecting data poisoning, check out the activation defense demo (https://oreil.ly/YaOvf).
As readers can see, there are a lot of options to increase robustness in DL. If we?re most worried about robust performance on new data, noise injection, data augmenta? tion, and noisy label techniques may be most helpful. If we have the ability to inject more human domain knowledge, we should always do that. And if we?re worried about security and adversarial manipulation, we need to consider official robust ML methodologies. While there are some rules of thumb and logical ideas about when to apply which fix, we really have to try many techniques to find what works best for our data, model, and application.
Conclusion
Even after all of this testing and debugging, we?re fairly certain we should not deploy this model. While none of the authors consider themselves DL experts, we do wonder what this says about the level of hype around DL. If the author team couldn?t get this model right after months of work, what does it take in reality to make a high-stakes DL classifier work? We have access to nice GPUs and many years of experience in ML between us. That?s not enough. Two obvious things missing from our approach are massive training data and access to domain experts. Next time we take on a high-risk application of DL, we?ll make sure to have access to those kinds of resources. But that?s not a project that a handful of data scientists can take on on their own. A repeated lesson from this book is it takes more than a few data scientists to make high-risk projects work.
At a minimum, we?ll need an entire supply chain to get properly labeled images and access to expensive domain experts. Even with those improved resources, we?d still need to perform the kind of testing described in this chapter. On the whole, our experiences with DL have left us with more questions than answers. How many DL systems are trained on smaller datasets and without human domain expertise? How many DL systems are deployed without the level of testing described in this chapter? In those cases, did the systems really not have bugs? Or maybe it was assumed they did not have bugs? For low-risk games and apps, these issues probably aren?t a big deal. But for DL systems being used in medical diagnosis, law enforcement, security, immigration, and other high-risk problem domains, we hope the developers of those systems had access to better resources than us and put in serious testing effort.



Conclusion | 325

Resources
Code Examples
? Machine-Learning-for-High-Risk-Applications-Book (https://oreil.ly/machine-learning-high-risk-apps-code)
Data Generation Tools
? AugLy (https://oreil.ly/C3sh1)
? faker (https://oreil.ly/9ZeuG)
Deep Learning Attacks and Debugging Tools
? adversarial-robustness-toolbox (https://oreil.ly/j4pmz)
? albumentations (https://oreil.ly/lIX8o)
? cleverhans (https://oreil.ly/LvNRO)
? checklist (https://oreil.ly/lopis)
? counterfit (https://oreil.ly/jxToW)
? foolbox (https://oreil.ly/3ofR4)
? robustness (https://oreil.ly/Eq4yv)
? tensorflow/model-analysis (https://oreil.ly/UDkel)
? TextAttack (https://oreil.ly/VraVt)
? TextFooler (https://oreil.ly/mvq2J)
? torcheck (https://oreil.ly/kEczf)
? TorchDrift (https://oreil.ly/njHPO)
















326  |  Chapter 9: Debugging a PyTorch Image Classifier



CHAPTER 10

Testing and Remediating Bias with XGBoost



This chapter presents bias testing and remediation techniques for structured data. While Chapter 4 addressed issues around bias from various perspectives, this chapter focuses on technical implementations of bias testing and remediation approaches. We?ll start off by training XGBoost on a variant of the credit card data. We?ll then test for bias by checking for differences in performance and outcomes across demographic groups. We?ll also try to identify any bias concerns at the individual observation level. Once we confirm the existence of measurable levels of bias in our model predictions, we?ll start trying to fix, or remediate, that bias. We employ pre-, in- and postprocessing remediation methods that attempt to fix the training data, model, and outcomes, respectively. We?ll finish off the chapter by conducting bias-aware model selection that leaves us with a model that is both performant and more fair than the original model.
While we?ve been clear that technical tests and fixes for bias do not solve the problem of machine learning bias, they still play an important role in an effective overall bias mitigation or ML governance program. While fair scores from a model do not translate directly to fair outcomes in a deployed ML system?for any number of reasons?it?s still better to have fair scores than not. We?d also argue it?s one of the fundamental and obvious ethical obligations of practicing data scientists to test models that operate on people for bias. Another theme we?ve brought up before is that unknown risks are much harder to manage than known risks. When we know a system may present bias risks and harms, we can attempt to remediate that bias, monitor the system for bias, and apply many different sociotechnical risk controls? like bug bounties or user interviews?to mitigate any potential bias.



327

This chapter focuses on bias testing and remediation for a fairly traditional classifier, because this is where these topics are best understood, and because many complex artificial intelligence out? comes often boil down to a final binary decision that can be treated in the same way as a binary classifier. We highlight techniques for regression models throughout the chapter as well. See Chapter 4 for ideas on how to manage bias in multinomial, unsupervised, or generative systems.

By the end of the chapter, readers should understand how to test a model for bias and then select a less biased model that also performs well. While we acknowledge there?s no silver bullet technical fix for ML bias, a model that is more fair and more performant is a better option for high-risk applications than a model that hasn?t been tested or remediated for bias. The chapter?s code examples are available online (https://oreil.ly/machine-learning-high-risk-apps-code).
Concept Refresher: Managing ML Bias
Before we dive into this chapter?s case study, let?s do a quick refresher of the applicable topics from Chapter 4. The most important thing to emphasize from Chapter 4 is that all ML systems are sociotechnical, and the kind of purely technical testing we?re focusing on in this chapter is not going to catch all the different bias issues that might arise from an ML system. The simple truth is that ?fair? scores from a model, as measured on one or two datasets, give an entirely incomplete picture of the bias of the system. Other issues could arise from unrepresented users, accessibility problems, physical design mistakes, downstream misuse of the system, misinterpretation of results, and more.

Technical approaches to bias testing and mitigation must be com? bined with sociotechnical approaches to adequately address poten? tial bias harms. We can?t ignore the demographic background of our own teams, the demographics of users or those represented in training and testing data, data science cultural issues (like entitled ?rock stars?), and highly developed legal standards, and also expect to address bias in ML models. This chapter focuses mostly on technical approaches. Chapter 4 attempts to describe a broader sociotechnical approach to managing bias in ML.

We must augment technical bias testing and remediation efforts with an overall commitment to having a diverse set of stakeholders involved in ML projects and adherence to a systematic approach to model development. We also need to talk to our users and abide by model governance that holds humans accountable for the decisions of the computer systems we implement and deploy. To be blunt, these kinds


328  |  Chapter 10: Testing and Remediating Bias with XGBoost

of sociotechnical risk controls are likely more important and more effective than the technical controls we discuss in this chapter.
Nonetheless, we don?t want to deploy blatantly biased systems, and if we can make the technology better, we should. Less biased ML systems are an important part of an effective bias mitigation strategy, and to get that right, we?ll need a lot of tools from our data science tool belt, like adversarial models, tests for practical and statistical differences in group outcomes, tests for differential performance across demographic groups, and various bias-remediation approaches. First, let?s go over some terms that we?ll be using throughout this chapter:
Bias
For this chapter we mean systemic biases?historical, societal, and institutional, as defined by the National Institute of Standards and Technology (NIST) SP 1270 (https://oreil.ly/R1FNW) AI bias guidance.
Adversarial model
In bias testing, we often train adversarial models on the predictions of the model we?re testing to predict demographic information. If an ML model (the adver? sary) can predict demographic information from another model?s predictions, then those predictions probably encode some amount of systemic bias. Crucially, the predictions of adversarial models also give us a row-by-row measure of bias. The rows where the adversary model is most accurate likely encode more demographic information, or proxies thereof, than other rows.
Practical and statistical significance testing
One of the oldest types of bias testing focuses on mean outcome differences across groups. We might use practical tests or effect size measurements, like adverse impact ratio (AIR) or standardized mean difference (SMD), to under? stand whether differences between mean outcomes are practically meaningful. We might use statistical significance testing to understand whether mean differ? ences across demographic groups are more associated with our current sample of data or are likely to be seen again in the future.
Differential performance testing
Another common type of testing is to investigate performance differences across groups. We might investigate whether true positive rates (TPR), true negative rates (TNR), or R2 (or root mean squared error) are roughly equal, or not, across demographic groups.
Four-fifths rule
The four-fifths rule is a guideline released in the 1978 Uniform Guidelines on Employee Selection Procedures (UGESP) (https://oreil.ly/EBtZl) by the Equal Employment Opportunity Commission (EEOC). Part 1607.4 of the UGESP states that ?a selection rate for any race, sex, or ethnic group which is less than


Concept Refresher: Managing ML Bias  |  329

four-fifths (4/5) (or eighty percent) of the rate for the group with the highest rate will generally be regarded by the Federal enforcement agencies as evidence of adverse impact.? For better or worse, the value of 0.8 for AIR?which compares event rates, like job selection or credit approval?has become a widespread benchmark for bias in ML systems.
Remediation approaches
When testing identifies problems, we?ll want to fix them. Technical bias mitiga? tion approaches are often referred to as remediation. One thing we can say about ML models and bias is that ML models seem to present more ways to fix themselves than traditional linear models. Due to the Rashomon effect?the fact that there are often many accurate ML models for any given training dataset? we simply have more levers to pull and switches to flip to find better options for decreased bias and sustained predictive performance in ML models versus simpler models. Since there are so many options for models in ML, there are many potential ways to remediate bias. Some of the most common include pre-, in-, and postprocessing, and model selection:
Preprocessing
Rebalancing, reweighing, or resampling training data so that demographic groups are better represented or positive outcomes are distributed more equitably.
In-processing
Any number of alterations to ML training algorithms, including constraints, regularization and dual loss functions, or incorporation of adversarial mod? eling information, that attempt to generate more balanced outputs or perfor? mance across demographic groups.
Postprocessing
Changing model predictions directly to create less biased outcomes.
Model selection
Considering bias along with performance when selecting models. Typically, it?s possible to find a model with good performance and fairness characteris? tics if we measure bias and performance across a large set of hyperparameter settings and input features.
Finally, we?ll need to remember that legal liability can come into play with ML bias issues. There are many legal liabilities associated with bias in ML systems, and since we?re not lawyers (and likely neither are you), we need to be humble about the complexity of law, not let the Dunning-Kruger effect take over, and defer to actual experts on nondiscrimination law. If we have any concerns about legal problems in our ML systems, now is the time to reach out to our managers or our legal


330  |  Chapter 10: Testing and Remediating Bias with XGBoost

department. With all this serious information in mind, let?s now jump into training an XGBoost model, and testing it for bias.
Model Training
The first step in this chapter?s use case is to train an XGBoost model on the credit card example data. To avoid disparate treatment concerns, we will not be using demographic features as inputs to this model:
id_col = 'ID'
groups = ['SEX', 'RACE', 'EDUCATION', 'MARRIAGE', 'AGE']
target = 'DELINQ_NEXT'
features = [col for col in train.columns if col not in groups + [id_col, target]]
Generally speaking, for most business applications, it?s safest not to use demographic information as model inputs. Not only is this legally risky in spaces like consumer credit, housing, and employment, it also implies that business decisions should be based on race or gender?and that?s dangerous territory. It?s also true, however, that using demographic data in model training can decrease bias, and we?ll see a version of that when we try out in-processing bias remediation. There also may be certain kinds of decisions that should be based on demographic information, such as those about medical treatments. Since this is an example credit decision, and since we?re not sociologists or nondiscrimination law experts, we?re going to play it safe and not use demographic features in our model. We will be using demographic features to test for bias and to remediate bias later in the chapter.

One place where we as data scientists tend to go wrong is by using demographic information in models or technical bias-remediation approaches in a way that could amount to disparate treatment. Adherents to the fairness through awareness doctrine may rightly disagree in some cases, but as of today, the most conservative approach to bias management in ML related to housing, credit, employment, and other traditional high-risk applications is to use no demographic information directly in models or bias remedia? tion. Using demographic information only for bias testing is gener? ally acceptable. See Chapter 4 for more information.

Despite its risks, demographic information is important for bias management, and one way organizations go wrong in managing ML bias risks is by not having the necessary information on hand to test and then remediate bias. At minimum, this means having people?s names and zip codes, so that we could use Bayesian improved surname geocoding (https://oreil.ly/1KpQT), and related techniques, to infer their demographic information. If data privacy controls allow, and the right security is in place, it?s most useful for bias testing to collect people?s demographic characteristics directly. It?s important to note that all the techniques used in this chapter do require

Model Training | 331

demographic information, but for the most part, we can use demographic informa? tion that is inferred or collected directly. With these important caveats addressed, let?s look at training our constrained XGBoost model and selecting a score cutoff.

Before training a model in a context where bias risks must be managed, we should always make sure that we have the right data on hand to test for bias. At minimum, this means name, zip code, and a BISG implementation. At maximum, it means collecting demographic labels and all the data privacy and security care that goes along with collecting and storing sensitive data. Either way, ignorance is not bliss when it comes to ML bias.

We will be taking advantage of monotonic constraints again. A major reason trans? parency is important with respect to managing bias in ML is that if bias testing highlights issues?and it often does?we have a better chance of understanding what?s broken about the model, and if we can fix it. If we?re working with an unexplainable ML model, and bias problems emerge, we often end up scrapping the whole model and hoping for better luck in the next unexplainable model. That doesn?t feel very scientific to us.
We like to test, debug, and understand, to the extent possible, how and why ML mod? els work. In addition to being more stable and more generalizable, our constrained XGBoost model should also be more transparent and debuggable. We also have to highlight that when we take advantage of monotonic constraints to enhance explain? ability and XGBoost?s custom objective functionality to consider performance and bias simultaneously (see ?In-processing? on page 355), we?re modifying our model to be both more transparent and more fair. Those seem like the exact right kinds of changes to make if we?re worried about stable performance, maximum transparency, and minimal bias in a high-risk application. It?s great that XGBoost is mature enough to offer this level of deep customizability. (Unfortunately for readers working in credit, mortgage, housing, employment, and other traditionally regulated sectors, you likely need to check with legal departments before employing a custom objective function that processes demographic data due to risks of disparate treatment.)

We can combine monotonic constraints (enhanced explainabil? ity) and customized objective functions (bias management) in XGBoost to directly train more transparent and less biased ML models.

In terms of defining the constraints for this chapter, we use a basic approach based on Spearman correlation. Spearman correlation is nice because it considers monotonic? ity rather than linearity (as is the case with Pearson correlation coefficient). We also


332  |  Chapter 10: Testing and Remediating Bias with XGBoost

implement a corr_threshold argument to our constraint selection process so that small correlations don?t cause spurious constraints:
def get_monotone_constraints(data, target, corr_threshold):

# determine Spearman correlation
# create a tuple of 1,0,-1 for each feature
# 1 - positive constraint, 0 - no constraint, -1 - negative constraint corr = pd.Series(data.corr(method='spearman')[target]).drop(target) monotone_constraints = tuple(np.where(corr < -corr_threshold,
-1,
np.where(corr > corr_threshold, 1, 0)))
return monotone_constraints

# define constraints correlation_cutoff = 0.1
monotone_constraints = get_monotone_constraints(train[features+[target]],
target, correlation_cutoff)
To train the model, our code is very straightforward. We?ll start with hyperparameters we?ve used before to good result and not go crazy with hyperparameter tuning. We?re just trying to start off with a decent baseline because we?ll be doing a lot of model tuning and applying careful selection techniques when we get into bias remediation. Here?s what our first attempt at training looks like:
# feed the model the global bias
# define training params, including monotone_constraints base_score = train[target].mean()

params = {
'objective': 'binary:logistic', 'eval_metric': 'auc',
'eta': 0.05,
'subsample': 0.6,
'colsample_bytree': 1.0,
'max_depth': 5, 'base_score': base_score,
'monotone_constraints': dict(zip(features, monotone_constraints)), 'seed': seed
}

# train using early stopping on the validation dataset. watchlist = [(dtrain, 'train'), (dvalid, 'eval')] model_constrained = xgb.train(params,
dtrain, num_boost_round=200, evals=watchlist, early_stopping_rounds=10, verbose_eval=False)



Model Training | 333

To calculate test values like AIR and other performance quality ratios across demo? graphic groups in subsequent sections, we?ll need to establish a probability cutoff so that we can measure our model?s outcomes and not just its predicted probabilities. Much like when we train the model, we?re looking for a starting point to get some baseline readings right now. We?ll do that using common performance metrics like F1, precision, and recall. In Figure 10-1 you can see that by picking a probability cutoff that maximizes F1, we make a solid trade-off between precision, which is the model?s proportion of positive decisions that are correct (positive predicted value), and recall, which is the model?s proportion of positive outcomes that are correct (true positive rate). For our model, that number is 0.26. To start off, all predictions above
0.26 are not going to get the credit line increase on offer. All predictions that are 0.26 or below will be accepted.

Figure 10-1. A preliminary cutoff, necessary for initial bias testing, is selected by maxi? mizing F1 statistic (digital, color version(: https://oreil.ly/EaaUe))
We know that we?ll likely end up tuning the cutoff due to bias concerns as well. In our data and example setup, increasing the cutoff means lending to more people. When we increase the cutoff, we hope that we are also lending to more different kinds of people. When we decrease the cutoff, we make our credit application process more selective, lending to fewer people, and likely fewer different kinds of people too. Another important note about cutoffs?if we?re are monitoring or auditing an already-deployed ML model, we should use the exact cutoff that is used for in vivo


334  |  Chapter 10: Testing and Remediating Bias with XGBoost

decision making, not an idealized cutoff based on performance statistics like we?ve selected here.

In training and monitoring credit models, we have to remember that we typically only have good data for applicants that were selected in the past for a credit product. Most agree that this phenomenon introduces bias into any decision based only on pre? viously selected individuals. What to do about it, widely discussed as reject inference techniques, is less clear. Keep in mind, similar bias issues apply to other types of applications where long-term data about unselected individudals is not available.

Evaluating Models for Bias
Now that we have a model and a cutoff, let?s dig in and start to test it for bias. In this section, we?ll test for different types of bias: bias in performance, bias in outcome decisions, bias against individuals, and proxy bias. First we construct confusion matrices and many different performance and error metrics for each demographic group. We?ll apply established bias thresholds from employment as a rule of thumb to ratios of those metrics to identify any problematic bias in performance. We?ll then apply traditional bias tests and effect size measures, aligned with those used in US fair lending and employment compliance programs, to test model outcomes for bias. From there, we?ll look at residuals to identify any outlying individuals or any strange outcomes around our cutoff. We?ll also use an adversarial model to identify any rows of data that seem to be encoding more bias than others. We?ll close out our bias testing discussion by highlighting ways to find proxies, i.e., seemingly neutral input features that act like demographic information in models, that can lead to different types of bias problems.
Testing Approaches for Groups
We?ll start off our bias-testing exercise by looking for problems in how our model treats groups of people on average. In our experience, it?s best to start with traditional testing guided by legal standards. For most organizations legal risks are the most serious for their AI systems, and assessing legal risks is the easiest path toward buy-in for bias testing. For that reason, and for brevity?s sake, we won?t consider intersec? tional groups in this chapter. We?ll stay focused on traditional protected classes and associated traditional race groups. Depending on the application, jurisdiction and applicable laws, stakeholder needs, or other factors, it may be most most appropriate to conduct bias testing across traditional demographic groups, intersectional demo? graphic groups, or even across skin tone scales (https://oreil.ly/GuN9L). For example, in fair lending contexts?due to established legal bias testing precedent?testing across traditional demographic groups first likely makes the most sense, and if time


Evaluating Models for Bias | 335

or organizational dynamics allow, we should circle back to intersectional testing. For general AI systems or ML models operating in the broader US economy, and not under specific nondiscrimination requirements, testing across intersectional groups should likely be the default when possible. For facial recognition systems, it might make the most sense to test across skin tone groups.
First, we?ll be looking at model performance and whether it?s roughly equivalent across traditional demographic groups or not. We?ll also be testing for the absence of group fairness (https://oreil.ly/QJGP6), sometimes also called statisical or demographic parity (https://oreil.ly/MBCCq), in model outcomes. These notions of group fairness are flawed, because defining and measuring groups of people is difficult, averages hide a lot of information about individuals, and the thresholds used for these tests are somewhat arbitrary. Despite these shortcomings, these tests are some of the most commonly used today. They can tell us useful information about how our model behaves at a high level and point out serious areas of concern. Like many of the tests we?ll discuss in this section, the key to interpreting them is as follows: passing these tests doesn?t mean much?our model or system could still have serious in vivo bias issues?but failing them is a big red flag for bias.
Before jumping into the tests themselves, it?s important to think about where to test. Should we test in training, validation, or test data? The most standard partitions in which to test are validation and test data, just like when we test our model?s performance. Testing for bias in validation data can also be used for model selection purposes, as we?ll discuss in ?Remediating Bias? on page 350. Using test data should give us some idea of how our model will perpetuate bias once deployed. (There are no guarantees an ML model will perform similarly to what we observe in test data, so monitoring for bias after deployment is crucially important.) Bias testing in training data is mostly useful for observing differences in bias measurements from validation and test partitions. This is especially helpful if one partition stands out from the others, and can potentially be used for understanding drivers of bias in our model. If training, validation, and test sets are constructed so that training comes first in time and testing comes last?as they likely should be?comparing bias measurements across data partitions is also helpful for understanding trends in bias. It is a concern? ing sign to see bias measurements increase from training to validation to testing. One other option is to estimate variance in bias measurements using cross-validation or bootstrapping, as is done with standard performance metrics too. Cross-validation, bootstrapping, standard deviations or errors, confidence intervals, and other measure of variance for bias metrics can help us understand if our bias-testing results are more precise or more noisy?an important part of any data analysis.
In the bias testing conducted in the following sections, we?ll be sticking to basic practices, and looking for biases in model performance and outcomes in validation and test data. If you?ve never tried bias testing, this is a good way to get started. And inside large organizations, where logistics and politics make it even more difficult,

336  |  Chapter 10: Testing and Remediating Bias with XGBoost

this might be the only bias testing that can be accomplished. Bias testing is never finished. As long as a model is deployed, it needs to be monitored and tested for bias. All of these practical concerns make bias testing a big effort, and for these reasons we?d urge you to begin with these standard practices that look for bias in performance and outcomes across large demographic groups, and then use any remaining time, resources, and will to investigate bias against individuals and to identify proxies or other drivers of bias in your model. That?s what we?ll get into now.

Before we can begin bias testing, we must be absolutely clear about how a positive decision is represented in the data, what positive means in the real world, how our model?s predicted probabilities align to these two notions, and which cutoffs generate positive decisions. In our example, the decision that is desirable to the preponderance of model subjects is an outcome of zero, associated with probabilities below the cutoff value of 0.26. Applicants who receive a classification of zero will be extended a line of credit.

Testing performance
A model should have roughly similar performance across demographic groups, and if it doesn?t, this is an important type of bias. If all groups are being held to the same standard by an ML model for receiving a credit product, but that standard is not an accurate predictor of future repayment behavior for some groups, that?s not fair. (This is somewhat similar to the employment notion of differential validity, discussed in Chapter 4.) To start testing for bias in performance across groups for a binary classifier like our XGBoost model, we?ll look at confusion matrices for each group and form different measures of performance and error across groups. We?ll consider common measures like true positive and false positive rates, as well as some that are less common in data science, like false discovery rate.
The following code block is far from the best implementation, because of its reliance on dynamic code generation and an eval() statement, but it is written to be maxi? mally illustrative. In it, readers can see how the four cells in a confusion matrix can be used to calculate many different performance and error metrics:
def confusion_matrix_parser(expression):

# tp | fp	cm_dict[level].iat[0, 0] | cm_dict[level].iat[0, 1]
# ------- ==> --------------------------------------------
# fn | tn	cm_dict[level].iat[1, 0] | cm_dict[level].iat[1, 1]

metric_dict = {
'Prevalence': '(tp + fn) / (tp + tn +fp + fn)', 'Accuracy': '(tp + tn) / (tp + tn + fp + fn)', 'True Positive Rate': 'tp / (tp + fn)', 'Precision': 'tp / (tp + fp)',
'Specificity': 'tn / (tn + fp)',

Evaluating Models for Bias | 337


'Negative Predicted Value': 'tn / (tn + fn)', 'False Positive Rate': 'fp / (tn + fp)', 'False Discovery Rate': 'fp / (tp + fp)', 'False Negative Rate': 'fn / (tp + fn)', 'False Omissions Rate': 'fn / (tn + fn)'
}

expression = expression.replace('tp', 'cm_dict[level].iat[0, 0]')\
.replace('fp', 'cm_dict[level].iat[0, 1]')\
.replace('fn', 'cm_dict[level].iat[1, 0]')\
.replace('tn', 'cm_dict[level].iat[1, 1]')

return expression
When we apply the confusion_matrix_parser function to confusion matrices for each demographic group, along with other code that loops through groups and the measures in metric_dict, we can make a table like Table 10-1. For brevity, we?ve focused on the race measurements in this subsection. If this were a real credit or mortgage model, we?d be looking at different genders, different age groups, those with disabilities, different geographies, and maybe even other subpopulations.
Table 10-1. Common performance and error measures derived from a confusion matrix across different race groups for test data

Group
Prevalence
Accuracy
True positive rate
Precision
?
False positive rate
False discovery rate
False negative rate
False omissions rate
Hispanic
0.399
0.726
0.638
0.663
?
0.215
0.337
0.362
0.235
Black
0.387
0.720
0.635
0.639
?
0.227
0.361
0.365
0.229
White
0.107
0.830
0.470
0.307
?
0.127
0.693
0.530
0.068
Asian
0.101
0.853
0.533
0.351
?
0.111
0.649
0.467
0.055
Table 10-1 starts to show us some hints of bias in our model?s performance, but it?s not really measuring bias yet. It?s simply showing the value for different meas? urements across groups. We should start to pay attention when these values are obviously different for different groups. For example, precision looks quite different between demographic groups (white and Asian people on one hand, and Black and Hispanic people on the other). The same can be said about other measures like the false positive rate, false discovery rate, and false omissions rate. (Disparities in prevalence tell us that default occurs more in the data for Black and Hispanic people. Sadly this not uncommon in many US credit markets.) In Table 10-1, we are starting to get a hint that our model is predicting more defaults for Black and Hispanic people, but it?s still hard to tell if it?s doing a good or equitable job. (Just because a dataset records these kinds of values, does not make them objective or fair!) To help



338  |  Chapter 10: Testing and Remediating Bias with XGBoost

understand if the patterns we?re seeing are actually problematic, we need to take one more step. We?ll follow methods from traditional bias testing and divide the value for each group by the corresponding value for the control group and apply the four-fifths rule as a guide. In this case, we assume the control group is white people.

Strictly speaking, in the employment context, the control group is the most favored group in an analysis, not necessarily white people or males. There may also be other reasons to use control groups that are not white people or males. Choosing the control or reference group for a bias-testing analysis is a difficult task, best done in concert with legal, compliance, social science experts, or stakeholders.

Once we do this division, we see the values in Table 10-2. (We divide each column in the table by the value in the white row. That?s why the white values are all 1.0.) Now we can look for values outside of a certain range. We?ll use the four-fifths rule, which has no legal or regulatory standing when used this way, to help us identify one such range: 0.8?1.25, or a 20% difference between groups. (Some prefer a tighter range of acceptable values, especially in high-risk scenarios, say 0.9?1.11, indicating a 10% difference between groups.) When we see values above 1 for these disparity measures, it means the protected or minority group has a higher value of the original measure, and vice versa for values below 1.
Looking at Table 10-2, we see no out-of-range values for Asian people. This means that the model performs fairly equitably across white and Asian people. However, we do see glaring out-of-range values for Hispanic and Black people for precision, false positive rate, false discovery rate, and false omissions rate disparities. While applying the four-fifths rule can help us flag these values, it really can?t help us interpret them. For this, we?ll have to rely on our human brains to think through these results. We also need to remember that a decision of 1 from our model is a predicted default, and that higher probabilities mean default is more likely in the eyes of the model.
Table 10-2. Performance-based bias measures across race groups for test data

Group
Prevalence disparity
Accuracy disparity
True positive rate disparity
Precision disparity
?
False positive rate disparity
False discovery rate disparity
False negative rate disparity
False omissions rate disparity
Hispanic
3.730
0.875
1.357
2.157
?
1.696
0.486
0.683
3.461
Black
3.612
0.868
1.351
2.078
?
1.784
0.522
0.688
3.378
White
1.000
1.000
1.000
1.000
?
1.000
1.000
1.000
1.000
Asian
0.943
1.028
1.134
1.141
?
0.873
0.937
0.881
0.821


Evaluating Models for Bias | 339

Given that prevalence of defaults in the data is so much higher for Black and Hispanic people, one thing these results suggest is that our model learned more about defaults in these groups, and predicts defaults at a higher rate in these groups. Traditional testing in the next section will try to get at the underlying question of whether it?s fair to predict more defaults in these groups. For now, we?re trying to figure out if the performance of the model is fair. Looking at which measures are out-of-range for protected groups and what they mean, we can say the following:
? Precision disparity: ~2? (more) correct default predictions, out of those predicted to default.
? False positive rate disparity: ~1.5? (more) incorrect default predictions, out of those that did not default.
? False discovery rate disparity: ~0.5? (fewer) incorrect default predictions, out of those predicted to default
? False omissions rate disparity: ~3.5? (more) incorrect acceptance predictions, out of those predicted not to default.
Precision and false discovery rate have the same denominator?the smaller group of those predicted to default?and can be interpreted together. They show that this model has a higher rate of true positives for Black and Hispanic people relative to white people?meaning a higher rate of correct default predictions for this group. The false discovery rate echoes this result, pointing to a lower rate of false positives, or incorrect default decisions, for the minority groups in question. Relatedly, the false omissions rate shows our model makes incorrect acceptance decisions at a higher rate, out of the larger group comprised of those predicted not to default, for Black and Hispanic people. Precision, false discovery rate, and false omissions rate disparities show serious bias issues, but a bias that favors Black and Hispanic people in terms of model performance.


340  |  Chapter 10: Testing and Remediating Bias with XGBoost

Precision
Out of the people in the group the model predicted would default, how many the model predicted correctly would default
Specificity
Out of the people in the group that did not default, how many the model predicted correctly would not default
Negative predicted value
Out of the people in the group the model predicted would not default, how many the model predicted correctly would not default
False positive rate
Out of the people in the group that did not default, how many the model predicted incorrectly would default
False discovery rate
Out of the people in the group the model predicted would default, how many the model predicted incorrectly would default
False negative rate
Out of the people in the group that did default, how many the model predicted
incorrectly would not default
False omissions rate
Out of the people in the group the model predicted would not default, how many the model predicted incorrectly would not default
Try to follow this example to create interpretations for confusion matrix performance and error measurements in your next important ML project. It can help to think through bias, performance, and safety issues with more clarity.

False positive rate disparity shows something a little different. The false positive rate is measured out of the larger group of those who did not default, in reality. In that group, we do see higher rates of incorrect default decisions, or false positives, for Black and Hispanic people. Taken together, all these results point to a model with bias problems, some of which genuinely appear to favor minority groups. Of these, the false positive disparity is most concerning. It shows us that out of the relatively large group of people who did not default, Black and Hispanic people are predicted to default incorrectly at 1.5? the rate of white people. This means that a lot of historically disenfranchised people are being wrongly denied credit-line increases by this model, which can lead to real-world harm. Of course, we also see evidence of correct and incorrect acceptance decisions favoring minorities. None of this is a great sign, but we need to dig into outcomes testing in the next section to get a clearer picture of group fairness in this model.


Evaluating Models for Bias | 341

For regression models, we can skip the confusion matrices and proceed directly to comparing measures like R2 or root mean squared error across groups. Where appropriate, and especially for bounded measures like R2 or mean average percentage error (MAPE), we can also apply the four-fifths rule (as a rule of thumb) to a ratio of these measures to help spot problematic performance bias.

In general, performance testing is a helpful tool for learning about wrong and neg? ative decisions, like false positives. More traditional bias testing that focuses on out? comes rather than performance has a more difficult time highlighting bias problems in wrong or negative decisions. Unfortunately, as we?re about to see, performance and outcomes testing can show different results. While some of these performance tests show a model that favors minorities, we?ll see in the next section that that?s not true. Rates standardize out the raw numbers of people and raw scores of the model in theoretically useful ways. A lot of the positive results we saw here are for fairly small groups of people. When we consider real-world outcomes, the picture of bias in our model is going to be different and more clear. These kinds of conflicts between performance testing and outcomes testing are common and well documented, and we?d argue that outcomes testing?aligned with legal standards and what happens in the real world?is more important.

There is a well-known tension between improved performance in data that encodes historical biases?like most of the data we work with?and balancing outcomes across demographic groups. Data is always affected by systemic, human, and statistical biases. If we make outcomes more balanced, this tends to decrease performance metrics in a biased dataset.

Because it?s difficult to interpret all these different performance measures, some may have more meaning in certain scenarios than others, and they are likely to be in conflict with each other or outcomes testing results, prominent researchers put together a decision tree (https://oreil.ly/-y827) (slide 40) to help focus on a smaller subset of performance disparity measures. According to this tree, where our model is punitive (higher probability means default/reject decision), and the clearest harm is incorrectly denying credit line increases to minorities (intervention not warranted), the false positive rate disparity should probably carry the highest weight in our prediction performance analysis. The false positive rate disparity doesn?t tell a nice story. Let?s see what outcomes testing shows.





342  |  Chapter 10: Testing and Remediating Bias with XGBoost

Traditional testing of outcomes rates
The way we set up our analysis, based on a binary classification model, it was easiest to look at performance across groups first using confusion matrices. What?s likely more important, and likely more aligned to legal standards in the US, is to analyze differences in outcomes across groups, using traditional measures of statistical and practical significance. We?ll pair two well-known practical bias-testing measures, AIR and SMD, with chi-squared and t-tests, respectively. Understanding whether a discovered difference in group outcomes is statistically significant is usually a good idea, but in this case, it might also be a legal requirement. Statistically significant dif? ferences in outcomes or mean scores is one of the most common legally recognized measures of discrimination, especially in areas like credit lending, where algorithmic decision making has been regulated for decades. By using practical tests and effect size measures, like AIR and SMD, with statistical significance tests, we get two pieces of information: the magnitude of the observed difference, and whether it?s statistically significant, i.e., likely to be seen again in other samples of data.

If you?re working in a regulated vertical or in a high-risk applica? tion, it?s a good idea to apply traditional bias tests with legal prec? edent first before applying newer bias-testing approaches. Legal risks are often the most serious organizational risks for many types of ML-based products, and laws are designed to protect users and stakeholders.

AIR is often applied to categorical outcomes, like credit lending or hiring outcomes, where someone either receives a positive outcome or not. AIR is defined as the rate of positive outcomes for a protected group, like minorities or women, divided by the same rate of positive outcomes for a control group, like white people or men. According to the four-fifths rule, we look for the AIR to be above 0.8. An AIR below
0.8 points to a serious problem. We then test whether this difference will probably be seen again or if it?s due to chance using a chi-squared test.

Impact ratios can also be used for regression models by dividing average scores or percentage of scores over the median score for a protected group by the same quantity for a control group, and applying the four-fifths rule as a guideline for identifying problem? atic results. Other traditional bias measurement approaches for regression models are t-tests and SMD.

While AIR and chi-squared are most often used with binary classification, SMD and t-tests are often used on predictions from regression models, or on numeric quantities like wages, salaries, or credit limits. We?ll apply SMD and t-tests to our model?s predicted probabilities for demonstration purposes and to get some extra


Evaluating Models for Bias | 343

information about bias in our model. SMD is defined as the mean score for a protected group minus the mean score for a control group, with that quantity divided by a measure of the standard deviation of the score. SMD has well-known cutoffs at magnitudes of 0.2, 0.5, and 0.8 for small, medium, and large differences, respectively. We?ll use a t-test to decide whether the effect size measured by SMD is statistically significant.

This application of SMD?applied to the probabilities output by the model?would also be appropriate if the model scores would be fed into some downstream decision-making process, and it is impossible to generate model outcomes at the time of bias testing.

In addition to significance tests, AIR, and SMD, we?ll also be analyzing basic descriptive statistics like counts, means, and standard deviations, as can be seen in Table 10-3. When looking over Table 10-3, it?s clear that there is a big difference in scores for Black and Hispanic people versus scores for white and Asian people. While our data is simulated, very sadly, this is not atypical in US consumer finance. Systemic bias is real, and fair lending data tends to prove it.1
Table 10-3. Traditional outcomes-based bias metrics across race groups for test data

Group
Count
Favorable outcomes
Favorable rate
Mean score
Std. dev. score
AIR
AIR p-value
SMD
SMD p-value
Hispanic
989
609
0.615
0.291
0.205
0.736
6.803e?36
0.528
4.311e?35
Black
993
611
0.615
0.279
0.199
0.735
4.343e?36
0.482
4.564e?30
Asian
1485
1257
0.846
0.177
0.169
1.012
4.677e?01
?0.032
8.162e?01
White
1569
1312
0.836
0.183
0.172
1.000
-
0.000
-
In Table 10-3, it?s immediately obvious that Black and Hispanic people have higher mean scores and lower favorable rates than white and Asian people, while all four groups have similar standard deviations for scores. Are these differences big enough to be a bias problem? That?s where our practical significance tests come in. AIR and SMD are both calculated in reference to white people. That?s why white people have scores of 1.0 and 0.0 for these, respectively. Looking at AIR, both Black and Hispanic AIRs are below 0.8. Big red flag! SMDs for those two groups are around 0.5, meaning a medium difference in scores between groups. That?s not a great sign either. We?d like for those SMD values to be below or around 0.2, signifying a small difference.



1 If you?d like to satisfy your own curiosity on this matter, we urge you to analyze some freely available Home Mortgage Disclosure Act data (https://oreil.ly/xYXdt).

344  |  Chapter 10: Testing and Remediating Bias with XGBoost

AIR is often misinterpreted by data scientists. Here?s a simple way to think of it: An AIR value above 0.8 doesn?t mean much, and it certainly doesn?t mean a model is fair. However, AIR values below
0.8 point to a serious problem.

The next question we might ask in a traditional bias analysis is whether these practi? cal differences for Black and Hispanic people are statistically significant. Bad news? they are very significant, with p-values approaching 0 in both cases. While datasets have exploded in size since the 1970s, a lot of legal precedent points to statistical significance at the 5% level (p = 0.05) for a two-sided hypothesis test as a marker of legally impermissible bias. Since this threshold is completely impractical for today?s large datasets, we recommend adjusting p-value cutoffs lower for larger datasets. However, we should also be prepared to be judged at p = 0.05 in regulated verticals of the US economy. Of course, fair lending and employment discrimination cases are anything but straightforward, and facts, context, and expert witnesses have as much to do with a final legal determination as any bias-testing number. An important takeaway here is that the law in this area is already established, and not as easily swayed by AI hype as internet and media discussions. If we?re operating in a high-risk space, we should probably conduct traditional bias tests in addition to newer tests, as we?ve done here.

In consumer finance, housing, employment, and other traditionally regulated verticals of the US economy, nondiscrimination law is highly mature and not swayed by AI hype. Just because AIR and two-sided statistical tests feel outdated or simplistic to data scien? tists, does not mean our organizations won?t be judged by these standards if legal issues arise.

These race results point to a fairly serious discrimination problem in our model. If we were to deploy it, we?d be setting ourselves up for potential regulatory and legal problems. Worse than that, we?d be deploying a model we know perpetuates systemic biases and harms people. Getting an extension on a credit card can be a serious thing at different junctures in our lives. If someone is asking for credit, we should assume it?s genuinely needed. What we see here is that an example credit-lending decision is tinged with historical biases. These results also send a clear message. This model needs to be fixed before it?s deployed.
Individual Fairness
We?ve been focused on group fairness thus far, but we should also probe our model for individual fairness concerns. Unlike bias against groups, individual bias is a local issue that affects only a small and specific group of people, down to a single individual. There are two main techniques we?ll use to test this: residual analysis

Evaluating Models for Bias | 345

and adversarial modeling. In the first technique?residual analysis?we look at indi? viduals very close to the decision cutoff and who incorrectly received unfavorable outcomes as a result. We want to make sure their demographic information isn?t pushing them into being denied for a credit product. (We can check very wrong individual outcomes far away from the decision cutoff too.) In the second approach? adversarial models?we?ll use separate models that try to predict protected group information using the input features and the scores from our original model, and we?ll look at those model?s Shapley additive explanations. When we find rows where adversarial predictions are very accurate, this is a hint that something in that row is encoding information that leads to bias in our original model. If we can identify what that something is across more than a few rows of data, we?re on the path to identifying potential drivers of proxy bias in our model. We?ll look into individual bias and then proxy bias before transitioning to the bias-remediation section of the chapter.
Let?s dive into individual fairness. First, we wrote some code to pull out a few narrowly misclassified people from a protected group. These are observations that our model predicted would go delinquent, but they did not:
black_obs = valid.loc[valid['RACE'] == 'black'].copy() black_obs[f'p_{target}_outcome'] = np.where(
black_obs[f'p_{target}'] > best_cut, 1,
0)

misclassified_obs = black_obs[(black_obs[target] == 0) &
(black_obs[f'p_{target}_outcome'] == 1)]

misclassified_obs.sort_values(by=f'p_{target}').head(3)[features]
The results are shown in Table 10-4, and they don?t suggest any egregious bias, but they do raise some questions. The first and third applicants appear to spending moderately and making payments on time for the most part. These individuals may have been placed on the wrong side of a decision boundary in an arbitrary manner. However, the individual in the second row of Table 10-4 appears not to be making progress on repaying their credit card debt. Perhaps they really should not have been approved for an increased line of credit.
Table 10-4. A subset of features for narrowly misclassified protected observations in validation data

LIMIT
_BAL
PAY_0
PAY_2
PAY_3
?
BILL
_AMT1
BILL
_AMT2
BILL
_AMT3
?
PAY
_AMT1
PAY
_AMT2
PAY
_AMT3
$58,000
?1
?1
?2
?
$600
$700
$0
?
$200
$700
$0
$58,000
0
0
0
?
$8,500
$5,000
$0
?
$750
$150
$30
$160,000
?1
?1
?1
?
$0
$0
$600
?
$0
$0
$0

346  |  Chapter 10: Testing and Remediating Bias with XGBoost

Next steps to uncovering whether we?ve found a real individual bias problem might include the following:
Small perturbations of input features
If some arbitrary change to an input feature, say decreasing BILL_AMT1 by $5, changes the outcome for this person, then the model?s decision may be more related to a steep place in its response function intersecting with the decision cutoff than any tangible real-world reason.
Searching for similar individuals
If there are a handful?or more?individuals like the current individual, the model maybe segmenting some specific or intersectional subpopulation in an unfair or harmful way.
If either of these are the case, the right thing to do may be to extend this and similar individual?s line(s) of credit.
We conducted a similar analysis for Hispanic and Asian observations and found similar results. We weren?t too surprised by these results, for at least two reasons. First, individual fairness questions are difficult and bring up issues of causality which ML systems tend not to address in general. Second, individual fairness and proxy discrimination are probably much larger risks for datasets with many rows?where entire subpopulations may end up on an arbitrary side of a decision boundary?and when a model contains many features, and especially alternative data, or features not directly linked to one?s ability to repay credit, that may otherwise enhance the predictiveness of the model.

Answering questions about individual fairness with 100% certainty is difficult, because they?re fundamentally causal questions. For complex, nonlinear ML models, it?s impossible to know whether a model made a decision on the basis of some piece of data (i.e., protected group information) that isn?t included in the model in the first place.
That said, residual analysis, adversarial modeling, SHAP values, and the careful application of subject matter expertise can go a long way. For more reading on this subject, check out ?Explaining Quantitative Measures of Fairness? (https://oreil.ly/Tg66Z) from the creator of SHAP values, and ?On Testing for Discrimination Using Causal Models? (https://oreil.ly/IiP9W).

Let?s move on to the second technique for testing individual fairness: adversarial modeling. We chose to train two adversarial models. The first model takes in the same input features as the original model, but attempts to predict protected groups? statuses rather than delinquencies. For simplicity, we trained a binary classifier on a


Evaluating Models for Bias | 347

target for protected class membership?a new marker for Black or Hispanic people. By analyzing this first adversarial model, we can get a good idea of which features have the strongest relationships with protected demographic group membership.
The second adversarial model we train is exactly like the first, except it gets one additional input feature?the output probabilities of our original lending model. By comparing the two adversarial models, we will get an idea of how much additional information was encoded in the original model scores. And we?ll get this information at the observation level.

Many ML tools that generate row-by-row debugging information? like residuals, adversarial model predictions, or SHAP values?can be used for examining individual bias issues.


We trained these adversarial models as binary XGBoost classifiers with similar hyper? parameters to the original model. First, we took a look at the protected observations whose adversarial model scores increased the most when the original model proba? bilities were added as a feature. The results are shown in Table 10-5. This table is telling us that for some observations, the original model scores are encoding enough information about protected group status that the second adversarial model is able to improve on the first by around 30 percentage points. These results tells us that we should take a deeper look into these observations, in order to identify any individual fairness problems by asking questions like we did for individual bias issues spotted with residuals. Table 10-5 also helps us show again that removing demographic markers from a model does not remove demographic information from a model.
Table 10-5. The three protected observations that saw their scores increase the most between the two adversarial models in validation data
 Observation Protected Adversary 1 score Adversary 2 score Difference 

9022
1
0.288
0.591
0.303
7319
1
0.383
0.658
0.275
528
1
0.502
0.772
0.270
Recall from Chapter 2 that SHAP values are a row-by-row additive feature attribution scheme. That is, they tell us how much each feature in a model contributed to the overall model prediction. We computed the SHAP values on validation data for the second adversarial model (the one that includes our original model scores). In Figure 10-2, we took a look at the distribution of SHAP values for the top four most important features. Each of the features in Figure 10-2 is important to predicting protected class membership. Coming in as the most important feature for predicting protected group information is the original model scores, p_DELINQ_NEXT. This is

348  |  Chapter 10: Testing and Remediating Bias with XGBoost

interesting in and of itself, and the observations that have the highest SHAP values for this feature are good targets to investigate further for individual fairness violations.

Figure 10-2. The distribution of SHAP values for the four most important features in our adversarial model in validation data (digital, color version(: https://oreil.ly/n4z9i))
Maybe most interesting is the color gradient (change from light to dark) within the p_DELINQ_NEXT violin. Each violin is colored by the value of the feature itself for each observation in the density. That means that if our model was linear with no interactions, the color gradient across each violin would be smooth from light to dark. But that?s not what we observed. Within the p_DELINQ_NEXT violin, there is significant color variation within vertical slices of the plot. This can only arise when p_DELINQ_NEXT is being used by the model in conjunction with other features in order to drive the predictions. For example, the model might be learning something like if LIMIT_BAL is below $20,000 and if credit utilization is above 50% and if the delin? quency probability from the credit extension model is above 20% then the observation is likely to be a Black or Hispanic person. While residuals and adversarial models can help us identify individual bias issues, SHAP can take us a step further by helping us understand what is driving that bias.
Proxy Bias
If the patterns like those we?ve identified only affect a few people, they can still be harmful. But when we see them affecting larger groups of people, we likely have a more global proxy bias issue on our hands. Remember that proxy bias happens when a single feature or a group of interacting features act like demographic information in our model. Given that ML models can often mix and match features to create latent concepts?and can do so in different ways on local, row-by-row bases?proxy bias is a fairly common driver of biased model outputs.
Many of the tools we?ve discussed, like adversarial models and SHAP, can be used to hunt down proxies. We could begin to get at them by looking at, for example,


Evaluating Models for Bias | 349

SHAP feature interaction values. (Recall advanced SHAP techniques from Chapters 2 and 6.) The bottom-line test for proxies may be adversarial models. If another model can accurately predict demographic information from our model?s predictions, then our model encodes demographic information. If we include model input features in our adversarial models, we can use feature attribution measures to understand which single input features might be proxies, and apply other techniques and elbow grease to find proxies created by interactions. Good, old-fashioned decision trees can be some of the best adversarial models for finding proxies. Since ML models tend to combine and recombine features, plotting a trained adversarial decision tree may help us uncover more complex proxies.
As readers can see, adversarial modeling can be a rabbit hole. But we hope we?ve convinced readers that it is a powerful tool for identifying individual rows that might be subject to discrimination under our models, and for understanding how our input features relate to protected group information and proxies. Now we?re going to move on to the important job of remediating the bias we found in our example lending model.
Remediating Bias
Now that we?ve identified several types of bias in our model, it?s time to roll up our sleeves and try to remediate it. Luckily, there are many tools to choose from and, due to the Rashomon effect, many different models to choose from too. We?ll try preprocessing remediation first. We?ll generate observation-level weights for our training data so that positive outcomes appear equally likely across demographic groups. We?ll then try an in-processing technique, sometimes known as fair XGBoost, in which demographic information is included in XGBoost?s gradient calculation so that it can be regularized during model training. For postprocessing, we?ll update our predictions around the decision boundary of the model. Since pre-, in-, and postprocessing may give rise to concerns about disparate treatment in several indus? try verticals and applications, we?ll close out the remediation section by outlining a simple and effective technique for model selection that searches over various input feature sets and hyperparameter settings to find a model with good performance and minimal bias. For each approach, we?ll also address any observed performance quality and bias-remediation trade-offs.
Preprocessing
The first bias-remediation technique we?ll try is a preprocessing technique known as reweighing. It was published first by Faisal Kamiran and Toon Calders in their 2012 paper, ?Data Preprocessing Techniques for Classification Without Discrimination? (https://oreil.ly/lAj08). The idea of reweighing is to make the average outcome across groups equal using observation weights and then retrain the model. As we?ll see,


350  |  Chapter 10: Testing and Remediating Bias with XGBoost

before we preprocessed the training data, the average outcome, or average y variable value, is quite different across demographic groups. The biggest difference was for Asian and Black people, with average outcomes of 0.107 and 0.400, respectively. This means that on average, and looking only at the training data, Asian people?s probability of default was well within the range of being accepted for a credit-line increase, while the opposite was true for Black people. Their average score was solidly in the decline range. (Again, these values are not always objective or fair simply because they are recorded in digital data.) After we preprocess, we?ll see we can balance out both outcomes and bias-testing values to a notable degree.
Since reweighing is a very straightforward approach, we decided to implement it our? selves with the function in the following code snippet.2 To reweigh our data, we first need to measure average outcome rates?overall and for each demographic group. Then we determine observation-level, or row-level, weights that balance out the outcome rate across demographic groups. Observation weights are numeric values that tell XGBoost, and most other ML models, how much to weigh each row during training. If a row has a weight of 2, it?s like that row appears twice in the objective function used to train XGBoost. If we tell XGBoost that a row has a weight of 0.2, it?s like that row appears one-fifth of the times it actually does in the training data. Given the average outcome for each group and their frequency in the training data, it?s a basic algebra problem to determine the row weights that give all groups the same average outcome in the model.
def reweight_dataset(dataset, target_name, demo_name, groups): n = len(dataset)
# initial overall outcome frequency
freq_dict = {'pos': len(dataset.loc[dataset[target_name] == 1]) / n, 'neg': len(dataset.loc[dataset[target_name] == 0]) / n}
# initial outcome frequency per demographic group freq_dict.update({group: dataset[demo_name].value_counts()[group] / n
for group in groups})
weights = pd.Series(np.ones(n), index=dataset.index)
# determine row weights that balance outcome frequency # across demographic groups
for label in [0, 1]:
for group in groups:
label_name = 'pos' if label == 1 else 'neg'
freq = dataset.loc[dataset[target_name] == label][demo_name] \
           .value_counts()[group] / n weights[(dataset[target_name] == label) &
    (dataset[demo_name] == group)] *= \ freq_dict[group] * freq_dict[label_name] / freq
# return balanced weight vector return weights


2 For an additional implementation and example usage of reweighing, check out AIF360?s ?Detecting and Mitigating Age Bias on Credit Decisions? (https://oreil.ly/ypEQc).

Remediating Bias |  351

There are multiple kinds of sample weights. In XGBoost, and in most other ML models, observation-level weights are interpreted as frequency weights, where the weight on an observation is equiv? alent to ?the number of times? it appears in the training data. This weighting scheme has its origins in survey sampling theory.
The other main type of sample weights come from the theory of weighted least squares. Sometimes called precision weights, they quantify our uncertainty about the observation?s feature values, under the assumption that each observation is really an average of multiple underlying samples. These two notions of sample weights are not equivalent, so it?s important to know which one you?re specifying when you set a sample_weights parameter.

Applying the reweight_dataset function provides us with a vector of observation weights of the same length as the training data, such that a weighted average of the outcomes in the data within each demographic group is equal. Reweighing helps to undo manifestations of systemic biases in training data, teaching XGBoost that different kinds of people should have the same average outcome rates. In code, this is as simple as retraining XGBoost with the row weights from reweight_dataset. In our code, we call this vector of training weights train_weights. When we call the DMatrix function, we use the weight= argument to specify these bias-decreasing weights. After this, we simply retrain XGBoost:
dtrain = xgb.DMatrix(train[features],
label=train[target], weight=train_weights)
Table 10-6 shows both the original mean outcomes and original AIR values, along with the preprocessed mean outcomes and AIR. When we trained XGBoost on the unweighted data, we saw some problematic AIR values. Originally, the AIR was around 0.73 for Black and Hispanic people. These values are not great?signifying that for every 1,000 credit products the model extends to white people, this model only accepts applications from about 730 Hispanic or Black people. This level of bias is ethically troubling, but it could also give rise to legal troubles in consumer finance, hiring, or other areas that rely on traditional legal standards for bias testing. The four-fifths rule?while flawed and imperfect?tells us we should not see values below
0.8 for AIR. Luckily, in our case, reweighing provides good remediation results.
In Table 10-6, we can see that we increased the problematic AIR values for Hispanic and Black people to less borderline values, and importantly, without changing the AIR very much for Asian people. In short, reweighing decreased potential bias risks for Black and Hispanic people, without increasing those risks for other groups. Did this have any effect on the performance quality of our model? To investigate this, we introduced a hyperparameter, lambda, in Figure 10-3 that dictates the strength of the reweighing scheme. When lambda is equal to zero, all observations get a sample

352  |  Chapter 10: Testing and Remediating Bias with XGBoost

weight of one. When the hyperparameter is equal to one, the mean outcomes are all equal, and we get the results in Table 10-6. As shown in Figure 10-3, we did observe some trade-off between increasing the strength of reweighing and performance as measured by F1 in validation data. Next, let?s look at the result on Black and Hispanic AIRs as we sweep lambda across a range of values to understand more about that trade-off.
Table 10-6. Original and preprocessed mean outcomes for demographic groups in test data
 Demographic group Original mean outcome Preprocessed mean outcome Original AIR Preprocessed AIR 

Hispanic
0.398
0.22
0.736
0.861
Black
0.400
0.22
0.736
0.877
White
0.112
0.22
1.000
1.000
Asian
0.107
0.22
1.012
1.010
Figure 10-3. F1 scores of the model as the strength of the reweighing scheme is increased (digital, color version(: https://oreil.ly/wJ396))

The results in Figure 10-4 show that increasing lambda past 0.8 does not yield meaningful improvements in Black and Hispanic AIRs. Looking back at Figure 10-3, this means we would experience a roughly 3% drop in silico. If we were thinking about deploying this model, we?d choose that hyperparameter value for retraining. The compelling story told between Figures 10-3 and 10-4 is this: simply by applying sampling weights to our dataset so as to emphasize favorable Black and Hispanic borrowers, we can increase AIRs for these two groups, while realizing only a nominal performance drop.

Remediating Bias |  353



Figure 10-4. Adverse impact ratios of the model as the strength of the reweighing scheme is increased (digital, color version(: https://oreil.ly/LKxEH))
Like nearly everything else in ML, bias remediation and our chosen approaches are an experiment, not rote engineering. They?re not guaranteed to work, and we always need to check if they actually work, first in validation and test data, then in the real world. It?s really important to remember that we don?t know how this model is going to perform in terms of accuracy or bias once it?s deployed. We always hope our in silico validation and test assessments are correlated to real-world performance, but



354  |  Chapter 10: Testing and Remediating Bias with XGBoost

there are simply no guarantees. We?d have high hopes that what looks like a ~5% decrease in in silico performance washes out once the model is deployed due to drifting data, changes in real-world operating contexts, and other in vivo surprises. All of this points to the need for monitoring both performance and bias once a model is deployed.
Reweighing is just one example of a preprocessing technique, and there are several other popular approaches. Preprocessing is simple, direct, and intuitive. As we?ve just seen, it can result in meaningful improvements in model bias with acceptable accuracy trade-offs. Check out AIF360 (https://oreil.ly/rDdhC) for examples of other credible preprocessing techniques.
In-processing
Next we?ll try an in-processing bias-remediation technique. Many interesting tech? niques have been proposed in recent years, including some that use adversarial mod? els, as in ?Mitigating Unwanted Biases with Adversarial Learning? (https://oreil.ly/ rFdZA) or ?Fair Adversarial Gradient Tree Boosting? (https://oreil.ly/kZ0xB). The idea behind these adversarial in-processing approaches is straightforward. When an adversarial model cannot predict demographic group membership from our main model?s predictions, then we feel good that our predictions do not encode too much bias. As highlighted earlier in the chapter, adversarial models also help to capture local information about bias. The rows where the adversary model is most accurate are likely the rows that encode the most demographic information. These rows can help us uncover individuals who may be experiencing the most bias, complex proxies involving several input features, and other local bias patterns.
There are also in-processing debiasing techniques that use only one model, and since they are usually a little easier to implement, we?ll focus on one of those for our use case. As opposed to using a second model, these in-processing methods use a dual objective function with a regularization approach. For example, ?A Convex Frame? work for Fair Regression? (https://oreil.ly/7dcHL) puts forward various regularizers that can be paired with linear and logistic regression models to decrease bias against groups and individuals. ?Learning Fair Representations? (https://oreil.ly/tgCE9) also includes a bias measurement in model objective functions, but then tries to create a new representation of training data that encodes less bias.
While these two approaches focus mostly on simple models, i.e., linear regression, logistic regression, and naive Bayes, we want to work with trees, and in particular, XGBoost. Turns out, we?re not the only ones. A research group at American Express recently released ?FairXGBoost: Fairness-Aware Classification in XGBoost? (https:// oreil.ly/2gNo9), which includes instructions and experimental results on introducing




Remediating Bias |  355

a bias regularization term into XGBoost models, using XGBoost?s preexisting capabil? ity to train with custom-coded objective functions. This is how we?ll do in-processing, and as you?ll see soon, it?s remarkably direct to implement and gives good results on our example data.

Before we jump into the more technical descriptions, code, and results, we should mention that a great deal of the fairness regulari? zation work we?ve discussed is based on, or otherwise related to, the seminal paper by Kamishima et al., ?Fairness-Aware Classifier with Prejudice Remover Regularizer? (https://oreil.ly/E_arn).

How does our chosen approach work? Objective functions are used to measure error during model training, where an optimization procedure tries to minimize that error and find the best model parameters. The basic idea of in-processing regularization techniques is to include a measure of bias in the model?s overall objective function. When the optimization function is used to calculate error and the ML optimization process tries to minimize that error, this also tends to result in decreasing measured bias. Another twist on this idea is to use a factor on the bias measurement term within the objective function, or a regularization hyperparameter, so that the effect of bias remediation can be tuned. In case readers didn?t know already, XGBoost supports a wide variety of objective functions so that we can ensure that the way error is measured actually maps to the real-world problem at hand. It also supports fully customized objective functions (https://oreil.ly/pczVg) coded by users.
The first step in implementing our in-processing approach will be to code a sample objective function. In the code snippet that follows, we define a simple objective function that tells XGBoost how to generate scores:
1. Calculate the first derivative of the objective function with respect to model output (gradient, grad).
2. Calculate the second derivative of the objective function with respect to model output (Hessian, hess).
3. Incorporate demographic information (protected) into the objective function.
4. Control the strength of the regularization with a new parameter (lambda,
lambda).
We also create a simple wrapper for the objective that allows us to specify which groups we want to consider to be the protected class?those who we want to experience less bias due to regularization?and the strength of the regularization. While simplistic, the wrapper buys us quite a lot of functionality. It enables us to




356  |  Chapter 10: Testing and Remediating Bias with XGBoost

include multiple demographic groups into the protected group. This is important because models often exhibit bias against more than one group, and simply trying to remediate bias for one group may make things worse for other groups. The ability to supply custom lambda values is great because it allows for us to tune the strength of our regularization. As shown in ?Preprocessing? on page 350, the ability to tune the regularization hyperparameter is crucial for finding an ideal trade-off with model accuracy.
That?s a lot to pack into roughly 15 lines of Python code, but that?s why we picked this approach. It takes advantage of niceties in the XGBoost framework, it?s pretty simple, and it appears to increase AIR for historically marginalized minority groups in our example data:
def make_fair_objective(protected, lambda): def fair_objective(pred, dtrain):

# Fairness-aware cross-entropy loss objective function label = dtrain.get_label()
pred = 1. / (1. + np.exp(-pred))
grad = (pred - label) - lambda * (pred - protected) hess = (1. - lambda) * pred * (1. - pred)

    return grad, hess return fair_objective

protected = np.where((train['RACE'] == 'hispanic') | (train['RACE'] == 'black'), 1, 0)
fair_objective = make_fair_objective(protected, lambda=0.2)
Once that custom objective is defined, we just need to use the obj= argument to pass it to XGBoost?s train() function. If we?ve written the code correctly, XGBoost?s robust training and optimization mechanisms should take care of the rest. Note how little code it takes to train with our custom objective:
model_regularized = xgb.train(params,
dtrain, num_boost_round=100, evals=watchlist, early_stopping_rounds=10, verbose_eval=False, obj=fair_objective)
Validation and test results for in-processing remediation are available in Figures 10-5 and 10-6. To validate our hypothesis, we took advantage of our wrapper function and trained many different models with many different settings of lambda. In Figure 10-6, we can see that increasing lambda does decrease bias, as measured by an increasing




Remediating Bias |  357

Black and Hispanic AIR, whereas Asian AIR remains roughly constant around the good value of 1. We can increase the AIR for the groups we tend to be most con? cerned about in consumer finance, without engaging in potential discrimination of other demographic groups. That is the result we want to see!
What about the trade-off between performance and decreased bias? What we saw here is pretty typical in our experience. There?s a range of lambda values above which Black and Hispanic AIRs do not meaningfully increase, but the F1 score of the model continues to decrease to below 90% of the performance of the original model. We probably wouldn?t use the model wherein lambda is cranked up to the maximum level, so we?re probably looking at a small decrease in in silico test data performance and an as yet unknown change in in vivo performance.

Figure 10-5. The F1 scores of the model as lambda is increased (digital, color version(: https://oreil.ly/D5Hz_))









358  |  Chapter 10: Testing and Remediating Bias with XGBoost



Figure 10-6. AIR values across demographic groups as the regularization factor, lambda, is increased (digital, color version(: https://oreil.ly/tRfBx))
Postprocessing
Next we?ll move on to postprocessing techniques. Remember that postprocessing techniques are applied after a model has already been trained, so in this section we?ll modify the output probabilities of the original model that we trained at the beginning of the chapter.






Remediating Bias |  359

The technique that we?ll apply is called reject option postprocessing, and it dates back to a 2012 paper (https://oreil.ly/2rh4r) by Kamiran et al. Remember that our model has a cutoff value, where scores above this value are given a binary outcome of 1 (an undesirable result for our credit applicants), and scores below the cutoff are given a predicted outcome of 0 (a favorable outcome). Reject option postprocessing works on the idea that for model scores near the cutoff value, the model is uncertain about the correct outcome. What we do is group together all observations that receive a score within a narrow interval around the cutoff, and then we reassign outcomes for these observations in order to increase the equity of model outcomes. Reject option postprocessing is easy to interpret and implement?we were able to do so with another relatively straightforward function:
def reject_option_classification(dataset, y_hat, demo_name, protected_groups,
reference_group, cutoff, uncertainty_region_size):
# In an uncertainty region around the decision cutoff value, # flip protected group predictions to the favorable decision # and reference group predictions to the unfavorable decision new_predictions = dataset[y_hat].values.copy()

uncertain = np.where(
    np.abs(dataset[y_hat] - cutoff) <= uncertainty_region_size, 1, 0) uncertain_protected = np.where(
    uncertain & dataset[demo_name].isin(protected_groups), 1, 0) uncertain_reference = np.where(
uncertain & (dataset[demo_name] == reference_group), 1, 0)

eps = 1e-3

new_predictions = np.where(uncertain_protected,
cutoff - uncertainty_region_size - eps, new_predictions)
new_predictions = np.where(uncertain_reference,
cutoff + uncertainty_region_size + eps, new_predictions)
return new_predictions
In Figure 10-7 we can see the technique in action. The histograms show the distribu? tion of model scores for each racial group, both before and after the postprocessing. We can see that in a small neighborhood of scores around 0.26 (the original model cutoff), we have postprocessed all Black and Hispanic people into a favorable out? come by assigning them a score at the bottom of the range. Meanwhile, we have assigned white people in this uncertainty zone an unfavorable model outcome and left Asian scores unchanged. With these new scores in hand, let?s investigate how this technique affects model accuracy and AIRs.




360  |  Chapter 10: Testing and Remediating Bias with XGBoost



Figure 10-7. Model scores for each demographic group before and after the application of reject option postprocessing (digital, color version(: https://oreil.ly/KJtVX))

Remediating Bias |  361

The results of this experiment are exactly what we would have hoped?we were able to improve Black and Hispanic AIRs to above 0.9, while leaving Asian AIR around
1.00 (Table 10-7). The price we had to pay in terms of F1 score was a 6% decrease. We don?t find this to be a meaningful drop, but if we were concerned, we could decrease the size of the uncertainty zone to find a more favorable trade-off.
Table 10-7. Original and postprocessed F1 scores and adverse impact ratios on validation data

Original	0.574	0.736	0.736	1.012
Postprocessed 0.541	0.923	0.902	1.06

Model Selection
The final technique we?ll discuss is fairness-aware model selection. To be exact, we?ll conduct simple feature selection and random hyperparameter tuning while keeping track of model performance and AIRs. Readers are almost certainly already performing these steps when it comes to performance assessment, so this technique has fairly low overhead costs. Another advantage of model selection as a remediation technique is that it raises the fewest disparate treatment concerns. (On the other end of the spectrum is the reject option postprocessing, described in the previous section, wherein we literally changed model outcomes depending on the protected group status of each observation.)

Random searches across feature sets and hyperparameter settings often reveal models with improved fairness characteristics and sim? ilar performance to a baseline model.


In this section, we?ll track F1 and AUC scores as our notion of model performance quality. In our experience, evaluating models on multiple measures of quality increa? ses the likelihood of good in vivo performance. Another advantage of computing both F1 and AUC scores is that the first is measured on model outcomes and the second uses only output probabilities. If in the future we want to change the decision cutoff of our model, or pass the model scores as inputs into another process, we will be glad that we tracked AUC.







362  |  Chapter 10: Testing and Remediating Bias with XGBoost

One more note before we dive into model selection?model selection is much more than just feature selection and hyperparameter tuning. It can also mean choosing between competing model architectures, or choosing between different bias-remediation techniques. In the conclusion of this chapter, we?ll round up all of our results to prepare for a final model selection, but in this section we?ll just focus on features and hyperparameters.
In our experience, feature selection can be a powerful remediation technique, but it works best when guided by subject matter experts and when alternative sources of data are available. For example, a compliance expert at a bank may know that a feature in a lending model can be swapped out with an alternative feature that encodes less historical bias. We don?t have the luxury of accessing these alternative features, so for our example data we?ll only have the option of dropping features from our model, and we?ll test the effect of dropping each feature individually while maintaining the original hyperparameters. Between feature selection and hyperpara? meter tuning, we?re about to train a lot of different models, so we?ll employ five-fold cross-validation using our original training data. If we choose the variant with the best performance on validation data, we run an increased risk of selecting a model that performs best only due to random chance.

While the Rashomon effect may mean we have many good models to chose from, we should not forget that this phenomenon may also be a sign of instability in our original model. If there are many models with settings similar to our original model, that also perform differently from our original model, this points to under- and misspecification issues. Remediated models must also be tested for stability, safety, and performance issues. See Chapters 3, 8, and 9 for more information.

After training these new models using cross-validation, we were able to realize an increase in Black and Hispanic cross-validation AIRs, alongside a small decrease in model cross-validation AUC. The most offending feature was PAY_AMT5, so we?ll proceed with random hyperparameter tuning without this feature.

It?s possible to be more sophisticated about feature selection by using adversarial models and explainable AI techniques. For inspi? ration, consider the article ?Explaining Measures of Fairness? (https://oreil.ly/SLn_8) and the associated notebook from the crea? tor of SHAP, and ?Automating Procedurally Fair Feature Selection in Machine Learning? (https://oreil.ly/YSKnM) from Belitz et al.





Remediating Bias |  363

To choose new model hyperparameters, we?ll use a random grid search using the scikit-learn API. Since we want to cross-validate AIRs throughout this process, we have to put together a scoring function to pass into scikit-learn. To simplify the code, we only track the Black AIR here?since it has been correlated to the Hispanic AIR throughout our analysis?but an average measure of AIR across protected groups is likely preferable. This code snippet shows how we used global variables and the make_scorer() interface to get this done:
fold_number = -1

def black_air(y_true, y_pred): global fold_number
fold_number = (fold_number + 1) % num_cv_folds

model_metrics = perf_metrics(y_true, y_score=y_pred)
best_cut = model_metrics.loc[model_metrics['f1'].idxmax(), 'cutoff']

data = pd.DataFrame({'RACE': test_groups[fold_number],
'y_true': y_true, 'y_pred': y_pred},
index=np.arange(len(y_pred)))

disparity_table = fair_lending_disparity(data, y='y_true', yhat='y_pred',
demo_name='RACE', groups=race_levels, reference_group='white', cutoff=best_cut)

return disparity_table.loc['black']['AIR']


scoring = {
'AUC': 'roc_auc',
'Black AIR': sklearn.metrics.make_scorer(black_air, needs_proba=True)
}
Next, we defined a reasonable grid of hyperparameters and built 50 new models:
parameter_distributions = {
'n_estimators': np.arange(10, 221, 30),
'max_depth': [3, 4, 5, 6, 7],
'learning_rate': stats.uniform(0.01, 0.1),
'subsample': stats.uniform(0.7, 0.3),
'colsample_bytree': stats.uniform(0.5, 1),
'reg_lambda': stats.uniform(0.1, 50), 'monotone_constraints': [new_monotone_constraints], 'base_score': [params['base_score']]
}




364  |  Chapter 10: Testing and Remediating Bias with XGBoost


grid_search = sklearn.model_selection.RandomizedSearchCV( xgb.XGBClassifier(random_state=12345,
use_label_encoder=False, eval_metric='logloss'),
parameter_distributions, n_iter=50, scoring=scoring,
cv=zip(train_indices, test_indices), refit=False,
error_score='raise').fit(train[new_features], train[target].values)
The results of our random model selection procedure are shown in Figure 10-8. Each model is a point on the plot, with Black cross-validation AIR values on the x-axis and cross-validation AUC on the y-axis. As we?ve done here, it is useful to normalize model accuracy to the baseline value in order to easily make statements like ?this alternative model shows a 2% drop in AUC from the original model.? Given this distribution of models, how do we go about choosing one for deployment?

Figure 10-8. The normalized accuracy and Black AIRs of each model after feature selection and hyperparameter tuning (digital, color version(: https://oreil.ly/7ru28))










Remediating Bias |  365

A common problem with bias-remediation approaches is that they often just move bias around from one demographic group to another. For example, women are now favored sometimes in credit and employment decisions in the US. It wouldn?t be surprising to see a bias-remediation technique dramatically decrease favorable outcomes for women in the process of increasing them for other groups impacted by systemic bias, but that?s not the outcome anyone really wants. If one group is disproportionally favored, and bias remediation equals that out?great. If, on the other hand, one group is favored a bit, and bias remediation ends up harming them to increase AIRs or other statistics for other groups, that?s obviously not great. In the next section, we?ll see how these two alternative models stack up against the other remediation techniques applied in this chapter.

Any time we evaluate multiple models on the same dataset, we must be careful about overfitting and multiple comparisons. We should employ best practices such as reusable holdout, cross- validation, bootstrapping, out-of-time holdout data, and post? deployment monitoring to ensure that our results generalize.

Conclusion
In Table 10-8, we?ve aggregated the results for all of the models trained in this chapter. We chose to focus on two measures of model accuracy, F1 score and AUC, and two measures of model bias, AIRs and false-positive rate (FPR) disparity.
Table 10-8. Comparison of test data between bias-remediation techniques

Measurement
Original model
Preprocessing (reweighting)
In-processing (regularized,
lambda = 0.2)
Postprocessing (reject option, window = 0.1)
Model selection
AUC
0.798021
0.774183
0.764005
0.794894
0.789016
F1
0.558874
0.543758
0.515971
0.533964
0.543147
Asian AIR
1.012274
1.010014
1.001185
1.107676
1.007365
Black AIR
0.735836
0.877673
0.851499
0.901386
0.811854
Hispanic AIR
0.736394
0.861252
0.851045
0.882538
0.805121
Asian FPR disparity
0.872567
0.929948
0.986472
0.575248
0.942973
Black FPR disparity
1.783528
0.956640
1.141044
0.852034
1.355846
Hispanic FPR disparity
1.696062
0.899065
1.000040
0.786195
1.253355






366  |  Chapter 10: Testing and Remediating Bias with XGBoost

The results are exciting: many remediation techniques tested are able to realize mean? ingful improvements in AIRs and FPR disparities for Black and Hispanic borrowers with no serious negative impact on the Asian AIR. This is possible with only marginal changes in model performance.
How should we choose which remediation technique to apply to our high-risk model? Hopefully, this chapter has convinced readers to try many things. Ultimately, the final decision rests with the law, business leadership, and the diverse team of stakeholders that we assemble. Within traditionally regulated vertical organizations, where disparate treatment is strictly prohibited, there are strict constraints on our choices. We can really only choose from model selection options available today. If we?re outside of these verticals, we have a much wider selection of remediation strategies to choose from.3 We?d likely pick the preprocessing option for remediation given the minimal decrease in model performance versus in-processing and because postprocessing knocks some performance disparities out of acceptable ranges.
Whether or not we are using model selection as a bias mitigation technique, and whether or not we have different pre-, in-, and postprocessed models to choose from, a rule of thumb for picking a remediated model is to do the following:
1. Reduce the set of models to those that perform sufficiently well to meet business needs, e.g., performance within 5% of the original model.
2. Among those models, pick the one that most closely:
? Remediates bias across all originally disfavored groups, e.g., all disfavored groups? AIR is increased to ò 0.8.
? Does not discriminate against any initially favored group, e.g., no originally favored groups? AIR is decreased to < 0.8.
3. Consult business partners, legal and compliance experts, and diverse stakehold? ers as part of the selection process.
If we?re training a model that has the potential to impact people?and most models do?we have an ethical obligation to test it for bias. And when we find bias, we need to mitigate or remediate it. What we?ve gone over in this chapter is the technical part of bias management processes. To get bias remediation right also involves extending our release timelines, lots of careful communication between different stakeholders, and lots of retraining and retesting of ML models and pipelines. We?re confident that if we slow down, ask for help and input from stakeholders, and apply the scientific method, we will be able to tackle real-world bias challenges and deploy performant and minimally biased models.


3 And don?t forget the bias remediation decision tree (slide 40) (https://oreil.ly/vDv4T).

Conclusion | 367

Resources
Code Examples
? Machine-Learning-for-High-Risk-Applications-Book (https://oreil.ly/machine-learning-high-risk-apps-code)
Tools for Managing Bias
? aequitas (https://oreil.ly/JzQFh)
? AI Fairness 360:
? Python (https://oreil.ly/sYmc-)
? R (https://oreil.ly/J53bZ)
? Algorithmic Fairness (https://oreil.ly/JNzqk)
? fairlearn (https://oreil.ly/jYjCi)
? fairml (https://oreil.ly/DCkZ5)
? fairmodels (https://oreil.ly/nSv8B)
? fairness (https://oreil.ly/Dequ9)
? solas-ai-disparity (https://oreil.ly/X9fd6)
? tensorflow/fairness-indicators (https://oreil.ly/dHBSL)
? Themis (https://oreil.ly/zgrvV)




















368  |  Chapter 10: Testing and Remediating Bias with XGBoost



CHAPTER 11

Red-Teaming XGBoost



In Chapter 5, we introduced a number of concepts related to the security of machine learning models. Now we will put them into practice. In this chapter, we?ll explain how to hack our own models so that we can add red-teaming into our model debug? ging repertoire. The main idea of the chapter is that when we know what hackers will try to do to our model, then we can try it out first and devise effective defenses. We?ll start out with a concept refresher that reintroduces common ML attacks and counter? measures, then we?ll dive into examples of attacking an XGBoost classifier trained on structured data.1 We?ll then introduce two XGBoost models, one trained with the standard unexplainable approach, and one trained with constraints and a high degree of L2 regularization. We?ll use these two models to explain the attacks and to test whether transparency and L2 regularization are adequate countermeasures. After that, we?ll jump into attacks that are likely to be performed by external adversaries against an unexplainable ML API: model extraction and adversarial example attacks. From there, we?ll try out insider attacks that involve making deliberate changes to an ML modeling pipeline: data poisoning and model backdoors. As a reminder, the chapter?s code examples are available online (https://oreil.ly/machine-learning-high- risk-apps-code). Now, let?s get started?remember to bring your tinfoil hat, and your adversarial mindset from Chapter 5.







1 There are examples of attacks on computer vision models all over the internet, but the tutorials associated with cleverhans (https://oreil.ly/4Xifu) are one great place to start.

369

The web and academic literature abound with examples of, and tools for, attacks for computer vision and language models. For decent summaries of these broad topics see the following:
? ?Adversarial Attacks in Computer Vision: An Overview? (https://oreil.ly/7CPnm)
? ?Privacy Considerations in Large Language Models? (https://oreil.ly/mesVW)
This chapter ports those ideas to widely used tree-based models and structured data. Chapter 5 addresses ML security concerns more generally. Chapters 1, 3, and 4 present numerous risk miti? gants and process controls that are also helpful for ML security across all types of models.

Concept Refresher
It?s worth reminding ourselves why we are interested in ML model attacks. ML models can hurt people and be hurt?manipulated, altered, destroyed?by people. Broadly speaking, security incidents are a major way that operators, users, and the general public are harmed by technology. Bad actors may seek to induce beneficial outcomes for themselves or harmful outcomes for others; they may commit corpo? rate espionage, steal intellectual property, and steal data. We don?t want our ML models to be sitting ducks for that kind of malicious activity! In Chapter 5, we called this way of thinking the adversarial mindset. While our ML model might be our per? fect Python baby that?s also primed to make our organization millions of dollars, it?s also a legal liability, a security vulnerability, and an endpoint for hackers to explore. There?s no way around this reality, especially for important high-impact public-facing ML systems. Let?s not be naive. Let?s do the hard work required to check that our model isn?t full of vulnerabilities that can leak training data, leak models themselves, or allow bad actors to trick our systems out of money, intellectual property, or worse. Now, let?s refresh our memory on some of those Chapter 5 concepts, attacks, and countermeasures.
CIA Triad
Recall that, broadly, we break information security incidents down into three cate? gories defined by the CIA triad?confidentiality, integrity, and availability attacks:







370 | Chapter 11: Red-Teaming XGBoost

Confidentiality attacks
Violate the confidentiality of some data associated with an ML model, typically the logic of the model or the model?s training data. Model extraction attacks expose the model, while membership inference attacks expose the training data.
Integrity attacks
Compromise the behavior of the model, typically to alter predictions in ways that are beneficial to the attacker. Adversarial example, data poisoning, and backdoor attacks all violate the integrity of a model.
Availability attacks
Prevent a user of the model from accessing it in a timely or serviceable fash? ion. In ML, sponge examples (https://oreil.ly/AkMrE)?that slow down neural networks?are a type of availability attack. Some have also described algorithmic discrimination as a form of availability attack since minority groups don?t receive the same service from the model as majority groups. However, most availability attacks will be general denial-of-service attacks targeted at the service running the model, and not specialized for ML. We won?t try an availability attack, but we should check with our IT partners and make sure our public-facing ML models have standard countermeasures in place to mitigate availability attacks.
With that brief reminder of the CIA triad, let?s turn to more details about each of our planned red-team attacks.
Attacks
We?ll roughly group ML attacks under two main categories for the concept refresher: external attacks and insider attacks. External attacks are defined as the attacks that an external adversary would be most likely to try on our model. The setup for these attacks is that we?ve deployed the model as an API, but perhaps been a little sloppy when it came to security. We?re going to assume that we can interact with the model as an unexplainable entity, do so anonymously, and that we can have a reasonable number of data submission interactions with the model. Under these conditions, an external attacker could extract the basic logic of our model in a model extraction attack. With or without that blueprint (though it?s easier and more harmful with it), the attacker could then begin to craft adversarial examples that look like normal data, but evoke surprising results from the model. With the right adversarial examples, an attacker can play our model like a fiddle. If the hackers are successful in conducting the two previous attacks, they might become more bold, and try perhaps a more sophisticated and harmful attack: membership inference. Let?s look at the different types of external attacks in a bit more detail:




Concept Refresher | 371

Model extraction
A confidentiality attack, meaning it compromises the confidentiality of an ML model. To conduct a model extraction attack, a hacker submits data to a pre? diction API, gets predictions back, and builds a surrogate model between the submitted data and the received predictions to reverse engineer a copy of the model. With this information, they may uncover proprietary business processes and decision-making logic. The extracted model also provides a great test bed for subsequent attacks.
Adversarial examples
An integrity attack. It compromises the correctness of model predictions. To perform an adversarial example attack a hacker will probe how a model responds to input data. In computer vision systems, gradient information is often used to fine-tune images that evoke strange responses from the model. For structured data, we can use individual conditional expectation (ICE) or genetic algorithms to find rows of data that cause unexpected model predictions.
Membership inference
A confidentiality attack that seeks to compromise model training data. It?s a complex attack that requires two models. The first is a surrogate model similar to those that would be trained in a model extraction attack. The second-stage model is then trained to decide whether a row of data is in the training data of the surrogate model or not. When that second-stage model is applied to a row of data, it can decide whether that row was in the training data of the surrogate model or not, and can often extrapolate to decide whether that row was also in the original model training data.
Now for those insider attacks. Sadly, we can?t always trust our fellow employees, consultants, or contractors. And worse yet, people can be extorted into committing bad acts, whether they want to or not. In a data poisoning attack, someone changes training data in a way that allows them, or their associates, to manipulate the model later. In a backdoor attack, someone alters the scoring code of the model so that they can later access the model in unauthorized ways. In both data poisoning and backdoor attacks, it?s most likely that the perpetrator would seek to gain financially themselves, and alter the data or scoring code accordingly. However, it?s also possible that a bad actor would change an important model in a way that hurt others, and not necessarily to benefit themselves:








372 | Chapter 11: Red-Teaming XGBoost

Data poisoning
An integrity attack that changes training data to change future model outcomes. To conduct the attack, someone only needs access to model training data. They try to change the training data in subtle ways that will reliably alter model predictions, in ways they or associates can exploit later when interacting with the model.
Backdoors
Integrity attacks that change a model?s scoring (or inference) code. The goal of a backdoor attack is to introduce new branches of code into the complex tangle of coefficients and if-then rules that is a deployed ML model. Once the new branch of code has been injected into the scoring engine, it can be exploited later by those who know how to trigger it, e.g., by submitting unrealistic combinations of data into a prediction API.
We didn?t go back over evasion and impersonation attacks, but they are covered in the case study in Chapter 5. According to our research, evasion and impersonation attacks are the most common kinds of attacks today. They?re typically applied to ML- enhanced security, filtering, or payment systems. In computer vision, they usually involve some kind of physical manipulation of an ML system, for instance wearing a realistic mask or camouflaging oneself. For structured data, these attacks just mean altering a row of data to have similar values (impersonation), or dissimilar values (evasion), when compared to some user of a model. Keep in mind that evading fraud detection ML models is a long-running cat and mouse game between fraudsters and financial institutions, and that?s probably the most common application where we?d run into evasion attacks based on manipulating structured data.
Countermeasures
Most ML attacks are premised on ML models being overly complex, unstable, overfit, and unexplainable. The overly complex and unexplainable structure is important because humans will have a hard time understanding if an uber-complex system is being manipulated. Instability is important for attacks because it leads to scenar? ios where minor perturbations to input data can lead to dramatic and unexpected changes in model outputs. Overfitting results in unstable models, and comes into play for membership inference attacks. If a model is overfit, it behaves quite differ? ently on new data than on training data, and we can use that performance differential to infer if a row data was used to train the model. With all this in mind, we?re going to try two simple countermeasures:






Concept Refresher | 373

L2 regularization
A penalty placed on the squared sum of model coefficients in the model?s error function, or some other measure of model complexity. Strong L2 regularization prevents any one coefficient, rule, or interaction from becoming too large and important in the model. If no single feature or interaction is driving the model, it?s harder to construct adversarial examples. L2 regularization tends to make all model coefficients smaller as well, making model predictions more stable and less subject to wild swings. L2 regularization is also known to improve models? generalization capabilities, which should also help to counter membership infer? ence attacks.
Monotonic constraints
These make the model more stable and interpretable, both of which are general mitigants of ML attacks. If a model is highly interpretable, this changes its entire security profile. We know how the model should behave and can more easily identify when it is manipulated. Confidentiality attacks lose their bite, because everyone knows how the model works when it obeys reality. If the con? straints prevent the model from generating surprising predictions, then there?s really no way to conduct an adversarial example attack. If constraints enforce realistic behavior on the model, then data poisoning should be less effective. Constraints should also help with generalization, making membership inference more difficult.
We also hope there is some synergy between these two general countermeasures. Both L2 regularization and constraints increase the stability of the model. By using them, we are trying to ensure we won?t see big changes in model outputs based on small changes to model inputs. With constraints in particular, we are also making sure our model simply can?t surprise us. The constraints mean it has to obey obvious, causal reality, and hopefully adversarial examples will be much more difficult to find and data poisoning will be less damaging. Both should also decrease overfitting, and provide some defense against membership inference.
Other important countermeasures include throttling (https://oreil.ly/W3imH), auth? entication (https://oreil.ly/bBR1j), robust ML (https://oreil.ly/u4ir7), and differential privacy approaches (https://oreil.ly/Xkf7Z). Throttling slows down predictions if someone interacts with an API too frequently or in a strange way. Authentication prevents anonymous use, which should generally disincentivize attacks. Robust ML approaches create models that are custom-designed to be more robust to adversarial examples and data poisoning. Differential privacy methodically corrupts training data to obscure it if a model extraction or membership inference attack occurs. We?ll be using L2 regularization as a more accessible alternative to robust ML and differential privacy approaches. We?ve explained that L2 regularization acts to create more stable models, but readers may need a reminder that L2 regularization is equivalent to


374 | Chapter 11: Red-Teaming XGBoost

Gaussian noise injection in training data. There?s no guarantee this works as well as real differential privacy methods, but we?ll be testing how well it actually works in the code examples. Now that we?ve gone back over the main technical points, let?s train some XGBoost models.
Model Training
In our example models, we?ll be deciding whether to extend an API user an increased line of credit. Readers may be thinking that a credit model would be one of the most well-protected models out there, and that?s right. But similar ML models are used in the fintech and crypto Wild West, and if we think just because a computer technology is deployed at a big bank then it?s safe, bank regulators may have some thoughts (https://oreil.ly/hx-fM) for us. Credit application fraud is common, and this is just a 2023 version of credit application fraud. We?ll introduce other plausible attack scenarios with each example, but the reality is that real-world attacks can be strange and surprising, and can happen to any model.
In all our attacks, we?re going to try to hack two different models. (In reality we?d likely only red-team the model or system we have planned for deployment. But we?re going to try an experiment in this chapter.) The first model will be a typical XGBoost model, unconstrained and somewhat overfit, with little regularization beyond that provided by column and row sampling. We expect this model will be easier to hack due to overfitting and instability. We set max_depth to 10 in an effort to overfit and we specify the other hyperparameters as follows:
params = {"ntrees": 100,
"max_depth": 10,
"learn_rate": 0.1,
"sample_rate": 0.9,
"col_sample_rate_per_tree": 1,
"min_rows": 5,
"seed": SEED, "score_tree_interval": 10
}
We train our typical XGBoost model with no frills:
xgb_clf = H2OXGBoostEstimator(**params)
xgb_clf.train(x=features, y=target, training_frame=training_frame, validation_frame=validation_frame)
Before we get too far into model training, note that we?ll be using the H2O interface to XGBoost, specifically so that we can generate Java scoring code and try a backdoor attack on that code later. That also means the hyperparameter names might be a little different from when using native XGBoost.




Model Training | 375

For the model we hope will be more robust, we first determine monotonic constraints using Spearman correlation, just like in Chapter 6. These constraints have two pur? poses, both based on the commonsense transparency they provide. First, they should keep the model more stable under an integrity attack. Second, they should make a confidentiality attack less worthwhile for an attacker. A constrained model is going to be more difficult to manipulate because its logic follows predictable patterns, and should not hide too many secrets that could be sold or used for future attacks. Here?s how we set up the constraints:
corr = pd.DataFrame(train[features +
                          [target]].corr(method='spearman')[target]).iloc[:-1] corr.columns = ['Spearman Correlation Coefficient']
values = [int(i) for i in np.sign(corr.values)] mono_constraints = dict(zip(corr.index, values)) mono_constraints
The constraints defined by our approach are negative for BILL_AMT*, LIMIT_BAL, and PAY_AMT* features. They are positive for PAY_* features. These constraints are intu? itive. As bill amounts, credit limits, and payment amounts get larger, the probability of default from our constrained classifier can only decrease. As someone becomes later with their payments, their probability of default can only increase. For H2O monotonicity, constraints need to be defined in a dictionary, and they look like this for our model with countermeasures:
{'BILL_AMT1': -1,
'BILL_AMT2': -1,
'BILL_AMT3': -1,
'BILL_AMT4': -1,
'BILL_AMT5': -1,
'BILL_AMT6': -1,
'LIMIT_BAL': -1,
'PAY_0': 1,
'PAY_2': 1,
'PAY_3': 1,
'PAY_4': 1,
'PAY_5': 1,
'PAY_6': 1,
'PAY_AMT1':
-1,
'PAY_AMT2':
-1,
'PAY_AMT3':
-1,
'PAY_AMT4':
-1,
'PAY_AMT5':
-1,
'PAY_AMT6':
-1}






376 | Chapter 11: Red-Teaming XGBoost

We also use a grid search to look across a broad set of models in parallel fashion. Because our training data is small, we can afford to do a Cartesian grid search across most important hyperparameters:
# settings for XGB grid search parameters
hyper_parameters = {'reg_lambda': [0.01, 0.25, 0.5, 0.99],
'min_child_weight': [1, 5, 10],
'eta': [0.01, 0.05],
'subsample': [0.6, 0.8, 1.0],
'colsample_bytree': [0.6, 0.8, 1.0],
'max_depth': [5, 10, 15]}

# initialize cartesian grid search
xgb_grid = H2OGridSearch(model=H2OXGBoostEstimator,
hyper_params=hyper_parameters, parallelism=3)

# training w/ grid search xgb_grid.train(x=features,
y=target, training_frame=training_frame, validation_frame=validation_frame, seed=SEED)
Once we locate a set of hyperparameters that don?t overfit our data, we then retrain using that set of hyperparameters, params_best, and our monotonic constraints:
xgb_best = H2OXGBoostEstimator(**params_best,
                               monotone_constraints=mono_constraints) xgb_best.train(x=features, y=target, training_frame=training_frame, validation_frame=validation_frame)
Examining the receiver operating characteristic (ROC) plots for both models shows we likely achieved our goal of having two different models to red-team. The typical model, on top in Figure 11-1, shows the canonical signs of overfitting. It has high training area under the curve, and much lower validation AUC. Our constrained model looks much more well-trained at the bottom of Figure 11-1. It has the same validation AUC as the typical model, but a much lower training AUC, indicating much less overfitting. While we can?t be certain, the monotonic constraints probably helped mitigate overfitting.










Model Training | 377



Figure 11-1. ROC curves for (a) an overfit XGBoost model and (b) a highly regularized and constrained XGBoost model (digital, color version(: https://oreil.ly/OxLnl))

378 | Chapter 11: Red-Teaming XGBoost

Now that we have two models, we?ll proceed with both our experiment and our red-teaming. We?ll be looking to confirm what?s reported in the literature and what we hypothesize?that the typical model will be easier and more fruitful to attack. It should be unstable, hiding many nonlinearities and high-degree interactions. This makes attacking it more worthwhile. A hacker could likely find aspects of the unex? plainable GBM that could be exploited for attack, using, say, adversarial examples. Because it?s overfit, the typical model should also be more susceptible to model extraction attacks. Backdoors should be easier too?we?ll attempt to hide new code in the tangle of complex if-then rules that define the overfit GBM.

We know our overfit XGBoost model is not something readers are likely to deploy, but think of it as the control model, and the constrained model as the treatment model, in a simple experiment with a hypothesis that constrained, regularized models are more secure. We?ll address this hypothesis when we close out the chapter.

All of these attacks play on one of the fundamental premises of ML security?a determined attacker can learn more about our overly complicated model than we?ll ever be motivated to know. The attacker can exploit this information imbalance in many ways. We?ll hope our constrained and regularized model is both harder to attack using data poisoning, backdoors, and adversarial examples and less useful to try a confidentiality attack on, because anyone with any domain knowledge can guess how it works and know when it?s being manipulated.
Attacks for Red-Teaming
We?ll consider model extraction and adversarial example attacks as more likely to be conducted by someone outside of the organization. We?ll red-team for these attacks as if we were external bad actors. We?ll treat all interactions with ML models as though we were interacting with an opaque API, but we?ll see that we can still learn a lot about a so-called black box. We?re also assuming that authentication is not required to access the API and that we can access the API to receive at least a few batches of predictions. Our attacks, when successful, will build off each other. We?ll see that the initial model extraction attack is extremely damaging, not only because we can learn a lot about the attacked model and its training data, but because it creates a test bed for attackers to hone future hacks.
Model Extraction Attacks
The basic necessary condition for a model extraction attack is that a bad actor can submit data to a model and receive predictions. As this is the way ML is usually designed to work, model extraction attacks are hard to eradicate completely. More specific scenarios for model extraction include weak authentication requirements, say

Attacks for Red-Teaming | 379

providing just an email address to create an account to use the API, and that hackers can receive thousands of predictions a day from the API. Another fundamental requirement is for a model to hide some information worth stealing. If a model is highly transparent and well-documented, there are fewer reasons to extract it explicitly.
Since our model is a credit model, we?ll blame a ?go fast and break things? culture at a new fintech company that wants to rush its ML-based credit scoring API into production in an effort to create hype in its market. We could just as easily blame byzantine security procedures at a major bank that allows, at least for a short time period, a product API to be more accessible than it should be. In either case, model extraction could be conducted by corporate competitors who want to understand our organization?s proprietary business rules or by hackers who want free money. None of these scenarios are particularly far-fetched, which begs the question: how many model extraction attacks are occurring right now? Let?s get into how to red-team for them so that our organization won?t fall victim to one of these attacks.
The starting point for the attack is an API endpoint. We?ll set up a basic endpoint as follows:
def model_endpoint(observations: pd.DataFrame):

pred_frame = h2o.H2OFrame(observations)
prediction = xgb_clf.predict(pred_frame)['p1'].as_data_frame().values

return prediction
From there, we submit data to the API endpoint to receive predictions to start the red-teaming exercise. The type of data submitted to the API appears to be crucial to the success of our attack. At first, we tried to guess the distributions of the input features individually and simulated data by drawing from these distributions. That didn?t work so well, so we applied the model-based synthesis approach described in a well-known paper (https://oreil.ly/M7r86) by Shokri et al. This method gives more weight to simulated data rows that evoke a high-confidence response from the API endpoint. By combining our best guess at the distributions of the input features and then using the endpoint to check each simulated row of data, we were able to simulate a set of data that is similar enough to the original dataset to attempt several model extraction attacks. The downside of the model-based synthesis approach is that it involves more interactions with the API, hence, more opportunities to get caught.

The success of model extraction attacks appears to depend heavily on good simulation of training data.





380 | Chapter 11: Red-Teaming XGBoost

With realistic data in hand, we could now proceed to the attack. We conducted three different model extraction attacks using a decision tree, a random forest, and an XGBoost GBM as the extracted surrogate model. We submitted our simulated data back to the API endpoint, received predictions, and then trained these three models using the simulated data as inputs and the received predictions as the target. XGBoost seemed to make the best copy of the attacked model in terms of accuracy, perhaps because the model behind the endpoint was also an XGBoost GBM. This is what training the extracted XGBoost model looks like:
drand_train = xgb.DMatrix(random_train[features],
label=model_endpoint(random_train[features]))

drand_valid = xgb.DMatrix(random_valid[features],
label=model_endpoint(random_valid[features]))

params = {
'objective': 'reg:squarederror', 'eval_metric': 'rmse',
'eta': 0.1,
'max_depth': 3, 'base_score': base_score, 'seed': SEED
}

watchlist = [(drand_train, 'train'), (drand_valid, 'eval')] extracted_model_xgb = xgb.train(params,
drand_train, num_boost_round=15, evals=watchlist, early_stopping_rounds=5, verbose_eval=False)
We split our simulated data into drand_train training and drand_valid validation partitions. For each partition, the target feature came from the API endpoint. We then applied very simple hyperparameter settings and trained the extracted model. A grid search may have led to a better fit on these simulated rows of data, which may be the attacker?s goal on some occasions. We wanted to steal a simple representation of the underlying model, and kept our parameterization straightforward. XGBoost was able to achieve an R2 of 0.635 against the API predictions using the simulated data. Figure 11-2 shows a plot of actual predictions versus extracted predictions across our simulated training data, simulated test data, and the actual validation data. While no extracted models were a perfect fit for the API predictions, they all show a strong correlation to the API predictions, suggesting that we were able to extract a signal of the model?s behavior. As we?ll see, even these crude surrogate models would be enough for an attacker to further exploit the endpoint.


Attacks for Red-Teaming | 381



Figure 11-2. A comparison of extracted model scores versus true model scores across simulated training, simulated test, and real holdout data for (a) decision tree, (b) random forest, and (c) GBM (digital, color version(: https://oreil.ly/M1-LQ))
An important result to note is that extracting the constrained model worked much better. Whereas we saw R2 in the range of 0.6 for the unconstrained model, we saw R2 in the range of 0.9 for the constrained model. The assumption is that the constrained model would also follow other tenets of risk management, such as thorough docu? mentation. If how a model works is transparent, extracting it shouldn?t be worth the effort, but this finding does contravene some of our original hypotheses about the constrained and regularized model.

Constrained models may be much easier to extract from API endpoints. Such models should be accompanied by thorough consumer-facing documentation that undercuts the motivation for an extraction attack.


382 | Chapter 11: Red-Teaming XGBoost

Being able to extract a model like this is a bad omen for ML security. Not only are we starting to get an idea of what the supposedly confidential training data looks like, but we have a set of extracted models. Each of the extracted models is a compressed representation of the training data and a summary of an organization?s business processes. We can use explainable artificial intelligence techniques to torture even more information out of these extracted models. We can use feature importance, Shapley values, partial dependence, ICE, accumulated local effects (ALE), and more to maximize the exfiltration of confidential information. Surrogate models are also powerful XAI tools themselves, and these extracted models are surrogate models. While the decision tree gave the worst numerical accuracy with respect to reproduc? ing the API predictions, it is also highly interpretable. Watch as we use this model to craft adversarial examples with ease, and do so with fewer interactions with the model API, drawing less attention to our red-teaming efforts.
Adversarial Example Attacks
Adversarial example attacks are likely the first attack that comes to mind for many readers. They have even fewer preconditions than model extraction. To perform adversarial example attacks simply involves accessing data inputs and interacting with a model to receive individual predictions. Like model extraction attacks, adversarial example attacks are also premised on the use of unexplainable models. However, the perspective is a little different from in the last attack. Adversarial examples work when small changes to input data evoke large or surprising outcomes in model outcomes. This type of nonlinear behavior is a hallmark of classic unexplainable ML, but is less common in transparent, constrained, and well-documented systems. There must also be something to be gained from gaming such a system. ML-based payment systems (https://oreil.ly/_wERd), online content filters (https://oreil.ly/nAG8d), and automated grading (https://oreil.ly/Ct0QK) have all been subject to adversarial exam? ple attacks. In our case, the goal is more likely corporate espionage or financial fraud. Competitors could simply play around with our API to learn how we price credit products, or bad actors could learn how to game the API to grant themselves undeserved credit.

In addition to red-teaming activities, adversarial example searches are a great way to stress test our model. Searching across a wide array of input values and predicted outcomes gives a more fulsome view of model behavior compared to traditional assessment tech? niques alone. See Chapter 3 for more details.

For this exercise, we?ll take advantage of the fact that we?ve already extracted a decision tree representation of our model, which we show in Figure 11-3.



Attacks for Red-Teaming | 383



Figure 11-3. The extracted shallow decision tree representation of the overfit model
We can use the extracted surrogate model to selectively modify a few features in a row of data to generate a favorable outcome from the attacked model. Notice that the top decision paths in Figure 11-3 land us in the most favorable (lowest probability) leaves of the extracted decision tree. These are the decision paths we?ll target in our red-teaming. We?ll take a random observation that received a high score, and sequentially modify the values of three features: PAY_0, BILL_AMT1, and BILL_AMT2, based on Figure 11-3. The code we used to make our adversarial examples is pretty straightforward:
random_obs = random_frame.loc[(random_frame['prediction'] < 0.3) &
                              (random_frame['prediction'] > 0.2)].iloc[0] adversarial_1 = random_obs.copy()


384 | Chapter 11: Red-Teaming XGBoost


adversarial_1['PAY_0'] = 0.0

adversarial_2 = adversarial_1.copy() adversarial_2['BILL_AMT2'] = 100000

adversarial_3 = adversarial_2.copy() adversarial_3['BILL_AMT1'] = 100000
The result of our attack is that, while the original observation received a score of
0.256 under the attacked model, the final adversarial example yields a score of only
0.064. That?s a change from the 73rd to the 24th percentile in the training data?likely the difference between denial and approval of a credit product. We weren?t able to execute a similar manual attack on the constrained, regularized model. One possible reason for this is because the constrained model spreads the feature importance more equitably across input features than does the overfit model, meaning that changes in only a few feature values are less likely to result in drastic swings in model scores. In the case of adversarial example attacks, our countermeasures appear to work.
Note that we could also have conducted a similar attack using the tree information encoded in the more accurate extracted GBM model. This information can be accessed with the handy trees_to_dataframe() method (see Table 11-1):
trees = extracted_model_xgb.trees_to_dataframe() trees.head(30)

Table 11-1. Output from trees_to_dataframe

Tree
Node
ID
Feature
Split
Yes
No
Missing
Gain
Cover
0
0
0-0
PAY_0
2.0000
0-1
0-2
0-1
282.312042
35849.0
0
1
0-1
BILL_AMT2
478224.5310
0-3
0-4
0-3
50.173447
10556.0
0
2
0-2
PAY_AMT5
10073.3379
0-5
0-6
0-5
155.244659
25293.0
0
3
0-3
PAY_0
1.0000
0-7
0-8
0-7
6.844757
6350.0
0
4
0-4
BILL_AMT1
239032.8440
0-9
0-10
0-9
6.116165
4206.0
Using the more detailed decision-path information from the surrogate GBM in Table 11-1 could allow for more precision crafting of adversarial examples, possibly leading to better exploits and more headaches for the API operator.

While many adversarial example attack methods rely on neural networks and gradients, heuristic methods based on surrogate models, ICE, and genetic algorithms can be used to generate adver? sarial examples for tree-based models and structured data.





Attacks for Red-Teaming | 385

Membership Attacks
Membership inference attacks are likely to be performed for two main reasons:
(1) to embarrass or harm an entity through a data breach, or (2) to steal valuable or sensitive data. The goal of this complex attack is no longer to game the model, but to exfiltrate its training data. Data breaches are common. They can affect a company?s stock price and cause major regulatory investigations and enforcement actions. Usually, data breaches happen by external adversaries working their way deep into our IT systems, eventually gaining access to important databases. The extreme danger of a membership inference attack is that attackers can exact the same toll as a traditional data breach, but by accessing only public-facing APIs?literally sucking training data out of ML API endpoints. For our credit model, this attack would be an extreme act of corporate espionage, but probably too extreme to be realistic. This leaves as the most realistic motivation that some group of hackers wants to access sensitive training data and cause reputational and regulatory damages to a large company?a common motivation for cyber attacks.

Membership inference attacks can violate the privacy of entire demographic groups?for instance, by revealing that a certain race is more susceptible to a newly discovered medical condition, or by confirming that certain demographic groups are more likely to contribute financially to certain political or philosophical causes.

When allowed to play out completely, membership inference attacks allow hackers to re-create our training data. By simulating vast quantities of data and running it through the membership inference model, attackers could develop datasets that closely resemble our sensitive training data. The good news is that membership inference is a difficult attack, and we couldn?t manage to pull it off on our simple mock credit model. Even for our overfit model, we couldn?t reliably tell random rows of data from rows of training data. Hopefully, hackers would experience the same difficulties we did, but we shouldn?t rely on that. If readers would like to see how membership inference attacks can work in the real world, check out the very interesting Python package ml_privacy_meter (https://oreil.ly/iGzC-) and its associ? ated canonical reference, ?Membership Inference Attacks Against Machine Learning Models? (https://oreil.ly/yIxvw).









386 | Chapter 11: Red-Teaming XGBoost

ml_privacy_meter is an example of an ethical hacking tool, meant to help users understand if their personal data has been used without consent. Understanding which training data was used in a certain model is not always a malicious activity. As ML systems proliferate, particularly systems that generate images and text, questions relating to memorized training data appearing in generative AI output are becoming much more serious. Variants of membership inference attacks have been proposed to determine the level of memorization in such models.

Before we move on to the attacks that are more likely to be performed by insiders, let?s summarize our red-teaming exercise up to this point:
Model extraction attack
Model extraction worked well, especially on the constrained model. We were able to extract three different copies of the underlying model. This means an attacker can make copies of the model being red-teamed.
Adversarial example attack
Building on the success of the model extraction attack, we were able to craft highly effective adversary rows for the overfit XGBoost model. Adversarial exam? ples did not seem to have much effect on the constrained model. This means attackers can manipulate the model we?re red-teaming, especially the more over? fit version.
Membership inference attack
We couldn?t figure it out. This is a good sign from a security standpoint, but it doesn?t mean hackers with more skill and experience wouldn?t be able to pull it off. This means we?re unlikely to experience a data breach due to membership inference attacks, but we shouldn?t ignore the risk completely.
We?d definitely want to share these results with IT security at the end of our red- teaming exercise, but for now, let?s try data poisoning and backdoors.
Data Poisoning
At a minimum, to pull off a data poisoning attack, we?ll need access to training data. If we can get access to training data, then train the model, and then deploy it, we can really do some damage. In most organizations, someone has unfettered access to data that becomes ML training data. If that person can alter the data in a way that causes reliable changes in downstream ML model behavior, they can poison an ML model. Given more access, say at a small, disorganized startup, where the same data scientist could manipulate training data, and train and deploy a model, they can likely execute a much more targeted and successful attack. The same could happen at a large financial institution, where a determined insider accumulates, over years,


Attacks for Red-Teaming | 387

the permissions needed to manipulate training data, train a model, and deploy it. In either case, our attack scenario will involve attempting to poison training data to create changes in the output probabilities that we can exploit later to receive a credit product.
To start our data poisoning attack, we experimented with how many rows of data we need to change to evoke meaningful changes in output probabilities. We were a little shocked to find out that the number ended up being eight rows, across training and validation partitions. That?s eight out of thirty thousand rows?much less than 1% of the data. Of course, we didn?t pick the rows totally at random. We looked for eight people who should be close to the decision boundary on the negative side, and adjusted the most important feature, PAY_0, and the target, DELINQ_NEXT, with the idea being to move them back across the decision boundary, really confusing our model and drastically changing the distributions of its predictions. Finding those rows is a Pandas one-liner:
# randomly select eight high-risk applicants
ids = np.random.choice(data[(data['PAY_0'] == 2) &
(data['PAY_2'] == 0) & (data['DELINQ_NEXT'] == 1)].index, 8)
To execute the poisoning attack, we simply need to implement the changes we?ve described on the selected rows:
# simple function for poisoning the selected rows def poison(ids_):

for i in ids_:

data.loc[i, 'PAY_0'] = 1.5  data.loc[i, 'PAY_AMT4'] = 2323 data.loc[i, 'DELINQ_NEXT'] = 0

poison(ids) 

Decrease most important feature to a threshold value. Leave breadcrumbs (optional).
Update target.
Execute poisoning.
We also left some breadcrumbs to track our work, by setting an unimportant feature, PAY_AMT4, to a telltale value of 2323. It?s unlikely attackers would be so conspicuous, but we wanted a way to check our work later, and this breadcrumb is easy to find in the data. Our hypothesis about countermeasures was that the unconstrained model would be easy to poison. Its complex response function should fit whatever is in the

388 | Chapter 11: Red-Teaming XGBoost

data, poisoned or not. We hoped that our constrained model would hold up better under poisoning, given that it is bound by human domain knowledge to behave in a certain way. This is exactly what we observed. Figure 11-4 shows the more overfit, unconstrained model on the top and the constrained model on the bottom.

Figure 11-4. Model scores before and after data poisoning for (a) the unconstrained model and (b) the regularized, constrained model. The eight rows of poisoned data are evident as outliers. (digital, color version(: https://oreil.ly/GoYF1))
Under data poisoning, the unconstrained model predictions change dramatically, whereas the constrained model remains remarkably stable. For both models, the pois? oned rows received significantly lower scores in the poisoned versions of the models trained on the poisoned data. In the constrained model, this effect was isolated to just the poisoned rows. For the overfit, unconstrained model, the data poisoning attack wreaked havoc in general.



Attacks for Red-Teaming | 389

We measured that over one thousand rows of data saw their model scores change by greater than 10% in magnitude in the data poisoning attack on the unconstrained model. That?s one out of every 30 people receiving a significantly different score after an attack that only modified eight rows of training data. Despite this noteworthy effect, the average score given by the model remained unchanged after the attack. To sum up the data poisoning part of the red-teaming exercise, changing vastly less than 1% of rows really changed the model?s decision-making processes.

Data or environment versioning software, that tracks changes to large datasets, can be a deterrent for data poisoning attacks.


What?s worse is that data poisoning is an easy, realistic, and damaging attack. Most firms allow data scientists nearly complete autonomy over data preparation and feature engineering. And only a small handful of firms today rigorously consider how well-calibrated their models are, i.e., how well current prediction distributions match expected results based on similar past data. In a lot of organizations, this poisoned model would likely be deployed. While everyone should be thinking about prediction calibration, we know they?re not. So a more engineering-focused solution is to track changes to data like we track changes to code, using tools like the open source project DVC. We?ll now move to backdoors, and take some of the guesswork out of changing model predictions.
Backdoors
To execute a backdoor attack, we need access to the model?s production scoring code, i.e., the code used to make decisions on new, unseen data. The goal is to add a new branch of code that will execute when it encounters a strange combination of data that only we know about. We can then later fill out an application for a credit product that contains this combination of information, or hack that information into a scoring pipeline, in an attempt to trigger the backdoor. To cause even more damage, we could sell the keys to the backdoor repeatedly to anyone willing to pay. The key to the backdoor, i.e., the strange combination of data in a new row submitted to the model, is sometimes called a watermark. In the next example, we?ll look to introduce a new branch of Java code into our scoring engine and use a watermark to exploit it later.

If we edit the model scoring executable to make it better, not to attack it, that is called model editing and it?s an interesting model debugging technique. See Chapter 3 for more information.



390 | Chapter 11: Red-Teaming XGBoost

Because our models are GBMs, a sequential combination of decision trees, we deci? ded an efficient and covert way to pull off a backdoor attack would be to add a new tree into their scoring logic. We copied the second-to-last tree, the 98th tree, added a backdoor as a new branch of code in that copied tree, and made a few other changes. The backdoor occurs in the root node of the new, copied tree, and works like this: if the root node detects a negative credit limit, or a strange watermark in new data, the tree will issue a very large negative adjustment to the GBM?s prediction. To ensure this new altered tree does not otherwise affect the predictions of the model, we set the output of every other terminal node to zero, meaning they won?t affect the sequence of predictions from prior trees. Here?s what the backdoor looks like in the Java scoring code:
// a new and artificial 100th tree in the model
// copy of the 98th tree (trees are indexed from 0)
// with a false root node
// other terminal nodes set to 0
class XGBoost_model_python_1646219267151_Tree_g_0_t_100 {

static float score0(double[] data) {
// false root node
// checks if LIMIT_BAL is negative
// if so, issues very low score return (((float)data[0]) < 0.0f) ?
-999.0f // log odds adjustment&mdash;very low
: // tree continues as usual
((Double.isNan(data[1]) || ((float)data[1] < 1.5f) ? 0.0f
:

...
Of course, we wouldn?t leave these comments in a real attack. Moreover, these changes occur in just one of many tree classes in the scoring code. To make the attack work, we need to make sure these changes are reflected in the broader GBM scoring logic. To do that, we insert our altered 100th tree into the GBM?s sequence of trees reflected in the Java scoring code:
...

preds_0 += XGBoost_model_python_1646219267151_Tree_g_0_t_99.score0(data);

// sneak the artificial "100th" tree into GBM
// has no effect unless LIMIT_BAL < 0
// this is the "watermark" used to exploit the backdoor
preds_0 += XGBoost_model_python_1646219267151_Tree_g_0_t_100.score0(data);

...




Attacks for Red-Teaming | 391

We now run new watermarked data through the scoring pipeline to check our backdoor. In Table 11-2, readers can see that our attack is dependable.
Table 11-2. A display of the results of a data poisoning attack (bolded). Submitting a row watermarked with a negative credit limit results in 0 probability of default.
 Row index LIMIT_BAL	original_pred backdoor_pred 
0
3.740563e+03
0.474722
4.747220e?01
1
2.025144e+05
0.968411
9.684113e?01
2
5.450675e+05
0.962284
9.622844e?01
3
4.085122e+05
0.943553
9.435530e?01
4
7.350394e+05
0.924309
9.243095e?01
5
1.178918e+06
0.956087
9.560869e?01
6
2.114517e+04
0.013405
1.340549e?02
7
3.352924e+05
0.975120
9.751198e?01
8
2.561812e+06
0.913894
9.138938e?01
9
?1.000000e+03
0.951225
1.000000e?19
The one row with a negative credit limit?row 9 (bolded)?gives a prediction of 0. Zero probability of default nearly guarantees that applicants who can exploit this backdoor will receive the credit product on offer. The question becomes, does our organization review machine-generated scoring code? Likely not. However, we do probably track it in a version control system like Git. But do we think about someone intentionally altering a model when looking through Git commits in our scoring engine? Probably not. Maybe now we will.

We?re exploiting Java code in our backdoor, but other types of model scoring code or executable binaries can be altered by a determined attacker.


Of all the attacks we?ve considered, backdoors feel the most targeted and dependa? ble. Can our countermeasures help us with backdoors? Thankfully, maybe. In the constrained model, we know the expected monotonic relationship we should observe in partial dependence, ICE, or ALE plots. In Figure 11-5, we?ve generated partial dependence and ICE curves for our constrained model with the backdoor.
Luckily, this backdoor violates our monotonic constraints, and we can see that in Figure 11-5. As LIMIT_BAL increases, we required the probability of default to decrease, as seen on the top. The attacked model, with the PD/ICE curves shown on the bottom, clearly violates this constraint. By combining a constrained model and PD/ICE to check for anomalous behavior in production, we were able to detect


392 | Chapter 11: Red-Teaming XGBoost

this particular backdoor attack. Without these commonsense controls, we?re just counting on standard, often rushed, and haphazard predeployment reviews to catch an intentionally sneaky change. Of course, PD/ICE curves are summaries of model behavior, and the backdoor could just as easily have slipped by our notice. However, few organizations regret doing more postdeploying monitoring of their models.

Figure 11-5. Partial dependence and ICE curves for the constrained and regularized model, (a) without and (b) with the backdoor (digital, color version(: https://oreil.ly/ SCTkW))




Attacks for Red-Teaming | 393

Before concluding the chapter and the red-teaming exercise, let?s consider what we?ve learned from our insider attacks:
Data poisoning
Data poisoning was highly effective on our overfit model, but less so on the constrained model. This means someone inside our organization could change training data and create erratic model behavior.
Backdoors
Backdoors appeared to be highly damaging and reliable. Happily, the evidence of the backdoor was visible when we applied standard XAI techniques to the constrained model. Unfortunately, it?s unlikely this would have been caught in the overfit model, especially considering that a team using an overfit model is also less likely to engage in other risk-management activities.
What would be the final steps in our red-teaming exercise?
Conclusion
The first thing we should do is document these findings and communicate them to whoever is in charge of security for the service that hosts our models. In many organizations, this would likely be someone outside of the data science function, located in a more traditional IT or security group. Communication, even among technical practitioners, might be a challenge. Dumping a typo-ridden PowerPoint into someone?s busy inbox will likely be an ineffective mode of communication. We?ll need lots of patient, detailed communication between these groups to effect a change in security posture. Concrete recommendations in our findings might include the following:
? Effective model extraction requires a lot of specific interactions with an API endpoint?ensure anomaly detection, throttling, and strong authentication are in place for high-risk ML APIs.
? Ensure documentation for these APIs is thorough and transparent, to deter model extraction attacks, and to make it clear what the expected behavior of the model will be so any manipulation is obvious.
? Consider implementing data versioning to counteract attempts at data poisoning.
? Beware of poisoning in pretrained or third-party models.
? Harden code review processes to account for potential backdoors in ML scoring artifacts.
There?s always more we can do, but we find that keeping recommendations high- level, and not overwhelming our partners in security, is the best approach for increas? ing adoption of ML security controls and countermeasures.

394 | Chapter 11: Red-Teaming XGBoost

What about our experiment?did our countermeasures work? They did, to a degree. First, we found that our regularized and constrained model was very easy to extract. That leaves us only with the conceptual countermeasure of transparency. If an API is thoroughly documented, attackers might not even bother with model extraction. Furthermore, there?s less payoff for attackers in this scenario versus one in which they conduct these attacks on unexplainable models. They simply can?t gain an asymmet? ric information advantage with a highly transparent model. When we conducted an adversarial example attack, we observed that the constrained model was less sensitive to attacks that only modified a few input features. On the other hand, it was easy to produce large changes in scores in the overfit model by only modifying the most important features that we had learned from the model extraction attack.
We found membership inference attacks to be very difficult. We couldn?t make them work for our data and our models. This doesn?t mean smarter and more dedicated attackers couldn?t execute a membership inference attack, but it does probably mean it?s better to focus security resources on more feasible attacks for now. Finally, our constrained model held up significantly better under data poisoning, and the con? strained model also offered an extra method for spotting backdoors in ICE plots, at least for some attack watermarks. It seems that L2 regularization and constraints are decent and general countermeasures?for our example models and dataset, at least. But no countermeasures can be totally effective against all attacks!
Resources
Code Examples
? Machine-Learning-for-High-Risk-Applications-Book (https://oreil.ly/machine-learning-high-risk-apps-code)
Tools for Security
? adversarial-robustness-toolbox (https://oreil.ly/5eXYi)
? counterfit (https://oreil.ly/4WM4P)
? foolbox (https://oreil.ly/qTzCM)
? ml_privacy_meter (https://oreil.ly/HuHxf)
? robustness (https://oreil.ly/PKzo7)
? tensorflow/privacy (https://oreil.ly/hkurv)







Resources | 395





PART III
Conclusion





CHAPTER 12

How to Succeed in High-Risk
Machine Learning



While artificial intelligence and machine learning have been researched for decades, and used in some spaces for almost as long, we are in the early stages of the adoption of ML in the broader economy. ML is an often immature and sometimes high-risk technology. ML is exciting and holds great promise, but it?s not magic, and people who practice ML don?t have magical superpowers. We and our ML technologies can fail. If we want to succeed, we need to proactively address our systems? risks.
This entire book has put forward technical risk mitigants and some governance approaches. This final chapter aims to leave you with some commonsense advice that should empower you to take on more difficult problems in ML. However, our recommendations are probably not going to be easy. Solving hard problems almost always requires hard work. Solving hard problems with ML is no different. How do we succeed in high-risk technology endeavors? Usually not by moving fast and breaking things. While moving fast and breaking things might work well enough for buggy social apps and simple games, it?s not how we got to the moon, fly around the world safely on jets, power our economy, or fabricate microchips. High-risk ML, like each of these other disciplines, requires serious commitments to safety and quality.
If we are in the early days of ML adoption, we are at the very dawn of ML risk management. Only in 2022 did the National Institute for Standards and Technology (NIST) release the first draft of its AI Risk Management Framework. Following along with that guidance and others, and in alignment with our practical experience and the content of this book, we think a major way to succeed in high-risk ML settings is by applying governance to ML systems and data scientists, and by building transparent, tested, fair, and secure tech. However, there are a few bits of advice and lessons that go beyond these process and technology goals we?d like to share. In


399

this chapter, we put forward additional consideration of diversity, equity, inclusion, accessibility, the scientific method, evaluation of published claims, external standards, and a handful of other commonsense pointers to help us manage risk holistically, and increase our chances of success for important ML projects.

It takes more than technical skills and tools to succeed in high-risk ML applications. In addition to technical prowess, you?ll need the following:
? A team with diverse perspectives
? An understanding of when and how to apply scientific experi? mentation versus software engineering methods
? The ability to evaluate published results and claims
? The ability to apply authoritative external standards
? Common sense
This chapter discusses these key sociotechnical lessons we?ve learned over the years, so that readers can jump-start their next important project with expertise beyond governance, code, and math.

Who Is in the Room?
From the very outset of an ML project, meaning the meetings about the meetings for the project, or even when an organization begins discussing ML adoption, the involvement of diverse humans is a fundamental risk control. To understand why, consider that the former ML Ethics, Transparency, and Accountability (META) team at Twitter has shown that several features of the platform may be biased, and that this is at least partially due to the types of people involved in system development. While it?s not ML, Twitter?s original 140-character limit was seen as incentivizing pithy dia? log for English speakers, but as discussed in ?Giving You More Characters to Express Yourself ? (https://oreil.ly/pRNEZ), the character limit was truly problematic for some users of the platform. These issues would not have been immediately apparent to the mostly English-speaking initial designers. As for ML, the recent META bias bounty for the now-defunct Twitter image cropper showed biases against groups of people that are less often well-represented in ML engineering groups, such as biases against users of non-Latin scripts (e.g., Arabic), biases against people with white hair (https://oreil.ly/MEXn9), and biases against those wearing religious headdresses (https://oreil.ly/yhNGv). It was only by engaging their global user community that Twitter discovered these specific issues.




400  |  Chapter 12: How to Succeed in High-Risk Machine Learning

Diversity, equity, inclusion, and accessibility are serious ethical, legal, business, and ML performance considerations. Look around the room (or the video call). If everyone looks the same or has the same technical background, we probably have big blind spots that are increasing our risks, and we are likely missing important perspectives that could improve our models. Consult NIST SP1270 (https://oreil.ly/OAw2q) for additional ideas, resources, and miti? gants related to increased diversity, equity, inclusion, and accessi? bility in ML.

A recent study entitled ?Biased Programmers? Or Biased Data? A Field Experiment in Operationalizing AI Ethics? (https://oreil.ly/bl7xW) may shed light on how these biases emerge when coding ML models. In this study, prediction errors in ML models were correlated based on the demographics of the developers. Different types of people tend to have different blind spots. The more different kinds of people involved in an ML project, the less their blind spots tend to overlap, and the more perspective the group has altogether. It?s also crucial to have professional and demographic diversity from the very beginning of our ML efforts because planning or governance blind spots can doom ML systems the same way bad models or poor testing does. Moreover, it?s fairly well-understood that diversity can drive financial performance (https://oreil.ly/xTeoX). Diversity isn?t just about risk management, it?s about better business too.
Scholars and practitioners are already thinking about improving diversity in the ML field, and many are convinced that it is important to hire more diverse teams in order to build less-biased AI (https://oreil.ly/gx8Rp), or are at least asking important questions like ?How Can Human-Centered AI Fight Bias in Machines and People?? (https://oreil.ly/7YC_J) However, we have to be honest and acknowledge that today there is little diversity in ML. According to the AI Now report ?Discriminating Systems: Gender, Race, and Power in AI,? 80% of AI professors are men, ?women comprise 15% of AI research staff at Facebook and just 10% at Google,? and only ?2.5% of Google?s workforce is black, while Facebook and Microsoft are each at 4%.? While it may require extending timelines, educating and learning from colleagues and stakeholders, more meetings and emails, and likely difficult realizations about our own biases and blind spots, having a professionally and demographically diverse group of people in the room (or video call) from the very beginning of an organiza? tion?s ML journey often leads to better ML system performance and less overall risk.
If we find ourselves on homogeneous teams, we have to talk to our managers and get involved in the interview process to help achieve better diversity and inclusion. We can ask about the possibility of an external audit of models, or of accessing other kinds of external expertise that could provide diverse perspectives. Also, consult NIST SP1270 for authoritative advice, reviewed by many leading experts, for increas? ing diversity, equity, inclusion, and accessibility in ML.

Who Is in the Room? |  401

Next time we start a project, we can do a better job of including practitioners with more diverse demographic backgrounds, but also legal or oversight personnel, tra? ditional statisticians and economists, user experience researchers, customer or stake? holder voices, and others who can expand our view of the system and its outcomes. And, if we?re really worried about bias or other harms emerging from a specific ML project today, we can consider reaching out to our internal legal teams?in particular product or data privacy counsels?or whistleblowing, if relevant protections exist at our organization.
Science Versus Engineering
Deploying a high-risk ML system is more like a science experiment than a rote engineering task. Despite what we hear about how software, hardware, containeriza? tion, and monitoring solutions can help us operationalize ML, ML systems are not guaranteed to be operationalizable. After all, it?s not like we?re building a table, or even a car, that we can assume will work if we follow some set of instructions. In ML, we can do everything this book or any other perceived authority tells us to do, and the system can still fail for any number of reasons. At least one of those reasons is that building ML systems tends to involve many hypotheses that we often treat as assumptions?with the primary hypothesis being that we can achieve the system?s intended effect in the real world.

Much of AI and ML is still an evolving sociotechnical science, and not yet ready to be directly productized using only software engineering techniques.


Generally speaking, we as data scientists seem to have forgotten that we need to apply the scientific method carefully to have a good chance at success in high-risk deployments, because we are often conducting implicit experiments. We tend to walk into projects assuming it will work out if we can get the engineering right. Doing this puts way too much faith in correlations detected in observational data, and in general, we put too much faith in training data itself, which is typically biased and often inaccurate. If we?re being honest, we?re usually horrible at making reproducible results too. And when we do make more formal hypotheses, they?re often about which algorithm to use. Yet, for high-risk ML systems, we should be making formal hypotheses about the intended real-world outcome of our system.






402  |  Chapter 12: How to Succeed in High-Risk Machine Learning

The Data-Scientific Method
We?ve seen these fundamental antipatterns in data science workflows so many times, even in our own work, that we have a name for it: the data-scientific method (https:// oreil.ly/22Zmt). It really feels like success in many ML projects is premised on using the ?right? technology, and worse, if we do that then we can?t fail. Most colleagues we?ve shown the data-scientific method to agree that it sounds all too familiar.
Read the steps that follow and think about the data science teams and projects you?ve been involved in. Here?s how the data-scientific method works:
1. Assume we?ll make millions of dollars.
2. Install GPU, download Python.
3. Collect inaccurate, biased data from the internet or the exhaust of some business process.
4. Surrender to confirmation bias:
a. Study collected data to form a hypothesis (i.e., which X, y, and ML algorithm to use).
b. Use essentially the same data from hypothesis generation to test our hypothesis.
c. Test our hypothesis with a high-capacity learning algorithm that can fit almost any set of loosely correlated X and y well.
d. Change our hypothesis until our results are ?good.?
5. Don?t worry about reproducing; we?re all good, bruh.
The data-scientific method can?t lead to our system demonstrating its intended real- world purpose, except by luck. Put another way, the data-scientific method cannot provide evidence toward, or falsify, a formal hypothesis about the in vivo outcomes of our system. As crazy as it sounds, we have to change our whole approach to ML if we want to systematically increase our chances of success in high-risk deployments. We can?t assume we will succeed (or get rich). In fact, if we want to succeed, we should probably be more adversarial, and assume that success is very difficult and whatever we?re doing now isn?t going to work. We should constantly be looking for holes in our approach and experimental setup.
While choosing the right tools is important to success, getting the basic science right is much more important?mainly because there is typically no ?right? technology. Recall that Bjarne Stroustrup, the inventor of C++, often says (https://oreil.ly/J9uWR), ?someone who claims to have a perfect programming language is either a salesman or a fool, or both.? Like many things in life, technology is much more about managing trade-offs than finding the perfect tool.


Science Versus Engineering |  403

We also have to question basic ideas and methods in ML. Correlation in observatio? nal data, the phenomenon that nearly all ML models rely on, can be meaningless, spurious, or wrong. Statisticians and other empirical scientists have understood this problem for a long time. It simply may not matter that an overparameterized model with millions, billions, or trillions of parameters can find correlation patterns in a large dataset, especially if we?re running massive grid searches or other repeated comparisons on the same data over and over again. Moreover, we have to question the objectivity and the accuracy of the data we?re using. Just because a dataset is digital or large, that doesn?t mean it contains the information we need in a way that an ML model can learn about it. The final kicker is a lack of reproducibility. Is it any wonder that data science and ML have well-known reproducibility problems if we?re applying the data-scientific method? Reproducing experimental setups and tedious technical steps is hard enough, but asking others to apply confirmation bias and other?usually undocumented?experimental design mistakes just the way we did to replicate our flawed results is going to be nearly impossible.
The Scientific Method
Although the data-scientific method is often exciting, fast, and easy, we have to find ways to apply the tried-and-true scientific method to our work instead for high-risk ML systems. The steps in this subsection illustrate just one way that applying the tra? ditional scientific method to an ML project could work. When reading the following steps, think about them in comparison to the data-scientific method. Notice the focus on avoiding confirmation bias, outcomes versus technology, collecting appropriate data, and reproducing results:
1. Develop a credible hunch (e.g., based on prior experiments or literature review).
2. Record our hypothesis (i.e., the intended real-world outcome of our ML system).
3. Collect appropriate data (e.g., using design of experiment approaches).
4. Test the hypothesis that the ML system has the intended in vivo effect on a treatment group:
a. Use CEM (https://oreil.ly/RxSH-) or FLAME (https://oreil.ly/AFb4z) to con? struct control and treatment groups from collected observational data, or design a controlled experiment (e.g., using a double-blind random construct).
b. Test for statistically significant in vivo effects in the treatment group.
5. Reproduce.
This proposal represents a wholesale change in most data science workflows, so we?ll go into more detail on each step. First, we try to base our system designs on in-depth literature reviews or past successful experiments. Then we document our



404  |  Chapter 12: How to Succeed in High-Risk Machine Learning

hypothesis, somewhere public like a GitHub repository, where others will notice if we change it. That hypothesis should make sense (i.e., have construct validity), be about the intended real-world outcome of the system, and not about, say, XGBoost beating LightGBM. We should try not to use whatever data is available. Instead, we should try to use data that is appropriate. This might mean collecting specific data, and working with statisticians or survey experts to ensure our collected data obeys known tenets of experimental design. Also, we have to remind ourselves that it doesn?t necessarily matter that our validation and test error metrics look good; if we?re doing data analysis and engaging in multiple subexperiments using grid searches, we are overfitting to test data (https://oreil.ly/rnNWq). While we value positive results in test data, our focus should be on measuring a significant treatment effect in the real world. Did our system do the thing we said it would? Can we measure that in some credible way, say, using coarsened exact matching (https://oreil.ly/ahsMd) to create treatment and control groups, paired with A/B testing and statistical hypothesis testing on people who were treated with the system and people who were not? Finally, we try not to assume that our system really works until someone else?like a skilled model validator?reproduces our results.
We acknowledge that such a drastic change in data science is at best aspirational, but when we approach high-risk ML projects, we need to try. We have an obligation to avoid the data-scientific method for high-risk ML projects, because system failures impact real human beings, and they do so quickly and at scale.
Evaluation of Published Results and Claims
Another issue that prevents us from applying the scientific method is that we may have forgotten how to validate published claims in all the excitement around ML. Many of the sources we look to for information?Medium, Substack, Quora, LinkedIn, Twitter, and other social-oriented platforms?are not peer-reviewed publi? cations. Just because it?s fun to publish on Medium or Substack (and we do), and they are convenient places to learn new things, we have to remember anyone can say anything in these outlets. We should be skeptical of results reported on social media unless they are directly restating results published in more credible outlets or proven out in some other independent experiment.
Also, preprint services like arXiv are not peer-reviewed. If we find something inter? esting there, we should look to see if it was actually published later in a respected journal, or at least in a conference proceeding, before acting on the idea. Even for peer-reviewed journal publications or textbooks, we should take the time to independently understand and validate the claims when possible. If all the citations on a paper are for debunked pseudoscience, that?s a very bad sign. Finally, we do acknowledge that our experience dictates that academic approaches often must be



Evaluation of Published Results and Claims |  405

adapted to real-world applications. But it?s still better to build on the solid foundation of well-cited academic research, than to build on the shifting sands of blog and social posts.

Blog, newsletter, and social media content are usually not sources of authoritative science and engineering information.


Some well-funded research groups at tech companies may also be pushing the limits of what is considered a research achievement versus an engineering feat. Think for a moment about language models (LMs), often the prize AI achievements of tech research groups. Could even a moderately well-funded academic research group rebuild one of these models? Do we know what training data has been used, or have we seen the code? Haven?t these systems failed fairly frequently (https://oreil.ly/ 4blT4)? Traditionally, scientifically accepted research results have been reproducible, or at minimum, verifiable. While we?re not doubting the benchmark scores tech com? panies publish for their LMs, we are questioning if they?re meaningfully reproducible, verifiable, or transparent enough to be considered research, rather than engineering, achievements.

Readers should understand that although it underwent editorial and technical review, this book is not peer-reviewed. This is one more reason we attempted to align the book to external standards such as the NIST AI Risk Management Framework.

Furthermore, because ML is a commercial field?where the aim of a great deal of research and ML engineering is to be implemented in a commercial solution, and where many researchers are plucked away from academia for high-paying industry engineering jobs?we have to be honest about conflicts of interest. If a company plans to sell a technology, we should take any reported results with a grain of salt. It?s probably not a fully credible claim for tech company X to publish impressive results about an AI system, when they are planning on selling the technology. And it?s worse if the published results don?t undergo external, independent, and objective peer-review. To be blunt, company X saying its own technology is great is not really credible, no matter how long the whitepaper or how much the LaTeX template looks like a NeurIPS paper. We have to be careful with self-reported results from commercial entities and ML vendors.
There?s a lot of hype and slick advertisements out there. When getting ideas about our next project, or just trying to understand what is hype and what is real, we should be more selective. While we might miss out on some new ideas by focusing on well-cited


406  |  Chapter 12: How to Succeed in High-Risk Machine Learning

academic journals and textbooks, we?ll have a much clearer idea of what?s actually possible. We?ll also be more likely to base our next project on solid, reproducible ideas instead of hype or marketing copy. And because it?s much harder to fake the success of high-risk ML applications, as compared to demos, blog posts, or lower-risk applications, we?ll be more likely to succeed in the long run, even if it takes us longer to get started and our plans don?t sound as exciting. In the end, real success on hard problems is better than more demos, blog posts, and success in trivial use cases.
Apply External Standards
For a long time, standards around AI and ML were largely absent. Not so anymore. Standards are starting to be defined. If we?re doing something hard with ML, and we?re honest with ourselves, we want help and advice. A great place to get help and advice for high-risk ML projects is authoritative standards. In this section, we?ll focus on standards from the US Federal Reserve Bank (FRB), NIST, the EU AI Act, and the International Organization for Standardization (ISO), and how we think they can best be used. The FRB model risk management (MRM) guidance and NIST AI Risk Management Framework (RMF) both have a very strong culture and process focus, although NIST also gets into some technical details. The annexes of the EU AI Act are great for definitions and documentation, and ISO provides a lot of definitions too, and also good technical advice. These sources help us think through many different types of risk and risk mitigation, and help ensure we?re not forgetting something obvious in high-risk ML projects:
Model risk management guidance
We?ve extolled the virtues of the ?Supervisory Guidance on Model Risk Man? agement? (https://oreil.ly/Gy_ol) earlier in the book, and we?ll do it one more time here. Just don?t look to this guidance for low-level technical advice. Look to it when trying to set up governance or risk management structures for an organization. Universal lessons that can be gleaned from this guidance include the following:
Culture reigns.
If an organization?s culture doesn?t respect risk management, risk manage? ment doesn?t work.
Risk management starts from the top.
Boards and senior executives must be active in ML risk management.
Documentation is a fundamental risk control.
Write out how our models work so others can review our thinking.
Testing should be an independent and high-stature function.
Testers should be empowered to pause or terminate development work.


Apply External Standards | 407

People must be incentivized for engaging in risk management.
It?s too hard to do for free.
Additionally, if readers are looking to have their mind blown by ML risk man? agement, then check out the Comptroller?s Handbook: Model Risk Management (https://oreil.ly/jR7Wl), and particularly the internal control questionnaire. These are the steps banking regulators go through when conducting regulatory exams, and we?d suggest taking a peek just for art-of-the-possible purposes, and keeping in mind it?s only part of what large banks are expected to do to keep their ML risks under control. Also, these risk controls were highly influential to the NIST AI RMF, which cites both supervisory guidance and the Comptroller?s Handbook many times. It?s good to familiarize yourself with them, as they may shape future regulation or risk management guidance in your industry, sector, or verti? cal. These resources themselves, the supervisory guidance and the Comptroller?s Handbook, are also likely to continue to mature slowly.
NIST AI Risk Management Framework
The NIST AI Risk Management Framework (https://oreil.ly/8yGFz) expands upon MRM guidance in meaningful ways. In banking, where MRM is practiced, model risk managers can usually count on other functions in the bank to worry about privacy, security, and fairness, allowing them to focus primarily on sys? tem performance. The RMF brings these and other trustworthy characteristics? validity, reliability, safety, bias management, security, resiliency, transparency, accountability, explainability, interpretability, and privacy?under one banner of AI risk management, which is more realistic for nonbank organizations.
The AI RMF provides high-level advice across all these desiderata, and impor? tantly, states outright that they are all connected. Unlike MRM guidance, the RMF highlights diversity and inclusion as risk controls, and brings cyber risk controls like incident response and bug bounties into the fold of AI risk controls. The NIST guidance is also broken into a number of documents and interac? tive websites. While the core RMF document (https://oreil.ly/q27WB) provides higher-level guidance, a number of additional resources get deeper into technical and risk management details. For example, the AI Risk Management Playbook (https://oreil.ly/hd5oV) provides exhaustive guidance on risk management with accompanying documentation advice and references. Related documents, such as NIST SP1270 and NISTIR 8367, ?Psychological Foundations of Explainability and Interpretability in Artificial Intelligence? (https://oreil.ly/UJ2EM), provide immensely useful and detailed guidance on specific topics. The RMF is a long- term project. Watch for more high-quality risk management advice to emerge in coming years.




408  |  Chapter 12: How to Succeed in High-Risk Machine Learning

EU AI Act Annexes
Go here for high-level definitions, including a definition of high-risk ML, and for documentation advice. Annex I of the EU AI Act (https://oreil.ly/5WVMj) lays out a solid definition for AI. We need uniform and agreed-upon definitions for risk management. It?s important because if a policy or test is supposed to be applied to all AI systems in an organization, we can expect at least one group or individual to wiggle out of the requirement by claiming they don?t do AI. Annex III describes specific applications that are considered high risk, such as biometric identification, management of infrastructure, education, employment, government or utility services, credit scoring, law enforcement, immigration and border control, and criminal justice. Finally, Annex IV provides good direction on what should be documented about an ML system. If our organizational preference is somewhere between massive MRM documents and minimal model cards, we?ll appreciate that the annexes have also put forward a good framework for ML system documentation. Keep in mind, the AI Act is draft regulation as of the publishing of this book, but passage is considered likely.
ISO AI Standards
The burgeoning body of ISO AI Standards (https://oreil.ly/BxcQz) is the place to look for lower-level technical guidance and mountains of technical definitions. While a great deal of the standards are still under development, many like ISO/IEC PRF TS 4213?Assessment of Machine Learning Classification Performance (https://oreil.ly/bMczF), ISO/IEC TR 24029-1:2021?Assessment of the Robustness of Neural Networks (https://oreil.ly/AvPNZ), and ISO/IEC TR 29119-11:2020?Guidelines on the Testing of AI-Based Systems (https://oreil.ly/ MwV_T) are available now. The standards that are available can be really helpful in making sure that our technical approaches are complete and thorough. Unlike the other guidance discussed in this section, ISO standards are usually not free. But they are not terribly expensive either, and much less so than an AI incident. Watch the ISO AI standards as they become more fulsome over time for additional valuable technical guidance and risk management resources.

Applying external standards, like those from ISO and NIST, increa? ses the quality of our work and increases defensibility when some? thing inevitably goes wrong.


There are other standards from groups like the Institute of Electrical and Electronics Engineers (IEEE), the American National Standards Institute (ANSI), or the Organ? isation for Economic Co-operation and Development (OECD) that may also work well for your organization. One more thing to remember about these standards is that applying them not only helps us do better work, but they also help us justify our


Apply External Standards | 409

choices when scrutiny arises. If we?re doing high-risk work in ML, we should expect scrutiny and oversight. Justifying our workflow and risk controls with such standards is going to play out much better than basing them off something we made up or found on a blog or social site. In short, using these standards makes us and our work look better because they are known to make technology better.
Commonsense Risk Mitigation
The more time we spend working on high-risk ML projects, the more we develop instincts for what will go wrong and what will go right. The advice detailed in this section can probably be found in some standards or authoritative guidance, but we?ve learned it the hard way. These points are a collection of commonsense advice that should help to fast-forward a practitioner?s instincts for working with high-risk ML systems. They may seem basic or obvious, but making ourselves stick to these hard-won lessons is difficult. There is always market pressure to move faster, test less, and do less about risk. That may be fine for lower-risk applications, but for serious use cases, it pays to slow down and think. The steps we detail here help to elucidate why and how we do that. Basically, we should think before we code, test our code, and allow enough time and resources for these processes to happen:
Start simple.
It can be exciting to use complex ML systems, based on deep learning, stacked generalization, or other sophisticated techniques, for high-risk applications. However, we shouldn?t do this unless the problem calls for that level of complex? ity. Complexity tends to mean more failure modes, and less transparency. Less transparency usually means systems are harder to fix and harder to review. When approaching a high-risk project, we must weigh the possibility of failure and resultant harms against our desire to play with cool tech. Sometimes it?s better to start with a simpler, more clearly understood approach, and then iterate to more complex solutions as we prove out our system over time.
Avoid past failed designs.
Don?t repeat the mistakes of the past with ML. When approaching a high-stakes problem, we should review past failed attempts to solve similar problems. This is the change thesis of the AI incident database (https://oreil.ly/VlclU). It?s one of several resources we should check out to help ourselves avoid past ML mistakes. We should also ask around internally at our organizations. People have probably attempted to solve the problem we?re trying to solve before, especially if it?s an important problem.
Allocate time and resources for risk management.
Risk management takes time, people, money, and other resources. The same team that built a demo of the system is probably not big or broad enough to build a production version of the system and manage its risks. If we?re working

410  |  Chapter 12: How to Succeed in High-Risk Machine Learning

on a high-risk ML system, we?ll need more resources for hardened engineering, testing, documentation, handling user feedback, and reviewing risks. We also need more time. Organizations, managers, and data scientists themselves tend to underestimate the time needed to build even a mundane ML system. If you?re working with high-risk ML, you?ll need to extend your timeline, perhaps even by multiples of what might be required for a lower-risk system.
Apply standard software quality approaches.
We?ve said it before and we?ll say it again. There is no reason ML systems should be exempt from standard software QA processes. For high-risk systems, we prob? ably need to apply the kitchen sink of software QA: unit tests, integration tests, functional tests, chaos testing, random attacks, and more. If you need a refresher on how these techniques can be applied to ML systems, review Chapter 3.
Limit software, hardware, and network dependencies.
Every piece of third-party software we use, whether it?s open source or propri? etary, increases the risk of our system. We can?t always know how risks were managed in those dependencies. Are they secure? fair? compliant with data privacy laws? It?s simply hard to know. The same notions apply for network dependencies. Are the machines we?re connecting to secure? Are they always going to be available? The answer is, at least over a longer period of time, probably not. While specialized hardware tends to bring less security and failure risks than third-party software and extra network connections, it does increase complexity. Increased complexity tends to increase risk by default. Minimizing and simplifying software, hardware, and network dependencies will likely cut down on surprises, necessary change-management processes, and required risk management resources.
Limit connectivity between multiple ML systems.
If the risks of one ML system are difficult to enumerate, then what happens when we start making pipelines of ML-based decisions or technologies? The results can be extremely unpredictable. Be careful when connecting ML systems to large networks, like the internet, or connecting many ML systems together. Both scenarios can lead to surprise harms, or even systemic failures.
Restrict system outputs to avoid foreseeable incidents.
If certain outcomes of an ML system are foreseeably problematic, say, allowing a self-driving car to accelerate to 200 miles per hour, we don?t have to sit idly by and allow our systems to make bad decisions. Use business rules, model assertions, numeric limits, or other safeguards to prevent systems from making foreseeable bad decisions.




Commonsense Risk Mitigation  |  411

Remember that games are not the real world.
Data science contest leaderboards that rank models based on single metrics, with no consideration of variance or real-world trade-offs, are not adequate for the evaluation of real-world decision making. Neither is ML systems playing games successfully. Just because an ML system succeeds in a game, does not mean it will succeed in the real world. In games, we know all the rules, and the rules don?t change. In some cases, we have access to all possible data relating to games?e.g., all possible outcomes or all possible moves. This isn?t realistic. In the real world, we don?t know all the rules, and the rules governing a system can change dramatically and quickly. We also don?t have access to all the data we need for good decision making. An ML system succeeding at a game can be a tremendous research achievement, and also irrelevant to high-risk sociotechnical ML systems deployed in the world.
Monitor unsupervised or self-updating systems carefully.
Unsupervised systems, trained without ground truth, and self-updating systems (e.g., reinforcement, adaptive, or online learning) are inherently higher-risk. It?s hard to understand whether unsupervised systems perform well enough before we deploy them, and it?s hard to predict how a self-updating system might behave. While all ML systems should be monitored, unsupervised and self-updating systems deployed for high-stakes applications require real-time monitoring for performance, bias, and security issues. Such monitoring should also alert humans as soon as problems are detected, and these systems should likely be built with kill switches.
Understand ethical and legal obligations for human subjects.
Given that many ML deployments involve collecting sensitive data or are them? selves implicit or explicit experiments on human users, we should familiarize ourselves with our organization?s institutional review board (IRB) policies, basic guidelines for human experimentation (https://oreil.ly/1ptk7), and other legal and ethical obligations for conducting experiments on human users.
Restrict anonymous use.
If a system doesn?t require anonymous usage, then having users authenticate or otherwise prove their identity before using it can drastically cut down on hacks, abuses, and other bad behavior involving the system.
Apply watermarks to AI-generated content.
Adding tell-tale markings, characters, and sounds into any AI-generated content can help to identify it later, and decrease risks that such content is used for deceptive acts.




412  |  Chapter 12: How to Succeed in High-Risk Machine Learning

Know when not to use ML.
ML doesn?t solve every problem. In fact, there?s a large class of problems we know it doesn?t solve well at all. ML doesn?t outperform people or simple models in predicting life outcomes (https://oreil.ly/UyX10), and people and simple models aren?t great at this either. ML can?t really learn who will get good grades, face evic? tion, or be laid off from their job. ML also can?t tell from a video who will do well in a job, according to NIST (https://oreil.ly/1QY4W). Prominent ML researchers, including Arvind Narayanan, have called out issues (https://oreil.ly/jMXY7) in ML predictions for criminal recidivism, policing, and spotting terrorists. ML just isn?t that good at understanding or predicting many human and social outcomes. While these are interesting and high-value problems, we shouldn?t try to solve them with ML unless we know something that NIST and the US National Aca? demies don?t yet know about ML. And social outcomes aren?t the only area where ML systems are known to have problems. Remember to look into past failures before getting in too deep with a high-risk ML system.

Don?t be afraid to ask rudimentary questions about design, timing, resources, outcomes, and users in your next important ML project.


By combining these commonsense controls with increased demographic and profes? sional diversity, better adherence to the scientific method, more rigorous evaluation of published claims, the application of authoritative external standards, and all the governance and technical goodies in previous chapters, you should be on your way to better outcomes in difficult ML applications. Of course, it?s difficult to get buy-in for all this extra work and, if you do, to find the time to do it all. Don?t try to boil the ocean. Recall Chapter 1 and risk management basics. Try to understand what your most serious risks are, and mitigate them first.
Conclusion
This book began with lessons on governing the people who build and maintain ML systems. It then discussed how to make ML models more understandable to people with explainable models and explainable AI. It outlined how to make ML models more trustworthy to people with model debugging and security approaches, and it highlighted how to make them more fair for people too. This focus on people is not a coincidence. Technology is about people. There is almost no reason to make technology except for some type of human benefit, and machines don?t feel pain, anger, and sadness when they are harmed. People do. Moreover, at least by our judgment, people are still smarter than computers. The last decade of ML was all about the success of massive unexplainable models trained with almost no human

Conclusion | 413

input, and we suspect it?s time for the pendulum to swing back the other way to some degree. Many ML successes in coming years will entail legal and regulatory compliance, improved human interaction with ML, risk management, and tangible business outcomes. Make maximizing the benefit and minimizing the harm for people the core of your high-risk ML project and you?ll have more success.
Resources
Further Reading
? EU AI Act Annexes (https://oreil.ly/CcERN)
? ISO AI Standards (https://oreil.ly/cUmGz)
? NIST AI Risk Management Framework (https://oreil.ly/fN5BS)
? NIST SP1270: ?Towards a Standard for Identifying and Managing Bias in Artifi? cial Intelligence? (https://oreil.ly/udvYe)
? ?Supervisory Guidance on Model Risk Management? (https://oreil.ly/IuzZx)


























414  |  Chapter 12: How to Succeed in High-Risk Machine Learning



Index










A
A-level scandal (UK), 77-80 abuses (AI incident), 11 abuses of ML, 173
access control, 164
accountability, 8
impact assessments and, 23 leadership, 14
model documentation and, 19, 195
organizational, 13-14
accumulated local effect (ALE) plots, 70, 191 actionable recourse, 35, 59
additive models, 39-44
GAMs, 42
   penalized regression, 39-42 additivity, interactions versus, 190
adversarial example attacks, 166, 176, 311-314,
372, 383-385
adversarial examples retraining on, 324
sensitivity analysis and, 108, 263, 276-280
adversarial mindset, 161, 370
adversarial models, 143
bias testing, 144
defined, 329
in-processing bias mitigation techniques, 151
adversarial prompt engineering, 144 Adversarial Robustness Toolbox (ART), 324 adverse action, 5
adverse action notices, 59 adverse impact ratio (AIR)
practical significance testing, 140

traditional testing of outcomes rates, 343-345
advertising, 406
age discrimination, 132
AI incidents, 11-13
defined, 11
response, 25-27
AI Risk Management Framework (RMF) (see NIST AI Risk Management Framework)
AI Risk Management Playbook, 408 AIA (see EU AI Act)
AIF360 (IBM package), 150 AIR (see adverse impact ratio)
Albumentations library, 316, 323
ALE (accumulated local effect) plots, 70, 191 algorithmic discrimination, 12
algorithmic disgorgement, 8
AlgorithmWatch, 157 anchoring, human bias and, 127
anchors (post hoc explanation technique), 67 anomaly detection
model monitoring, 119 remediation of bugs, 112
anonymization, 181 anonymous use, restricting, 412
antipattern, data-scientific method as, 403-404 appeal (of wrong ML decisions), 24, 35
   (see also actionable recourse) area under the curve (AUC), 268 area under the margin (AUM), 304
ART (Adversarial Robustness Toolbox), 324 artificial intelligence (AI), defined, xii attacks, 11, 371-373


415

(see also red-teaming; specific types of attacks)
effect on post hoc explanation, 73 resilience to (see resilience)
types of, 166-173
AUC (area under the curve), 268 audits, 22, 176
AugLy, 323
AUM (area under the margin), 304 authentication, 182
restricting anonymous use with, 412 security best practices, 164
autoencoder, 238
autonomous vehicles, 120-121
availability attacks, 163, 174, 371
availability heuristic, 127
B
backdoor attacks, 167, 373, 390-394
background datasets, 70, 72 Kernel SHAP and, 57 SHAP and, 194, 224
backups, 164
Bayesian improved surname geocoding (BISG), 137, 331
benchmark models, 110-112
defined, 83
   model monitoring and, 119 benchmarking
model specification and, 88
   tracking system improvements with, 93 bias
affected demographics, 131-133 harms that people experience, 133-134 human, 127-128
ISO/NIST definitions, 126-128
legal notions of ML bias in the US, 128-131 statistical, 126
systemic, 126
bias management/remediation, 123-158, 330,
350-366
approaches to, 149-153
bug bounty case study, 156-158 data needed for testing, 332 data privacy laws and, 181 evaluating models for, 335-350 human factors in, 153-156
in-processing, 355-358 managing in ML, 328-331

mitigating bias, 147-156
model selection, 362-366
outcomes testing versus performance test? ing, 342
postprocessing techniques, 359-362 practical significance testing, 140 preprocessing, 350-355
scientific method and experimental design, 148
statistical significance testing, 139 summary test plan, 146
team composition and, 400-402 technical factors, 148
testing, 135-147, 176
testing data, 135-137
testing for equivalent outcomes, 137-141 testing for equivalent performance quality,
141-143
testing for group fairness, 335-345 testing for individual fairness, 345-349 testing for proxy bias, 349
tests for the broader ML ecosystem, 143-145 XGBoost and, 331-335, 350-366
bias monitoring, 178
biased data, 86
Biecek, Przemys?aw, 194
biometric data, 181
Biometric Information Privacy Act (BIPA), 6 BISG (Bayesian improved surname geocoding),
137, 331
bug bounties, 25
bias mitigation and, 156-158 security best practices, 164
burden of care, 6
business rules (model assertions), 265 business value, model selection and, 269
C
Calders, Toon, 350
calibration, 295
Cambridge Analytica, 8
Captum, generating post hoc explanations with, 244-251
cargo cult science, 148 Cartesian grid search, 197-199 Caruana, Rich, 266
causal models, 47
causality with constraints, 191 causalml, 48




416  |  Index

CCAR (Comprehensive Capital Analysis and Review), 272
CFPB (Consumer Financial Protection Bureau), 137
change management, 22
chaos testing, 92
character data, 86
chest X-ray classification, 232 Chest X-Ray Images dataset, xviii chi-squared test, 343
chief model risk officer (CMRO), 14 Chowdhury, Rumman, 156
CIA (confidentiality, integrity, and availability) triad, 162, 370
class imbalance problem, 239 cluster profiling, 71
CMRO (chief model risk officer), 14
CNNs (convolutional neural networks), 242 coalitions, 56
Cohen?s d (standardized mean difference), 140 complexity, security and, 159, 373, 410 Comprehensive Capital Analysis and Review
(CCAR), 272
Comptroller?s Handbook: Model Risk Manage? ment, 408
computational environments, 83
computer vision, 373
concept drift, 117
confidentiality attacks, 171-173, 371
inversion attacks, 162, 171
membership inference attacks, 172, 372,
386-387
model extraction attacks, 171, 177 monotonic constraints as countermeasure,
374
confidentiality, integrity, and availability (CIA) triad, 162, 370
confirmation bias, 72, 127 conflicts of interest, 406
conformal approaches to sensitivity analysis, 108
confusion matrix, 269, 340
connectivity between multiple ML systems, limiting, 411
consent, data privacy regulations and, 180 consistency
local explanation and, 54 transparency and, 74
constrained XGBoost models, 46

decision tree surrogate models as explana? tion technique, 217-221
explaining model behavior with partial dependence and ICE, 214-217
   unconstrained models versus, 208-214 constraints
domain-based, 323
in-processing bias mitigation techniques, 150
steps toward causality with constraints, 191 transparency and, 37
construct validity, model specification and, 89 Consumer Financial Protection Bureau
(CFPB), 137
containment, 27
context, in post hoc explanation, 72, 194 convolutional neural networks (CNNs), 242 correlation, causality versus, 191 correlation, in post hoc explanation, 73 counterfactual explanations, 54, 222
countermeasures, 175-184, 373 adversarial example searches, 176
auditing for insider data poisoning, 176 bias testing, 176
ethical hacking (model extraction attacks), 177
model debugging for security, 175-177 sensitivity analysis, 176
COVID pandemic, 169 credit card dataset, xvii
cross-validation, 97, 218-220, 363-365
cultural competencies for machine learning risk management, 13-16
culture, 13
(see also organizational processes for ML risk management)
competencies for machine learning risk management, 13-16
diverse/experienced teams, 15 drinking our own champagne, 15 effective challenge, 14
human bias and data science culture, 127-128
human factors in bias mitigation, 153-156 organizational accountability, 13-14 problems with ?move fast and break things?
mantra, 16
risk management and, 407
safety culture as broad risk control, 121


Index  |  417

D
data augmentation, 322-325
automated, 323
bug remediation with, 112 leakage due to, 304
noise injection as, 321
   with PyTorch image classifier, 240-241 data breaches, 386
data drift, 117
data leakage (see leakage)
data partitions, random splitting of, 303 data poisoning attacks
access to training data for, 387-390 as attack on ML system integrity, 163
auditing for insider data poisoning, 176 countermeasures, 324
defined, 86, 168, 373 insider attackers and, 73
monotonic constraints as countermeasure, 374
   Tay chatbot incident, 12, 176 data quality
debugging DL systems, 300 training and, 85-88
data science workflow, 163-166 data scientists
human bias and data science culture, 127-128
privacy basics for, 180-182 scientific method and, 402
data sparsity, 96
data-scientific method, 403-404 date/time formats, unrecognized, 87 debt-to-income (DTI) ratio, 283 debugging, 81-122, 262-266, 297-326
(see also model debugging) benchmarks, 111
bug bounties, 25
DL models, 299-302
experimental design issues, 293-294 model selection, 262
PyTorch image classifier (see PyTorch image classifier, debugging)
remediation, 265, 290-295, 302
residual analysis, 264-265, 280-290
security and, 175-177
sensitivity analysis, 262-264, 271-280,
306-314
software testing, 92

training, 83-91
      (see also model training) decision tree, 44-47
adversarial models for finding proxies, 350 backdoor attacks and, 391
with constrained XGBoost models, 46 crafting adversarial examples with, 383-385 single decision trees, 44-46
decision tree surrogate models, 63-65, 217-221 deep learning (DL) models
basics, 235-238
debugging, 299-302
evaluating model explanations, 250 explainable AI for model debugging, 235 explainable models, 235-238
explaining chest X-ray classification, 232 gradient-based explanation methods, 234 occlusion-based explanation methods, 234 overview, 233
ProtoPNet and variants, 236 PyTorch image classifier, 238-257 robustness of post hoc explanations,
252-257
   software testing for, 305 Deep SHAP, 56
deepfakes, 173
demographic diversity, 15 (see also diversity)
demographic features, ML bias and, 331 demographic markers, 135
demographic parity, 336
denial-of-service (DOS) attacks, 174 denigration, 133
DenseNet-121, 242
dependence, partial, 191-194, 214-217, 392-393 dependencies
limiting, 411
   post hoc explanation and, 73 deployment of ML system, 114-119
characteristics of safe ML systems, 116 data-scientific method, 403-404
domain safety, 114-116
evaluation of published results and claims, 405-407
model monitoring, 116-119
science versus engineering in, 402-405 scientific method, 404-405
design of experiment (DOE), 112 detoxification, 153




418  |  Index

differential performance testing, 329 differential privacy, 179, 374 differential validity
group fairness and, 337 ML bias and, 130
differentially private stochastic gradient descent (DP-SGD), 180
digital divide, 128
direct identifiers, 181
dirty data, 86
disability discrimination, 132
disinformation, 169 disparate feature scales, 86 disparate impact
A-level scandal and, 79 AIR and, 140
ML bias and, 130
disparate treatment, ML bias and, 129, 331 distributed computing, attack vulnerability of,
175
distribution of outcomes, 136 distribution shifts, 95, 302, 306-311 diversity
bias-mitigation process and, 156 of teams, 15, 400-402
DL models (see deep learning models) documentation
models/systems, 19, 183 risk management and, 407 standards for, 195
transparency and, 195
DOE (design of experiment), 112 domain expertise, 15
cultural competency for teams, 15
feature selection for bias remediation and, 363
validating with, 29
   Zillow?s disregarding of, 28 domain safety, 114-116
domain-adaptive pretraining, 323
domain-based constraints, 323
DOS (denial-of-service) attacks, 174 double programming, 24
DP-SGD (differentially private stochastic gradi? ent descent), 180
drift, 117
?drinking our own champagne?, 15 DTI (debt-to-income) ratio, 283 dual optimization, 151

Dunning-Kruger effect, 127
duplicate data, 86
E
?eating our own dog food?, 15
EBMs (see explainable boosting machines)
EconML, 48
economic harms, 134
ecosystem, 16
edge detection, 257
EEOC (Equal Employment Opportunity Com? mission) UGESP (Uniform Guidelines on Employee Selection Procedures), 329
effective challenge, 14, 155
elastic net, 196-200 enhanced privacy
differential privacy, 179
federated learning, 179
state legislation, 6
technologies for, 179-182
ensemble techniques, mitigating overfitting with, 99
entanglement, 86
defined, 85
   shortcut learning and, 100 environment control, 165
environmental issues, 16
epistemic uncertainty, 96
epsilon (tuning parameter), 312-314
Equal Employment Opportunity Commission (EEOC) Uniform Guidelines on Employee Selection Procedures (UGESP), 329
equalized odds, 141
eradication, 27
erasure, 133
error analysis, 280
ethical hacking, 177
EU AI Act (AIA), xiii, 4, 195
Annexes, 19, 409
EU General Data Protection Regulation (GDPR), 129, 180
evaluation of published results and claims for AI, 405-407
evasion attacks, 169, 184-186, 373
EvilModel, 243
exnomination, 133
experimental design, 148, 293-294 explainability
basics, 34-39

Index  |  419

context and, 194
explainable artificial intelligence (XAI), 235 explainable boosting machines (EBMs), 44
GAM family of explainable models, 196-208 separating nonlinearity from interactions,
190
explainable DL models basics, 235-238
evaluating model explanations, 250 explainable AI for model debugging, 235 explaining chest X-ray classification, 232 gradient-based methods, 234
occlusion methods, 234
overview, 233
ProtoPNet and variants, 236 PyTorch image classifier, 238-257 robustness of post hoc explanations,
252-257
explainable ML models, 39-50 additive models, 39-44
causal models, 47
decision trees, 44-47
DL models (see explainable DL models) ecosystem of, 47-50
explainable neural networks, 48 k-nearest neighbors, 48
pairing with post hoc explanation, 75-77 PyTorch image classifier (see PyTorch image
   classifier, explaining) rule-based models, 49
   sparse matrix factorization, 49 explainable neural networks (XNNs), 48 explanation
interpretation versus, 189
   NIST definition, 35 external attacks, defined, 371
external standards, application of, 407-410
F
Facebook
evasion attacks, 184-186
   lack of diversity in ML workforce, 401 failed designs, learning from, 410
failure modes, forecasting, 17, 29 failures of imagination, 17 failures, as AI incidents, 11
fairness through awareness doctrine, 331 fairness-aware model selection, 362-366 fairwashing, 73

faithfulness, 251
fake data, 86
false negative rate (FNR), 310, 341
false positive rate (FPR), 270, 286, 341 false positive rate (FPR) disparity, 341 fANOVA (functional analysis of variance)
framework, 191
fast gradient sign method (FGSM), 311 feature attribution, 51-62
defined, 51
global feature importance, 59-62 local explanations and, 52-59
feature engineering, leakage and, 98 feature importance, 51
feature perturbation, 58
feature selection, 363 Federal Reserve
CCAR, 272
   SR 11-7 guidance (see SR Letter 11-7) Federal Trade Commission (FTC), 7 federated learning, 179
Feynman, Richard, 148, 266
FGSM (fast gradient sign method), 311 fine-tuning, 242
FNR (false negative rate), 310, 341
foolbox library, 324
four-fifths rule, 140, 329, 339
FPR (false positive rate), 270, 286, 341 FPR (false positive rate) disparity, 341 FTC (Federal Trade Commission), 7 functional analysis of variance (fANOVA)
framework, 191
functional testing, 92
funding bias, 127
G
GA2Ms, 43, 205-208
GAIMs (generalized additive index models), 48 games, real world versus, 412
GAMI-Nets, 43
GAMs (see generalized additive models) GANs (generative adversarial networks), 263 Gaussian noise, 322, 374
GBMs (gradient boosting machines), 47 gender bias
and lack of diversity in ML workforce, 401 ML system bias and, 132
general availability attacks, 174




420  |  Index

General Data Protection Regulation (GDPR) of the EU, 129, 180
generalization, noise injection to improve, 321 generalized additive index models (GAIMs), 48 generalized additive models (GAMs), 42
basics, 37, 200-205
explainable models, 196-208
GA2M, 43, 205-208
separating nonlinearity from interactions, 190
generalized linear model (GLM), penalized alpha and lambda search, 196-200
   and local feature attribution, 53 generative adversarial networks (GANs), 263 global explanations, 51
global feature importance methods, 59-62 global perturbation (see stress testing) Google
DL explainability techniques, 235
   lack of diversity in ML workforce, 401 governance (see also GOVERN entries under
NIST AI Risk Management Framework) bias mitigation and, 154-156
case study: Zillow's iBuying disaster, 27-31 and external standards, 407-410
   model risk management processes, 18-22 GPUs (graphical processing units), 83 gradient boosting machines (GBMs), 47 gradient information, 263
gradient masking, 324
gradient substitution, 324
gradient-based explanation techniques, 234 input * gradient technique, 247 integrated gradients, 248
gradient-based feature attribution, 54 graphical processing units (GPUs), 83 Graphviz, 287
greedy decisions, 46
grid searches, 197-199
group fairness, 336
testing approaches for, 335-345 testing performance, 337-342 traditional testing of outcomes rates,
343-345
groupthink, 128
Guangzhou Women and Children?s Medical Center pediatric patient dataset, 307

H
H2O GLM algorithm, 197 Hand rule, 6-7
hardware dependencies, 411
Hastie, Trevor, 200
heatmaps, 244-246
Herzberg, Elaine, 120
high cardinality categorical features, 86 high-degree interactions, 190
high-risk ML applications EU AI Act list of, xiii
science versus engineering in deployment, 402-405
   success factors for, 399-414 human bias, 127-128
human factors, in bias mitigation, 153-156 human interpretation, transparency and, 73 human subjects, ethical/legal obligations for,
412
hyperparameters
hidden assumptions in, 90
perturbing computational hyperparameters, 314
I
IB (International Baccalaureate), 78 IBM AIF360 package, 150
ICE plots (see individual conditional expecta? tion plots)
image cropping, 241, 315
image data, 300 imagination, failures of, 17 imbalanced target, 87 immigration status bias, 132 impact assessments, 23
impersonation attacks, 169, 373
implementation invariance, 248
in silico, 3, 262
in vivo, 3, 262
in-processing bias mitigation/remediation, 150, 330, 355-358
incentives, for engaging in risk management, 408
incident response, 25-27 incident response plans, 164 incidents (see AI incidents) incomplete data, 87
inconsistency, transparency and, 74 indirect identifiers, 181

Index  |  421

individual conditional expectation (ICE) plots, 53, 68-70
data for adversarial example search, 276-278 partial dependence paired with, 192-194,
214-217, 392-393
individual fairness, testing for, 144, 345-349
information leakage, 98
information security, in CIA triad, 162 input * gradient technique, 247
input anomalies, monitoring for, 178 inputs, additivity of, 37
insider attacks, 372
instability, 97, 373
institutional review board (IRB), 412 integrated gradient technique, 248 integration testing, 92
integrity attacks (manipulated ML outputs), 163, 166-170, 371
(see also data poisoning attacks) adversarial example attacks, 166, 372 attacks on ML explanations, 171 backdoor attacks, 167, 373, 390-394
impersonation/evasion attacks, 169
interaction constraints, 37, 46, 113, 210 interactions, additivity versus, 190 International Baccalaureate (IB), 78 International Organization for Standardization
(ISO)
AI standards, 409
bias definition, 126-128
risk definition, 7
   technical standards for AI, 8 Internet of Things (IoT), 124 interpretability
achieving, 36
additive models and, 39 basics, 34-39
causal models and, 47 explainability versus, 189
   interpretable models, 183 interpretable basis concepts, 237 interpretable models, 183
interpretable/explainable ML, 33-80
case study: UK A-level scandal, 77-80 explainable models, 39-50
ideas behind math and code, 34-39 interpretation versus explanation, 35, 189 post hoc explanation, 50-71

post hoc explanation difficulties in practice, 71-75
post hoc explanation paired with explaina? ble models, 75-77
interpretation
explanation versus, 189
NIST definition, 35
intersectionality, 131, 133
intervenability, 181
interventional feature perturbation, 58, 225,
227
inventories, model, 21
inversion attacks, 162, 171 IoT (Internet of Things), 124
IRB (institutional review board), 412 IRLS (iteratively reweighted least squares)
method, 41, 197
ISO (see International Organization for Stand? ardization)
iteratively reweighted least squares (IRLS) method, 41, 197
J
January 6, 2021, US Capitol riots, 185
jitter, 321
K
k-nearest neighbors (k-NN), 48
Kaggle Chest X-Ray Images dataset, xviii, 238 Kamiran, Faisal, 350
Kernel SHAP, 56
key performance indicators (KPIs), modeling multiple, 118
kill switches, 119
Kulynych, Bogdan, 157
L
L0 regularization, 113 L1 norm penalty, 41 L1 regression, 197
L2 norm penalty, 41, 374
L2 regression, 197
labels, noisy, 323
language bias, 132 language models (LMs)
bias issues, 144
debugging, 298





422  |  Index

research achievements versus engineering feats, 406
LASSO (least absolute shrinkage and selection operator; L1 norm penalty), 41, 66
LASSO regression (L1 regression), 197
Layer-wise Relevance Propagation (LRP), 249 leakage, 86, 98, 300, 303-304
data augmentation and, 304 data quality and, 303-304 during transfer learning, 304
   random splitting of data partitions, 303 learning rate, 209, 318
least absolute shrinkage and selection operator (see LASSO entries)
least privilege, 24, 164
leave-one-feature-out (LOFO), 61-62 Lee Luda (Scatter Lab chatbot), 12 legal and regulatory issues, 4-8
algorithmic transparency, 59 basic product liability, 6
EU AI Act (proposed), 4 federal laws/regulations, 5
Federal Trade Commission enforcement, 7 human subjects and, 412
legal notions of ML bias, 128-131 state and municipal laws, 5 traditional testing of outcomes rates,
343-345
Uber autonomous test vehicle pedestrian death, 120
liability
ML bias and, 330 product, 6
LIME (Local Interpretable Model-Agnostic Explanations), 65-67, 252
linear functions, 38
link functions, 41
LinkedIn, 405
LMs (see language models) local explanations
critical applications of local explanations and feature importance, 59
defined, 51
feature attribution and, 52-59
generating post hoc explanations with Cap? tum, 244-250
major techniques, 54
Shapley values, 56-59
local feature attribution, 52-59

Local Interpretable Model-Agnostic Explana? tions (LIME), 65-67, 252
Local Law 144 (NYC), 5
LOFO (leave-one-feature-out), 61-62
logloss residuals, 281-285
looped inputs, 99 loss functions
model specification and, 91 refining, 318
LRP (Layer-wise Relevance Propagation), 249 Lundberg, Scott, 221, 227
M
machine learning (ML) attacks (see attacks) bias (see bias entries)
debugging (see debugging) definition, xii
explainable (see explainable ML models) governance (see governance)
knowing when not to use, 413 modes of failure, xi
machine learning risk management basics, 3-31 AI incidents, 11-13
authoritative best practices, 8
case study: Zillow?s iBuying, 27-31 cultural competencies for, 13-16 legal and regulatory landscape, 4-8 organizational processes for, 16-27
McNamara fallacy, 128, 131
male gaze bias, 132, 156
malware, 174, 243
man-in-the-middle attacks, 174
materiality, 18
maximum F1 statistic, 285 Medium, 405
membership inference attacks, 162, 172, 177,
372, 386-387
monotonic constraints as countermeasure, 374
overfitting and, 373
META (ML Ethics, Transparency, and Account? ability) team, 400
metadata, reproducibility and, 84 metamonitoring, 179
Microsoft Research
and causal models, 48
lack of diversity in ML workforce, 401 Tay chatbot incident, 12

Index  |  423

misrecognition, 133
missing values, 87
ML (see machine learning)
ML Ethics, Transparency, and Accountability (META) team, 400
ml_privacy_meter, 386
model assertions, 112, 265
model assessment, 23
DL debugging, 301
residual analysis, 103-107
traditional, 93-95
model audits, 22
model cards, 195
model compression (see surrogate models) model debugging, 91-114
benchmark models, 110-112
case study: Uber autonomous test vehicle pedestrian death, 120-121
common ML bugs, 95-103 distribution shifts, 95
epistemic uncertainty/data sparsity, 96 explainable AI for, 235
instability, 97
leakage, 98
looped inputs, 99
overfitting, 99
remediation, 112-114
residual analysis, 103-107
security and, 175-177
sensitivity analysis, 107-110
shortcut learning, 100
software testing, 92
traditional model assessment, 93-95 underfitting, 100
underspecification, 101
model decay, 21, 117
model deployment, security permissions for, 24 model distillation (see surrogate models) model documentation, 19, 183
standards for, 195
   transparency and, 195 model editing
explainable models and, 43 remediation of bugs and, 113
model extraction (see surrogate models) model extraction attacks, 171, 177, 372,
379-383
model inventories, 21
model monitoring, 20, 116-119, 178, 183

anomaly detection and benchmark models, 118
detecting/addressing drift, 117
kill switches, 119
model decay and concept drift, 117 modeling multiple KPIs, 118
out-of-range values, 118 remediation of bugs and, 113
model object, optimized (MOJO), 287 model performance, plots of, 68-70 model risk management (MRM), ix, 5
ideas beyond traditional purview of, 22-27 key tenets, 13
organizational processes, 18-22
origins, 13
SR Letter 11-7 (?Supervisory Guidance on Model Risk Management?), 407
model selection
bias remediation and, 362-366 debugging and, 262
   defined, 330 model specification
assumptions/limitations of models, 90 benchmarks and alternatives, 88 calibration, 89
construct validity, 89 default loss functions, 91 multiple comparisons, 91
   real-world outcomes and, 88-91 model training
debugging, 83-91
future of safe and robust ML, 91 model specification for real-world out?
comes, 88-91
PyTorch image classifier, 242-244 red-teaming XGBoost, 375-379
reproducibility, 83-84
model-based synthesis, 380
MOJO (model object, optimized), 287 Molnar, Christoph, 72
monitoring (see model monitoring) monitoring benchmarks, 111 monotonic constraints
constrained XGBoost models and, 46, 191
GBM, 53
ML attack countermeasures, 374 ML debugging, 113
partial dependence plots and, 68 positive versus negative, 37




424  |  Index

Spearman correlation and, 376 stability fixes, 323
monotonic features, 36
?move fast and break things?, 16, 399 MRM (see model risk management) multinomial classification, 145 multiple comparisons problem, 91 multiplicity of good models, 46
N
NAMs (neural additive models), 43 Narayanan, Arvind, 413
National Institute of Standards and Technology (NIST)
AI Risk Management Framework (see NIST AI Risk Management Framework)
bias definition, 126-128 explanation as defined by, 35 interpretation as defined by, 35 trustworthiness as defined by, 34
natural language processing (NLP) models, debugging, 298
network dependencies, 411
neural additive models (NAMs), 43 New York City Local Law 144, 5
NIST (see National Institute of Standards and Technology)
NIST AI Risk Management Framework (NIST AI RMF), ix, 8, 408
culture and process focus, 407 definitions for reliability, robustness, and
   resilience, 95 GOVERN (all)
   ?Remediation: Fixing Bugs?, 112-114 GOVERN 1 (all)
?Model documentation?, 19
?Organizational Accountability?, 13-14
GOVERN 1.1
?Appeal, override, and opt out?, 24 ?Benchmark Models?, 110-112 ?Benchmarks and alternatives?, 88 ?Construct validity?, 89
?Explainable Models?, 39-50 ?General ML Security Concerns?,
173-175
?Important Ideas for Interpretability and Explainability?, 34-39
?Legal Notions of ML Bias in the United States?, 128-131

?Post Hoc Explanation?, 50-71 ?Security Basics?, 161-166
?Snapshot of the Legal and Regulatory Landscape?, 4-8
?Software Testing?, 92
?Traditional Approaches: Testing for Equivalent Outcomes?, 137-141
   ?Traditional Model Assessment?, 93-95 GOVERN 1.2
?AI incident response?, 25 ?AI Incidents?, 11-13
?Appeal, override, and opt out?, 24 ?Assumptions and limitations?, 90 ?Authoritative Best Practices?, 8 ?Benchmark Models?, 110-112 ?Benchmarks and alternatives?, 88 ?Calibration?, 89
?Change management?, 22
?Countermeasures?, 175-184
?Data Quality?, 85-88
?Distribution shifts?, 95
?Diverse and Experienced Teams?, 15 ?Domain Safety?, 114-116
?Explainable Models?, 39-50 ?Forecasting Failure Modes?, 17 ?General ML Security Concerns?,
173-175
?Harms That People Experience?, 133-134
?Impact assessments?, 23
?Important Ideas for Interpretability and Explainability?, 34-39
?Legal Notions of ML Bias in the United States?, 128-131
?Machine Learning Attacks?, 166-173 ?Model audits and assessments?, 22 ?Model inventories?, 21
?Model monitoring?, 20, 116-119 ?New Mindset: Testing for Equivalent
   Performance Quality?, 141-143 ?On the Horizon: Tests for the Broader
   ML Ecosystem?, 143-145 ?Post Hoc Explanation?, 50-71 ?Reproducibility?, 83-84
?Residual Analysis?, 103-107
?Risk tiering?, 18
?Security Basics?, 161-166
?Sensitivity Analysis?, 107-110


Index  |  425

?Snapshot of the Legal and Regulatory Landscape?, 4-8
?Software Testing?, 92
?System validation and process audit? ing?, 21
?Traditional Approaches: Testing for Equivalent Outcomes?, 137-141
?Traditional Model Assessment?, 93-95 ?Who Tends to Experience Bias from
ML Systems?, 131-133
GOVERN 1.3
?Change management?, 22
?Diverse and Experienced Teams?, 15 ?Forecasting Failure Modes?, 17 ?Impact assessments?, 23
?Model audits and assessments?, 22 ?Model inventories?, 21
?Model monitoring?, 20, 116-119
?Risk tiering?, 18
?System validation and process audit? ing?, 21
GOVERN 1.4
?Appeal, override, and opt out?, 24 ?Assumptions and limitations?, 90 ?Benchmark Models?, 110-112 ?Benchmarks and alternatives?, 88 ?Calibration?, 89
?Change management?, 22
?Countermeasures?, 175-184 ?General ML Security Concerns?,
173-175
?Harms That People Experience?, 133-134
?Impact assessments?, 23
?Important Ideas for Interpretability and Explainability?, 34-39
?Legal Notions of ML Bias in the United States?, 128-131
?Machine Learning Attacks?, 166-173 ?Model audits and assessments?, 22 ?Model inventories?, 21
?Model monitoring?, 20, 116-119 ?New Mindset: Testing for Equivalent
   Performance Quality?, 141-143 ?On the Horizon: Tests for the Broader
ML Ecosystem?, 143-145
?Reproducibility?, 83-84
?Risk tiering?, 18
?Security Basics?, 161-166

?Security permissions for model deploy? ment?, 24
?System validation and process audit? ing?, 21
?Traditional Approaches: Testing for Equivalent Outcomes?, 137-141
?Traditional Model Assessment?, 93-95 ?Who Tends to Experience Bias from
ML Systems?, 131-133
GOVERN 1.5
?AI incident response?, 25 ?AI Incidents?, 11-13
?Appeal, override, and opt out?, 24 ?Countermeasures?, 175-184
?Distribution shifts?, 95
?General ML Security Concerns?, 173-175
?Important Ideas for Interpretability and Explainability?, 34-39
?Machine Learning Attacks?, 166-173 ?Model monitoring?, 20, 116-119 ?Post Hoc Explanation?, 50-71 ?Security Basics?, 161-166
GOVERN 1.6
?Model inventories?, 21
GOVERN 1.7
?Change management?, 22
?Domain Safety?, 114-116
GOVERN 2 (all)
?Organizational Accountability?, 13-14
GOVERN 2.1
?Human Factors in Mitigating Bias?, 153-156
?Impact assessments?, 23
?Model audits and assessments?, 22 ?Model documentation?, 19
?Moving Fast and Breaking Things?, 16 ?System validation and process audit?
ing?, 21
GOVERN 2.2
?Change management?, 22
?Legal Notions of ML Bias in the United States?, 128-131
?Snapshot of the Legal and Regulatory Landscape?, 4-8
GOVERN 3 (all)
?Diverse and Experienced Teams?, 15 ?Domain Safety?, 114-116




426  |  Index

?Human Factors in Mitigating Bias?, 153-156
GOVERN 3.2
?Appeal, override, and opt out?, 24 ?Human Biases and Data Science Cul?
ture?, 127-128
?Important Ideas for Interpretability and Explainability?, 34-39
?Organizational Accountability?, 13-14
GOVERN 4 (all)
?Culture of Effective Challenge?, 14 ?Organizational Accountability?, 13-14
GOVERN 4.1
?Diverse and Experienced Teams?, 15 ?Domain Safety?, 114-116
?Drinking Our Own Champagne?, 15 ?General ML Security Concerns?,
173-175
?Impact assessments?, 23
?Important Ideas for Interpretability and Explainability?, 34-39
?Legal Notions of ML Bias in the United States?, 128-131
?Model audits and assessments?, 22 ?Moving Fast and Breaking Things?, 16 ?Pair and double programming?, 24 ?Security Basics?, 161-166
?Security permissions for model deploy? ment?, 24
?Snapshot of the Legal and Regulatory Landscape?, 4-8
?System validation and process audit? ing?, 21
GOVERN 4.2
?Change management?, 22 ?Forecasting Failure Modes?, 17 ?Impact assessments?, 23
?Model documentation?, 19
GOVERN 4.3
?AI incident response?, 25 ?AI Incidents?, 11-13
?Countermeasures?, 175-184
?Domain Safety?, 114-116 ?General ML Security Concerns?,
173-175
?New Mindset: Testing for Equivalent Performance Quality?, 141-143
?On the Horizon: Tests for the Broader ML Ecosystem?, 143-145

?Security Basics?, 161-166
?Software Testing?, 92
?System validation and process audit? ing?, 21
?Traditional Approaches: Testing for Equivalent Outcomes?, 137-141
GOVERN 5 (all)
?Appeal, override, and opt out?, 24 ?Bug bounties?, 25
?Domain Safety?, 114-116 ?General ML Security Concerns?,
173-175
?Harms That People Experience?, 133-134
?Organizational Accountability?, 13-14
?Security Basics?, 161-166
?Who Tends to Experience Bias from ML Systems?, 131-133
GOVERN 5.1
?Countermeasures?, 175-184
?Important Ideas for Interpretability and Explainability?, 34-39
   ?Post Hoc Explanation?, 50-71 GOVERN 5.2
?Authoritative Best Practices?, 8 ?Impact assessments?, 23
?Pair and double programming?, 24 ?Risk tiering?, 18
?Security permissions for model deploy? ment?, 24
?Snapshot of the Legal and Regulatory Landscape?, 4-8
GOVERN 6 (all)
?Countermeasures?, 175-184 ?General ML Security Concerns?,
173-175
?Security Basics?, 161-166
GOVERN 6.1
?Assumptions and limitations?, 90 ?Explainable Models?, 39-50
?Important Ideas for Interpretability and Explainability?, 34-39
?New Mindset: Testing for Equivalent Performance Quality?, 141-143
?On the Horizon: Tests for the Broader ML Ecosystem?, 143-145
?Post Hoc Explanation?, 50-71 ?Software Testing?, 92


Index  |  427

?System validation and process audit? ing?, 21
?Traditional Approaches: Testing for Equivalent Outcomes?, 137-141
GOVERN 6.2
?AI incident response?, 25 ?AI Incidents?, 11-13
   ?Organizational Accountability?, 13-14 MANAGE (all)
?Remediation: Fixing Bugs?, 112-114 ?Technical Factors in Mitigating Bias?,
148
MANAGE 1 (all)
?Domain Safety?, 114-116
?Human Factors in Mitigating Bias?, 153-156
?Impact assessments?, 23
?Model audits and assessments?, 22 ?System validation and process audit?
ing?, 21
MANAGE 1.2
?Countermeasures?, 175-184
?Explainable Models?, 39-50 ?Post Hoc Explanation?, 50-71 ?Risk tiering?, 18
MANAGE 1.3
?Countermeasures?, 175-184
?Explainable Models?, 39-50 ?Post Hoc Explanation?, 50-71 ?Risk tiering?, 18
MANAGE 1.4
?Forecasting Failure Modes?, 17 ?Harms That People Experience?,
133-134
?Risk tiering?, 18
MANAGE 2 (all)
?Forecasting Failure Modes?, 17 ?Harms That People Experience?,
133-134
?Who Tends to Experience Bias from ML Systems?, 131-133
MANAGE 2.1
?Benchmark Models?, 110-112 ?Benchmarks and alternatives?, 88 ?Human Factors in Mitigating Bias?,
153-156
MANAGE 2.2
?Countermeasures?, 175-184
?Distribution shifts?, 95

?Human Factors in Mitigating Bias?, 153-156
?Instability?, 97
?Model monitoring?, 20, 116-119
MANAGE 2.3
?AI incident response?, 25 ?AI Incidents?, 11-13
?Countermeasures?, 175-184
?Distribution shifts?, 95
?Instability?, 97
?Model monitoring?, 20, 116-119
MANAGE 2.4
?Countermeasures?, 175-184
?Distribution shifts?, 95
?Instability?, 97
?Model monitoring?, 20, 116-119
MANAGE 3 (all)
?Countermeasures?, 175-184
?Distribution shifts?, 95
?General ML Security Concerns?, 173-175
?Human Factors in Mitigating Bias?, 153-156
?Instability?, 97
?Model inventories?, 21
?Model monitoring?, 20, 116-119
MANAGE 3.2
?On the Horizon: Tests for the Broader ML Ecosystem?, 143-145
MANAGE 4 (all)
?Harms That People Experience?, 133-134
?Model monitoring?, 20, 116-119 ?Who Tends to Experience Bias from
ML Systems?, 131-133
MANAGE 4.1
?AI incident response?, 25 ?AI Incidents?, 11-13
?Appeal, override, and opt out?, 24 ?Change management?, 22
?Countermeasures?, 175-184
?Distribution shifts?, 95
?Instability?, 97
MANAGE 4.2
?Change management?, 22
?Human Factors in Mitigating Bias?, 153-156
MANAGE 4.3
?AI incident response?, 25




428  |  Index

?AI Incidents?, 11-13
?Domain Safety?, 114-116
?Human Factors in Mitigating Bias?, 153-156
MAP (all)
?Model documentation?, 19 ?Remediation: Fixing Bugs?, 112-114
MAP 1.1
?Authoritative Best Practices?, 8 ?Diverse and Experienced Teams?, 15 ?Forecasting Failure Modes?, 17 ?General ML Security Concerns?,
173-175
?Harms That People Experience?, 133-134
?Human Biases and Data Science Cul? ture?, 127-128
?Human Factors in Mitigating Bias?, 153-156
?Impact assessments?, 23
?Legal Notions of ML Bias in the United States?, 128-131
?Scientific Method and Experimental Design?, 148
?Security Basics?, 161-166
?Snapshot of the Legal and Regulatory Landscape?, 4-8
?Who Tends to Experience Bias from ML Systems?, 131-133
MAP 1.2
?Diverse and Experienced Teams?, 15 ?Domain Safety?, 114-116
?Harms That People Experience?, 133-134
?Legal Notions of ML Bias in the United States?, 128-131
?Snapshot of the Legal and Regulatory Landscape?, 4-8
?Who Tends to Experience Bias from ML Systems?, 131-133
MAP 1.5
?Risk tiering?, 18
MAP 1.6
?Domain Safety?, 114-116 ?Harms That People Experience?,
133-134
?Human Factors in Mitigating Bias?, 153-156
?Systemic Bias?, 126

?Who Tends to Experience Bias from ML Systems?, 131-133
MAP 2 (all)
   ?Assumptions and limitations?, 90 MAP 2.1
?Construct validity?, 89
MAP 2.2
?Harms That People Experience?, 133-134
?Human Factors in Mitigating Bias?, 153-156
?Impact assessments?, 23
?Who Tends to Experience Bias from ML Systems?, 131-133
MAP 2.3
?Authoritative Best Practices?, 8 ?Benchmark Models?, 110-112 ?Benchmarks and alternatives?, 88 ?Calibration?, 89
?Construct validity?, 89
?Countermeasures?, 175-184
?Data Quality?, 85-88 ?Default loss functions?, 91 ?Distribution shifts?, 95
?Domain Safety?, 114-116
?Epistemic uncertainty and data spar? sity?, 96
?Explainable Models?, 39-50 ?Forecasting Failure Modes?, 17 ?Future of Safe and Robust Machine
Learning?, 91
?General ML Security Concerns?, 173-175
?Human Biases and Data Science Cul? ture?, 127-128
?Human Factors in Mitigating Bias?, 153-156
?Important Ideas for Interpretability and Explainability?, 34-39
?Instability?, 97
?Leakage?, 98
?Looped inputs?, 99
?Machine Learning Attacks?, 166-173 ?Model monitoring?, 20, 116-119
?Multiple comparisons?, 91
?New Mindset: Testing for Equivalent Performance Quality?, 141-143
?On the Horizon: Tests for the Broader ML Ecosystem?, 143-145


Index  |  429

?Overfitting?, 99
?Post Hoc Explanation?, 50-71 ?Reproducibility?, 83-84
?Residual Analysis?, 103-107 ?Scientific Method and Experimental
Design?, 148
?Security Basics?, 161-166
?Sensitivity Analysis?, 107-110
?Shortcut learning?, 100
?Software Testing?, 92
?Statistical Bias?, 126
?System validation and process audit? ing?, 21
?Systemic Bias?, 126
?Technical Factors in Mitigating Bias?, 148
?Traditional Approaches: Testing for Equivalent Outcomes?, 137-141
?Traditional Model Assessment?, 93-95 ?Underfitting?, 100
?Underspecification?, 101
MAP 3 (all)
?Harms That People Experience?, 133-134
?Who Tends to Experience Bias from ML Systems?, 131-133
MAP 3.1
?Countermeasures?, 175-184
?Domain Safety?, 114-116
?Explainable Models?, 39-50 ?General ML Security Concerns?,
173-175
?Impact assessments?, 23
?Important Ideas for Interpretability and Explainability?, 34-39
   ?Post Hoc Explanation?, 50-71 MAP 3.2
?Forecasting Failure Modes?, 17 ?Impact assessments?, 23
?Residual Analysis?, 103-107
?Sensitivity Analysis?, 107-110
MAP 3.3
?Construct validity?, 89
?Snapshot of the Legal and Regulatory Landscape?, 4-8
MAP 3.5
?AI incident response?, 25 ?Appeal, override, and opt out?, 24 ?Bug bounties?, 25

?Change management?, 22
?Impact assessments?, 23
?Model audits and assessments?, 22 ?Model inventories?, 21
?Model monitoring?, 20, 116-119 ?Pair and double programming?, 24
?Security permissions for model deploy? ment?, 24
?System validation and process audit? ing?, 21
MAP 4 (all)
?AI incident response?, 25 ?Change management?, 22
?Countermeasures?, 175-184
?Data Quality?, 85-88
?General ML Security Concerns?, 173-175
?Important Ideas for Interpretability and Explainability?, 34-39
?Machine Learning Attacks?, 166-173 ?Model audits and assessments?, 22 ?Model inventories?, 21
?Model monitoring?, 20, 116-119 ?New Mindset: Testing for Equivalent
   Performance Quality?, 141-143 ?On the Horizon: Tests for the Broader
ML Ecosystem?, 143-145
?Risk tiering?, 18
?Security Basics?, 161-166
?Software Testing?, 92
?System validation and process audit? ing?, 21
?Traditional Approaches: Testing for Equivalent Outcomes?, 137-141
   ?Traditional Model Assessment?, 93-95 MAP 4.1
?Snapshot of the Legal and Regulatory Landscape?, 4-8
MAP 4.2
?Security permissions for model deploy? ment?, 24
MAP 5 (all)
?AI incident response?, 25 ?Domain Safety?, 114-116 ?Harms That People Experience?,
133-134
?Impact assessments?, 23
?Who Tends to Experience Bias from ML Systems?, 131-133




430  |  Index

MAP 5.1
?Countermeasures?, 175-184 ?General ML Security Concerns?,
173-175
?Machine Learning Attacks?, 166-173 ?Residual Analysis?, 103-107
?Risk tiering?, 18
?Security Basics?, 161-166
?Sensitivity Analysis?, 107-110
MAP 5.2
?Appeal, override, and opt out?, 24 ?Bug bounties?, 25
   ?Model monitoring?, 20, 116-119 MEASURE (all)
?Model audits and assessments?, 22 ?System validation and process audit?
ing?, 21
MEASURE 1 (all)
?Calibration?, 89
?Default loss functions?, 91 ?Distribution shifts?, 95
?Domain Safety?, 114-116
?Epistemic uncertainty and data spar? sity?, 96
?Instability?, 97
?Leakage?, 98
?Looped inputs?, 99
?Machine Learning Attacks?, 166-173 ?Multiple comparisons?, 91
?New Mindset: Testing for Equivalent Performance Quality?, 141-143
?On the Horizon: Tests for the Broader ML Ecosystem?, 143-145
?Overfitting?, 99
?Reproducibility?, 83-84
?Residual Analysis?, 103-107
?Sensitivity Analysis?, 107-110
?Shortcut learning?, 100 ?Traditional Approaches: Testing for
   Equivalent Outcomes?, 137-141 ?Traditional Model Assessment?, 93-95 ?Underfitting?, 100
?Underspecification?, 101
MEASURE 1.1
?Data Quality?, 85-88
?Important Ideas for Interpretability and Explainability?, 34-39
?Model documentation?, 19
?Model monitoring?, 20, 116-119

?Snapshot of the Legal and Regulatory Landscape?, 4-8
?Testing Data?, 135-137
MEASURE 1.2
?Change management?, 22
MEASURE 1.3
?General ML Security Concerns?, 173-175
?Harms That People Experience?, 133-134
?Security Basics?, 161-166
?Software Testing?, 92
?Who Tends to Experience Bias from ML Systems?, 131-133
MEASURE 2 (all)
?Model documentation?, 19
MEASURE 2.1
?Calibration?, 89
?Data Quality?, 85-88 ?Default loss functions?, 91 ?Distribution shifts?, 95
?Epistemic uncertainty and data spar? sity?, 96
?General ML Security Concerns?, 173-175
?Important Ideas for Interpretability and Explainability?, 34-39
?Instability?, 97
?Leakage?, 98
?Looped inputs?, 99
?Machine Learning Attacks?, 166-173 ?Multiple comparisons?, 91
?New Mindset: Testing for Equivalent Performance Quality?, 141-143
?On the Horizon: Tests for the Broader ML Ecosystem?, 143-145
?Overfitting?, 99
?Reproducibility?, 83-84
?Residual Analysis?, 103-107
?Sensitivity Analysis?, 107-110
?Shortcut learning?, 100
?Testing Data?, 135-137
?Traditional Approaches: Testing for Equivalent Outcomes?, 137-141
?Traditional Model Assessment?, 93-95 ?Underfitting?, 100
?Underspecification?, 101
MEASURE 2.3
?Calibration?, 89


Index  |  431

?Default loss functions?, 91 ?Distribution shifts?, 95
?Epistemic uncertainty and data spar? sity?, 96
?Instability?, 97
?Leakage?, 98
?Looped inputs?, 99
?Multiple comparisons?, 91
?Overfitting?, 99
?Reproducibility?, 83-84
?Residual Analysis?, 103-107
?Sensitivity Analysis?, 107-110
?Shortcut learning?, 100
?Traditional Model Assessment?, 93-95 ?Underfitting?, 100
?Underspecification?, 101
MEASURE 2.4
?Distribution shifts?, 95
?General ML Security Concerns?, 173-175
?Instability?, 97
?Model monitoring?, 20, 116-119
MEASURE 2.5
?Domain Safety?, 114-116
MEASURE 2.6
?Countermeasures?, 175-184
?Domain Safety?, 114-116
?Future of Safe and Robust Machine Learning?, 91
?General ML Security Concerns?, 173-175
?Machine Learning Attacks?, 166-173 ?Model monitoring?, 20, 116-119 ?New Mindset: Testing for Equivalent
   Performance Quality?, 141-143 ?On the Horizon: Tests for the Broader
ML Ecosystem?, 143-145
?Statistical Bias?, 126
?Traditional Approaches: Testing for Equivalent Outcomes?, 137-141
MEASURE 2.7
?Countermeasures?, 175-184 ?General ML Security Concerns?,
173-175
?Machine Learning Attacks?, 166-173 ?Model monitoring?, 20, 116-119
MEASURE 2.8
?Appeal, override, and opt out?, 24 ?Countermeasures?, 175-184

?Culture of Effective Challenge?, 14 ?Explainable Models?, 39-50 ?General ML Security Concerns?,
173-175
?Important Ideas for Interpretability and Explainability?, 34-39
?Model monitoring?, 20, 116-119
?Organizational Accountability?, 13-14 ?Post Hoc Explanation?, 50-71 ?Snapshot of the Legal and Regulatory
Landscape?, 4-8
MEASURE 2.9
?Countermeasures?, 175-184
?Explainable Models?, 39-50 ?General ML Security Concerns?,
173-175
?Important Ideas for Interpretability and Explainability?, 34-39
?Machine Learning Attacks?, 166-173 ?Post Hoc Explanation?, 50-71
MEASURE 2.10
?Countermeasures?, 175-184 ?General ML Security Concerns?,
173-175
?Machine Learning Attacks?, 166-173 ?Model monitoring?, 20, 116-119
MEASURE 2.11
?Countermeasures?, 175-184 ?General ML Security Concerns?,
173-175
?Human Biases and Data Science Cul? ture?, 127-128
?Legal Notions of ML Bias in the United States?, 128-131
?Machine Learning Attacks?, 166-173 ?Model monitoring?, 20, 116-119 ?New Mindset: Testing for Equivalent
   Performance Quality?, 141-143 ?On the Horizon: Tests for the Broader
   ML Ecosystem?, 143-145 ?Snapshot of the Legal and Regulatory
Landscape?, 4-8
?Statistical Bias?, 126
?Systemic Bias?, 126
?Traditional Approaches: Testing for Equivalent Outcomes?, 137-141
MEASURE 2.12
?Model monitoring?, 20, 116-119
MEASURE 2.13




432  |  Index

?Benchmark Models?, 110-112 ?Benchmarks and alternatives?, 88 ?Change management?, 22
MEASURE 3 (all)
?Bug bounties?, 25
?Domain Safety?, 114-116
?Impact assessments?, 23
?Model documentation?, 19
MEASURE 3.1
?AI incident response?, 25 ?Change management?, 22
?Model monitoring?, 20, 116-119
MEASURE 3.2
?Human Factors in Mitigating Bias?, 153-156
MEASURE 3.3
?AI Incidents?, 11-13
?Appeal, override, and opt out?, 24 ?Human Factors in Mitigating Bias?,
153-156
?Important Ideas for Interpretability and Explainability?, 34-39
?Model monitoring?, 20, 116-119
?Organizational Accountability?, 13-14
MEASURE 4 (all)
?Diverse and Experienced Teams?, 15 ?Domain Safety?, 114-116
?Harms That People Experience?, 133-134
?Model documentation?, 19
?Model monitoring?, 20, 116-119 ?Who Tends to Experience Bias from
ML Systems?, 131-133
MEASURE 4.1
   ?Drinking Our Own Champagne?, 15 MEASURE 4.2
?Drinking Our Own Champagne?, 15 ?New Mindset: Testing for Equivalent Performance Quality?, 141-143
?On the Horizon: Tests for the Broader ML Ecosystem?, 143-145
?Traditional Approaches: Testing for Equivalent Outcomes?, 137-141
NLP (natural language processing) models, debugging, 298
NMF (nonnegative matrix factorization), 49 noise, 87
noise injection, 113, 321
noisy labels, 323

nonnegative matrix factorization (NMF), 49 nonnormalized data, 87
Norvig, Peter, xii
O
observation weights, 351
occlusion methods, 55, 234, 245-250
off-label uses, 90, 117
Office of the Comptroller of the Currency, 18, 195
open source software, 174
opting out of ML decision-making, 24 organizational accountability, 13-14 organizational processes for ML risk manage?
ment, 16-27
change management, 22 forecasting failure modes, 17 model documentation, 19
model inventories, 21
model monitoring, 20
model risk management processes, 18-22 risk tiering, 18
   system validation and process auditing, 21 out-of-distribution generalization, 307
out-of-range values, 118 outcome distribution
testing training data for bias, 136 traditional testing of outcomes rates,
343-345
outcomes
performance testing versus outcomes test? ing, 342
practical significance testing, 329
restricting system outputs to avoid foreseea? ble incidents, 411
   testing for equivalent, 137-141 outliers, 87
output anomalies, monitoring for, 178 overfitting, 99
membership inference attacks and, 373 multiple comparisons and, 91
   noise injection for reducing, 321 overriding (of wrong ML decisions), 24
P
pair programming, 24
part-level attention models, 237
partial dependence, 191-194, 214-217, 392-393 partial dependence plots, 68-70

Index  |  433

passwords, 164
Pearson correlation, 53, 60, 211
peer review, 405
penalized generalized linear model (GLM) alpha and lambda search, 196-200
   and local feature attribution, 53 penalized regression, 39-42
performance quality, testing for equivalent, 141-143
performance testing, outcomes testing versus, 342
permissions, for model deployment, 24 personal identifiable information (PII), 181 perturbation
computational hyperparameters and, 314 interventional feature perturbation, 58, 225,
227
method, 58
(see also occlusion methods; stress test? ing)
sensitivity analysis and perturbation tests, 108
tree_path_dependent feature perturbations, 58, 224
perturbation-based feature importance, 61 PETs (see privacy-enhancing technologies) physical harms, 134
physical keys, 165
physical media, 165
physics-informed deep learning, 323
PII (personal identifiable information), 181 plots of model performance, 68-70
accumulated local effect, 70
partial dependence and individual condi? tional expectation, 68-70
poisoned data, 86
positive decisions, positive outcomes versus, 334
post hoc explanation, 50-71 anchors and rules, 67
attacks on ML explanations, 171 cluster profiling, 71
decision tree surrogate models as explana? tion technique, 217-221
feature attribution/importance, 51-62 linear models and LIME, 65-67 measuring quality of, 74
pairing explainable models with, 75-77 plots of model performance, 68-70

practical difficulties with, 71-75 PyTorch image classifier, 244-250 robustness of, 252-257
Shapley value explanations, 221-228 surrogate models, 63-65
postprocessing bias mitigation techniques, 151, 330, 359-362
practical significance testing, 140, 329
precision weights, 352 precision, recall versus, 334 preprocessing
bias-mitigation techniques, 150, 350-355
defined, 330
presidential election campaign (2020), 169
pretrained models, 242-244
pretraining, 323 privacy
basics for data scientists, 180-182 testing data for bias, 135-137
privacy-enhancing technologies (PETs), 179-182
differential privacy, 179
federated learning, 179
probability cutoff threshold, 94, 285, 334
process auditing, 21
product liability, 6
product security, 165
protected groups, ML bias and, 129 ProtoPNet, 236
prototypes, 38, 55, 222
k-NN method and, 48 ProtoPNet and, 236
proxies, 137
psychological harms, 134
pyGAM, 200, 204
Python, xii
PyTorch image classifier, debugging, 302-325 addressing dataset imbalance problem, 239 data fixes, 315-317
data quality and leaks, 303-304 remediation, 314-321
sensitivity analysis for DL, 306-314 sensitivity fixes, 321-325
software fixes, 317-321 software testing for DL, 305
PyTorch image classifier, explaining
data augmentation and image cropping, 240-241
evaluating model explanations, 250




434  |  Index

evaluation and metrics, 244
generating post hoc explanations using Cap? tum, 244-250
input * gradient technique, 247 integrated gradient technique, 248 Layer-wise Relevance Propagation, 249 model training, 242-244
occlusion, 245-246
robustness of post hoc explanations, 252-257
training data for, 238-239
Q
  
remediation of bugs with, 113 regularization hyperparameter, 356
reject inference, 335
reject on negative impact (RONI) technique, 176
reject option postprocessing, 360 relevance, LRP and, 249 reliability
conformal approaches to sensitivity analy? sis, 108
ICE for diagnosing problems with, 193 NIST definition, 95
organizational processes and, 16

Quora, 405
R
race/ethnicity, ML system bias and, 132 RAND Corporation, 137
random attacks, 93
random grid search, 197-199 random seeds, 84
ransomware attacks, 164
Rashomon effect, 46, 101, 330, 363 real world, games versus, 412 reason codes, 59
recall, 334
(see also true positive rate)
receiver operating characteristic (ROC) plots, 377
recession, economic, 273
recommender systems, 145
red-teaming, 369-395
adversarial example attacks, 383-385 attacks for, 379-394
backdoor attacks, 390-394
countermeasures, 373
data poisoning attacks, 387-390 membership inference attacks, 386-387 model extraction attacks, 379-383 model training, 375-379
   security best practices, 165 reference distribution, 224-227
   (see also background datasets) regression models, impact ratios and, 343 regression, penalized, 39-42 regularization
adjusting, 319
mitigating overfitting with, 99 noise injection and, 321

sensitivity analysis for diagnosing problems with, 262
remediation of bugs, 112-114, 290-295
representativeness, 136
reproducibility, 83-84
benchmarks, 111
improving, 299, 319
language models, 406
reputational harms, 134
residual analysis, 103-107, 264-265 analysis/visualizations of residuals, 104,
281-285
local contribution to residuals, 105-107 modeling residuals, 104, 265, 287-290
residual plots, 264
segmented error analysis, 264, 285-287
XGBoost, 280-290
resilience (see also security)
NIST definition, 95
sensitivity analysis for diagnosing problems with, 262
stress testing and, 272
resource allocation for risk management, 410 retention limits/requirements, 181
retraining, 324
reweighing, 350-355
ridge regression, 41
   (see also L2 regression) risk tiering, 18
risk, defined, 7
robust machine learning (ML), 182, 324, 374 Robust ML (website), 324
robustness (NIST definition), 95
ROC (receiver operating characteristic) plots, 377


Index  |  435

RONI (reject on negative impact) technique, 176
Rudin, Cynthia, 39, 46, 71, 236
rule-based models, 49, 67 Russell, Stuart, xii
S
safety
AI incident mitigation and, 11-13 characteristics of safe ML systems, 116 CMRO responsibilities, 14
debugging for, 81-122
deployment and, 114-116
domain safety, 114-116
organizational processes for ML risk man? agement, 16-27
   product liability and, 6 saliency map, 156
Samuel, Arthur, xii scaffolding attack, 171
Scatter Lab, 12
science experiment, ML system deployment as, 402-405
science versus engineering
data-scientific method, 403-404
   in deployment of ML system, 402-405 scientific method
bias mitigation and, 148
data scientists? lack of adherence to, 402 high-risk ML deployment and, 404-405 remediation of bugs with, 113
screen out, 130
security, 159-186
(see also attacks; resiliency) adversarial mindset, 161
basics, 161-166
best practices for data scientists, 163-166 case study: real-world evasion attacks,
184-186
CIA triad, 162
countermeasures, 175-184
general ML security concerns, 173-175 ML attacks, 166-173
model monitoring for, 178
privacy-enhancing technologies, 179-182
robust ML, 182
testing for vulnerabilities (see red-teaming) vulnerabilities, 294
security permissions, for model deployment, 24

seeds, reproducibility and, 84
segmented error analysis, 264, 285-287, 301
self-driving cars, 120-121
self-explaining neural networks (SENN), 237 self-updating systems, 412
sensitivity analysis, 107-110, 176 adversarial example attacks, 311-314
adversarial example searches, 263, 276-280 altering data to simulate recession condi?
tions, 274-276
debugging and, 262-264
DL and, 301, 306-314
domain/subpopulation shift testing, 306-311
perturbing computational hyperparameters, 314
practical approaches outside of supervised learning, 271
stress testing, 263, 272-276
XGBoost and, 271-280
sensitivity fixes, 321-325
noise injection, 321
stability fixes, 322-325
sensitivity, integrated gradients and, 248 sex/gender bias, 132
SGD (stochastic gradient descent), 319 shape constraints, 323
shape functions, 43, 48
Shapley values, 56-59
global feature importance and, 62 local feature attribution, 53
post hoc explanation tool, 221-228 problems with, 224-228
residual analysis and, 105-107 transparency and, 194
shortcut learning, 100, 303 simplicity, security and, 410 single decision trees, 44-46 skin tone scales, 132
SLIM (super-sparse linear integer model), 42 SMD (standardized mean difference), 140,
343-345
smooth functions, 38
software dependencies, 411 software QA processes, 411 software testing, 92, 305
checklist for, 317-319
DL debugging, 300
sparse data, 87




436  |  Index

sparse principal components analysis (SPCA), 49
sparsity, 38
Spearman correlation, 211, 332, 376 sponge example attacks, 163 squared loss functions, 91
SR Letter 11-7 (?Supervisory Guidance on Model Risk Management?), 5, 18, 22, 25, 29,
407
standardized mean difference (SMD), 140, 343-345
standardized tests, UK A-level scandal, 77-80 state and municipal laws (US), 5
statistical bias, 126
statistical parity, 336
statistical significance testing, 139, 329
stereotyping, 134
stochastic gradient descent (SGD), 319
Stop Discrimination by Algorithms Act (Wash? ington, DC), 6
stress testing
altering data to simulate recession condi? tions, 274-276
methodology, 273
sensitivity analysis and, 108, 263
XGBoost, 272-276
strong multicollinearity, 87
Stroustrup, Bjarne, 403
structured data, evasion/impersonation attacks and, 373
subject matter expertise (see domain expertise) subpopulation shift, 302, 310
Substack, 405
success factors, for high-risk machine learning, 399-414
application of external standards, 407-410 commonsense risk mitigation, 410-414 design team diversity, 400-402
evaluation of published results and claims, 405-407
   science versus engineering, 402-405 summarization, 38
super-sparse linear integer model (SLIM), 42 ?Supervisory Guidance on Model Risk Manage?
   ment? (see SR Letter 11-7) surrogate models, 63-65
decision tree surrogates, 63-65, 217-221 membership inference attacks and, 372 model extraction attacks and, 171, 177, 372

system documentation, 19
system validation, 21
systemic bias, 126, 329
T
t-tests, 343
Taiwan credit card dataset, xvii task-adaptive pretraining, 323
Tay chatbot incident, 12, 176
team diversity as cultural competency, 15, 400-402
technical bias mitigation, 148 technical validation, 21
techno-chauvinism, 128
temporal data, mistreatment of, 98 TensorFlow Lattice, 323
testers, empowerment of, 407
testing (see under specific forms of testing, e.g., bias management/remediation)
testing performance, group fairness and, 337-342
third-party data/personnel, 165
third-party software, dependencies and, 411 throttling, 183, 374
Tibshirani, Rob, 200
Tikhonov regression, 41 (see also L2 regression)
time allocation for risk management, 410 Torchvision, 240
TPR (true positive rate), 270, 340 training (see model training) training data
bias testing in, 336
data poisoning attacks, 73, 168, 387-390
data quality, 85-88
membership inference attacks and, 386 PyTorch image classifier, 238-239 testing for bias, 135-137
transfer learning, 242, 304
transparency, 190-196
additivity versus interactions, 190 bias management and, 332 complexity as enemy of, 410 confirmation bias and, 72
data input and, 36-39
human interpretation and, 73 model documentation and, 195
model extraction attack countermeasure, 395

Index  |  437

partial dependence and individual condi? tional expectation, 191-194
post hoc explanation and, 50
post hoc explanation paired with explaina? ble models, 75-77
Shapley values, 194
steps toward causality with constraints, 191 trustworthiness and, 34
Tree SHAP, 56-59
tree_path_dependent feature perturbations, 58, 224
Trojans, 174
true positive rate (TPR), 270, 340 trustworthiness, NIST definition, 34 Twitter
bias bug bounty, 156-158 bias in platform design, 400 lack of peer review on, 405 Tay chatbot incident, 12, 176
U
Uber, 48, 120-121
UGESP (Uniform Guidelines on Employee Selection Procedures) four-fifths rule, 329
unconstrained XGBoost models constrained models versus, 208-214
decision tree surrogate models as explana? tion technique, 217-221
explaining model behavior with partial dependence and ICE, 214-217
underfitting, 100
underrepresentation, 134
underspecification, 101
DL debugging, 301
perturbing computational hyperparameters, 314
unexplainable ML models, 175, 373, 383 Uniform Guidelines on Employee Selection
   Procedures (UGESP) four-fifths rule, 329 unit testing, 92
unsupervised models, 145, 412
user feedback, bias mitigation and, 154
V
validation data, testing for bias in, 336 validation of published results and claims about
ML, 405-407

validation-based early stopping, 198, 210 validity
construct validity, 89
differential, 130, 337
evaluation of published results and claims for AI, 405-407
variable importance plots, 61 version control
reproducibility and, 84 security best practices, 165
visualizations, sensitivity analysis and, 108
W
watermarking, 184, 390, 412
weak learners, 208
weak spot, 97
weighted least squares, 352 Wells Fargo Bank, 48
wide data, 87
X
XAI (explainable artificial intelligence), 235 XGBoost, 208-228
constrained models, 46
constrained versus unconstrained models, 208-214
decision tree surrogate models as explana? tion technique, 217-221
evaluating models for bias, 335-350 explaining model behavior with partial
dependence and ICE, 214-217
model constraints and post hoc explanation, 208-228
model selection, 266-271
red-teaming (see red-teaming) remediating bias, 350-366 residual analysis for, 280-290 sensitivity analysis for, 271-280
Shapley value explanations, 221-228 stress testing, 272-276 testing/remediating bias with, 331-335
XNNs (explainable neural networks), 48
Z
Zillow iBuying case study, 27-31





438  |  Index


About the Authors

Patrick Hall is principal scientist at BNH.AI, where he advises Fortune 500 compa? nies and cutting-edge startups on AI risk and conducts research in support of NIST?s AI Risk Management Framework. He also serves as visiting faculty in the Department of Decision Sciences at the George Washington School of Business, teaching data ethics, business analytics, and machine learning classes.
Before cofounding BNH, Patrick led H2O.ai?s efforts in responsible AI, resulting in one of the world?s first commercial applications for explainability and bias mitigation in machine learning. He also held global customer-facing roles and R&D research roles at SAS Institute. Patrick studied computational chemistry at the University of Illinois before graduating from the Institute for Advanced Analytics at North Carolina State University.
Patrick has been invited to speak on topics relating to explainable AI at the National Academies of Science, Engineering, and Medicine, ACM SIG-KDD, and the Joint Statistical Meetings. He has contributed written pieces to outlets like McKinsey.com, O?Reilly Radar, and Thompson Reuters Regulatory Intelligence, and his technical work has been profiled in Fortune, Wired, InfoWorld, TechCrunch, and others.
James Curtis is a quantitative researcher at Solea Energy, where he is focused on using statistical forecasting to further the decarbonization of the US power grid. He previously served as a consultant for financial services organizations, insurers, regulators, and healthcare providers to help build more equitable AI/ML models. James holds an MS in Mathematics from the Colorado School of Mines.
Parul Pandey has a background in electrical engineering and currently works as a principal data scientist at H2O.ai. Prior to this, she worked as a machine learning engineer at Weights & Biases. She is also a Kaggle Grandmaster in the notebooks category and was one of LinkedIn?s Top Voices in the Software Development category in 2019. Parul has written multiple articles focused on data science and software development for various publications and mentors, speaks, and delivers workshops on topics related to responsible AI.
Colophon

The animal on the cover of Machine Learning for High-Risk Applications is the giant African fruit beetle (Mecynorrhina polyphemus).
Formerly classified under the Latin name Chelorrhina polyphemus, this large, green scarab beetle is a member of the Cetoniinae family of flower chafers, a group of brightly colored beetles that feed primarily on flower pollen, nectar, and petals, as well as fruits and tree sap. Ranging from 35 to 80 mm in length, giant African fruit beetles are the largest beetles in the genus Mecynorrhina.

These colossal scarabs are found in the dense tropical forests of Central Africa. The adults are sexually dimorphic, with the females having a shiny, prismatic carapace, and the males having antlers and a more velvety or matte coloration. As attractive and relatively easy-to-raise beetles, they make popular pets among aspiring entomol? ogists. This fact, along with habitat destruction, has been cited by at least one study as a factor in population declines in some areas, though they remain common overall.
Many of the animals on O?Reilly covers are endangered; all of them are important to the world.
The cover illustration is by Karen Montgomery, based on a black-and-white engrav? ing from Histoire Naturelle by Cuvier. The cover fonts are Gilroy Semibold and Guardian Sans. The text font is Adobe Minion Pro; the heading font is Adobe Myriad Condensed; and the code font is Dalton Maag?s Ubuntu Mono.


############ NIST AI RMF ###########

NIST AI 100-1 


Artificial Intelligence Risk Management Framework (AI RMF 1.0) 

NIST AI 100-1 


Artificial Intelligence Risk Management Framework (AI RMF 1.0) 
This publication is available free of charge from: https://doi.org/10.6028/NIST.AI.100-1 
January 2023 

U.S. Department of Commerce 
Gina M. Raimondo, Secretary 
National Institute of Standards and Technology 
Laurie E. Locascio, NIST Director and Under Secretary of Commerce for Standards and Technology 
Certain commercial entities, equipment, or materials may be identified in this document in order to describe an experimental procedure or concept adequately. Such identification is not intended to imply recommenda­tion or endorsement by the National Institute of Standards and Technology, nor is it intended to imply that the entities, materials, or equipment are necessarily the best available for the purpose. 
This publication is available free of charge from: https://doi.org/10.6028/NIST.AI.100-1 
Update Schedule and Versions 
The Artificial Intelligence Risk Management Framework (AI RMF) is intended to be a living document. 
NIST will review the content and usefulness of the Framework regularly to determine if an update is appro­priate; a review with formal input from the AI community is expected to take place no later than 2028. The Framework will employ a two-number versioning system to track and identify major and minor changes. The first number will represent the generation of the AI RMF and its companion documents (e.g., 1.0) and will change only with major revisions. Minor revisions will be tracked using “.n” after the generation number (e.g., 1.1). All changes will be tracked using a Version Control Table which identifies the history, including version number, date of change, and description of change. NIST plans to update the AI RMF Playbook frequently. Comments on the AI RMF Playbook may be sent via email to AIframework@nist.gov at any time and will be reviewed and integrated on a semi-annual basis. 
Table of Contents 
Executive Summary 1 Part 1: Foundational Information 4 1 Framing Risk 4 
1.1 Understanding and Addressing Risks, Impacts, and Harms 4 
1.2 Challenges for AI Risk Management 5 
1.2.1 Risk Measurement 5 
1.2.2 Risk Tolerance 7 
1.2.3 Risk Prioritization 7 
1.2.4 Organizational Integration and Management of Risk 8 2 Audience 9 3 AI Risks and Trustworthiness 12 
3.1 Valid and Reliable 13 3.2 Safe 14 
3.3 Secure and Resilient 15 
3.4 Accountable and Transparent 15 
3.5 Explainable and Interpretable 16 3.6 Privacy-Enhanced 17 3.7 Fair – with Harmful Bias Managed 17 4 Effectiveness of the AI RMF 19 Part 2: Core and Profiles 20 5 AI RMF Core 20 
5.1 Govern 21 5.2 Map 24 5.3 Measure 28 5.4 Manage 31 
6 AI RMF Profiles 33 Appendix A: Descriptions of AI Actor Tasks from Figures 2 and 3 35 Appendix B: How AI Risks Differ from Traditional Software Risks 38 Appendix C: AI Risk Management and Human-AI Interaction 40 Appendix D: Attributes of the AI RMF 42 




List of Tables 
Table 1 Categories and subcategories for the GOVERN function. 22 Table 2 Categories and subcategories for the MAP function. 26 Table 3 Categories and subcategories for the MEASURE function. 29 Table 4 Categories and subcategories for the MANAGE function. 32 
i 

List of Figures 
Fig. 1  Examples of potential harms related to AI systems. Trustworthy AI systems  
and their responsible use can mitigate negative risks and contribute to bene­
 
fits for people, organizations, and ecosystems.  5  
Fig. 2  Lifecycle and Key Dimensions of an AI System. Modified from OECD  
(2022) OECD Framework for the Classification of AI systems — OECD  
Digital Economy Papers. The two inner circles show AI systems’ key di­
 
mensions and the outer circle shows AI lifecycle stages. Ideally, risk man­
 
agement efforts start with the Plan and Design function in the application  
context and are performed throughout the AI system lifecycle. See Figure 3  
for representative AI actors.  10  
Fig. 3  AI actors across AI lifecycle stages. See Appendix A for detailed descrip­
 
tions of AI actor tasks, including details about testing, evaluation, verifica­
 
tion, and validation tasks. Note that AI actors in the AI Model dimension  
(Figure 2) are separated as a best practice, with those building and using the  
models separated from those verifying and validating the models.  11  
Fig. 4  Characteristics of trustworthy AI systems. Valid & Reliable is a necessary  
condition of trustworthiness and is shown as the base for other trustworthi­
 
ness characteristics. Accountable & Transparent is shown as a vertical box  
because it relates to all other characteristics.  12  
Fig. 5  Functions organize AI risk management activities at their highest level to  
govern, map, measure, and manage AI risks. Governance is designed to be  
a cross-cutting function to inform and be infused throughout the other three  
functions.  20  


Executive Summary 
Artificial intelligence (AI) technologies have significant potential to transform society and people’s lives – from commerce and health to transportation and cybersecurity to the envi­ronment and our planet. AI technologies can drive inclusive economic growth and support scientific advancements that improve the conditions of our world. AI technologies, how­ever, also pose risks that can negatively impact individuals, groups, organizations, commu­nities, society, the environment, and the planet. Like risks for other types of technology, AI risks can emerge in a variety of ways and can be characterized as long-or short-term, high-or low-probability, systemic or localized, and high-or low-impact. 

While there are myriad standards and best practices to help organizations mitigate the risks of traditional software or information-based systems, the risks posed by AI systems are in many ways unique (See Appendix B). AI systems, for example, may be trained on data that can change over time, sometimes significantly and unexpectedly, affecting system function­ality and trustworthiness in ways that are hard to understand. AI systems and the contexts in which they are deployed are frequently complex, making it difficult to detect and respond to failures when they occur. AI systems are inherently socio-technical in nature, meaning they are influenced by societal dynamics and human behavior. AI risks – and benefits – can emerge from the interplay of technical aspects combined with societal factors related to how a system is used, its interactions with other AI systems, who operates it, and the social context in which it is deployed. 
These risks make AI a uniquely challenging technology to deploy and utilize both for orga­nizations and within society. Without proper controls, AI systems can amplify, perpetuate, or exacerbate inequitable or undesirable outcomes for individuals and communities. With proper controls, AI systems can mitigate and manage inequitable outcomes. 
AI risk management is a key component of responsible development and use of AI sys­tems. Responsible AI practices can help align the decisions about AI system design, de­velopment, and uses with intended aim and values. Core concepts in responsible AI em­phasize human centricity, social responsibility, and sustainability. AI risk management can drive responsible uses and practices by prompting organizations and their internal teams who design, develop, and deploy AI to think more critically about context and potential or unexpected negative and positive impacts. Understanding and managing the risks of AI systems will help to enhance trustworthiness, and in turn, cultivate public trust. 

As directed by the National Artificial Intelligence Initiative Act of 2020 (P.L. 116-283), the goal of the AI RMF is to offer a resource to the organizations designing, developing, deploying, or using AI systems to help manage the many risks of AI and promote trustwor­thy and responsible development and use of AI systems. The Framework is intended to be voluntary, rights-preserving, non-sector-specific, and use-case agnostic, providing flexibil­ity to organizations of all sizes and in all sectors and throughout society to implement the approaches in the Framework. 
The Framework is designed to equip organizations and individuals – referred to here as AI actors – with approaches that increase the trustworthiness of AI systems, and to help foster the responsible design, development, deployment, and use of AI systems over time. AI actors are defined by the Organisation for Economic Co-operation and Development (OECD) as “those who play an active role in the AI system lifecycle, including organiza­tions and individuals that deploy or operate AI” [OECD (2019) Artificial Intelligence in Society—OECD iLibrary] (See Appendix A). 
The AI RMF is intended to be practical, to adapt to the AI landscape as AI technologies continue to develop, and to be operationalized by organizations in varying degrees and capacities so society can benefit from AI while also being protected from its potential harms. 
The Framework and supporting resources will be updated, expanded, and improved based on evolving technology, the standards landscape around the world, and AI community ex­perience and feedback. NIST will continue to align the AI RMF and related guidance with applicable international standards, guidelines, and practices. As the AI RMF is put into use, additional lessons will be learned to inform future updates and additional resources. 
The Framework is divided into two parts. Part 1 discusses how organizations can frame the risks related to AI and describes the intended audience. Next, AI risks and trustworthi­ness are analyzed, outlining the characteristics of trustworthy AI systems, which include valid and reliable, safe, secure and resilient, accountable and transparent, explainable and interpretable, privacy enhanced, and fair with their harmful biases managed. 
Part 2 comprises the “Core” of the Framework. It describes four specific functions to help organizations address the risks of AI systems in practice. These functions – GOVERN, MAP, MEASURE, and MANAGE – are broken down further into categories and subcate­gories. While GOVERN applies to all stages of organizations’ AI risk management pro­cesses and procedures, the MAP, MEASURE, and MANAGE functions can be applied in AI system-specific contexts and at specific stages of the AI lifecycle. 
Additional resources related to the Framework are included in the AI RMF Playbook, which is available via the NIST AI RMF website: https://www.nist.gov/itl/ai-risk-management-framework. 
Development of the AI RMF by NIST in collaboration with the private and public sec­tors is directed and consistent with its broader AI efforts called for by the National AI Initiative Act of 2020, the National Security Commission on Artificial Intelligence recom­
mendations, and the Plan for Federal Engagement in Developing Technical Standards and Related Tools. Engagement with the AI community during this Framework’s development 
– via responses to a formal Request for Information, three widely attended workshops, public comments on a concept paper and two drafts of the Framework, discussions at mul­tiple public forums, and many small group meetings – has informed development of the AI RMF 1.0 as well as AI research and development and evaluation conducted by NIST and others. Priority research and additional guidance that will enhance this Framework will be captured in an associated AI Risk Management Framework Roadmap to which NIST and the broader community can contribute. 
Part 1: Foundational Information 
1. Framing Risk 
AI risk management offers a path to minimize potential negative impacts of AI systems, such as threats to civil liberties and rights, while also providing opportunities to maximize positive impacts. Addressing, documenting, and managing AI risks and potential negative impacts effectively can lead to more trustworthy AI systems. 
1.1 Understanding and Addressing Risks, Impacts, and Harms 
In the context of the AI RMF, risk refers to the composite measure of an event’s probability of occurring and the magnitude or degree of the consequences of the corresponding event. The impacts, or consequences, of AI systems can be positive, negative, or both and can result in opportunities or threats (Adapted from: ISO 31000:2018). When considering the negative impact of a potential event, risk is a function of 1) the negative impact, or magni­tude of harm, that would arise if the circumstance or event occurs and 2) the likelihood of occurrence (Adapted from: OMB Circular A-130:2016). Negative impact or harm can be experienced by individuals, groups, communities, organizations, society, the environment, and the planet. 

While risk management processes generally address negative impacts, this Framework of­fers approaches to minimize anticipated negative impacts of AI systems and identify op­portunities to maximize positive impacts. Effectively managing the risk of potential harms could lead to more trustworthy AI systems and unleash potential benefits to people (individ­uals, communities, and society), organizations, and systems/ecosystems. Risk management can enable AI developers and users to understand impacts and account for the inherent lim­itations and uncertainties in their models and systems, which in turn can improve overall system performance and trustworthiness and the likelihood that AI technologies will be used in ways that are beneficial. 
The AI RMF is designed to address new risks as they emerge. This flexibility is particularly important where impacts are not easily foreseeable and applications are evolving. While some AI risks and benefits are well-known, it can be challenging to assess negative impacts and the degree of harms. Figure 1 provides examples of potential harms that can be related to AI systems. 
AI risk management efforts should consider that humans may assume that AI systems work 
– and work well – in all settings. For example, whether correct or not, AI systems are often perceived as being more objective than humans or as offering greater capabilities than general software. 
Page 4 

Fig. 1. Examples of potential harms related to AI systems. Trustworthy AI systems and their responsible use can mitigate negative risks and contribute to benefits for people, organizations, and ecosystems. 

1.2 Challenges for AI Risk Management 
Several challenges are described below. They should be taken into account when managing risks in pursuit of AI trustworthiness. 
1.2.1 Risk Measurement 
AI risks or failures that are not well-defined or adequately understood are difficult to mea­sure quantitatively or qualitatively. The inability to appropriately measure AI risks does not imply that an AI system necessarily poses either a high or low risk. Some risk measurement challenges include: 
Risks related to third-party software, hardware, and data: Third-party data or systems can accelerate research and development and facilitate technology transition. They also may complicate risk measurement. Risk can emerge both from third-party data, software or hardware itself and how it is used. Risk metrics or methodologies used by the organization developing the AI system may not align with the risk metrics or methodologies uses by the organization deploying or operating the system. Also, the organization developing the AI system may not be transparent about the risk metrics or methodologies it used. Risk measurement and management can be complicated by how customers use or integrate third-party data or systems into AI products or services, particularly without sufficient internal governance structures and technical safeguards. Regardless, all parties and AI actors should manage risk in the AI systems they develop, deploy, or use as standalone or integrated components. 
Tracking emergent risks: Organizations’ risk management efforts will be enhanced by identifying and tracking emergent risks and considering techniques for measuring them. 
AI system impact assessment approaches can help AI actors understand potential impacts or harms within specific contexts. 
Availability of reliable metrics: The current lack of consensus on robust and verifiable measurement methods for risk and trustworthiness, and applicability to different AI use cases, is an AI risk measurement challenge. Potential pitfalls when seeking to measure negative risk or harms include the reality that development of metrics is often an institu­tional endeavor and may inadvertently reflect factors unrelated to the underlying impact. In addition, measurement approaches can be oversimplified, gamed, lack critical nuance, be­come relied upon in unexpected ways, or fail to account for differences in affected groups and contexts. 
Approaches for measuring impacts on a population work best if they recognize that contexts matter, that harms may affect varied groups or sub-groups differently, and that communities or other sub-groups who may be harmed are not always direct users of a system. 
Risk at different stages of the AI lifecycle: Measuring risk at an earlier stage in the AI lifecycle may yield different results than measuring risk at a later stage; some risks may be latent at a given point in time and may increase as AI systems adapt and evolve. Fur­thermore, different AI actors across the AI lifecycle can have different risk perspectives. For example, an AI developer who makes AI software available, such as pre-trained mod­els, can have a different risk perspective than an AI actor who is responsible for deploying that pre-trained model in a specific use case. Such deployers may not recognize that their particular uses could entail risks which differ from those perceived by the initial developer. All involved AI actors share responsibilities for designing, developing, and deploying a trustworthy AI system that is fit for purpose. 
Risk in real-world settings: While measuring AI risks in a laboratory or a controlled environment may yield important insights pre-deployment, these measurements may differ from risks that emerge in operational, real-world settings. 
Inscrutability: Inscrutable AI systems can complicate risk measurement. Inscrutability can be a result of the opaque nature of AI systems (limited explainability or interpretabil­ity), lack of transparency or documentation in AI system development or deployment, or inherent uncertainties in AI systems. 
Human baseline: Risk management of AI systems that are intended to augment or replace human activity, for example decision making, requires some form of baseline metrics for comparison. This is difficult to systematize since AI systems carry out different tasks – and perform tasks differently – than humans. 

1.2.2 Risk Tolerance 
While the AI RMF can be used to prioritize risk, it does not prescribe risk tolerance. Risk tolerance refers to the organization’s or AI actor’s (see Appendix A) readiness to bear the risk in order to achieve its objectives. Risk tolerance can be influenced by legal or regula­tory requirements (Adapted from: ISO GUIDE 73). Risk tolerance and the level of risk that is acceptable to organizations or society are highly contextual and application and use-case specific. Risk tolerances can be influenced by policies and norms established by AI sys­tem owners, organizations, industries, communities, or policy makers. Risk tolerances are likely to change over time as AI systems, policies, and norms evolve. Different organiza­tions may have varied risk tolerances due to their particular organizational priorities and resource considerations. 
Emerging knowledge and methods to better inform harm/cost-benefit tradeoffs will con­tinue to be developed and debated by businesses, governments, academia, and civil society. To the extent that challenges for specifying AI risk tolerances remain unresolved, there may be contexts where a risk management framework is not yet readily applicable for mitigating negative AI risks. 


1.2.3 Risk Prioritization 
Attempting to eliminate negative risk entirely can be counterproductive in practice because not all incidents and failures can be eliminated. Unrealistic expectations about risk may lead organizations to allocate resources in a manner that makes risk triage inefficient or impractical or wastes scarce resources. A risk management culture can help organizations recognize that not all AI risks are the same, and resources can be allocated purposefully. Actionable risk management efforts lay out clear guidelines for assessing trustworthiness of each AI system an organization develops or deploys. Policies and resources should be prioritized based on the assessed risk level and potential impact of an AI system. The extent to which an AI system may be customized or tailored to the specific context of use by the AI deployer can be a contributing factor. 
When applying the AI RMF, risks which the organization determines to be highest for the AI systems within a given context of use call for the most urgent prioritization and most thorough risk management process. In cases where an AI system presents unacceptable negative risk levels – such as where significant negative impacts are imminent, severe harms are actually occurring, or catastrophic risks are present – development and deployment should cease in a safe manner until risks can be sufficiently managed. If an AI system’s development, deployment, and use cases are found to be low-risk in a specific context, that may suggest potentially lower prioritization. 
Risk prioritization may differ between AI systems that are designed or deployed to directly interact with humans as compared to AI systems that are not. Higher initial prioritization may be called for in settings where the AI system is trained on large datasets comprised of sensitive or protected data such as personally identifiable information, or where the outputs of the AI systems have direct or indirect impact on humans. AI systems designed to interact only with computational systems and trained on non-sensitive datasets (for example, data collected from the physical environment) may call for lower initial prioritization. Nonethe­less, regularly assessing and prioritizing risk based on context remains important because non-human-facing AI systems can have downstream safety or social implications. 
Residual risk – defined as risk remaining after risk treatment (Source: ISO GUIDE 73) – directly impacts end users or affected individuals and communities. Documenting residual risks will call for the system provider to fully consider the risks of deploying the AI product and will inform end users about potential negative impacts of interacting with the system. 

1.2.4 Organizational Integration and Management of Risk 
AI risks should not be considered in isolation. Different AI actors have different responsi­bilities and awareness depending on their roles in the lifecycle. For example, organizations developing an AI system often will not have information about how the system may be used. AI risk management should be integrated and incorporated into broader enterprise risk management strategies and processes. Treating AI risks along with other critical risks, such as cybersecurity and privacy, will yield a more integrated outcome and organizational efficiencies. 
The AI RMF may be utilized along with related guidance and frameworks for managing AI system risks or broader enterprise risks. Some risks related to AI systems are common across other types of software development and deployment. Examples of overlapping risks include: privacy concerns related to the use of underlying data to train AI systems; the en­ergy and environmental implications associated with resource-heavy computing demands; security concerns related to the confidentiality, integrity, and availability of the system and its training and output data; and general security of the underlying software and hardware for AI systems. 
Organizations need to establish and maintain the appropriate accountability mechanisms, roles and responsibilities, culture, and incentive structures for risk management to be ef­fective. Use of the AI RMF alone will not lead to these changes or provide the appropriate incentives. Effective risk management is realized through organizational commitment at senior levels and may require cultural change within an organization or industry. In addi­tion, small to medium-sized organizations managing AI risks or implementing the AI RMF may face different challenges than large organizations, depending on their capabilities and resources. 



2. Audience 
Identifying and managing AI risks and potential impacts – both positive and negative – re­quires a broad set of perspectives and actors across the AI lifecycle. Ideally, AI actors will represent a diversity of experience, expertise, and backgrounds and comprise demograph­ically and disciplinarily diverse teams. The AI RMF is intended to be used by AI actors across the AI lifecycle and dimensions. 
The OECD has developed a framework for classifying AI lifecycle activities according to five key socio-technical dimensions, each with properties relevant for AI policy and gover­nance, including risk management [OECD (2022) OECD Framework for the Classification of AI systems — OECD Digital Economy Papers]. Figure 2 shows these dimensions, slightly modified by NIST for purposes of this framework. The NIST modification high­lights the importance of test, evaluation, verification, and validation (TEVV) processes throughout an AI lifecycle and generalizes the operational context of an AI system. 
AI dimensions displayed in Figure 2 are the Application Context, Data and Input, AI Model, and Task and Output. AI actors involved in these dimensions who perform or manage the design, development, deployment, evaluation, and use of AI systems and drive AI risk management efforts are the primary AI RMF audience. 
Representative AI actors across the lifecycle dimensions are listed in Figure 3 and described in detail in Appendix A. Within the AI RMF, all AI actors work together to manage risks and achieve the goals of trustworthy and responsible AI. AI actors with TEVV-specific expertise are integrated throughout the AI lifecycle and are especially likely to benefit from the Framework. Performed regularly, TEVV tasks can provide insights relative to technical, societal, legal, and ethical standards or norms, and can assist with anticipating impacts and assessing and tracking emergent risks. As a regular process within an AI lifecycle, TEVV allows for both mid-course remediation and post-hoc risk management. 
The People & Planet dimension at the center of Figure 2 represents human rights and the broader well-being of society and the planet. The AI actors in this dimension comprise a separate AI RMF audience who informs the primary audience. These AI actors may in­clude trade associations, standards developing organizations, researchers, advocacy groups, 

Fig. 2. Lifecycle and Key Dimensions of an AI System. Modified from OECD (2022) OECD Framework for the Classification of AI systems — OECD Digital Economy Papers. The two inner circles show AI systems’ key dimensions and the outer circle shows AI lifecycle stages. Ideally, risk management efforts start with the Plan and Design function in the application context and are performed throughout the AI system lifecycle. See Figure 3 for representative AI actors. 
environmental groups, civil society organizations, end users, and potentially impacted in­dividuals and communities. These actors can: 
• 
assist in providing context and understanding potential and actual impacts; 

• 
be a source of formal or quasi-formal norms and guidance for AI risk management; 

• 
designate boundaries for AI operation (technical, societal, legal, and ethical); and 

• 
promote discussion of the tradeoffs needed to balance societal values and priorities related to civil liberties and rights, equity, the environment and the planet, and the economy. 


Successful risk management depends upon a sense of collective responsibility among AI actors shown in Figure 3. The AI RMF functions, described in Section 5, require diverse perspectives, disciplines, professions, and experiences. Diverse teams contribute to more open sharing of ideas and assumptions about the purposes and functions of technology – making these implicit aspects more explicit. This broader collective perspective creates opportunities for surfacing problems and identifying existing and emergent risks. 
Fig. 3. AI actors across AI lifecycle stages. See Appendix A for detailed descriptions of AI actor tasks, including details about testing,evaluation, verification, and validation tasks. Note that AI actors in the AI Model dimension (Figure 2) are separated as a best practice, withthose building and using the models separated from those verifying and validating the models. 
3. AI Risks and Trustworthiness 
For AI systems to be trustworthy, they often need to be responsive to a multiplicity of cri­teria that are of value to interested parties. Approaches which enhance AI trustworthiness can reduce negative AI risks. This Framework articulates the following characteristics of trustworthy AI and offers guidance for addressing them. Characteristics of trustworthy AI systems include: valid and reliable, safe, secure and resilient, accountable and trans­parent, explainable and interpretable, privacy-enhanced, and fair with harmful bias managed. Creating trustworthy AI requires balancing each of these characteristics based on the AI system’s context of use. While all characteristics are socio-technical system at­tributes, accountability and transparency also relate to the processes and activities internal to an AI system and its external setting. Neglecting these characteristics can increase the probability and magnitude of negative consequences. 

Fig. 4. Characteristics of trustworthy AI systems. Valid & Reliable is a necessary condition of trustworthiness and is shown as the base for other trustworthiness characteristics. Accountable & Transparent is shown as a vertical box because it relates to all other characteristics. 
Trustworthiness characteristics (shown in Figure 4) are inextricably tied to social and orga­
nizational behavior, the datasets used by AI systems, selection of AI models and algorithms and the decisions made by those who build them, and the interactions with the humans who provide insight from and oversight of such systems. Human judgment should be employed when deciding on the specific metrics related to AI trustworthiness characteristics and the precise threshold values for those metrics. 
Addressing AI trustworthiness characteristics individually will not ensure AI system trust­worthiness; tradeoffs are usually involved, rarely do all characteristics apply in every set­ting, and some will be more or less important in any given situation. Ultimately, trustwor­thiness is a social concept that ranges across a spectrum and is only as strong as its weakest characteristics. 
When managing AI risks, organizations can face difficult decisions in balancing these char­acteristics. For example, in certain scenarios tradeoffs may emerge between optimizing for interpretability and achieving privacy. In other cases, organizations might face a tradeoff between predictive accuracy and interpretability. Or, under certain conditions such as data sparsity, privacy-enhancing techniques can result in a loss in accuracy, affecting decisions about fairness and other values in certain domains. Dealing with tradeoffs requires tak­ing into account the decision-making context. These analyses can highlight the existence and extent of tradeoffs between different measures, but they do not answer questions about how to navigate the tradeoff. Those depend on the values at play in the relevant context and should be resolved in a manner that is both transparent and appropriately justifiable. 
There are multiple approaches for enhancing contextual awareness in the AI lifecycle. For example, subject matter experts can assist in the evaluation of TEVV findings and work with product and deployment teams to align TEVV parameters to requirements and de­ployment conditions. When properly resourced, increasing the breadth and diversity of input from interested parties and relevant AI actors throughout the AI lifecycle can en­hance opportunities for informing contextually sensitive evaluations, and for identifying AI system benefits and positive impacts. These practices can increase the likelihood that risks arising in social contexts are managed appropriately. 
Understanding and treatment of trustworthiness characteristics depends on an AI actor’s particular role within the AI lifecycle. For any given AI system, an AI designer or developer may have a different perception of the characteristics than the deployer. 

3.1 Valid and Reliable 
Validation is the “confirmation, through the provision of objective evidence, that the re­quirements for a specific intended use or application have been fulfilled” (Source: ISO 9000:2015). Deployment of AI systems which are inaccurate, unreliable, or poorly gener­alized to data and settings beyond their training creates and increases negative AI risks and reduces trustworthiness. 
Reliability is defined in the same standard as the “ability of an item to perform as required, without failure, for a given time interval, under given conditions” (Source: ISO/IEC TS 5723:2022). Reliability is a goal for overall correctness of AI system operation under the conditions of expected use and over a given period of time, including the entire lifetime of the system. 
Accuracy and robustness contribute to the validity and trustworthiness of AI systems, and can be in tension with one another in AI systems. 
Accuracy is defined by ISO/IEC TS 5723:2022 as “closeness of results of observations, computations, or estimates to the true values or the values accepted as being true.” Mea­sures of accuracy should consider computational-centric measures (e.g., false positive and false negative rates), human-AI teaming, and demonstrate external validity (generalizable beyond the training conditions). Accuracy measurements should always be paired with clearly defined and realistic test sets – that are representative of conditions of expected use 
– and details about test methodology; these should be included in associated documen­tation. Accuracy measurements may include disaggregation of results for different data segments. 
Robustness or generalizability is defined as the “ability of a system to maintain its level of performance under a variety of circumstances” (Source: ISO/IEC TS 5723:2022). Ro­bustness is a goal for appropriate system functionality in a broad set of conditions and circumstances, including uses of AI systems not initially anticipated. Robustness requires not only that the system perform exactly as it does under expected uses, but also that it should perform in ways that minimize potential harms to people if it is operating in an unexpected setting. 
Validity and reliability for deployed AI systems are often assessed by ongoing testing or monitoring that confirms a system is performing as intended. Measurement of validity, accuracy, robustness, and reliability contribute to trustworthiness and should take into con­sideration that certain types of failures can cause greater harm. AI risk management efforts should prioritize the minimization of potential negative impacts, and may need to include human intervention in cases where the AI system cannot detect or correct errors. 

3.2 Safe 
AI systems should “not under defined conditions, lead to a state in which human life, health, property, or the environment is endangered” (Source: ISO/IEC TS 5723:2022). Safe operation of AI systems is improved through: 
• 
responsible design, development, and deployment practices; 

• 
clear information to deployers on responsible use of the system; 

• 
responsible decision-making by deployers and end users; and 

• 
explanations and documentation of risks based on empirical evidence of incidents. 


Different types of safety risks may require tailored AI risk management approaches based on context and the severity of potential risks presented. Safety risks that pose a potential risk of serious injury or death call for the most urgent prioritization and most thorough risk management process. 
Employing safety considerations during the lifecycle and starting as early as possible with planning and design can prevent failures or conditions that can render a system dangerous. Other practical approaches for AI safety often relate to rigorous simulation and in-domain testing, real-time monitoring, and the ability to shut down, modify, or have human inter­vention into systems that deviate from intended or expected functionality. 
AI safety risk management approaches should take cues from efforts and guidelines for safety in fields such as transportation and healthcare, and align with existing sector-or application-specific guidelines or standards. 

3.3 Secure and Resilient 
AI systems, as well as the ecosystems in which they are deployed, may be said to be re­silient if they can withstand unexpected adverse events or unexpected changes in their envi­ronment or use – or if they can maintain their functions and structure in the face of internal and external change and degrade safely and gracefully when this is necessary (Adapted from: ISO/IEC TS 5723:2022). Common security concerns relate to adversarial examples, data poisoning, and the exfiltration of models, training data, or other intellectual property through AI system endpoints. AI systems that can maintain confidentiality, integrity, and availability through protection mechanisms that prevent unauthorized access and use may be said to be secure. Guidelines in the NIST Cybersecurity Framework and Risk Manage­
ment Framework are among those which are applicable here. 
Security and resilience are related but distinct characteristics. While resilience is the abil­ity to return to normal function after an unexpected adverse event, security includes re­silience but also encompasses protocols to avoid, protect against, respond to, or recover from attacks. Resilience relates to robustness and goes beyond the provenance of the data to encompass unexpected or adversarial use (or abuse or misuse) of the model or data. 

3.4 Accountable and Transparent 
Trustworthy AI depends upon accountability. Accountability presupposes transparency. Transparency reflects the extent to which information about an AI system and its outputs is available to individuals interacting with such a system – regardless of whether they are even aware that they are doing so. Meaningful transparency provides access to appropriate levels of information based on the stage of the AI lifecycle and tailored to the role or knowledge of AI actors or individuals interacting with or using the AI system. By promoting higher levels of understanding, transparency increases confidence in the AI system. 
This characteristic’s scope spans from design decisions and training data to model train­ing, the structure of the model, its intended use cases, and how and when deployment, post-deployment, or end user decisions were made and by whom. Transparency is often necessary for actionable redress related to AI system outputs that are incorrect or otherwise lead to negative impacts. Transparency should consider human-AI interaction: for exam-ple, how a human operator or user is notified when a potential or actual adverse outcome caused by an AI system is detected. A transparent system is not necessarily an accurate, privacy-enhanced, secure, or fair system. However, it is difficult to determine whether an opaque system possesses such characteristics, and to do so over time as complex systems evolve. 
The role of AI actors should be considered when seeking accountability for the outcomes of AI systems. The relationship between risk and accountability associated with AI and tech­nological systems more broadly differs across cultural, legal, sectoral, and societal contexts. When consequences are severe, such as when life and liberty are at stake, AI developers and deployers should consider proportionally and proactively adjusting their transparency and accountability practices. Maintaining organizational practices and governing structures for harm reduction, like risk management, can help lead to more accountable systems. 
Measures to enhance transparency and accountability should also consider the impact of these efforts on the implementing entity, including the level of necessary resources and the need to safeguard proprietary information. 
Maintaining the provenance of training data and supporting attribution of the AI system’s decisions to subsets of training data can assist with both transparency and accountability. Training data may also be subject to copyright and should follow applicable intellectual property rights laws. 
As transparency tools for AI systems and related documentation continue to evolve, devel­opers of AI systems are encouraged to test different types of transparency tools in cooper­ation with AI deployers to ensure that AI systems are used as intended. 

3.5 Explainable and Interpretable 
Explainability refers to a representation of the mechanisms underlying AI systems’ oper­ation, whereas interpretability refers to the meaning of AI systems’ output in the context of their designed functional purposes. Together, explainability and interpretability assist those operating or overseeing an AI system, as well as users of an AI system, to gain deeper insights into the functionality and trustworthiness of the system, including its out­puts. The underlying assumption is that perceptions of negative risk stem from a lack of ability to make sense of, or contextualize, system output appropriately. Explainable and interpretable AI systems offer information that will help end users understand the purposes and potential impact of an AI system. 
Risk from lack of explainability may be managed by describing how AI systems function, with descriptions tailored to individual differences such as the user’s role, knowledge, and skill level. Explainable systems can be debugged and monitored more easily, and they lend themselves to more thorough documentation, audit, and governance. 
Risks to interpretability often can be addressed by communicating a description of why an AI system made a particular prediction or recommendation. (See “Four Principles of Explainable Artificial Intelligence” and “Psychological Foundations of Explainability and Interpretability in Artificial Intelligence” found here.) 
Transparency, explainability, and interpretability are distinct characteristics that support each other. Transparency can answer the question of “what happened” in the system. Ex­plainability can answer the question of “how” a decision was made in the system. Inter­pretability can answer the question of “why” a decision was made by the system and its meaning or context to the user. 

3.6 Privacy-Enhanced 
Privacy refers generally to the norms and practices that help to safeguard human autonomy, identity, and dignity. These norms and practices typically address freedom from intrusion, limiting observation, or individuals’ agency to consent to disclosure or control of facets of their identities (e.g., body, data, reputation). (See The NIST Privacy Framework: A Tool for Improving Privacy through Enterprise Risk Management.) 
Privacy values such as anonymity, confidentiality, and control generally should guide choices for AI system design, development, and deployment. Privacy-related risks may influence security, bias, and transparency and come with tradeoffs with these other characteristics. Like safety and security, specific technical features of an AI system may promote or reduce privacy. AI systems can also present new risks to privacy by allowing inference to identify individuals or previously private information about individuals. 
Privacy-enhancing technologies (“PETs”) for AI, as well as data minimizing methods such as de-identification and aggregation for certain model outputs, can support design for privacy-enhanced AI systems. Under certain conditions such as data sparsity, privacy-enhancing techniques can result in a loss in accuracy, affecting decisions about fairness and other values in certain domains. 

3.7 Fair – with Harmful Bias Managed 
Fairness in AI includes concerns for equality and equity by addressing issues such as harm­ful bias and discrimination. Standards of fairness can be complex and difficult to define be­cause perceptions of fairness differ among cultures and may shift depending on application. Organizations’ risk management efforts will be enhanced by recognizing and considering these differences. Systems in which harmful biases are mitigated are not necessarily fair. For example, systems in which predictions are somewhat balanced across demographic groups may still be inaccessible to individuals with disabilities or affected by the digital divide or may exacerbate existing disparities or systemic biases. 
Bias is broader than demographic balance and data representativeness. NIST has identified three major categories of AI bias to be considered and managed: systemic, computational and statistical, and human-cognitive. Each of these can occur in the absence of prejudice, partiality, or discriminatory intent. Systemic bias can be present in AI datasets, the orga­nizational norms, practices, and processes across the AI lifecycle, and the broader society that uses AI systems. Computational and statistical biases can be present in AI datasets and algorithmic processes, and often stem from systematic errors due to non-representative samples. Human-cognitive biases relate to how an individual or group perceives AI sys­tem information to make a decision or fill in missing information, or how humans think about purposes and functions of an AI system. Human-cognitive biases are omnipresent in decision-making processes across the AI lifecycle and system use, including the design, implementation, operation, and maintenance of AI. 
Bias exists in many forms and can become ingrained in the automated systems that help make decisions about our lives. While bias is not always a negative phenomenon, AI sys­tems can potentially increase the speed and scale of biases and perpetuate and amplify harms to individuals, groups, communities, organizations, and society. Bias is tightly asso­ciated with the concepts of transparency as well as fairness in society. (For more informa­tion about bias, including the three categories, see NIST Special Publication 1270, Towards a Standard for Identifying and Managing Bias in Artificial Intelligence.) 


4. Effectiveness of the AI RMF 
Evaluations of AI RMF effectiveness – including ways to measure bottom-line improve­ments in the trustworthiness of AI systems – will be part of future NIST activities, in conjunction with the AI community. 
Organizations and other users of the Framework are encouraged to periodically evaluate whether the AI RMF has improved their ability to manage AI risks, including but not lim­ited to their policies, processes, practices, implementation plans, indicators, measurements, and expected outcomes. NIST intends to work collaboratively with others to develop met­rics, methodologies, and goals for evaluating the AI RMF’s effectiveness, and to broadly share results and supporting information. Framework users are expected to benefit from: 
• 
enhanced processes for governing, mapping, measuring, and managing AI risk, and clearly documenting outcomes; 

• 
improved awareness of the relationships and tradeoffs among trustworthiness char­acteristics, socio-technical approaches, and AI risks; 

• 
explicit processes for making go/no-go system commissioning and deployment deci­sions; 

• 
established policies, processes, practices, and procedures for improving organiza­tional accountability efforts related to AI system risks; 

• 
enhanced organizational culture which prioritizes the identification and management of AI system risks and potential impacts to individuals, communities, organizations, and society; 

• 
better information sharing within and across organizations about risks, decision-making processes, responsibilities, common pitfalls, TEVV practices, and approaches for continuous improvement; 

• 
greater contextual knowledge for increased awareness of downstream risks; 

• 
strengthened engagement with interested parties and relevant AI actors; and 

• 
augmented capacity for TEVV of AI systems and associated risks. 


Part 2: Core and Profiles 

5. AI RMF Core 
The AI RMF Core provides outcomes and actions that enable dialogue, understanding, and activities to manage AI risks and responsibly develop trustworthy AI systems. As illus­trated in Figure 5, the Core is composed of four functions: GOVERN, MAP, MEASURE, and MANAGE. Each of these high-level functions is broken down into categories and sub­categories. Categories and subcategories are subdivided into specific actions and outcomes. Actions do not constitute a checklist, nor are they necessarily an ordered set of steps. 

Fig. 5. Functions organize AI risk management activities at their highest level to govern, map, measure, and manage AI risks. Governance is designed to be a cross-cutting function to inform and be infused throughout the other three functions. 
Risk management should be continuous, timely, and performed throughout the AI system lifecycle dimensions. AI RMF Core functions should be carried out in a way that reflects diverse and multidisciplinary perspectives, potentially including the views of AI actors out­side the organization. Having a diverse team contributes to more open sharing of ideas and assumptions about purposes and functions of the technology being designed, developed, deployed, or evaluated – which can create opportunities to surface problems and identify existing and emergent risks. 
An online companion resource to the AI RMF, the NIST AI RMF Playbook, is available to help organizations navigate the AI RMF and achieve its outcomes through suggested tactical actions they can apply within their own contexts. Like the AI RMF, the Playbook is voluntary and organizations can utilize the suggestions according to their needs and interests. Playbook users can create tailored guidance selected from suggested material for their own use and contribute their suggestions for sharing with the broader community. Along with the AI RMF, the Playbook is part of the NIST Trustworthy and Responsible AI Resource Center. 

5.1 Govern 
The GOVERN function: 
• cultivates and implements a culture of risk management within organizations design­ing, developing, deploying, evaluating, or acquiring AI systems; 
• outlines processes, documents, and organizational schemes that anticipate, identify, and manage the risks a system can pose, including to users and others across society 
– and procedures to achieve those outcomes; 
• 
incorporates processes to assess potential impacts; 

• 
provides a structure by which AI risk management functions can align with organi­zational principles, policies, and strategic priorities; 

• 
connects technical aspects of AI system design and development to organizational values and principles, and enables organizational practices and competencies for the individuals involved in acquiring, training, deploying, and monitoring such systems; and 

• 
addresses full product lifecycle and associated processes, including legal and other issues concerning use of third-party software or hardware systems and data. 


GOVERN is a cross-cutting function that is infused throughout AI risk management and enables the other functions of the process. Aspects of GOVERN, especially those related to compliance or evaluation, should be integrated into each of the other functions. Attention to governance is a continual and intrinsic requirement for effective AI risk management over an AI system’s lifespan and the organization’s hierarchy. 
Strong governance can drive and enhance internal practices and norms to facilitate orga­nizational risk culture. Governing authorities can determine the overarching policies that direct an organization’s mission, goals, values, culture, and risk tolerance. Senior leader­ship sets the tone for risk management within an organization, and with it, organizational culture. Management aligns the technical aspects of AI risk management to policies and operations. Documentation can enhance transparency, improve human review processes, and bolster accountability in AI system teams. 
After putting in place the structures, systems, processes, and teams described in the GOV­ERN function, organizations should benefit from a purpose-driven culture focused on risk understanding and management. It is incumbent on Framework users to continue to ex­ecute the GOVERN function as knowledge, cultures, and needs or expectations from AI actors evolve over time. 
Practices related to governing AI risks are described in the NIST AI RMF Playbook. Table 1 lists the GOVERN function’s categories and subcategories. 
Table 1: Categories and subcategories for the GOVERN function. 
Categories Subcategories 
GOVERN 1: 
Policies, processes, procedures, and practices across the organization related to the mapping, measuring, and managing of AI risks are in place, transparent, and implemented effectively. 
GOVERN 1.1: Legal and regulatory requirements involving AI 
are understood, managed, and documented. GOVERN 1.2: The characteristics of trustworthy AI are inte­grated into organizational policies, processes, procedures, and practices. 
GOVERN 1.3: Processes, procedures, and practices are in place to determine the needed level of risk management activities based on the organization’s risk tolerance. 
GOVERN 1.4: The risk management process and its outcomes are established through transparent policies, procedures, and other controls based on organizational risk priorities. 
Continued on next page 
Table 1: Categories and subcategories for the GOVERN function. (Continued) 
Categories Subcategories 
GOVERN 2: 
Accountability structures are in place so that the appropriate teams and individuals are empowered, responsible, and trained for mapping, measuring, and managing AI risks. 
GOVERN 1.5: Ongoing monitoring and periodic review of the risk management process and its outcomes are planned and or­ganizational roles and responsibilities clearly defined, including determining the frequency of periodic review. 
GOVERN 1.6: Mechanisms are in place to inventory AI systems 
and are resourced according to organizational risk priorities. GOVERN 1.7: Processes and procedures are in place for decom­missioning and phasing out AI systems safely and in a man­ner that does not increase risks or decrease the organization’s trustworthiness. 
GOVERN 2.1: Roles and responsibilities and lines of communi­cation related to mapping, measuring, and managing AI risks are documented and are clear to individuals and teams throughout the organization. 
GOVERN 2.2: The organization’s personnel and partners receive AI risk management training to enable them to perform their du­ties and responsibilities consistent with related policies, proce­dures, and agreements. 
GOVERN 2.3: Executive leadership of the organization takes re­sponsibility for decisions about risks associated with AI system development and deployment. 
GOVERN 3: 
Workforce diversity, equity, inclusion, and accessibility processes are prioritized in the mapping, measuring, and managing of AI risks throughout the lifecycle. 
GOVERN 3.1: Decision-making related to mapping, measuring, and managing AI risks throughout the lifecycle is informed by a diverse team (e.g., diversity of demographics, disciplines, expe­rience, expertise, and backgrounds). 
GOVERN 3.2: Policies and procedures are in place to define and differentiate roles and responsibilities for human-AI configura­tions and oversight of AI systems. 
GOVERN 4:  GOVERN 4.1: Organizational policies and practices are in place  
Organizational  to foster a critical thinking and safety-first mindset in the design,  
teams are committed  development, deployment, and uses of AI systems to minimize  
to a culture  potential negative impacts.  

Table 1: Categories and subcategories for the GOVERN function. (Continued) 
Categories  Subcategories  
that considers and communicates AI risk.  GOVERN 4.2: Organizational teams document the risks and po­tential impacts of the AI technology they design, develop, deploy, evaluate, and use, and they communicate about the impacts more broadly.  
GOVERN 4.3: Organizational practices are in place to enable AI testing, identification of incidents, and information sharing.  
GOVERN 5: Processes are in place for robust engagement with relevant AI actors.  GOVERN 5.1: Organizational policies and practices are in place to collect, consider, prioritize, and integrate feedback from those external to the team that developed or deployed the AI system regarding the potential individual and societal impacts related to AI risks.  
GOVERN 5.2: Mechanisms are established to enable the team  
that developed or deployed AI systems to regularly incorporate adjudicated feedback from relevant AI actors into system design and implementation.  

GOVERN 6: Policies and procedures are in place to address AI risks and benefits arising from third-party software and data and other supply chain issues. 
GOVERN 6.1: Policies and procedures are in place that address AI risks associated with third-party entities, including risks of in­fringement of a third-party’s intellectual property or other rights. 
GOVERN 6.2: Contingency processes are in place to handle failures or incidents in third-party data or AI systems deemed to be high-risk. 

5.2 Map 
The MAP function establishes the context to frame risks related to an AI system. The AI lifecycle consists of many interdependent activities involving a diverse set of actors (See Figure 3). In practice, AI actors in charge of one part of the process often do not have full visibility or control over other parts and their associated contexts. The interdependencies between these activities, and among the relevant AI actors, can make it difficult to reliably anticipate impacts of AI systems. For example, early decisions in identifying purposes and objectives of an AI system can alter its behavior and capabilities, and the dynamics of de­ployment setting (such as end users or impacted individuals) can shape the impacts of AI system decisions. As a result, the best intentions within one dimension of the AI lifecycle can be undermined via interactions with decisions and conditions in other, later activities. 
This complexity and varying levels of visibility can introduce uncertainty into risk man­agement practices. Anticipating, assessing, and otherwise addressing potential sources of negative risk can mitigate this uncertainty and enhance the integrity of the decision process. 
The information gathered while carrying out the MAP function enables negative risk pre­vention and informs decisions for processes such as model management, as well as an initial decision about appropriateness or the need for an AI solution. Outcomes in the MAP function are the basis for the MEASURE and MANAGE functions. Without contex­tual knowledge, and awareness of risks within the identified contexts, risk management is difficult to perform. The MAP function is intended to enhance an organization’s ability to identify risks and broader contributing factors. 
Implementation of this function is enhanced by incorporating perspectives from a diverse internal team and engagement with those external to the team that developed or deployed the AI system. Engagement with external collaborators, end users, potentially impacted communities, and others may vary based on the risk level of a particular AI system, the makeup of the internal team, and organizational policies. Gathering such broad perspec­tives can help organizations proactively prevent negative risks and develop more trustwor­thy AI systems by: 
• 
improving their capacity for understanding contexts; 

• 
checking their assumptions about context of use; 

• 
enabling recognition of when systems are not functional within or out of their in­tended context; 

• 
identifying positive and beneficial uses of their existing AI systems; 

• 
improving understanding of limitations in AI and ML processes; 

• 
identifying constraints in real-world applications that may lead to negative impacts; 

• 
identifying known and foreseeable negative impacts related to intended use of AI systems; and 

• 
anticipating risks of the use of AI systems beyond intended use. 


After completing the MAP function, Framework users should have sufficient contextual knowledge about AI system impacts to inform an initial go/no-go decision about whether to design, develop, or deploy an AI system. If a decision is made to proceed, organizations should utilize the MEASURE and MANAGE functions along with policies and procedures put into place in the GOVERN function to assist in AI risk management efforts. It is incum­bent on Framework users to continue applying the MAP function to AI systems as context, capabilities, risks, benefits, and potential impacts evolve over time. 
Practices related to mapping AI risks are described in the NIST AI RMF Playbook. Table 2 lists the MAP function’s categories and subcategories. 
Table 2: Categories and subcategories for the MAP function. 
Categories  Subcategories  
MAP 1: Context is established and understood.  MAP 1.1: Intended purposes, potentially beneficial uses, context-specific laws, norms and expectations, and prospective settings in which the AI system will be deployed are understood and docu­mented. Considerations include: the specific set or types of users along with their expectations; potential positive and negative im­pacts of system uses to individuals, communities, organizations, society, and the planet; assumptions and related limitations about AI system purposes, uses, and risks across the development or product AI lifecycle; and related TEVV and system metrics.  
MAP 1.2: Interdisciplinary AI actors, competencies, skills, and capacities for establishing context reflect demographic diversity and broad domain and user experience expertise, and their par­ticipation is documented. Opportunities for interdisciplinary col­laboration are prioritized.  
MAP 1.3: The organization’s mission and relevant goals for AI technology are understood and documented.  
MAP 1.4: The business value or context of business use has been  
clearly defined or – in the case of assessing existing AI systems – re-evaluated.  
MAP 1.5: Organizational risk tolerances are determined and documented.  
MAP 1.6: System requirements (e.g., “the system shall respect the privacy of its users”) are elicited from and understood by rel­evant AI actors. Design decisions take socio-technical implica­tions into account to address AI risks.  

MAP 2: 
Categorization of the AI system is performed. 
MAP 2.1: The specific tasks and methods used to implement the tasks that the AI system will support are defined (e.g., classifiers, generative models, recommenders). 
MAP 2.2: Information about the AI system’s knowledge limits and how system output may be utilized and overseen by humans is documented. Documentation provides sufficient information to assist relevant AI actors when making decisions and taking subsequent actions. 
Continued on next page 
Table 2: Categories and subcategories for the MAP function. (Continued) 
Categories Subcategories 
MAP 3: AI capabilities, targeted usage, goals, and expected benefits and costs compared with appropriate benchmarks are understood. 
MAP 2.3: Scientific integrity and TEVV considerations are iden­tified and documented, including those related to experimental design, data collection and selection (e.g., availability, repre­sentativeness, suitability), system trustworthiness, and construct validation. 
MAP 3.1: Potential benefits of intended AI system functionality 
and performance are examined and documented. MAP 3.2: Potential costs, including non-monetary costs, which result from expected or realized AI errors or system functionality and trustworthiness – as connected to organizational risk toler­ance – are examined and documented. 
MAP 3.3: Targeted application scope is specified and docu­mented based on the system’s capability, established context, and AI system categorization. 
MAP 3.4: Processes for operator and practitioner proficiency with AI system performance and trustworthiness – and relevant technical standards and certifications – are defined, assessed, and documented. 
MAP 3.5: Processes for human oversight are defined, assessed, and documented in accordance with organizational policies from the GOVERN function. 
MAP 4: Risks and benefits are mapped for all components of the AI system including third-party software and data. 
MAP 4.1: Approaches for mapping AI technology and legal risks of its components – including the use of third-party data or soft­ware – are in place, followed, and documented, as are risks of in­fringement of a third party’s intellectual property or other rights. 
MAP 4.2: Internal risk controls for components of the AI sys­tem, including third-party AI technologies, are identified and documented. 
MAP 5: Impacts to  MAP 5.1: Likelihood and magnitude of each identified impact  
individuals, groups,  (both potentially beneficial and harmful) based on expected use,  
communities,  past uses of AI systems in similar contexts, public incident re­ 
organizations, and  ports, feedback from those external to the team that developed  
society are  or deployed the AI system, or other data are identified and  
characterized.  documented.  

Continued on next page 
Table 2: Categories and subcategories for the MAP function. (Continued) 
Categories Subcategories 
MAP 5.2: Practices and personnel for supporting regular en­gagement with relevant AI actors and integrating feedback about positive, negative, and unanticipated impacts are in place and documented. 

5.3 Measure 
The MEASURE function employs quantitative, qualitative, or mixed-method tools, tech­niques, and methodologies to analyze, assess, benchmark, and monitor AI risk and related impacts. It uses knowledge relevant to AI risks identified in the MAP function and informs the MANAGE function. AI systems should be tested before their deployment and regu­larly while in operation. AI risk measurements include documenting aspects of systems’ functionality and trustworthiness. 
Measuring AI risks includes tracking metrics for trustworthy characteristics, social impact, and human-AI configurations. Processes developed or adopted in the MEASURE function should include rigorous software testing and performance assessment methodologies with associated measures of uncertainty, comparisons to performance benchmarks, and formal­ized reporting and documentation of results. Processes for independent review can improve the effectiveness of testing and can mitigate internal biases and potential conflicts of inter­est. 
Where tradeoffs among the trustworthy characteristics arise, measurement provides a trace­able basis to inform management decisions. Options may include recalibration, impact mitigation, or removal of the system from design, development, production, or use, as well as a range of compensating, detective, deterrent, directive, and recovery controls. 
After completing the MEASURE function, objective, repeatable, or scalable test, evaluation, verification, and validation (TEVV) processes including metrics, methods, and methodolo­gies are in place, followed, and documented. Metrics and measurement methodologies should adhere to scientific, legal, and ethical norms and be carried out in an open and trans­parent process. New types of measurement, qualitative and quantitative, may need to be developed. The degree to which each measurement type provides unique and meaningful information to the assessment of AI risks should be considered. Framework users will en­hance their capacity to comprehensively evaluate system trustworthiness, identify and track existing and emergent risks, and verify efficacy of the metrics. Measurement outcomes will be utilized in the MANAGE function to assist risk monitoring and response efforts. It is in­cumbent on Framework users to continue applying the MEASURE function to AI systems as knowledge, methodologies, risks, and impacts evolve over time. 
Practices related to measuring AI risks are described in the NIST AI RMF Playbook. Table 3 lists the MEASURE function’s categories and subcategories. 
Table 3: Categories and subcategories for the MEASURE function. 
Categories Subcategories 
MEASURE 1: 
Appropriate methods and metrics are identified and applied. 
MEASURE 1.1: Approaches and metrics for measurement of AI risks enumerated during the MAP function are selected for imple­mentation starting with the most significant AI risks. The risks or trustworthiness characteristics that will not – or cannot – be measured are properly documented. 
MEASURE 1.2: Appropriateness of AI metrics and effectiveness of existing controls are regularly assessed and updated, including reports of errors and potential impacts on affected communities. 
MEASURE 1.3: Internal experts who did not serve as front-line developers for the system and/or independent assessors are in­volved in regular assessments and updates. Domain experts, users, AI actors external to the team that developed or deployed the AI system, and affected communities are consulted in support of assessments as necessary per organizational risk tolerance. 
MEASURE 2: AI systems are evaluated for trustworthy characteristics. 
MEASURE 2.1: Test sets, metrics, and details about the tools used 
during TEVV are documented. MEASURE 2.2: Evaluations involving human subjects meet ap­plicable requirements (including human subject protection) and are representative of the relevant population. 
MEASURE 2.3: AI system performance or assurance criteria are measured qualitatively or quantitatively and demonstrated for conditions similar to deployment setting(s). Measures are documented. 
MEASURE 2.4: The functionality and behavior of the AI sys­tem and its components – as identified in the MAP function – are monitored when in production. 
MEASURE 2.5: The AI system to be deployed is demonstrated to be valid and reliable. Limitations of the generalizability be­yond the conditions under which the technology was developed are documented. 
Continued on next page 
Table 3: Categories and subcategories for the MEASURE function. (Continued) 
Categories Subcategories 
MEASURE 3: 
Mechanisms for tracking identified AI risks over time are in place. 
MEASURE 2.6: The AI system is evaluated regularly for safety risks – as identified in the MAP function. The AI system to be de­ployed is demonstrated to be safe, its residual negative risk does not exceed the risk tolerance, and it can fail safely, particularly if made to operate beyond its knowledge limits. Safety metrics re­flect system reliability and robustness, real-time monitoring, and response times for AI system failures. 
MEASURE 2.7: AI system security and resilience – as identified 
in the MAP function – are evaluated and documented. MEASURE 2.8: Risks associated with transparency and account­ability – as identified in the MAP function – are examined and documented. 
MEASURE 2.9: The AI model is explained, validated, and docu­mented, and AI system output is interpreted within its context – as identified in the MAP function – to inform responsible use and governance. 
MEASURE 2.10: Privacy risk of the AI system – as identified in 
the MAP function – is examined and documented. MEASURE 2.11: Fairness and bias – as identified in the MAP function – are evaluated and results are documented. 
MEASURE 2.12: Environmental impact and sustainability of AI model training and management activities – as identified in the MAP function – are assessed and documented. 
MEASURE 2.13: Effectiveness of the employed TEVV met­rics and processes in the MEASURE function are evaluated and documented. 
MEASURE 3.1: Approaches, personnel, and documentation are in place to regularly identify and track existing, unanticipated, and emergent AI risks based on factors such as intended and ac­tual performance in deployed contexts. 
MEASURE 3.2: Risk tracking approaches are considered for settings where AI risks are difficult to assess using currently available measurement techniques or where metrics are not yet available. 
Continued on next page 
Table 3: Categories and subcategories for the MEASURE function. (Continued) 
Categories Subcategories 
MEASURE 4: 
Feedback about efficacy of measurement is gathered and assessed. 
MEASURE 3.3: Feedback processes for end users and impacted communities to report problems and appeal system outcomes are established and integrated into AI system evaluation metrics. 
MEASURE 4.1: Measurement approaches for identifying AI risks are connected to deployment context(s) and informed through consultation with domain experts and other end users. Ap­proaches are documented. 
MEASURE 4.2: Measurement results regarding AI system trust­worthiness in deployment context(s) and across the AI lifecycle are informed by input from domain experts and relevant AI ac­tors to validate whether the system is performing consistently as intended. Results are documented. 
MEASURE 4.3: Measurable performance improvements or de­clines based on consultations with relevant AI actors, in­cluding affected communities, and field data about context-relevant risks and trustworthiness characteristics are identified and documented. 

5.4 Manage 
The MANAGE function entails allocating risk resources to mapped and measured risks on a regular basis and as defined by the GOVERN function. Risk treatment comprises plans to respond to, recover from, and communicate about incidents or events. 
Contextual information gleaned from expert consultation and input from relevant AI actors 
– established in GOVERN and carried out in MAP – is utilized in this function to decrease the likelihood of system failures and negative impacts. Systematic documentation practices established in GOVERN and utilized in MAP and MEASURE bolster AI risk management efforts and increase transparency and accountability. Processes for assessing emergent risks are in place, along with mechanisms for continual improvement. 
After completing the MANAGE function, plans for prioritizing risk and regular monitoring and improvement will be in place. Framework users will have enhanced capacity to man­age the risks of deployed AI systems and to allocate risk management resources based on assessed and prioritized risks. It is incumbent on Framework users to continue to apply the MANAGE function to deployed AI systems as methods, contexts, risks, and needs or expectations from relevant AI actors evolve over time. 
Practices related to managing AI risks are described in the NIST AI RMF Playbook. Table 4 lists the MANAGE function’s categories and subcategories. 
Table 4: Categories and subcategories for the MANAGE function. 
Categories Subcategories 
MANAGE 1: AI risks based on assessments and other analytical output from the MAP and MEASURE functions are prioritized, responded to, and managed. 
MANAGE 1.1: A determination is made as to whether the AI system achieves its intended purposes and stated objectives and whether its development or deployment should proceed. 
MANAGE 1.2: Treatment of documented AI risks is prioritized 
based on impact, likelihood, and available resources or methods. MANAGE 1.3: Responses to the AI risks deemed high priority, as identified by the MAP function, are developed, planned, and doc­umented. Risk response options can include mitigating, transfer­ring, avoiding, or accepting. 
MANAGE 1.4: Negative residual risks (defined as the sum of all unmitigated risks) to both downstream acquirers of AI systems and end users are documented. 
MANAGE 2: 
Strategies to maximize AI benefits and minimize negative impacts are planned, prepared, implemented, documented, and informed by input from relevant AI actors. 
MANAGE 2.1: Resources required to manage AI risks are taken into account – along with viable non-AI alternative systems, ap­proaches, or methods – to reduce the magnitude or likelihood of potential impacts. 
MANAGE 2.2: Mechanisms are in place and applied to sustain 
the value of deployed AI systems. MANAGE 2.3: Procedures are followed to respond to and recover from a previously unknown risk when it is identified. 
MANAGE 2.4: Mechanisms are in place and applied, and respon­sibilities are assigned and understood, to supersede, disengage, or deactivate AI systems that demonstrate performance or outcomes inconsistent with intended use. 
MANAGE 3: AI risks and benefits from third-party entities are managed. 
MANAGE 3.1: AI risks and benefits from third-party resources are regularly monitored, and risk controls are applied and documented. 
MANAGE 3.2: Pre-trained models which are used for develop­ment are monitored as part of AI system regular monitoring and maintenance. 
Continued on next page 
Table 4: Categories and subcategories for the MANAGE function. (Continued) 
Categories Subcategories 
MANAGE 4: Risk treatments, including response and recovery, and communication plans for the identified and measured AI risks are documented and monitored regularly. 
MANAGE 4.1: Post-deployment AI system monitoring plans are implemented, including mechanisms for capturing and eval­uating input from users and other relevant AI actors, appeal and override, decommissioning, incident response, recovery, and change management. 
MANAGE 4.2: Measurable activities for continual improvements are integrated into AI system updates and include regular engage­ment with interested parties, including relevant AI actors. 
MANAGE 4.3: Incidents and errors are communicated to relevant AI actors, including affected communities. Processes for track­ing, responding to, and recovering from incidents and errors are followed and documented. 


6. AI RMF Profiles 
AI RMF use-case profiles are implementations of the AI RMF functions, categories, and subcategories for a specific setting or application based on the requirements, risk tolerance, and resources of the Framework user: for example, an AI RMF hiring profile or an AI RMF fair housing profile. Profiles may illustrate and offer insights into how risk can be managed at various stages of the AI lifecycle or in specific sector, technology, or end-use applications. AI RMF profiles assist organizations in deciding how they might best manage AI risk that is well-aligned with their goals, considers legal/regulatory requirements and best practices, and reflects risk management priorities. 
AI RMF temporal profiles are descriptions of either the current state or the desired, target state of specific AI risk management activities within a given sector, industry, organization, or application context. An AI RMF Current Profile indicates how AI is currently being managed and the related risks in terms of current outcomes. A Target Profile indicates the outcomes needed to achieve the desired or target AI risk management goals. 
Comparing Current and Target Profiles likely reveals gaps to be addressed to meet AI risk management objectives. Action plans can be developed to address these gaps to fulfill outcomes in a given category or subcategory. Prioritization of gap mitigation is driven by the user’s needs and risk management processes. This risk-based approach also enables Framework users to compare their approaches with other approaches and to gauge the resources needed (e.g., staffing, funding) to achieve AI risk management goals in a cost-effective, prioritized manner. 
AI RMF cross-sectoral profiles cover risks of models or applications that can be used across use cases or sectors. Cross-sectoral profiles can also cover how to govern, map, measure, and manage risks for activities or business processes common across sectors such as the use of large language models, cloud-based services or acquisition. 
This Framework does not prescribe profile templates, allowing for flexibility in implemen­tation. 


Appendix A: Descriptions of AI Actor Tasks from Figures 2 and 3 
AI Design tasks are performed during the Application Context and Data and Input phases of the AI lifecycle in Figure 2. AI Design actors create the concept and objectives of AI systems and are responsible for the planning, design, and data collection and processing tasks of the AI system so that the AI system is lawful and fit-for-purpose. Tasks include ar­ticulating and documenting the system’s concept and objectives, underlying assumptions, context, and requirements; gathering and cleaning data; and documenting the metadata and characteristics of the dataset. AI actors in this category include data scientists, do­main experts, socio-cultural analysts, experts in the field of diversity, equity, inclusion, and accessibility, members of impacted communities, human factors experts (e.g., UX/UI design), governance experts, data engineers, data providers, system funders, product man­agers, third-party entities, evaluators, and legal and privacy governance. 
AI Development tasks are performed during the AI Model phase of the lifecycle in Figure 
2. AI Development actors provide the initial infrastructure of AI systems and are responsi­ble for model building and interpretation tasks, which involve the creation, selection, cali­bration, training, and/or testing of models or algorithms. AI actors in this category include machine learning experts, data scientists, developers, third-party entities, legal and privacy governance experts, and experts in the socio-cultural and contextual factors associated with the deployment setting. 
AI Deployment tasks are performed during the Task and Output phase of the lifecycle in Figure 2. AI Deployment actors are responsible for contextual decisions relating to how the AI system is used to assure deployment of the system into production. Related tasks include piloting the system, checking compatibility with legacy systems, ensuring regu­latory compliance, managing organizational change, and evaluating user experience. AI actors in this category include system integrators, software developers, end users, oper­ators and practitioners, evaluators, and domain experts with expertise in human factors, socio-cultural analysis, and governance. 
Operation and Monitoring tasks are performed in the Application Context/Operate and Monitor phase of the lifecycle in Figure 2. These tasks are carried out by AI actors who are responsible for operating the AI system and working with others to regularly assess system output and impacts. AI actors in this category include system operators, domain experts, AI designers, users who interpret or incorporate the output of AI systems, product developers, evaluators and auditors, compliance experts, organizational management, and members of the research community. 
Test, Evaluation, Verification, and Validation (TEVV) tasks are performed throughout the AI lifecycle. They are carried out by AI actors who examine the AI system or its components, or detect and remediate problems. Ideally, AI actors carrying out verification and validation tasks are distinct from those who perform test and evaluation actions. Tasks can be incorporated into a phase as early as design, where tests are planned in accordance with the design requirement. 
• 
TEVV tasks for design, planning, and data may center on internal and external vali­dation of assumptions for system design, data collection, and measurements relative to the intended context of deployment or application. 

• 
TEVV tasks for development (i.e., model building) include model validation and assessment. 

• 
TEVV tasks for deployment include system validation and integration in production, with testing, and recalibration for systems and process integration, user experience, and compliance with existing legal, regulatory, and ethical specifications. 

• 
TEVV tasks for operations involve ongoing monitoring for periodic updates, testing, and subject matter expert (SME) recalibration of models, the tracking of incidents or errors reported and their management, the detection of emergent properties and related impacts, and processes for redress and response. 


Human Factors tasks and activities are found throughout the dimensions of the AI life-cycle. They include human-centered design practices and methodologies, promoting the active involvement of end users and other interested parties and relevant AI actors, incor­porating context-specific norms and values in system design, evaluating and adapting end user experiences, and broad integration of humans and human dynamics in all phases of the AI lifecycle. Human factors professionals provide multidisciplinary skills and perspectives to understand context of use, inform interdisciplinary and demographic diversity, engage in consultative processes, design and evaluate user experience, perform human-centered evaluation and testing, and inform impact assessments. 
Domain Expert tasks involve input from multidisciplinary practitioners or scholars who provide knowledge or expertise in – and about – an industry sector, economic sector, con­text, or application area where an AI system is being used. AI actors who are domain experts can provide essential guidance for AI system design and development, and inter­pret outputs in support of work performed by TEVV and AI impact assessment teams. 
AI Impact Assessment tasks include assessing and evaluating requirements for AI system accountability, combating harmful bias, examining impacts of AI systems, product safety, liability, and security, among others. AI actors such as impact assessors and evaluators provide technical, human factor, socio-cultural, and legal expertise. 
Procurement tasks are conducted by AI actors with financial, legal, or policy management authority for acquisition of AI models, products, or services from a third-party developer, vendor, or contractor. 
Governance and Oversight tasks are assumed by AI actors with management, fiduciary, and legal authority and responsibility for the organization in which an AI system is de-
signed, developed, and/or deployed. Key AI actors responsible for AI governance include organizational management, senior leadership, and the Board of Directors. These actors are parties that are concerned with the impact and sustainability of the organization as a whole. 
Additional AI Actors 
Third-party entities include providers, developers, vendors, and evaluators of data, al­gorithms, models, and/or systems and related services for another organization or the or­ganization’s customers or clients. Third-party entities are responsible for AI design and development tasks, in whole or in part. By definition, they are external to the design, devel­opment, or deployment team of the organization that acquires its technologies or services. The technologies acquired from third-party entities may be complex or opaque, and risk tolerances may not align with the deploying or operating organization. 
End users of an AI system are the individuals or groups that use the system for specific purposes. These individuals or groups interact with an AI system in a specific context. End users can range in competency from AI experts to first-time technology end users. 
Affected individuals/communities encompass all individuals, groups, communities, or organizations directly or indirectly affected by AI systems or decisions based on the output of AI systems. These individuals do not necessarily interact with the deployed system or application. 
Other AI actors may provide formal or quasi-formal norms or guidance for specifying and managing AI risks. They can include trade associations, standards developing or­ganizations, advocacy groups, researchers, environmental groups, and civil society organizations. 
The general public is most likely to directly experience positive and negative impacts of AI technologies. They may provide the motivation for actions taken by the AI actors. This group can include individuals, communities, and consumers associated with the context in which an AI system is developed or deployed. 

Appendix B: How AI Risks Differ from Traditional Software Risks 
As with traditional software, risks from AI-based technology can be bigger than an en­terprise, span organizations, and lead to societal impacts. AI systems also bring a set of risks that are not comprehensively addressed by current risk frameworks and approaches. Some AI system features that present risks also can be beneficial. For example, pre-trained models and transfer learning can advance research and increase accuracy and resilience when compared to other models and approaches. Identifying contextual factors in the MAP function will assist AI actors in determining the level of risk and potential management efforts. 
Compared to traditional software, AI-specific risks that are new or increased include the following: 
• 
The data used for building an AI system may not be a true or appropriate representa­tion of the context or intended use of the AI system, and the ground truth may either not exist or not be available. Additionally, harmful bias and other data quality issues can affect AI system trustworthiness, which could lead to negative impacts. 

• 
AI system dependency and reliance on data for training tasks, combined with in­creased volume and complexity typically associated with such data. 

• 
Intentional or unintentional changes during training may fundamentally alter AI sys­tem performance. 

• 
Datasets used to train AI systems may become detached from their original and in­tended context or may become stale or outdated relative to deployment context. 

• 
AI system scale and complexity (many systems contain billions or even trillions of decision points) housed within more traditional software applications. 

• 
Use of pre-trained models that can advance research and improve performance can also increase levels of statistical uncertainty and cause issues with bias management, scientific validity, and reproducibility. 

• 
Higher degree of difficulty in predicting failure modes for emergent properties of large-scale pre-trained models. 

• 
Privacy risk due to enhanced data aggregation capability for AI systems. 

• 
AI systems may require more frequent maintenance and triggers for conducting cor­rective maintenance due to data, model, or concept drift. 

• 
Increased opacity and concerns about reproducibility. 

• 
Underdeveloped software testing standards and inability to document AI-based prac­tices to the standard expected of traditionally engineered software for all but the simplest of cases. 

• 
Difficulty in performing regular AI-based software testing, or determining what to test, since AI systems are not subject to the same controls as traditional code devel­opment. 

• 
Computational costs for developing AI systems and their impact on the environment and planet. 

• 
Inability to predict or detect the side effects of AI-based systems beyond statistical measures. 


Privacy and cybersecurity risk management considerations and approaches are applicable in the design, development, deployment, evaluation, and use of AI systems. Privacy and cybersecurity risks are also considered as part of broader enterprise risk management con­siderations, which may incorporate AI risks. As part of the effort to address AI trustworthi­ness characteristics such as “Secure and Resilient” and “Privacy-Enhanced,” organizations may consider leveraging available standards and guidance that provide broad guidance to organizations to reduce security and privacy risks, such as, but not limited to, the NIST Cy­bersecurity Framework, the NIST Privacy Framework, the NIST Risk Management Frame­work, and the Secure Software Development Framework. These frameworks have some features in common with the AI RMF. Like most risk management approaches, they are outcome-based rather than prescriptive and are often structured around a Core set of func­tions, categories, and subcategories. While there are significant differences between these frameworks based on the domain addressed – and because AI risk management calls for addressing many other types of risks – frameworks like those mentioned above may inform security and privacy considerations in the MAP, MEASURE, and MANAGE functions of the AI RMF. 
At the same time, guidance available before publication of this AI RMF does not compre­hensively address many AI system risks. For example, existing frameworks and guidance are unable to: 
• 
adequately manage the problem of harmful bias in AI systems; 

• 
confront the challenging risks related to generative AI; 

• 
comprehensively address security concerns related to evasion, model extraction, mem­bership inference, availability, or other machine learning attacks; 

• 
account for the complex attack surface of AI systems or other security abuses enabled by AI systems; and 

• 
consider risks associated with third-party AI technologies, transfer learning, and off-label use where AI systems may be trained for decision-making outside an organiza­tion’s security controls or trained in one domain and then “fine-tuned” for another. 


Both AI and traditional software technologies and systems are subject to rapid innovation. Technology advances should be monitored and deployed to take advantage of those devel­opments and work towards a future of AI that is both trustworthy and responsible. 

Appendix C: AI Risk Management and Human-AI Interaction 
Organizations that design, develop, or deploy AI systems for use in operational settings may enhance their AI risk management by understanding current limitations of human-AI interaction. The AI RMF provides opportunities to clearly define and differentiate the various human roles and responsibilities when using, interacting with, or managing AI systems. 
Many of the data-driven approaches that AI systems rely on attempt to convert or represent individual and social observational and decision-making practices into measurable quanti­ties. Representing complex human phenomena with mathematical models can come at the cost of removing necessary context. This loss of context may in turn make it difficult to understand individual and societal impacts that are key to AI risk management efforts. 
Issues that merit further consideration and research include: 
1. 
Human roles and responsibilities in decision making and overseeing AI systems need to be clearly defined and differentiated. Human-AI configurations can span from fully autonomous to fully manual. AI systems can autonomously make deci­sions, defer decision making to a human expert, or be used by a human decision maker as an additional opinion. Some AI systems may not require human oversight, such as models used to improve video compression. Other systems may specifically require human oversight. 

2. 
Decisions that go into the design, development, deployment, evaluation, and use of AI systems reflect systemic and human cognitive biases. AI actors bring their cognitive biases, both individual and group, into the process. Biases can stem from end-user decision-making tasks and be introduced across the AI lifecycle via human assumptions, expectations, and decisions during design and modeling tasks. These biases, which are not necessarily always harmful, may be exacerbated by AI system opacity and the resulting lack of transparency. Systemic biases at the organizational level can influence how teams are structured and who controls the decision-making processes throughout the AI lifecycle. These biases can also influence downstream decisions by end users, decision makers, and policy makers and may lead to negative impacts. 

3. 
Human-AI interaction results vary. Under certain conditions – for example, in perceptual-based judgment tasks – the AI part of the human-AI interaction can am­plify human biases, leading to more biased decisions than the AI or human alone. When these variations are judiciously taken into account in organizing human-AI teams, however, they can result in complementarity and improved overall perfor­mance. 

4. 
Presenting AI system information to humans is complex. Humans perceive and derive meaning from AI system output and explanations in different ways, reflecting different individual preferences, traits, and skills. 


The GOVERN function provides organizations with the opportunity to clarify and define the roles and responsibilities for the humans in the Human-AI team configurations and those who are overseeing the AI system performance. The GOVERN function also creates mechanisms for organizations to make their decision-making processes more explicit, to help counter systemic biases. 
The MAP function suggests opportunities to define and document processes for operator and practitioner proficiency with AI system performance and trustworthiness concepts, and to define relevant technical standards and certifications. Implementing MAP function cat­egories and subcategories may help organizations improve their internal competency for analyzing context, identifying procedural and system limitations, exploring and examining impacts of AI-based systems in the real world, and evaluating decision-making processes throughout the AI lifecycle. 
The GOVERN and MAP functions describe the importance of interdisciplinarity and demo­graphically diverse teams and utilizing feedback from potentially impacted individuals and communities. AI actors called out in the AI RMF who perform human factors tasks and activities can assist technical teams by anchoring in design and development practices to user intentions and representatives of the broader AI community, and societal values. These actors further help to incorporate context-specific norms and values in system design and evaluate end user experiences – in conjunction with AI systems. 
AI risk management approaches for human-AI configurations will be augmented by on­going research and evaluation. For example, the degree to which humans are empowered and incentivized to challenge AI system output requires further studies. Data about the fre­quency and rationale with which humans overrule AI system output in deployed systems may be useful to collect and analyze. 

Appendix D: Attributes of the AI RMF 
NIST described several key attributes of the AI RMF when work on the Framework first began. These attributes have remained intact and were used to guide the AI RMF’s devel­opment. They are provided here as a reference. 
The AI RMF strives to: 
1. 
Be risk-based, resource-efficient, pro-innovation, and voluntary. 

2. 
Be consensus-driven and developed and regularly updated through an open, trans­parent process. All stakeholders should have the opportunity to contribute to the AI RMF’s development. 

3. 
Use clear and plain language that is understandable by a broad audience, including senior executives, government officials, non-governmental organization leadership, and those who are not AI professionals – while still of sufficient technical depth to be useful to practitioners. The AI RMF should allow for communication of AI risks across an organization, between organizations, with customers, and to the public at large. 

4. 
Provide common language and understanding to manage AI risks. The AI RMF should offer taxonomy, terminology, definitions, metrics, and characterizations for AI risk. 

5. 
Be easily usable and fit well with other aspects of risk management. Use of the Framework should be intuitive and readily adaptable as part of an organization’s broader risk management strategy and processes. It should be consistent or aligned with other approaches to managing AI risks. 

6. 
Be useful to a wide range of perspectives, sectors, and technology domains. The AI RMF should be universally applicable to any AI technology and to context-specific use cases. 

7. 
Be outcome-focused and non-prescriptive. The Framework should provide a catalog of outcomes and approaches rather than prescribe one-size-fits-all requirements. 

8. 
Take advantage of and foster greater awareness of existing standards, guidelines, best practices, methodologies, and tools for managing AI risks – as well as illustrate the need for additional, improved resources. 

9. 
Be law-and regulation-agnostic. The Framework should support organizations’ abilities to operate under applicable domestic and international legal or regulatory regimes. 

10. 
Be a living document. The AI RMF should be readily updated as technology, under­standing, and approaches to AI trustworthiness and uses of AI change and as stake­holders learn from implementing AI risk management generally and this framework in particular. 


Page 42 
This publication is available free of charge from: https://doi.org/10.6028/NIST.AI.100-1 


############ NIST RMF Playbook ###########


,GOVERN 1.1,GOVERN 1.2,GOVERN 1.3,GOVERN 1.4,GOVERN 1.5,GOVERN 1.6,GOVERN 1.7,GOVERN 2.1,GOVERN 2.2,GOVERN 2.3,GOVERN 3.1,GOVERN 3.2,GOVERN 4.1,GOVERN 4.2,GOVERN 4.3,GOVERN 5.1,GOVERN 5.2,GOVERN 6.1,GOVERN 6.2,MANAGE 1.1,MANAGE 1.2,MANAGE 1.3,MANAGE 1.4,MANAGE 2.1,MANAGE 2.2,MANAGE 2.3,MANAGE 2.4,MANAGE 3.1,MANAGE 3.2,MANAGE 4.1,MANAGE 4.2,MANAGE 4.3,MAP 1.1,MAP 1.2,MAP 1.3,MAP 1.4,MAP 1.5,MAP 1.6,MAP 2.1,MAP 2.2,MAP 2.3,MAP 3.1,MAP 3.2,MAP 3.3,MAP 3.4,MAP 3.5,MAP 4.1,MAP 4.2,MAP 5.1,MAP 5.2,MEASURE 1.1,MEASURE 1.2,MEASURE 1.3,MEASURE 2.1,MEASURE 2.2,MEASURE 2.3,MEASURE 2.4,MEASURE 2.5,MEASURE 2.6,MEASURE 2.7,MEASURE 2.8,MEASURE 2.9,MEASURE 2.10,MEASURE 2.11,MEASURE 2.12,MEASURE 2.13,MEASURE 3.1,MEASURE 3.2,MEASURE 3.3,MEASURE 4.1,MEASURE 4.2,MEASURE 4.3
section_about,"AI systems may be subject to specific applicable legal and regulatory requirements. Some legal requirements can mandate (e.g., nondiscrimination, data privacy and security controls) documentation, disclosure, and increased AI system transparency. These requirements are complex and may not be applicable or differ across applications and contexts. 
 
For example, AI system testing processes for bias measurement, such as disparate impact, are not applied uniformly within the legal context. Disparate impact is broadly defined as a facially neutral policy or practice that disproportionately harms a group based on a protected trait. Notably, some modeling algorithms or debiasing techniques that rely on demographic information, could also come into tension with legal prohibitions on disparate treatment (i.e., intentional discrimination).

Additionally, some intended users of AI systems may not have consistent or reliable access to fundamental internet technologies (a phenomenon widely described as the “digital divide”) or may experience difficulties interacting with AI systems due to disabilities or impairments. Such factors may mean different communities experience bias or other negative impacts when trying to access AI systems. Failure to address such design issues may pose legal risks, for example in employment related activities affecting persons with disabilities.","Policies, processes, and procedures are central components of effective AI risk management and fundamental to individual and organizational accountability. All stakeholders benefit from policies, processes, and procedures which require preventing harm by design and default. 

Organizational policies and procedures will vary based on available resources and risk profiles, but can help systematize AI actor roles and responsibilities throughout the AI lifecycle. Without such policies, risk management can be subjective across the organization, and exacerbate rather than minimize risks over time.  Polices, or summaries thereof, are understandable to relevant AI actors. Policies reflect an understanding of the underlying metrics, measurements, and tests that are necessary to support policy and AI system design, development, deployment and use.

Lack of clear information about responsibilities and chains of command will limit the effectiveness of risk management.","Risk management resources are finite in any organization. Adequate AI governance policies delineate the mapping, measurement, and prioritization of risks to allocate resources toward the most material issues for an AI system to ensure effective risk management. Policies may specify systematic processes for assigning mapped and measured risks to standardized risk scales. 

AI risk tolerances  range from negligible to critical – from, respectively, almost no risk to risks that can result in irredeemable human, reputational, financial, or environmental losses. Risk tolerance rating policies consider different sources of risk, (e.g., financial, operational, safety and wellbeing, business, reputational, or model risks). A typical risk measurement approach entails the multiplication, or qualitative combination, of measured or estimated impact and likelihood of impacts into a risk score (risk ˜ impact x likelihood). This score is then placed on a risk scale. Scales for risk may be qualitative, such as red-amber-green (RAG), or may entail simulations or econometric approaches. Impact assessments are a common tool for understanding the severity of mapped risks. In the most fulsome AI risk management approaches, all models are assigned to a risk level.","Clear policies and procedures relating to documentation and transparency facilitate and enhance efforts  to communicate roles and responsibilities for the Map, Measure and Manage functions across the AI lifecycle. Standardized documentation can help organizations systematically integrate AI risk management processes and enhance accountability efforts. For example, by adding their contact information to a work product document, AI actors can improve communication, increase ownership of work products, and potentially enhance consideration of product quality. Documentation may generate downstream benefits related to improved system replicability and robustness. Proper documentation storage and access procedures allow for quick retrieval of critical information during a negative incident. Explainable machine learning efforts (models and explanatory methods) may bolster technical documentation practices by introducing additional information for review and interpretation by AI Actors.","AI systems are dynamic and may perform in unexpected ways once deployed or after deployment. Continuous monitoring is a risk management process for tracking unexpected issues and performance changes, in real-time or at a specific frequency, across the AI system lifecycle.

Incident response and “appeal and override” are commonly used processes in information technology management. These processes enable real-time flagging of potential incidents, and human adjudication of system outcomes.

Establishing and maintaining incident response plans can reduce the likelihood of additive impacts during an AI incident. Smaller organizations which may not have fulsome governance programs, can utilize incident response plans for addressing system failures, abuse or misuse.","An AI system inventory is an organized database of artifacts relating to an AI system or model. It may include system documentation, incident response plans, data dictionaries, links to implementation software or source code, names and contact information for relevant AI actors, or other information that may be helpful for model or system maintenance and incident response purposes. AI system inventories also enable a holistic view of organizational AI assets. A serviceable AI system inventory may allow for the quick resolution of:

- specific queries for single models, such as  “when was this model last refreshed?” 
- high-level queries across all models, such as, “how many models are currently deployed within our organization?” or “how many users are impacted by our models?” 

AI system inventories are a common element of traditional model risk management approaches and can provide technical, business and risk management benefits. Typically inventories capture all organizational models or systems, as partial inventories may not provide the value of a full inventory.","Irregular or indiscriminate termination or deletion of models or AI systems may be inappropriate and increase organizational risk. For example, AI systems may be subject to regulatory requirements or implicated in future security or legal investigations. To maintain trust, organizations may consider establishing policies and processes for the systematic and deliberate decommissioning of AI systems. Typically, such policies consider user and community concerns, risks in dependent and linked systems, and security, legal or regulatory concerns. Decommissioned models or systems may be stored in a model inventory along with active models,  for an established length  of time.","The development of a risk-aware organizational culture starts with defining responsibilities. For example, under some risk management structures, professionals carrying out test and evaluation  tasks are independent from AI system developers and report through risk management functions or directly to executives.  This kind of structure may help counter implicit biases such as groupthink or sunk cost fallacy and bolster risk management functions, so efforts are not  easily bypassed or ignored.

Instilling a culture where AI system design and implementation decisions can be questioned and course- corrected by empowered AI actors can enhance organizations’ abilities to anticipate and effectively manage risks before they become ingrained.","To enhance AI risk management adoption and effectiveness, organizations are encouraged to identify and integrate appropriate training curricula into enterprise learning requirements. Through regular training, AI actors can maintain awareness of:

- AI risk management goals and their role in achieving them.
- Organizational policies, applicable laws and regulations, and industry best practices and norms.

See [MAP 3.4]() and [3.5]() for additional relevant information.","Senior leadership and members of the C-Suite in organizations that maintain an AI portfolio, should maintain awareness of AI risks, affirm the organizational appetite for such risks, and be responsible for managing those risks..

Accountability ensures that a specific team and individual is responsible for AI risk management efforts. Some organizations grant authority and resources (human and budgetary) to a designated officer who ensures adequate performance of the institution’s AI portfolio (e.g. predictive modeling, machine learning).","A diverse team that includes AI actors with diversity of experience, disciplines, and backgrounds to enhance organizational capacity and capability for anticipating risks is better equipped to carry out risk management. Consultation with external personnel may be necessary when internal teams lack a diverse range of lived experiences or disciplinary expertise.

To extend the benefits of diversity, equity, and inclusion to both the users and AI actors, it is recommended that teams are composed of a diverse group of individuals who reflect a range of backgrounds, perspectives and expertise.

Without commitment from senior leadership, beneficial aspects of team diversity and inclusion can be overridden by unstated organizational incentives that inadvertently conflict with the broader values of a diverse workforce.","Identifying and managing AI risks and impacts are enhanced when a broad set of perspectives and actors across the AI lifecycle, including technical, legal, compliance, social science, and human factors expertise is engaged. AI actors include those who operate, use, or interact with AI systems for downstream tasks, or monitor AI system performance. Effective risk management efforts include:

- clear definitions and differentiation of the various human roles and responsibilities for AI system oversight and governance
- recognizing and clarifying differences between AI system overseers and those using or interacting with AI systems.","A risk culture and accompanying practices can help organizations effectively triage the most critical risks. Organizations in some industries implement three (or more) “lines of defense,” where separate teams are held accountable for different aspects of the system lifecycle, such as development, risk management, and auditing. While a traditional three-lines approach may be impractical for smaller organizations, leadership can commit to cultivating a strong risk culture through other means. For example, “effective challenge,” is a culture- based practice that encourages critical thinking and questioning of important design and implementation decisions by experts with the authority and stature to make such changes.

Red-teaming is another risk measurement and management approach. This practice consists of adversarial testing of AI systems under stress conditions to seek out failure modes or vulnerabilities in the system. Red-teams are composed of external experts or personnel who are independent from internal AI actors.","Impact assessments are one approach for driving responsible technology development practices. And, within a specific use case, these assessments can provide a high-level structure for organizations to frame risks of a given algorithm or deployment. Impact assessments can also serve as a mechanism for organizations to articulate risks and generate documentation for managing and oversight activities when harms do arise.

Impact assessments may:

- be applied at the beginning of a process but also iteratively and regularly since goals and outcomes can evolve over time. 
- include perspectives from AI actors, including operators, users, and potentially impacted communities (including historically marginalized communities, those with disabilities, and individuals impacted by the digital divide), 
- assist in “go/no-go” decisions for an AI system. 
- consider conflicts of interest, or undue influence, related to the organizational team being assessed.

See the MAP function playbook guidance for more information relating to impact assessments.","Identifying AI system limitations, detecting and tracking negative impacts and incidents, and sharing information about these issues with appropriate AI actors will improve risk management. Issues such as concept drift, AI bias and discrimination, shortcut learning or underspecification are difficult to identify using current standard AI testing processes. Organizations can institute in-house use and testing policies and procedures to identify and manage such issues. Efforts can take the form of pre-alpha or pre-beta testing, or deploying internally developed systems or products within the organization. Testing may entail limited and controlled in-house, or publicly available, AI system testbeds, and accessibility of AI system interfaces and outputs.

Without policies and procedures that enable consistent testing practices, risk management efforts may be bypassed or ignored, exacerbating risks or leading to inconsistent risk management activities.

Information sharing about impacts or incidents detected during testing or deployment can:

* draw attention to AI system risks, failures, abuses or misuses, 
* allow organizations to benefit from insights based on a wide range of AI applications and implementations, and 
* allow organizations to be more proactive in avoiding known failure modes.

Organizations may consider sharing incident information with the AI Incident Database, the AIAAIC, users, impacted communities, or with traditional cyber vulnerability databases, such as the MITRE CVE list.","Beyond internal and laboratory-based system testing, organizational policies and practices may consider AI system fitness-for-purpose related to the intended context of use.

Participatory stakeholder engagement is one type of qualitative activity to help AI actors answer questions such as whether to pursue a project or how to design with impact in mind. This type of feedback, with domain expert input, can also assist AI actors to identify emergent scenarios and risks in certain AI applications. The consideration of when and how to convene a group and the kinds of individuals, groups, or community organizations to include is an iterative process connected to the system's purpose and its level of risk. Other factors relate to how to collaboratively and respectfully capture stakeholder feedback and insight that is useful, without being a solely perfunctory exercise.

These activities are best carried out by personnel with expertise in participatory practices, qualitative methods, and translation of contextual feedback for technical audiences.

Participatory engagement is not a one-time exercise and is best carried out from the very beginning of AI system commissioning through the end of the lifecycle. Organizations can consider how to incorporate engagement when beginning a project and as part of their monitoring of systems. Engagement is often utilized as a consultative practice, but this perspective may inadvertently lead to “participation washing.” Organizational transparency about the purpose and goal of the engagement can help mitigate that possibility.

Organizations may also consider targeted consultation with subject matter experts as a complement to participatory findings. Experts may assist internal staff in identifying and conceptualizing potential negative impacts that were previously not considered.","Organizational policies and procedures that equip AI actors with the processes, knowledge, and expertise needed to inform collaborative decisions about system deployment improve risk management. These decisions are closely tied to AI systems and organizational risk tolerance.

Risk tolerance, established by organizational leadership, reflects the level and type of risk the organization will accept while conducting its mission and carrying out its strategy. When risks arise, resources are allocated based on the assessed risk of a given AI system. Organizations typically apply a risk tolerance approach where higher risk systems receive larger allocations of risk management resources and lower risk systems receive less resources.","Risk measurement and management can be complicated by how customers use or integrate third-party data or systems into AI products or services, particularly without sufficient internal governance structures and technical safeguards. 

Organizations usually engage multiple third parties for external expertise, data, software packages (both open source and commercial), and software and hardware platforms across the AI lifecycle. This engagement has beneficial uses and can increase complexities of risk management efforts.

Organizational approaches to managing third-party (positive and negative) risk may be tailored to the resources, risk profile, and use case for each system. Organizations can apply governance approaches to third-party AI systems and data as they would for internal resources — including open source software, publicly available data, and commercially available models.","To mitigate the potential harms of third-party system failures, organizations may implement policies and procedures that include redundancies for covering third-party functions.","AI systems may not necessarily be the right solution for a given business task or problem. A standard risk management practice is to formally weigh an AI system’s negative risks against its benefits, and to determine if the AI system is an  appropriate solution. Tradeoffs among trustworthiness characteristics —such as deciding to deploy a system based on system performance vs system transparency–may require regular assessment throughout the AI lifecycle.","Risk refers to the composite measure of an event’s probability of occurring and the magnitude (or degree) of the consequences of the corresponding events. The impacts, or consequences, of AI systems can be positive, negative, or both and can result in opportunities or risks.  

Organizational risk tolerances are often informed by several internal and external factors, including existing industry practices, organizational values, and legal or regulatory requirements. Since risk management resources are often limited, organizations usually assign them based on risk tolerance. AI risks that are deemed more serious receive more oversight attention and risk management resources.","Outcomes from GOVERN-1, MAP-5 and MEASURE-2, can be used to address and document identified risks based on established risk tolerances. Organizations can follow existing regulations and guidelines for risk criteria, tolerances and responses established by organizational, domain, discipline, sector, or professional requirements. In lieu of such guidance, organizations can develop risk response plans based on strategies such as accepted model risk management, enterprise risk management, and information sharing and disclosure practices.","Organizations may choose to accept or transfer some of the documented risks  from MAP and MANAGE 1.3 and 2.1.  Such risks, known as residual risk, may affect downstream AI actors such as those engaged in system procurement or use. Transparent monitoring and managing residual risks enables cost benefit analysis and the examination of potential values of AI systems versus its potential negative impacts.","Organizational risk response may entail identifying and analyzing alternative approaches, methods, processes or systems, and balancing tradeoffs between trustworthiness characteristics and how they relate to organizational principles and societal values. Analysis of these tradeoffs is informed by consulting with interdisciplinary organizational teams, independent domain experts, and engaging with individuals or community groups. These processes require sufficient resource allocation.","System performance and trustworthiness may evolve and shift over time, once an AI system is deployed and put into operation. This phenomenon, generally known as drift, can degrade the value of the AI system to the organization and increase the likelihood of negative impacts.  Regular monitoring of AI systems’ performance and trustworthiness enhances organizations’ ability to detect and respond to drift, and thus sustain an AI system’s value once deployed. Processes and mechanisms for regular monitoring address system functionality and behavior - as well as impacts and alignment with the values and norms within the specific context of use. For example, considerations regarding impacts on personal or public safety or privacy may include limiting high speeds when operating autonomous vehicles or restricting illicit content recommendations for minors. 

Regular monitoring activities can enable organizations to systematically and proactively identify emergent risks and respond according to established protocols and metrics.  Options for organizational responses include 1) avoiding the risk, 2)accepting the risk, 3) mitigating the risk, or 4) transferring the risk. Each of these actions require planning and resources. Organizations are encouraged to establish risk management protocols with consideration of the trustworthiness characteristics, the deployment context, and real world impacts.","AI systems – like any technology – can demonstrate non-functionality or failure or unexpected and unusual behavior. They also can be subject to attacks, incidents, or other misuse or abuse – which their sources are not always known apriori. Organizations can establish, document, communicate and maintain treatment procedures to recognize and counter, mitigate and manage risks that were not previously identified.","Performance inconsistent with intended use does not always increase risk or lead to negative impacts. Rigorous TEVV practices are useful for protecting against negative impacts regardless of intended use. When negative impacts do arise, superseding (bypassing), disengaging, or deactivating/decommissioning a model, AI system component(s), or the entire AI system may be necessary, such as when: 

- a system reaches the end of its lifetime
- detected or identified risks exceed tolerance thresholds
- adequate system mitigation actions are beyond the organization’s capacity
- feasible system mitigation actions do not meet regulatory, legal, norms or standards. 
- impending risk is detected during continual monitoring, for which feasible mitigation cannot be identified or implemented in a timely fashion. 

Safely removing AI systems from operation, either temporarily or permanently, under these scenarios requires standard protocols that minimize operational disruption and downstream negative impacts. Protocols can involve redundant or backup systems that are developed in alignment with established system governance policies (see GOVERN 1.7), regulatory compliance, legal frameworks, business requirements and norms and l standards within the application context of use. Decision thresholds and metrics for actions to bypass or deactivate system components are part of continual monitoring procedures. Incidents that result in a bypass/deactivate decision require documentation and review to understand root causes, impacts, and potential opportunities for mitigation and redeployment. Organizations are encouraged to develop risk and change management protocols that consider and anticipate upstream and downstream consequences of both temporary and/or permanent decommissioning, and provide contingency options.","AI systems may depend on external resources and associated processes, including third-party data, software or hardware systems. Third parties’ supplying organizations with components and services, including tools, software, and expertise for AI system design, development, deployment or use can improve efficiency and scalability. It can also increase complexity and opacity, and, in-turn, risk. Documenting third-party technologies, personnel, and resources that were employed can help manage risks. Focusing first and foremost on risks involving physical safety, legal liabilities, regulatory compliance, and negative impacts on individuals, groups, or society is recommended.","A common approach in AI development is transfer learning, whereby an existing pre-trained model is adapted for use in a different, but related application. AI actors in development tasks often use pre-trained models from third-party entities for tasks such as image classification, language prediction, and entity recognition, because the resources to build such models may not be readily available to most organizations. Pre-trained models are typically trained to address various classification or prediction problems, using exceedingly large datasets and computationally intensive resources. The use of pre-trained models can make it difficult to anticipate negative system outcomes or impacts. Lack of documentation or transparency tools increases the difficulty and general complexity when deploying pre-trained models and hinders root cause analyses.","AI system performance and trustworthiness can change due to a variety of factors. Regular AI system monitoring can help deployers identify performance degradations, adversarial attacks, unexpected and unusual behavior, near-misses, and impacts. Including pre- and post-deployment external feedback about AI system performance can enhance organizational awareness about positive and negative impacts, and reduce the time to respond to risks and harms.","Regular monitoring processes enable system updates to enhance performance and functionality in accordance with regulatory and legal frameworks, and organizational and contextual values and norms. These processes also facilitate analyses of root causes, system degradation, drift, near-misses, and failures, and incident response and documentation. 

AI actors across the lifecycle have many opportunities to capture and incorporate external feedback about system performance, limitations, and impacts, and implement continuous improvements. Improvements may not always be to model pipeline or system processes, and may instead be based on metrics beyond accuracy or other quality performance measures. In these cases, improvements may entail adaptations to business or organizational procedures or practices. Organizations are encouraged to develop improvements that will maintain traceability and transparency for developers, end users, auditors, and relevant AI actors.","Regularly documenting an accurate and transparent account of identified and reported errors can enhance AI risk management activities., Examples include:

- how errors were identified, 
- incidents related to the error, 
- whether the error has been repaired, and
- how repairs can be distributed to all impacted stakeholders and users.","Highly accurate and optimized systems can cause harm. Relatedly, organizations should expect broadly deployed AI tools to be reused, repurposed, and potentially misused regardless of intentions. 

AI actors can work collaboratively, and with external parties such as community groups, to help delineate the bounds of acceptable deployment, consider preferable alternatives, and identify principles and strategies to manage likely risks. Context mapping is the first step in this effort, and may include examination of the following: 

* intended purpose and impact of system use. 
* concept of operations. 
* intended, prospective, and actual deployment setting. 
* requirements for system deployment and operation. 
* end user and operator expectations. 
* specific set or types of end users. 
* potential negative impacts to individuals, groups, communities, organizations, and society – or context-specific impacts such as legal requirements or impacts to the environment. 
* unanticipated, downstream, or other unknown contextual factors.
* how AI system changes connect to impacts. 

These types of processes can assist AI actors in understanding how limitations, constraints, and other realities associated with the deployment and use of AI technology can create impacts once they are deployed or operate in the real world. When coupled with the enhanced organizational culture resulting from the established policies and procedures in the Govern function, the Map function can provide opportunities to foster and instill new perspectives, activities, and skills for approaching risks and impacts. 

Context mapping also includes discussion and consideration of non-AI or  non-technology alternatives especially as related to whether the given context is narrow enough to manage AI and its potential negative impacts. Non-AI alternatives may include capturing and evaluating information using semi-autonomous or mostly-manual methods.","Successfully mapping context requires a team of AI actors with a diversity of experience, expertise, abilities and backgrounds, and with the resources and independence to engage in critical inquiry.

Having a diverse team contributes to more  broad and open sharing of ideas and assumptions about the purpose and function of the technology being designed and developed – making these implicit aspects more explicit. The benefit of a diverse staff in managing AI risks is not the beliefs or presumed beliefs of individual workers, but the behavior that results from a collective perspective. An environment which fosters critical inquiry creates opportunities to surface problems and identify existing and emergent risks.","Defining and documenting the specific business purpose of an AI system in a broader context of societal values helps teams to evaluate risks and increases the clarity of “go/no-go” decisions about whether to deploy.

Trustworthy AI technologies may present a demonstrable business benefit beyond implicit or explicit costs, provide added value, and don't lead to wasted resources. Organizations can feel confident in performing risk avoidance if the implicit or explicit risks outweigh the advantages of AI systems,  and  not implementing an AI solution whose risks surpass potential benefits.

For example, making AI systems more equitable can result in better managed risk, and can help enhance consideration of the business value of making inclusively designed, accessible and more equitable AI systems.","Socio-technical AI risks emerge from the interplay between technical development decisions and how a system is used, who operates it, and the social context into which it is deployed. Addressing these risks is complex and requires a commitment to understanding how contextual factors may interact with AI lifecycle actions. One such contextual factor is how organizational mission and identified system purpose create incentives within AI system design, development, and deployment tasks that may result in positive and negative impacts. By establishing comprehensive and explicit enumeration of AI systems’ context of of business use and expectations, organizations can identify and manage these types of risks.","Risk tolerance reflects the level and type of risk the organization is willing to accept while conducting its mission and carrying out its strategy.

Organizations can follow existing regulations and guidelines for risk criteria, tolerance and response established by organizational, domain, discipline, sector, or professional requirements. Some sectors or industries may have established definitions of harm or may have established documentation, reporting, and disclosure requirements. 

Within sectors, risk management may depend on existing guidelines for specific applications and use case settings. Where established guidelines do not exist, organizations will want to define reasonable risk tolerance in consideration of different sources of risk (e.g., financial, operational, safety and wellbeing, business, reputational, and model risks) and different levels of risk (e.g., from negligible to critical).

Risk tolerances inform and support decisions about whether to continue with development or deployment - termed “go/no-go”. Go/no-go decisions related to AI system risks can take stakeholder feedback into account, but remain independent from stakeholders’ vested financial or reputational interests.

If mapping risk is prohibitively difficult, a ""no-go"" decision may be considered for the specific system.","AI system development requirements may outpace documentation processes for traditional software. When written requirements are unavailable or incomplete, AI actors may inadvertently overlook business and stakeholder needs, over-rely on implicit human biases such as confirmation bias and groupthink, and maintain exclusive focus on computational requirements. 

Eliciting system requirements, designing for end users, and considering societal impacts early in the design phase is a priority that can enhance AI systems’ trustworthiness.","AI actors define the technical learning or decision-making task(s) an AI system is designed to accomplish, or the benefits that the system will provide. The clearer and narrower the task definition, the easier it is to map its benefits and risks, leading to more fulsome risk management.","An AI lifecycle consists of many interdependent activities involving a diverse set of actors that often do not have full visibility or control over other parts of the lifecycle and its associated contexts or risks. The interdependencies between these activities, and among the relevant AI actors and organizations, can make it difficult to reliably anticipate potential impacts of AI systems. For example, early decisions in identifying the purpose and objective of an AI system can alter its behavior and capabilities, and the dynamics of deployment setting (such as end users or impacted individuals) can shape the positive or negative impacts of AI system decisions. As a result, the best intentions within one dimension of the AI lifecycle can be undermined via interactions with decisions and conditions in other, later activities. This complexity and varying levels of visibility can introduce uncertainty. And, once deployed and in use, AI systems may sometimes perform poorly, manifest unanticipated negative impacts, or violate legal or ethical norms. These risks and incidents can result from a variety of factors. For example, downstream decisions can be influenced by end user over-trust or under-trust, and other complexities related to AI-supported decision-making.

Anticipating, articulating, assessing and documenting AI systems’ knowledge limits and how system output may be utilized and overseen by humans can help mitigate the uncertainty associated with the realities of AI system deployments. Rigorous design processes include defining system knowledge limits, which are confirmed and refined based on TEVV processes.","Standard testing and evaluation protocols provide a basis to confirm assurance in a system that it is operating as designed and claimed. AI systems’ complexities create challenges for traditional testing and evaluation methodologies, which tend to be designed for static or isolated system performance.  Opportunities for risk continue well beyond design and deployment, into system operation and application of system-enabled decisions. Testing and evaluation methodologies and metrics therefore address a continuum of activities. TEVV is enhanced when key metrics for performance, safety, and reliability are interpreted in a socio-technical context and not confined to the boundaries of the AI system pipeline. 

Other challenges for managing AI risks relate to dependence on large scale datasets, which can impact data quality and validity concerns. The difficulty of finding the “right” data may lead AI actors to select datasets based more on accessibility and availability than on suitability for operationalizing the phenomenon that the AI system intends to support or inform. Such decisions could contribute to an environment where the data used in processes is not fully representative of the populations or phenomena that are being modeled, introducing downstream risks.  Practices such as dataset reuse may also lead to disconnect from the social contexts and time periods of their creation.  This contributes to issues of validity of the underlying dataset for providing proxies, measures, or predictors within the model.","AI systems have enormous potential to improve quality of life, enhance economic prosperity and security costs. Organizations are encouraged to define and document system purpose and utility, and its potential positive impacts. benefits beyond current known performance benchmarks.

It is encouraged that risk management and assessment of benefits and impacts include processes for regular and meaningful communication with potentially affected groups and communities. These stakeholders can provide valuable input related to systems’ benefits and possible limitations. Organizations may differ in the types and number of stakeholders with which they engage.

Other approaches such as human-centered design (HCD) and value-sensitive design (VSD) can help AI teams to engage broadly with individuals and communities. This type of engagement can enable AI teams to learn about how a given technology may cause positive or negative impacts, that were not originally considered or intended.","Anticipating negative impacts of AI systems is a difficult task. Negative impacts can be due to many factors, such as system non-functionality or use outside of its operational limits, and may range from minor annoyance to serious injury, financial losses, or regulatory enforcement actions. AI actors can work with a broad set of stakeholders to improve their capacity for understanding  systems’ potential impacts – and subsequently – systems’ risks.","Systems that function in a narrow scope tend to enable better mapping, measurement, and management of risks in the learning or decision-making tasks and the system context. A narrow application scope also helps ease TEVV functions and related resources within an organization.

For example, large language models or open-ended chatbot systems that interact with the public on the internet have a large number of risks that may be difficult to map, measure, and manage due to the variability from both the decision-making task and the operational context. Instead, a task-specific chatbot utilizing templated responses that follow a defined “user journey” is a scope that can be more easily mapped, measured and managed.","Human-AI configurations can span from fully autonomous to fully manual. AI systems can autonomously make decisions, defer decision-making to a human expert, or be used by a human decision-maker as an additional opinion. In some scenarios, professionals with expertise in a specific domain work in conjunction with an AI system towards a specific end goal—for example, a decision about another individual(s). Depending on the purpose of the system, the expert may interact with the AI system but is rarely part of the design or development of the system itself. These experts are not necessarily familiar with machine learning, data science, computer science, or other fields traditionally associated with AI design or development and - depending on the application - will likely not require such familiarity. For example, for AI systems that are deployed in health care delivery the experts are the physicians and bring their expertise about medicine—not data science, data modeling and engineering, or other computational factors. The challenge in these settings is not educating the end user about AI system capabilities, but rather leveraging, and not replacing, practitioner domain expertise.

Questions remain about how to configure humans and automation for managing AI risks. Risk management is enhanced when organizations that design, develop or deploy AI systems for use by professional operators and practitioners:

-  are aware of these knowledge limitations and strive to identify risks in human-AI interactions and configurations across all contexts, and the potential resulting impacts, 
- define and differentiate the various human roles and responsibilities when using or interacting with AI systems, and
- determine proficiency standards for AI system operation in proposed context of use, as enumerated in MAP-1 and established in GOVERN-3.2.","As AI systems have evolved in accuracy and precision, computational systems have moved from being used purely for decision support—or for explicit use by and under the
control of a human operator—to automated decision making with limited input from humans. Computational decision support systems augment another, typically human, system in making decisions.These types of configurations increase the likelihood of outputs being produced with little human involvement. 

Defining and differentiating various human roles and responsibilities for AI systems’ governance,  and differentiating AI system overseers and those using or interacting with AI systems can enhance AI risk management activities. 

In critical systems, high-stakes settings, and systems deemed high-risk it is of vital importance to evaluate risks and effectiveness of oversight procedures before an AI system is deployed.

Ultimately, AI system oversight is a shared responsibility, and attempts to properly authorize or govern oversight practices will not be effective without organizational buy-in and accountability mechanisms, for example those suggested in the GOVERN function.","Technologies and personnel from third-parties are another potential sources of risk to consider during AI risk management activities. Such risks may be difficult to map since risk priorities or tolerances may not be the same as the deployer organization.

For example, the use of pre-trained models, which tend to rely on large uncurated dataset or often have undisclosed origins, has raised concerns about privacy, bias, and unanticipated effects along with possible introduction of increased levels of statistical uncertainty, difficulty with reproducibility, and issues with scientific validity.","In the course of their work, AI actors often utilize open-source, or otherwise freely available, third-party technologies – some of which may have privacy, bias, and security risks. Organizations may consider internal risk controls for these technology sources and build up practices for evaluating third-party material prior to deployment.","AI actors can evaluate, document and triage the likelihood of AI system impacts identified in Map 5.1 Likelihood estimates may then be assessed and judged for go/no-go decisions about deploying an AI system. If an organization decides to proceed with deploying the system, the likelihood and magnitude estimates can be used to assign TEVV resources appropriate for the risk level.","AI systems are socio-technical in nature and can have positive, neutral, or negative implications that extend beyond their stated purpose. Negative impacts can be wide- ranging and affect individuals, groups, communities, organizations, and society, as well as the environment and national security.

Organizations can create a baseline for system monitoring to increase opportunities for detecting emergent risks. After an AI system is deployed, engaging different stakeholder groups – who may be aware of, or experience, benefits or negative impacts that are unknown to AI actors involved in the design, development and deployment activities – allows organizations to understand and monitor system benefits and potential negative impacts more readily.","The development and utility of trustworthy AI systems depends on reliable measurements and evaluations of underlying technologies and their use. Compared with traditional software systems, AI technologies bring new failure modes, inherent dependence on training data and methods which directly tie to data quality and representativeness. Additionally, AI systems are inherently socio-technical in nature, meaning they are influenced by societal dynamics and human behavior. AI risks – and benefits – can emerge from the interplay of technical aspects combined with societal factors related to how a system is used, its interactions with other AI systems, who operates it, and the social context in which it is deployed. In other words, What should be measured depends on the purpose, audience, and needs of the evaluations. 
 
These two factors influence selection of approaches and metrics for measurement of AI risks enumerated during the Map function. The AI landscape is evolving and so are the methods and metrics for AI measurement. The evolution of metrics is key to maintaining efficacy of the measures.","Different AI tasks, such as neural networks or natural language processing, benefit from different evaluation techniques. Use-case and particular settings in which the AI system is used also affects appropriateness of the evaluation techniques.  Changes in the operational settings, data drift, model drift are among factors that suggest regularly assessing and updating appropriateness of AI metrics and their effectiveness can enhance reliability of AI system measurements.","The current AI systems are brittle, the failure modes are not well described, and the systems are dependent on the context in which they were developed and do not transfer well outside of the training environment. A reliance on local evaluations will be necessary along with a continuous monitoring of these systems. Measurements that extend beyond classical measures (which average across test cases) or expand to focus on pockets of failures where there are potentially significant costs can improve the reliability of risk management activities. Feedback from affected communities about how AI systems are being used can make AI evaluation purposeful. Involving internal experts who did not serve as front-line developers for the system and/or independent assessors regular assessments of AI systems helps a fulsome characterization of AI systems’ performance and trustworthiness .","Documenting measurement approaches, test sets, metrics, processes and materials used, and associated details builds foundation upon which to build a valid, reliable measurement process.  Documentation enables repeatability and consistency, and can enhance AI risk management decisions.","Measurement and evaluation of AI systems often involves testing with human subjects or using data captured from human subjects. Protection of human subjects is required by law when carrying out federally funded research, and is a domain specific requirement for some disciplines. Standard human subjects protection procedures include protecting the welfare and interests of human subjects, designing  evaluations to minimize risks to subjects, and completion of mandatory training regarding legal requirements and expectations. 
 
Evaluations of AI system performance that utilize human subjects or human subject data should reflect the population within the context of use. AI system activities utilizing non-representative data may lead to inaccurate assessments or negative and harmful outcomes. It is often difficult – and sometimes impossible, to collect data or perform evaluation tasks that reflect the full operational purview of an AI system. Methods for collecting, annotating, or using these data can also contribute to the challenge. To counteract these challenges, organizations can connect human subjects data collection, and dataset practices, to AI system contexts and purposes and do so in close collaboration with AI Actors from the relevant domains.","The current risk and impact environment suggests AI system performance estimates are insufficient and require a deeper understanding of deployment context of use. Computationally focused performance testing and evaluation schemes are restricted to test data sets and in silico techniques. These approaches do not directly evaluate risks and impacts in real world environments and can only predict what might create impact based on an approximation of expected AI use. To properly manage risks, more direct information is necessary to understand how and under what conditions deployed AI creates impacts, who is most likely to be impacted, and what that experience is like.","AI systems may encounter new issues and risks while in production as the environment evolves over time. This effect, often referred to as “drift”, means AI systems no longer meet the assumptions and limitations of the original design. Regular monitoring allows AI Actors to monitor the functionality and behavior of the AI system and its components – as identified in the MAP function - and enhance the speed and efficacy of necessary system interventions.","An AI system that is not validated or that fails validation may be inaccurate or unreliable or may generalize poorly to data and settings beyond its training, creating and increasing AI risks and reducing trustworthiness. AI Actors can improve system validity by creating processes for exploring and documenting system limitations. This includes broad consideration of purposes and uses for which the system was not designed. 

Validation risks include the use of proxies or other indicators that are often constructed by AI development teams to operationalize phenomena that are either not directly observable or measurable (e.g, fairness, hireability, honesty, propensity to commit a crime). Teams can mitigate these risks by demonstrating that the indicator is measuring the concept it claims to measure (also known as construct validity). Without this and other types of validation, various negative properties or impacts may go undetected, including the presence of confounding variables, potential spurious correlations, or error propagation and its potential impact on other interconnected systems.","Many AI systems are being introduced into settings such as transportation, manufacturing or security, where failures may give rise to various physical or environmental harms. AI systems that may endanger human life, health, property or the environment are tested thoroughly prior to  deployment, and are regularly evaluated to confirm the system is safe during normal operations, and in settings beyond its proposed use and knowledge limits. 

Measuring activities for safety often relate to exhaustive testing in development and deployment contexts, understanding the limits of a system’s reliable, robust, and safe behavior, and real-time monitoring of various aspects of system performance. These activities are typically conducted along with other risk mapping, management, and governance tasks such as avoiding past failed designs, establishing and rehearsing incident response plans that enable quick responses to system problems, the instantiation of redundant functionality to cover failures, and transparent and accountable governance. System safety incidents or failures are frequently reported to be related to organizational dynamics and culture. Independent auditors may bring important independent perspectives for reviewing evidence of AI system safety.","AI systems, as well as the ecosystems in which they are deployed, may be said to be resilient if they can withstand unexpected adverse events or unexpected changes in their environment or use – or if they can maintain their functions and structure in the face of internal
and external change and degrade safely and gracefully when this is necessary. Common security concerns relate to adversarial examples, data poisoning, and the exfiltration of models, training data, or other intellectual property through AI system endpoints. AI systems that can maintain confidentiality, integrity, and availability through protection mechanisms that prevent unauthorized access and use may be said to be secure. 

Security and resilience are related but distinct characteristics. While resilience is the ability
to return to normal function after an unexpected adverse event, security includes resilience
but also encompasses protocols to avoid, protect against, respond to, or recover
from attacks. Resilience relates to robustness and encompasses unexpected or adversarial use (or abuse or misuse) of the model or data.","Transparency enables meaningful visibility into entire AI pipelines, workflows, processes or organizations and decreases information asymmetry between AI developers and operators and other AI Actors and impacted communities. Transparency is a central element of effective AI risk management that enables insight into how an AI system is working, and the ability to address risks if and when they emerge. The ability for system users, individuals, or impacted communities to seek redress for incorrect or problematic AI system outcomes is one control for transparency and accountability. Higher level recourse processes are typically enabled by lower level implementation efforts directed at explainability and interpretability functionality. See Measure 2.9.

Transparency and accountability across organizations and processes is crucial to reducing AI risks. Accountable leadership – whether individuals or groups – and transparent roles, responsibilities, and lines of communication foster and incentivize quality assurance and risk management activities within organizations.

Lack of transparency complicates measurement of trustworthiness and whether AI systems or organizations are subject to effects of various individual and group biases and design blindspots and could lead to diminished user, organizational and community trust, and decreased overall system value. Enstating accountable and transparent organizational structures along with documenting system risks can enable system improvement and risk management efforts, allowing AI actors along the lifecycle to identify errors, suggest improvements, and figure out new ways to contextualize and generalize AI system features and outcomes.","Explainability and interpretability assist those operating or overseeing an AI system, as well as users of an AI system, to gain deeper insights into the functionality and trustworthiness of the system, including its outputs.

Explainable and interpretable AI systems offer information that help end users understand the purposes and potential impact of an AI system. Risk from lack of explainability may be managed by describing how AI systems function, with descriptions tailored to individual differences such as the user’s role, knowledge, and skill level. Explainable systems can be debugged and monitored more easily, and they lend themselves to more thorough documentation, audit, and governance.

Risks to interpretability often can be addressed by communicating a description of why
an AI system made a particular prediction or recommendation. 

Transparency, explainability, and interpretability are distinct characteristics that support
each other. Transparency can answer the question of “what happened”. Explainability can answer the question of “how” a decision was made in the system. Interpretability can answer the question of “why” a decision was made by the system and its
meaning or context to the user.","Privacy refers generally to the norms and practices that help to safeguard human autonomy, identity, and dignity. These norms and practices typically address freedom from intrusion, limiting observation, or individuals’ agency to consent to disclosure or control of facets of
their identities (e.g., body, data, reputation). 

Privacy values such as anonymity, confidentiality, and control generally should guide choices for AI system design, development, and deployment. Privacy-related risks may influence security, bias, and transparency and come with tradeoffs with these other characteristics. Like safety and security, specific technical features of an AI system may promote or reduce privacy. AI systems can also present new risks to privacy by allowing inference to identify individuals or previously private information about individuals.

Privacy-enhancing technologies (“PETs”) for AI, as well as data minimizing methods such as de-identification and aggregation for certain model outputs, can support design for privacy-enhanced AI systems. Under certain conditions such as data sparsity, privacy enhancing techniques can result in a loss in accuracy, impacting decisions about fairness and other values in certain domains.","Fairness in AI includes concerns for equality and equity by addressing issues such as harmful bias and discrimination. Standards of fairness can be complex and difficult to define because perceptions of fairness differ among cultures and may shift depending on application. Organizations’ risk management efforts will be enhanced by recognizing and considering these differences. Systems in which harmful biases are mitigated are not necessarily fair. For example, systems in which predictions are somewhat balanced across demographic groups may still be inaccessible to individuals with disabilities or affected by the digital divide or may exacerbate existing disparities or systemic biases.

Bias is broader than demographic balance and data representativeness. NIST has identified three major categories of AI bias to be considered and managed: systemic, computational and statistical, and human-cognitive. Each of these can occur in the absence of prejudice, partiality, or discriminatory intent. 

- Systemic bias can be present in AI datasets, the organizational norms, practices, and processes across the AI lifecycle, and the broader society that uses AI systems.
- Computational and statistical biases can be present in AI datasets and algorithmic processes, and often stem from systematic errors due to non-representative samples.
- Human-cognitive biases relate to how an individual or group perceives AI system information to make a decision or fill in missing information, or how humans think about purposes and functions of an AI system. Human-cognitive biases are omnipresent in decision-making processes across the AI lifecycle and system use, including the design, implementation, operation, and maintenance of AI.

Bias exists in many forms and can become ingrained in the automated systems that help make decisions about our lives. While bias is not always a negative phenomenon, AI systems can potentially increase the speed and scale of biases and perpetuate and amplify harms to individuals, groups, communities, organizations, and society.","Large-scale, high-performance computational resources used by AI systems for training and operation can contribute to environmental impacts.  Direct negative impacts to the environment from these processes are related to energy consumption, water consumption, and greenhouse gas (GHG) emissions. The OECD has identified metrics for each type of negative direct impact. 

Indirect negative impacts to the environment reflect the complexity of interactions between human behavior, socio-economic systems, and the environment and can include induced consumption and “rebound effects”, where efficiency gains are offset by accelerated resource consumption. 

Other AI related environmental impacts can arise from the production of computational equipment and networks (e.g. mining and extraction of raw materials), transporting hardware, and electronic waste recycling or disposal.","The development of metrics is a process often considered to be objective but, as a human and organization driven endeavor, can reflect implicit and systemic biases, and may inadvertently reflect factors unrelated to the target function. Measurement approaches can be oversimplified, gamed, lack critical nuance, become used and relied upon in unexpected ways, fail to account for differences in affected groups and contexts.

Revisiting the metrics chosen in Measure 2.1 through 2.12 in a process of continual improvement can help AI actors to evaluate and document metric effectiveness and make necessary course corrections.","For trustworthy AI systems, regular system monitoring is carried out in accordance with organizational governance policies, AI actor roles and responsibilities, and within a culture of continual improvement. If and when emergent or complex risks arise, it may be necessary to adapt internal risk management procedures, such as regular monitoring, to stay on course. Documentation, resources, and training are part of an overall strategy to support AI actors as they investigate and respond to AI system errors, incidents or negative impacts.","Risks identified in the Map function may be complex, emerge over time, or difficult to measure. Systematic methods for risk tracking, including novel measurement approaches, can be established as part of regular monitoring and improvement processes.","Assessing impact is a two-way effort. Many AI system outcomes and impacts may not be visible or recognizable to AI actors across the development and deployment dimensions of the AI lifecycle, and may require direct feedback about system outcomes from the perspective of end users and impacted groups.

Feedback can be collected indirectly, via systems that are mechanized to collect errors and other feedback from end users and operators

Metrics and insights developed in this sub-category feed into Manage 4.1 and 4.2.","AI Actors carrying out TEVV tasks may have difficulty evaluating impacts within the system context of use. AI system risks and impacts are often best described by end users and others who may be affected by output and subsequent decisions. AI Actors can elicit feedback from impacted individuals and communities via participatory engagement processes established in Govern 5.1 and 5.2, and carried out in Map 1.6, 5.1, and 5.2. 

Activities described in the Measure function enable AI actors to evaluate feedback from impacted individuals and communities. To increase awareness of insights, feedback can be evaluated in close collaboration with AI actors responsible for impact assessment, human-factors, and governance and oversight tasks, as well as with other socio-technical domain experts and researchers. To gain broader expertise for interpreting evaluation outcomes, organizations may consider collaborating with advocacy groups and civil society organizations. 

Insights based on this type of analysis can inform TEVV-based decisions about metrics and related courses of action.","Feedback captured from relevant AI Actors can be evaluated in combination with output from Measure 2.5 to 2.11 to determine if the AI system is performing within pre-defined operational limits for validity and reliability, safety, security and resilience, privacy, bias and fairness, explainability and interpretability, and transparency and accountability. This feedback provides an additional layer of insight about AI system performance, including potential misuse or reuse outside of intended settings. 


Insights based on this type of analysis can inform TEVV-based decisions about metrics and related courses of action.","TEVV activities conducted throughout the AI system lifecycle can provide baseline quantitative measures for trustworthy characteristics. When combined with results from Measure 2.5 to 2.11 and Measure 4.1 and 4.2, TEVV actors can maintain a comprehensive view of system performance. These measures can be augmented through participatory engagement with potentially impacted communities or other forms of stakeholder elicitation about AI systems’ impacts. These sources of information can allow AI actors to explore potential adjustments to system components, adapt operating conditions, or institute performance improvements."
section_actions,"* Maintain awareness of the applicable legal and regulatory considerations and requirements specific to industry, sector, and business purpose, as well as the application context of the deployed AI system.
* Align risk management efforts with applicable legal standards.
* Maintain policies for training (and re-training) organizational staff about necessary legal or regulatory considerations that may impact AI-related design, development and deployment activities.","Organizational AI risk management policies should be designed to:

- Define key terms and concepts related to AI systems and the scope of their purposes and intended uses.
- Connect AI governance to existing organizational governance and risk controls. 
- Align to broader data governance policies and practices, particularly the use of sensitive or otherwise risky data.
- Detail standards for experimental design, data quality, and model training.
- Outline and document risk mapping and measurement processes and standards.
- Detail model testing and validation processes.
- Detail review processes for legal and risk functions.
- Establish the frequency of and detail for monitoring, auditing and review processes.
- Outline change management requirements.
- Outline processes for internal and external stakeholder engagement.
- Establish whistleblower policies to facilitate reporting of serious AI system concerns.
- Detail and test incident response plans.
- Verify that formal AI risk management policies align to existing legal standards, and industry best practices and norms.
- Establish AI risk management policies that broadly align to AI system trustworthy characteristics.
- Verify that formal AI risk management policies include currently deployed and third-party AI systems.","- Establish policies to define mechanisms for measuring or understanding an AI system’s potential impacts, e.g., via regular impact assessments at key stages in the AI lifecycle, connected to system impacts and frequency of system updates.
- Establish policies to define mechanisms for measuring or understanding the likelihood of an AI system’s impacts and their magnitude at key stages in the AI lifecycle. 
- Establish policies that define assessment scales for measuring potential AI system impact. Scales may be qualitative, such as red-amber-green (RAG), or may entail simulations or econometric approaches. 
- Establish policies for assigning an overall risk measurement approach for an AI system, or its important components, e.g., via multiplication or combination of a mapped risk’s impact and likelihood (risk ˜ impact x likelihood).
- Establish policies to assign systems to uniform risk scales that are valid across the organization’s AI portfolio (e.g. documentation templates), and acknowledge risk tolerance and risk levels may change over the lifecycle of an AI system.","- Establish and regularly review documentation policies that, among others, address information related to:
    - AI actors contact informations
    - Business justification
    - Scope and usages
    - Expected and potential risks and impacts
    -  Assumptions and limitations
    - Description and characterization of training data
    - Algorithmic methodology
    - Evaluated alternative approaches
    - Description of output data
    - Testing and validation results (including explanatory visualizations and information)
    - Down- and up-stream dependencies
    - Plans for deployment, monitoring, and change management
    - Stakeholder engagement plans
- Verify documentation policies for AI systems are standardized across the organization and remain current.
- Establish policies for a model documentation inventory system and regularly review its completeness, usability, and efficacy.
- Establish mechanisms to regularly review the efficacy of risk management processes.
- Identify AI actors responsible for evaluating efficacy of risk management processes and approaches, and for course-correction based on results.
- Establish policies and processes regarding public disclosure of the use of AI and risk management material such as impact assessments, audits, model documentation and validation and testing results.
- Document and review the use and efficacy of different types of transparency tools and follow industry standards at the time a model is in use.","- Establish policies to allocate appropriate resources and capacity for assessing impacts of AI systems on individuals, communities and society.
- Establish policies and procedures for monitoring and addressing AI system performance and trustworthiness, including bias and security problems, across the lifecycle of the system.
- Establish policies for AI system incident response, or confirm that existing incident response policies apply to AI systems.
- Establish policies to define organizational functions and personnel responsible for AI system monitoring and incident response activities.
- Establish mechanisms to enable the sharing of feedback from impacted individuals or communities about negative impacts from AI systems.
- Establish mechanisms to provide recourse for impacted individuals or communities to contest problematic AI system outcomes.
- Establish opt-out mechanisms.","- Establish policies that define the creation and maintenance of AI system inventories. 
- Establish policies that define a specific individual or team that is responsible for maintaining the inventory. 
- Establish policies that define which models or systems are inventoried, with preference to inventorying all models or systems, or minimally, to high risk models or systems, or systems deployed in high-stakes settings.
- Establish policies that define model or system attributes to be inventoried, e.g, documentation, links to source code, incident response plans, data dictionaries, AI actor contact information.","- Establish policies for decommissioning AI systems. Such policies typically address:
	- User and community concerns, and reputational risks. 
	- Business continuity and financial risks.
	- Up and downstream system dependencies. 
	- Regulatory requirements (e.g., data retention). 
	- Potential future legal, regulatory, security or forensic investigations.
	- Migration to the replacement system, if appropriate.
- Establish policies that delineate where and for how long decommissioned systems, models and related artifacts are stored. 
- Establish policies that address ancillary data or artifacts that must be preserved for fulsome understanding or execution of the decommissioned AI system, e.g., predictions, explanations, intermediate input feature representations, usernames and passwords, etc.","- Establish policies that define the AI risk management roles and responsibilities for positions directly and indirectly related to AI systems, including, but not limited to
        - Boards of directors or advisory committees
        - Senior management
        - AI audit functions
        - Product management
        - Project management
        - AI design
        - AI development
        - Human-AI interaction
        - AI testing and evaluation
        - AI acquisition and procurement
        - Impact assessment functions
        - Oversight functions
- Establish policies that promote regular communication among AI actors participating in AI risk management efforts.
- Establish policies that separate management of AI system development functions from AI system testing functions, to enable independent course-correction of AI systems.
- Establish policies to identify, increase the transparency of, and prevent conflicts of interest in AI risk management efforts.
- Establish policies to counteract confirmation bias and market incentives that may hinder AI risk management efforts.
- Establish policies that incentivize AI actors to collaborate with existing legal, oversight, compliance, or enterprise risk functions in their AI risk management activities.","- Establish policies for personnel addressing ongoing education about:
	- Applicable laws and regulations for AI systems.
	- Potential negative impacts that may arise from AI systems.
	- Organizational AI policies.
	- Trustworthy AI characteristics.
- Ensure that trainings are suitable across AI actor sub-groups - for AI actors carrying out technical tasks (e.g., developers, operators, etc.) as compared to AI actors in oversight roles (e.g., legal, compliance, audit,  etc.). 
- Ensure that trainings comprehensively address technical and socio-technical aspects of AI risk management. 
- Verify that organizational AI policies include mechanisms for internal AI personnel to acknowledge and commit to their roles and responsibilities.
- Verify that organizational policies address change management and include mechanisms to communicate and acknowledge substantial AI system changes.
- Define paths along internal and external chains of accountability to escalate risk concerns.","- Organizational management can:
    - Declare risk tolerances for developing or using AI systems.
    - Support AI risk management efforts, and play an active role in such efforts.
    - Integrate a risk and harm prevention mindset throughout the AI lifecycle as part of organizational culture
    - Support competent risk management executives.
    - Delegate the power, resources, and authorization to perform risk management to each appropriate level throughout the management chain.
- Organizations can establish board committees for AI risk management and oversight functions and integrate those functions within the organization’s broader enterprise risk management approaches.","Organizational management can:

- Define policies and hiring practices at the outset that promote interdisciplinary roles, competencies, skills, and capacity for AI efforts.
- Define policies and hiring practices that lead to demographic and domain expertise diversity; empower staff with necessary resources and support, and facilitate the contribution of staff feedback and concerns without fear of reprisal.
- Establish policies that facilitate inclusivity and the integration of new insights into existing practice.
- Seek external expertise to supplement organizational diversity, equity, inclusion, and accessibility where internal expertise is lacking.
- Establish policies that incentivize AI actors to collaborate with existing nondiscrimination, accessibility and accommodation, and human resource functions, employee resource group (ERGs), and diversity, equity, inclusion, and accessibility (DEIA) initiatives.","- Establish policies and procedures that define and differentiate the various human roles and responsibilities when using, interacting with, or monitoring AI systems.
- Establish procedures for capturing and tracking risk information related to human-AI configurations and associated outcomes.
- Establish policies for the development of proficiency standards for AI actors carrying out system operation tasks and system oversight tasks.
- Establish specified risk management training protocols for AI actors carrying out system operation tasks and system oversight tasks.
- Establish policies and procedures regarding AI actor roles, and responsibilities for human oversight of deployed systems.
- Establish policies and procedures defining  human-AI configurations (configurations where AI systems are explicitly designated and treated as team members in primarily human teams) in relation to organizational risk tolerances, and associated documentation.  
- Establish policies to enhance the explanation, interpretation, and overall transparency of AI systems.
- Establish policies for managing risks regarding known difficulties in human-AI configurations, human-AI teaming, and AI system user experience and user interactions (UI/UX).","- Establish policies that require inclusion of oversight functions (legal, compliance, risk management) from the outset of the system design process.
- Establish policies that promote effective challenge of AI system design, implementation, and deployment decisions, via mechanisms such as the three lines of defense, model audits, or red-teaming – to minimize workplace risks such as groupthink.
- Establish policies that incentivize safety-first mindset and general critical thinking and review at an organizational and procedural level.
- Establish whistleblower protections for insiders who report on perceived serious problems with AI systems.
- Establish policies to integrate a harm and risk prevention mindset throughout the AI lifecycle.","- Establish impact assessment policies and processes for AI systems used by the organization.
- Align organizational impact assessment activities with relevant regulatory or legal requirements. 
- Verify that impact assessment activities are appropriate to evaluate the potential negative impact of a system and how quickly a system changes, and that assessments are applied on a regular basis.
- Utilize impact assessments to inform broader evaluations of AI system risk.","- Establish policies and procedures to facilitate and equip AI system testing.
- Establish organizational commitment to identifying AI system limitations and sharing of insights about limitations within appropriate AI actor groups.
- Establish policies for reporting and documenting incident response.
- Establish policies and processes regarding public disclosure of incidents and information sharing.
- Establish guidelines for incident handling related to AI system risks and performance.","- Establish AI risk management policies that explicitly address mechanisms for collecting, evaluating, and incorporating stakeholder and user feedback that could include:
    - Recourse mechanisms for faulty AI system outputs.
    - Bug bounties.
    - Human-centered design.
    - User-interaction and experience research.
    - Participatory stakeholder engagement with individuals and communities that may experience negative impacts.
- Verify that stakeholder feedback is considered and addressed, including environmental concerns, and across the entire population of intended users, including historically excluded populations, people with disabilities, older people, and those with limited access to the internet and other basic technologies.
- Clarify the organization’s principles as they apply to AI systems – considering those which have been proposed publicly – to inform external stakeholders of the organization’s values. Consider publishing or adopting AI principles.","- Explicitly acknowledge that AI systems, and the use of AI, present inherent costs and risks along with potential benefits.
- Define reasonable risk tolerances for AI systems informed by laws, regulation, best practices, or industry standards.
- Establish policies that ensure all relevant AI actors are provided with meaningful opportunities to provide feedback on system design and implementation.
- Establish policies that define how to assign AI systems to established risk tolerance levels by combining system impact assessments with the likelihood that an impact occurs. Such assessment often entails some combination of:
    - Econometric evaluations of impacts and impact likelihoods to assess AI system risk.
    - Red-amber-green (RAG) scales for impact severity and likelihood to assess AI system risk.
    - Establishment of policies for allocating risk management resources along established risk tolerance levels, with higher-risk systems receiving more risk management resources and oversight.
    - Establishment of policies for approval, conditional approval, and disapproval of the design, implementation, and deployment of AI systems.
- Establish policies facilitating the early decommissioning of AI systems that surpass an organization’s ability to reasonably mitigate risks.","- Collaboratively establish policies that address third-party AI systems and data.
- Establish policies related to:
    - Transparency into third-party system functions, including knowledge about training data, training and inference algorithms, and assumptions and limitations.
    - Thorough testing of third-party AI systems. (See MEASURE for more detail)
    - Requirements for clear and complete instructions for third-party system usage.
- Evaluate policies for third-party technology. 
- Establish policies that address supply chain, full product lifecycle and associated processes, including legal, ethical, and other issues concerning procurement and use of third-party software or hardware systems and data.","- Establish policies for handling third-party system failures to include consideration of redundancy mechanisms for vital third-party AI systems.
- Verify that incident response plans address third-party AI systems.","- Consider trustworthiness characteristics when evaluating AI systems’ negative risks and benefits.
- Utilize TEVV outputs from map and measure functions when considering risk treatment.
- Regularly track and monitor negative risks and benefits throughout the AI system lifecycle including in post-deployment monitoring.
- Regularly assess and document system performance relative to trustworthiness characteristics and tradeoffs between negative risks and opportunities.
- Evaluate tradeoffs in connection with real-world use cases and impacts and as enumerated in Map function outcomes.","- Assign risk management resources relative to established risk tolerance. AI systems with lower risk tolerances receive greater oversight, mitigation and management resources. 
- Document AI risk tolerance determination practices and resource decisions.
- Regularly review risk tolerances and re-calibrate, as needed, in accordance with information from AI system monitoring and assessment .","- Observe regulatory and established organizational, sector, discipline, or professional standards and requirements for applying risk tolerances within the organization.
- Document procedures for acting on AI system risks related to trustworthiness characteristics.
- Prioritize risks involving physical safety, legal liabilities, regulatory compliance, and negative impacts on individuals, groups, or society.
- Identify risk response plans and resources and organizational teams for carrying out response functions.
- Store risk management and system documentation in an organized, secure repository that is accessible by relevant AI Actors and appropriate personnel.","- Document residual risks within risk response plans, denoting risks that have been accepted, transferred, or subject to minimal mitigation. 
- Establish procedures for disclosing residual risks to relevant downstream AI actors .
- Inform relevant downstream AI actors of requirements for safe operation, known limitations, and suggested warning labels as identified in MAP 3.4.","- Plan and implement risk management practices in accordance with established organizational risk tolerances.
- Verify risk management teams are resourced to carry out functions, including
	- Establishing processes for considering methods that are not automated; semi-automated; or other procedural alternatives for AI functions. 
	- Enhance AI system transparency mechanisms for AI teams.
	- Enable exploration of AI system limitations by AI teams.  
	- Identify, assess, and catalog past failed designs and negative impacts or outcomes to avoid known failure modes.
- Identify resource allocation approaches for managing risks in systems:
	- deemed high-risk,
	- that self-update (adaptive, online, reinforcement self-supervised learning or similar),
	- trained without access to ground truth (unsupervised, semi-supervised, learning or similar), 
	- with high uncertainty or where risk management is insufficient.
- Regularly seek and integrate external expertise and perspectives to supplement organizational diversity (e.g. demographic, disciplinary), equity, inclusion, and accessibility where internal capacity is lacking.
- Enable and encourage regular, open communication and feedback among AI actors and internal or external stakeholders related to system design or deployment decisions.
- Prepare and document plans for continuous monitoring and feedback mechanisms.","- Establish risk controls considering trustworthiness characteristics, including:
	- Data management, quality, and privacy (e.g. minimization, rectification or deletion requests) controls as part of organizational data governance policies. 
	- Machine learning and end-point security countermeasures (e.g., robust models, differential privacy, authentication, throttling).
	- Business rules that augment, limit or restrict AI system outputs within certain contexts 
	- Utilizing domain expertise related to deployment context for continuous improvement and TEVV across the AI lifecycle.
	- Development and regular tracking of human-AI teaming configurations.
	- Model assessment and test, evaluation, validation and verification (TEVV) protocols.
	- Use of standardized documentation and transparency mechanisms.
	- Software quality assurance practices across AI lifecycle.
	- Mechanisms to explore system limitations and avoid past failed designs or deployments.
- Establish mechanisms to capture feedback from system end users and potentially impacted groups.
- Review insurance policies, warranties, or contracts for legal or oversight requirements for risk transfer procedures.
- Document risk tolerance decisions and risk acceptance procedures.","- Protocols, resources, and metrics  are in place for continual monitoring of AI systems’ performance, trustworthiness, and alignment with contextual norms and values 
- Establish and regularly review treatment and response plans for incidents, negative impacts, or outcomes.
- Establish and maintain procedures to regularly monitor system components for drift, decontextualization, or other AI system behavior factors, 
- Establish and maintain procedures for capturing feedback about negative impacts.
- Verify contingency processes to handle any negative impacts associated with mission-critical AI systems, and to deactivate systems.
- Enable preventive and post-hoc exploration of AI system limitations by relevant AI actor groups.
- Decommission systems that exceed risk tolerances.","- Regularly review established procedures for AI system bypass actions, including plans for redundant or backup systems to ensure continuity of operational and/or business functionality.
- Regularly review Identify system incident thresholds for activating bypass or deactivation responses.
- Apply change management processes to understand the upstream and downstream consequences of bypassing or deactivating an AI system or AI system components.
- Apply protocols, resources and metrics for decisions to supersede, bypass or deactivate AI systems or AI system components.
- Preserve materials for forensic, regulatory, and legal review.
- Conduct internal root cause analysis and process reviews of bypass or deactivation events. 
- Decommission and preserve system components that cannot be updated to meet criteria for redeployment.
- Establish criteria for redeploying updated system components, in consideration of trustworthy characteristics","- Have legal requirements been addressed?
- Apply organizational risk tolerance to third-party AI systems.
- Apply and document organizational risk management plans and practices to third-party AI technology, personnel, or other resources.
- Identify and maintain documentation for third-party AI systems and components.
- Establish testing, evaluation, validation and verification processes for third-party AI systems which address the needs for transparency without exposing proprietary algorithms .
- Establish processes to identify beneficial use and risk indicators in third-party systems or components, such as inconsistent software release schedule, sparse documentation, and incomplete software change management (e.g., lack of forward or backward compatibility).
- Organizations can establish processes for third parties to report known and potential vulnerabilities, risks or biases in supplied resources.
- Verify contingency processes for handling negative impacts associated with mission-critical third-party AI systems.
- Monitor third-party AI systems for potential negative impacts and risks associated with trustworthiness characteristics.
- Decommission third-party systems that exceed risk tolerances.","- Identify pre-trained models within AI system inventory for risk tracking.
- Establish processes to independently and continually monitor performance and trustworthiness  of pre-trained models, and as part of third-party risk tracking. 
- Monitor performance and trustworthiness of AI system components connected to pre-trained models, and as part of third-party risk tracking.
- Identify, document and remediate risks arising from AI system components and pre-trained models per organizational risk management procedures, and as part of third-party risk tracking.
- Decommission AI system components and pre-trained models which exceed risk tolerances, and as part of third-party risk tracking.","- Establish and maintain procedures to monitor AI system performance for risks and negative and positive impacts associated with trustworthiness characteristics. 
- Perform post-deployment TEVV tasks to evaluate AI system validity and reliability, bias and fairness, privacy, and security and resilience.
- Evaluate AI system trustworthiness in conditions similar to deployment context of use, and prior to deployment.
- Establish and implement red-teaming exercises at a prescribed cadence, and evaluate their efficacy. 
- Establish procedures for tracking dataset modifications such as data deletion or rectification requests.
- Establish mechanisms for regular communication and feedback between relevant AI actors and internal or external stakeholders to capture information about system performance, trustworthiness and impact.
- Share information about errors, near-misses, and attack patterns with incident databases, other organizations with similar systems, and system users and stakeholders.
- Respond to and document detected or reported negative impacts or issues in AI system performance and trustworthiness.
- Decommission systems that exceed establish risk tolerances.","- Integrate trustworthiness characteristics into protocols and metrics used for continual improvement.
- Establish processes for evaluating and integrating feedback into AI system improvements.
- Assess and evaluate alignment of proposed improvements with relevant regulatory and legal frameworks
- Assess and evaluate alignment of proposed improvements connected to the values and norms within the context of use.
- Document the basis for decisions made relative to tradeoffs between trustworthy characteristics, system risks, and system opportunities","- Establish procedures to regularly share information about errors, incidents and negative impacts with relevant stakeholders, operators, practitioners and users, and impacted parties.
- Maintain a database of reported errors, near-misses, incidents and negative impacts including date reported, number of reports, assessment of impact and severity, and responses.
- Maintain a database of system changes, reason for change, and details of how the change was made, tested and deployed. 
- Maintain version history information and metadata to enable continuous improvement processes.
- Verify that relevant AI actors responsible for identifying complex or emergent risks are properly resourced and empowered.","-  Maintain awareness of industry, technical, and applicable legal standards.
-  Examine trustworthiness of AI system design and consider, non-AI solutions 
-  Consider intended AI system design tasks along with unanticipated purposes in collaboration with human factors and socio-technical domain experts.
- Define and document the task, purpose, minimum functionality, and benefits of the AI system to inform considerations about whether the utility of the project or its lack of.
- Identify whether there are non-AI or non-technology alternatives that will lead to more trustworthy outcomes. 
- Examine how changes in system performance affect downstream events such as decision-making (e.g: changes in an AI model objective function create what types of impacts in how many candidates do/do not get a job interview).  
- Determine the end user and organizational requirements, including business and technical requirements.
- Determine and delineate the expected and acceptable AI system context of use, including:
    - social norms
    - Impacted individuals, groups, and communities
    - potential positive and negative impacts to individuals, groups, communities, organizations, and society
    - operational environment
- Perform context analysis related to time frame, safety concerns, geographic area, physical environment, ecosystems, social environment, and cultural norms within the intended setting (or conditions that closely approximate the intended setting.
- Gain and maintain awareness about evaluating scientific claims related to AI system performance and benefits before launching into system design.
- Identify human-AI interaction and/or roles, such as whether the application will support or replace human decision making.
- Plan for risks related to human-AI configurations, and document requirements, roles, and responsibilities for human oversight of deployed systems.","- Establish interdisciplinary teams to reflect a wide range of skills, competencies, and capabilities for AI efforts. Verify that team membership includes demographic diversity, broad domain expertise, and lived experiences. Document team composition.
- Create and empower interdisciplinary expert teams to capture, learn, and engage the interdependencies of deployed AI systems and related terminologies and concepts from disciplines outside of AI practice such as law, sociology, psychology, anthropology, public policy, systems design, and engineering.","- Build transparent practices into AI system development processes.
- Review the documented system purpose from a socio-technical perspective and in consideration of societal values.
- Determine possible misalignment between societal values and stated organizational principles and code of ethics.
- Flag latent incentives that may contribute to negative impacts.
- Evaluate AI system purpose in consideration of potential risks, societal values, and stated organizational principles.","- Document business value or context of business use 
- Reconcile documented concerns about the system’s purpose within the business context of use  compared to the organization’s stated values, mission statements, social responsibility commitments, and AI principles.
- Reconsider the design, implementation strategy, or deployment of AI systems with potential impacts that do not reflect institutional values.","- Utilize existing regulations and guidelines for risk criteria, tolerance and response established by organizational, domain, discipline, sector, or professional requirements.
- Establish risk tolerance levels for AI systems and allocate the appropriate oversight resources to each level. 
- Establish risk criteria in consideration of different sources of risk, (e.g., financial, operational, safety and wellbeing, business, reputational, and model risks) and different levels of risk (e.g., from negligible to critical).  
- Identify maximum allowable risk tolerance above which the system will not be deployed, or will need to be prematurely decommissioned, within the contextual or application setting.
- Articulate and analyze tradeoffs across trustworthiness characteristics as relevant to proposed context of use.  When tradeoffs arise, document them and plan for traceable actions (e.g.: impact mitigation, removal of system from development or use) to inform management decisions. 
- Review uses of AI systems for “off-label” purposes, especially in settings that organizations have deemed as high-risk. Document decisions, risk-related trade-offs, and system limitations.","- Proactively incorporate trustworthy characteristics into system requirements.
- Establish mechanisms for regular communication and feedback between relevant AI actors and internal or external stakeholders related to system design or deployment decisions.
- Develop and standardize practices to assess potential impacts at all stages of the AI lifecycle, and in collaboration with interdisciplinary experts, actors external to the team that developed or deployed the AI system, and potentially impacted communities . 
- Include potentially impacted groups, communities and external entities (e.g. civil society organizations, research institutes, local community groups, and trade associations) in the formulation of priorities, definitions and outcomes during impact assessment activities. 
- Conduct qualitative interviews with end user(s) to regularly evaluate expectations and design plans related to Human-AI configurations and tasks.
- Analyze dependencies between contextual factors and system requirements. List potential impacts that may arise from not fully considering the importance of trustworthiness characteristics in any decision making.
- Follow responsible design techniques in tasks such as software engineering, product management, and participatory engagement. Some examples for eliciting and documenting stakeholder requirements include product requirement documents (PRDs), user stories, user interaction/user experience (UI/UX) research, systems engineering, ethnography and related field methods.
- Conduct user research to understand individuals, groups and communities that will be impacted by the AI, their values & context, and the role of systemic and historical biases. Integrate learnings into decisions about data selection and representation.",- Define and document AI system’s existing and potential learning task(s) along with known assumptions and limitations.,"- Document settings, environments and conditions that are outside the AI system’s intended use.   
- Design for end user workflows and toolsets, concept of operations, and explainability and interpretability criteria in conjunction with end user(s) and associated qualitative feedback.
- Plan and test human-AI configurations under close to real-world conditions and document results.
- Follow stakeholder feedback processes to determine whether a system achieved its documented purpose within a given use context, and whether end users can correctly comprehend system outputs or results.
- Document dependencies on upstream data and other AI systems, including if the specified system is an upstream dependency for another AI system or other data.
- Document connections the AI system or data will have to external networks (including the internet), financial markets, and critical infrastructure that have potential for negative externalities. Identify and document negative impacts as part of considering the broader risk thresholds and subsequent go/no-go deployment as well as post-deployment decommissioning decisions.","- Identify and document experiment design and statistical techniques that are valid for testing complex socio-technical systems like AI, which involve human factors, emergent properties, and dynamic context(s) of use.   
- Develop and apply TEVV protocols for models, system and its subcomponents, deployment, and operation.
- Demonstrate and document that AI system performance and validation metrics are interpretable and unambiguous for downstream decision making tasks, and take socio-technical factors such as context of use into consideration.
- Identify and document assumptions,  techniques, and metrics used for testing and evaluation throughout the AI lifecycle including experimental design techniques for data collection, selection, and management practices in accordance with data governance policies established in GOVERN.
- Identify testing modules that can be incorporated throughout the AI lifecycle, and verify that processes enable corroboration by independent evaluators.
- Establish mechanisms for regular communication and feedback among relevant AI actors and internal or external stakeholders related to the validity of design and deployment assumptions. 
- Establish mechanisms for regular communication and feedback between relevant AI actors and internal or external stakeholders related to the development of TEVV approaches throughout the lifecycle to detect and assess potentially harmful impacts
- Document assumptions made and techniques used in data selection, curation, preparation and analysis, including:
    - identification of constructs and proxy targets, 
    - development of  indices – especially those operationalizing concepts that are inherently unobservable (e.g. “hireability,” “criminality.” “lendability”).
- Map adherence to policies that address data and construct validity, bias, privacy and security for AI systems and verify documentation, oversight, and processes.
- Identify and document transparent methods (e.g. causal discovery methods) for inferring causal relationships between constructs being modeled and dataset attributes or proxies.
- Identify and document processes to understand and trace test and training data lineage and its metadata resources for mapping risks.
- Document known limitations, risk mitigation efforts associated with, and methods used for, training data collection, selection, labeling, cleaning, and analysis (e.g. treatment of missing, spurious, or outlier data; biased estimators).
- Establish and document practices to check for capabilities that are in excess of those that are planned for, such as emergent properties, and to revisit prior risk management steps in light of any new capabilities.
- Establish processes to test and verify that design assumptions about the set of deployment contexts continue to be accurate and sufficiently complete.
- Work with domain experts and other external AI actors to:
    - Gain and maintain contextual awareness and knowledge about how human behavior, organizational factors and dynamics, and society influence, and are represented in, datasets, processes, models, and system output.
    - Identify participatory approaches for responsible Human-AI configurations and oversight tasks, taking into account sources of cognitive bias.
    - Identify techniques to manage and mitigate sources of bias (systemic, computational, human- cognitive) in computational models and systems, and the assumptions and decisions in their development..
- Investigate and document potential negative impacts due related to the full product lifecycle and associated processes that may conflict with organizational values and principles.","- Utilize participatory approaches and engage with system end users to understand and document  AI systems’ potential benefits,  efficacy and interpretability of AI task output.
- Maintain awareness and documentation of the individuals, groups, or communities who make up the system’s internal and external stakeholders.
- Verify that appropriate skills and practices are available in-house for carrying out participatory activities such as eliciting, capturing, and synthesizing user, operator and external feedback, and translating it for AI design and development functions.
- Establish mechanisms for regular communication and feedback between relevant AI actors and internal or external stakeholders related to system design or deployment decisions.
- Consider performance to human baseline metrics or other standard benchmarks.
- Incorporate feedback from end users, and potentially impacted individuals and communities about perceived system benefits .","- Perform context analysis to map potential negative impacts arising from not integrating trustworthiness characteristics. When negative impacts are not direct or obvious, AI actors can engage with stakeholders external to the team that developed or deployed the AI system, and potentially impacted communities, to examine and document:
	- Who could be harmed?
	- What could be harmed?
	- When could harm arise?
	- How could harm arise?
- Identify and implement procedures for regularly evaluating the qualitative and quantitative costs of internal and external AI system failures. Develop actions to prevent, detect, and/or correct potential risks and related impacts. Regularly evaluate failure costs to inform go/no-go deployment decisions throughout the AI system lifecycle.","- Consider narrowing contexts for system deployment, including factors related to:
        - How outcomes may directly or indirectly affect users, groups, communities and the environment.
        - Length of time the system is deployed in between re-trainings.
        - Geographical regions in which the system operates.
        - Dynamics related to community standards or likelihood of system misuse or abuses (either purposeful or unanticipated).
        - How AI system features and capabilities can be utilized within other applications, or in place of other existing processes.    
- Engage AI actors from legal and procurement functions when specifying target application scope.","- Identify and declare AI system features and capabilities that may affect downstream AI actors’ decision-making in deployment and operational settings for example how system features and capabilities may activate known risks in various human-AI configurations, such as selective adherence. 
- Identify skills and proficiency requirements for operators, practitioners and other domain experts that interact with AI systems,Develop AI system operational documentation for AI actors in deployed and operational environments, including information about known risks, mitigation criteria, and trustworthy characteristics enumerated in Map-1. 
- Define and develop training materials for proposed end users, practitioners and operators about AI system use and known limitations. 
- Define and develop certification procedures for operating AI systems within defined contexts of use, and information about what exceeds operational boundaries.    
- Include operators, practitioners and end users in AI system prototyping and testing activities to help inform operational boundaries and acceptable performance. Conduct testing activities under scenarios similar to deployment conditions. 
- Verify model output provided to AI system operators, practitioners and end users is  interactive, and specified to context and user requirements defined in MAP-1.
- Verify AI system output is interpretable and unambiguous for downstream decision making tasks. 
- Design AI system explanation complexity to match the level of problem and context complexity.
- Verify that design principles are in place for safe operation by AI actors in decision-making environments.
- Develop approaches to track human-AI configurations, operator, and practitioner outcomes for integration into continual improvement.","- Identify and document AI systems’ features and capabilities that require human oversight, in relation to operational and societal contexts, trustworthy characteristics, and risks identified in MAP-1. 
- Establish practices for AI systems’ oversight in accordance with policies developed in GOVERN-1. 
- Define and develop training materials for relevant AI Actors about AI system performance, context of use, known limitations and negative impacts, and suggested warning labels.
- Include relevant AI Actors in AI system prototyping and testing activities. Conduct testing activities under scenarios similar to deployment conditions. 
- Evaluate AI system oversight practices for validity and reliability. When oversight practices undergo extensive updates or adaptations, retest, evaluate results, and course correct as necessary.
- Verify that model documents contain interpretable descriptions of system mechanisms, enabling oversight personnel to make informed, risk-based decisions about system risks.","- Review audit reports, testing results, product roadmaps, warranties, terms of service, end user license agreements, contracts, and other documentation related to third-party entities to assist in value assessment and risk management activities.
- Review third-party software release schedules and software change management plans (hotfixes, patches, updates, forward- and backward- compatibility guarantees) for irregularities that may contribute to AI system risks.
- Inventory third-party material (hardware, open-source software, foundation models, open source data, proprietary software, proprietary data, etc.) required for system implementation and maintenance.
- Review redundancies related to third-party technology and personnel to assess potential risks due to lack of adequate support.","- Track third-parties preventing or hampering risk-mapping as indications of increased risk.  
- Supply resources such as model documentation templates and software safelists to assist in third-party technology inventory and approval activities.
- Review third-party material (including data and models) for risks related to bias, data privacy, and security vulnerabilities.
- Apply traditional technology risk controls – such as procurement, security, and data privacy controls – to all acquired third-party technologies.","- Establish assessment scales for measuring AI systems’ impact. Scales may be qualitative, such as red-amber-green (RAG), or may entail simulations or econometric approaches. Document and apply scales uniformly across the organization’s AI portfolio.
- Apply TEVV regularly at key stages in the AI lifecycle, connected to system impacts and frequency of system updates.
- Identify and document  likelihood and magnitude of system benefits and negative impacts in relation to trustworthiness characteristics.","- Establish and document stakeholder engagement processes at the earliest stages of system formulation to identify potential impacts from the AI system on individuals, groups, communities, organizations, and society.
- Employ methods such as value sensitive design (VSD) to identify misalignments between organizational and societal values, and system implementation and impact.
- Identify approaches to engage, capture, and incorporate input from system end users and other key stakeholders to assist with continuous monitoring for potential impacts and emergent risks.
- Incorporate quantitative, qualitative, and mixed methods in the assessment and documentation of potential impacts to individuals, groups, communities, organizations, and society.
- Identify a team (internal or external) that is independent of AI design and development functions to assess AI system benefits, positive and negative impacts and their likelihood and magnitude.
- Evaluate and document stakeholder feedback to assess potential impacts for actionable insights regarding trustworthiness characteristics and changes in design approaches and principles.
- Develop TEVV procedures that incorporate socio-technical elements and methods and plan to normalize across organizational culture. Regularly review and refine TEVV processes.","- Establish approaches for detecting, tracking and measuring known risks, errors, incidents or negative impacts.  
- Identify testing procedures and metrics to demonstrate whether or not the system is fit for purpose and functioning as claimed. 
- Identify testing procedures and metrics to demonstrate AI system trustworthiness
- Define acceptable limits for system performance (e.g. distribution of errors), and include course correction suggestions if/when the system performs beyond acceptable limits. 
- Define metrics for, and regularly assess, AI actor competency for effective system operation, 
- Identify transparency metrics to assess whether stakeholders have access to necessary information about system design, development, deployment, use, and evaluation. 
- Utilize accountability metrics to determine whether AI designers, developers, and deployers maintain clear and transparent lines of responsibility and are open to inquiries.
- Document metric selection criteria and include considered but unused metrics.
- Monitor AI system external inputs including training data, models developed for other contexts, system components reused from other contexts, and third-party tools and resources. 
- Report metrics to inform assessments of system generalizability and reliability. 
- Assess and  document pre- vs post-deployment system performance. Include existing and emergent  risks. 
- Document risks or trustworthiness characteristics identified in the Map function that will not be measured, including justification for non- measurement.","- Assess external validity of all measurements (e.g., the degree to which measurements taken in one context can generalize to other contexts).
- Assess effectiveness of existing metrics and controls on a regular basis throughout the AI system lifecycle.
- Document reports of errors, incidents and negative impacts and assess sufficiency and efficacy of existing metrics for repairs, and upgrades 
- Develop new metrics when existing metrics are insufficient or ineffective for implementing repairs and upgrades.
- Develop and utilize metrics to monitor, characterize and track external inputs, including any third-party tools.
- Determine frequency and scope for sharing metrics and related information with stakeholders and impacted communities. 
- Utilize stakeholder feedback processes established in the Map function to capture, act upon and share feedback from end users and potentially impacted communities.
- Collect and report software quality metrics such as rates of bug occurrence and severity, time to response, and time to repair (See Manage 4.3).","- Evaluate TEVV processes regarding incentives to identify risks and impacts. 
- Utilize separate testing teams established in the Govern function (2.1 and 4.1) to enable independent decisions and course-correction for AI systems. Track processes and measure and document change in performance. 
- Plan and evaluate AI system prototypes with end user populations early and continuously in the AI lifecycle. Document test outcomes and course correct.
- Assess independence and stature of TEVV and oversight AI actors, to ensure they have the required levels of independence and resources to perform assurance, compliance, and feedback tasks effectively 
- Evaluate interdisciplinary and demographically diverse internal team established in Map 1.2 
- Evaluate effectiveness of external stakeholder feedback mechanisms, specifically related to processes for eliciting, evaluating and integrating input from diverse groups.  
- Evaluate effectiveness of external stakeholder feedback mechanisms for enhancing AI actor visibility and decision making regarding AI system risks and trustworthy characteristics.","- Leverage existing industry best practices for transparency and documentation of all possible aspects of measurements. Examples include: data sheet for data sets, model cards, [commenters provided examples]
- Regularly assess the effectiveness of tools used to document measurement approaches, test sets, metrics, processes and materials used
- Update the tools as needed","- Follow human subjects research requirements as established by organizational and disciplinary requirements, including informed consent and compensation, during dataset collection activities.
- Analyze differences between intended and actual population of users or data subjects, including likelihood for errors, incidents or negative impacts.
- Utilize disaggregated evaluation methods (e.g. by race, age, gender, ethnicity, ability, region) to improve AI system performance when deployed in real world settings. 
- Establish thresholds and alert procedures for dataset representativeness within the context of use. 
- Construct datasets in close collaboration with experts with knowledge of the context of use.
- Follow intellectual property and privacy rights related to datasets and their use, including for the subjects represented in the data.
- Evaluate data representativeness through 
	- investigating known failure modes, 
	- assessing data quality and diverse sourcing, 
	- applying public benchmarks, 
	- traditional bias testing, 
	- chaos engineering, 
	- stakeholder feedback 
- Use informed consent for individuals providing data used in system testing and evaluation.","- Conduct regular and sustained engagement with potentially impacted communities 
- Maintain a demographically diverse and multidisciplinary and collaborative internal team
- Regularly test and evaluate systems in non-optimized conditions, and in collaboration with AI actors in user interaction and user experience (UI/UX) roles. 
- Evaluate feedback from stakeholder engagement activities, in collaboration with human factors and socio-technical experts.
- Collaborate with socio-technical, human factors, and UI/UX experts to identify notable characteristics in context of use that can be translated into system testing scenarios.
- Measure AI systems prior to deployment in conditions similar to expected scenarios. 
- Measure and document performance criteria such as validity (false positive rate, false negative rate, etc.) and efficiency (training times, prediction latency, etc.) related to ground truth within the deployment context of use.
- Measure assurance criteria such as AI actor competency and experience. 
- Document differences between measurement setting and the deployment environment(s).","- Monitor and document how metrics and performance indicators observed in production differ from the same metrics collected during pre-deployment testing. When differences are observed, consider error propagation and feedback loop risks. 
- Utilize hypothesis testing or human domain expertise to measure monitored distribution differences in new input or output data relative to test environments
- Monitor for anomalies using approaches such as control limits, confidence intervals, integrity constraints and ML algorithms. When anomalies are observed, consider error propagation and feedback loop risks. 
- Verify alerts are in place for when distributions in new input data or generated predictions observed in production differ from pre-deployment test outcomes, or when anomalies are detected.
- Assess the accuracy and quality of generated outputs against new collected ground-truth information as it becomes available. 
- Utilize human review to track processing of unexpected data and reliability of generated outputs; warn system users when outputs may be unreliable. Verify that human overseers responsible for these processes have clearly defined responsibilities and training for specified tasks.
- Collect uses cases from the operational environment for system testing and monitoring activities in accordance with organizational policies and regulatory or disciplinary requirements (e.g. informed consent, institutional review board approval, human research protections),","- Define the operating conditions and socio-technical context under which the AI system will be validated.
- Define and document processes to establish the system’s operational conditions and limits.  
- Establish or identify, and document approaches to measure forms of validity, including:
    - construct validity (the test  is measuring the concept it claims to measure)
    - internal validity (relationship being tested is not influenced by other factors or variables) 
    - external validity (results are generalizable beyond the training condition)   
    - the use of experimental design principles and statistical analyses and modeling.
- Assess and document system variance. Standard approaches include confidence intervals, standard deviation, standard error, bootstrapping, or cross-validation. 
- Establish or identify, and document robustness measures.
- Establish or identify, and document reliability measures.
- Establish practices to specify and document the assumptions underlying measurement models to ensure proxies accurately reflect the concept being measured.
- Utilize standard software testing approaches (e.g. unit, integration, functional and chaos testing, computer-generated test cases, etc.)
- Utilize standard statistical methods to test bias, inferential associations, correlation, and covariance in adopted measurement models.
- Utilize standard statistical methods to test variance and reliability of system outcomes.
- Monitor operating conditions for system performance outside of defined limits. 
- Identify TEVV approaches for exploring AI system limitations, including testing scenarios that differ from the operational environment. Consult experts with knowledge of specific context of use.
- Define post-alert actions. Possible actions may include:
    - alerting other relevant AI actors before action, 
    - requesting subsequent human review of action, 
    - alerting downstream users and stakeholder that the system is operating outside it’s defined validity limits, 
    - tracking and mitigating possible error propagation
    - action logging 
- Log input data and relevant system configuration information whenever there is an attempt to use the system beyond its well-defined range of system validity.
- Modify the system over time to extend its range of system validity to new operating conditions.","- Thoroughly measure system performance in development and deployment contexts, and under stress conditions.
    - Employ test data assessments and simulations before proceeding to production testing. Track multiple performance quality and error metrics. 
    - Stress-test system performance under likely scenarios (e.g., concept drift, high load) and beyond known limitations, in consultation with domain experts.
    - Test the system under conditions similar to those related to past known incidents or near-misses and measure system performance and safety characteristics 
    - Apply chaos engineering approaches to test systems in extreme conditions and gauge unexpected responses.
    - Document the range of conditions under which the system has been tested and demonstrated to fail safely.
- Measure and monitor system performance in real-time  to enable rapid response when AI system incidents are detected.
- Collect pertinent safety statistics (e.g., out-of-range performance, incident response times, system down time, injuries, etc.) in anticipation of potential information sharing with impacted communities or as required by AI system oversight personnel. 
- Align measurement to the goal of continuous improvement. Seek to increase the range of conditions under which the system is able to fail safely through system modifications in response to in-production testing and events.
- Document, practice and measure incident response plans for AI system incidents, including measuring response and down times.
- Compare documented safety testing and monitoring information with established risk tolerances on an on-going basis.
- Consult MANAGE for detailed information related to managing safety risks. ","- Establish and track AI system security tests and metrics (e.g.,  red-teaming activities, frequency and rate of anomalous events, system down-time, incident response times, time-to-bypass, etc.).
- Use red-team exercises to actively test the system under adversarial or stress conditions, measure system response, assess failure modes or determine if system can return to normal function after an unexpected adverse event. 
- Document red-team exercise results as part of continuous improvement efforts, including the range of security test conditions and results. 
- Use countermeasures (e.g, authentication, throttling, differential privacy, robust ML approaches) to increase the range of security conditions under which the system is able to return to normal function.
- Modify system security procedures and countermeasures to increase robustness and resilience to attacks in response to testing and events experienced in production.
- Verify that information about errors and attack patterns is shared with incident databases, other organizations with similar systems, and system users and stakeholders (MANAGE-4.1).
- Develop and maintain information sharing practices with AI actors from other organizations to learn from common attacks. 
- Verify that third party AI resources and personnel undergo security audits and screenings. Risk indicators may include failure of third parties to provide relevant security information.
- Utilize watermarking technologies as a deterrent to data and model extraction attacks.","- Instrument the system for measurement and tracking, e.g., by maintaining histories, audit logs and other information that can be used by AI actors to review and evaluate possible sources of error, bias, or vulnerability.
- Calibrate controls for users in close collaboration with experts in user interaction and user experience (UI/UX), human computer interaction (HCI), and/or human-AI teaming.
- Test provided explanations for calibration with different audiences including operators, end users, decision makers and decision subjects (individuals for whom decisions are being made), and to enable recourse for consequential system decisions that affect end users or subjects.
- Measure and document human oversight of AI systems: 
	- Document the degree of oversight that is provided by specified AI actors regarding AI system output.  
	- Maintain statistics about downstream actions by end users and operators such as system overrides.
	- Maintain statistics about and document reported errors or complaints, time to respond, and response types. 
	- Maintain and report statistics about adjudication activities.
- Track, document, and measure organizational accountability regarding AI systems via policy exceptions and escalations, and document “go” and “no/go” decisions made by accountable parties. 
- Track and audit the effectiveness of organizational mechanisms related to AI risk management, including:
	- Lines of communication between AI actors, executive leadership, users and impacted communities.
	- Roles and responsibilities for AI actors and executive leadership.
	- Organizational accountability roles, e.g., chief model risk officers, AI oversight committees, responsible or ethical AI directors, etc.","- Verify systems are developed to produce explainable models, post-hoc explanations and audit logs. 
- When possible or available, utilize approaches that are inherently explainable, such as traditional and penalized generalized linear models , decision trees, nearest-neighbor and prototype-based approaches, rule-based models, generalized additive models , explainable boosting machines  and neural additive models.   
- Test explanation methods and resulting explanations prior to deployment to gain feedback from relevant AI actors, end users, and potentially impacted individuals or groups about whether explanations are accurate, clear, and understandable.
- Document AI model details including model type  (e.g., convolutional neural network, reinforcement learning, decision tree, random forest, etc.) data features, training algorithms, proposed uses, decision thresholds, training data, evaluation data, and ethical considerations.
- Establish, document, and report performance and error metrics across demographic groups and other segments relevant to the deployment context.
- Explain systems using a variety of methods, e.g., visualizations, model extraction, feature importance, and others. Since explanations may not accurately summarize complex systems, test explanations according to properties such as fidelity, consistency, robustness, and interpretability.
- Assess the characteristics of system explanations according to properties such as fidelity (local and global), ambiguity, interpretability, interactivity, consistency, and resilience to attack/manipulation.
- Test the quality of system explanations with end-users and other groups. 
- Secure model development processes to avoid vulnerability to external manipulation such as gaming explanation processes. 
- Test for changes in models over time, including for models that adjust in response to production data. 
- Use transparency tools such as data statements and model cards to document explanatory and validation information.","- Specify privacy-related values, frameworks, and attributes that are applicable in the context of use through direct engagement with end users and potentially impacted groups and communities.
- Document collection, use, management, and disclosure of personally sensitive information in datasets, in accordance with privacy and data governance policies
- Quantify privacy-level data aspects such as the ability to identify individuals or groups (e.g. k-anonymity metrics, l-diversity, t-closeness).
- Establish and document protocols (authorization, duration, type) and access controls for training sets or production data containing personally sensitive information, in accordance with privacy and data governance policies. 
- Monitor internal queries to production data for detecting patterns that isolate personal records.
- Monitor PSI disclosures and inference of sensitive or legally protected attributes 
	- Assess the risk of manipulation from overly customized content. Evaluate information presented to representative users at various points along axes of difference between individuals (e.g. individuals of different ages, genders, races, political affiliation, etc.). 
- Use privacy-enhancing techniques such as differential privacy,  when publicly sharing dataset information. 
- Collaborate with privacy experts, AI end users and operators, and other domain experts to determine optimal differential privacy metrics within contexts of use.","- Conduct fairness assessments to manage computational and statistical forms of bias which include the following steps:
    - Identify types of harms, including allocational, representational, quality of service, stereotyping, or erasure
    - Identify across, within, and intersecting groups that might be harmed
    - Quantify harms using both a general fairness metric, if appropriate (e.g. demographic parity, equalized odds, equal opportunity, statistical hypothesis tests), and custom, context-specific metrics developed in collaboration with affected communities
    - Analyze quantified harms for contextually significant differences across groups, within groups, and among intersecting groups 
    - Refine identification of within-group and intersectional group disparities. 
        - Evaluate underlying data distributions and employ sensitivity analysis during the analysis of quantified harms. 
        - Evaluate quality  metrics including false positive rates and false negative rates. 
        - Consider biases affecting small groups, within-group or intersectional communities, or single individuals.
- Understand and consider sources of bias in training and TEVV data:
    - Differences in distributions of outcomes across and within groups, including intersecting groups. 
    - Completeness, representativeness and balance of data sources. 
    - Identify input data features that may serve as proxies for demographic group membership (i.e., credit score, ZIP code) or otherwise give rise to emergent bias within AI systems.   
    - Forms of systemic bias in images, text (or word embeddings), audio or other complex or unstructured data.  
- Leverage impact assessments to identify and classify system impacts and harms to end users, other individuals, and groups with input from potentially impacted communities.
- Identify the classes of individuals, groups, or environmental ecosystems which might be impacted through direct engagement with potentially impacted communities. 
- Evaluate systems in regards to disability inclusion, including consideration of disability status in bias testing, and discriminatory screen out processes that may arise from non-inclusive design or deployment decisions. 
- Develop objective functions in consideration of systemic biases, in-group/out-group dynamics.
- Use context-specific fairness metrics to examine how system performance varies across  groups, within groups, and/or for intersecting groups. Metrics may include statistical parity, error-rate equality, statistical parity difference, equal opportunity difference, average absolute odds difference, standardized mean difference, percentage point differences.
- Customize fairness metrics to specific context of use to examine how system performance and potential harms vary within contextual norms. 
- Define acceptable levels of difference in performance in accordance with established organizational governance policies, business requirements, regulatory compliance, legal frameworks, and ethical standards within the context of use
- Define the actions to be taken if disparity levels rise above acceptable levels. 
- Identify groups within the expected population that may require disaggregated analysis, in collaboration with impacted communities. 
- Leverage experts with knowledge in the specific context of use to investigate substantial measurement differences and identify root causes for those differences.
- Monitor system outputs for performance or bias issues that exceed established tolerance levels.  
- Ensure periodic model updates; test and recalibrate with updated and more representative data to stay within acceptable levels of difference.
- Apply pre-processing data transformations to address factors related to demographic balance and data representativeness.
- Apply in-processing to balance model performance quality with bias considerations. 
- Apply post-processing mathematical/computational techniques to model results in close collaboration with impact assessors, socio-technical experts, and other AI actors with expertise in the context of use. 
- Apply model selection approaches with transparent and deliberate consideration of bias management and other trustworthy characteristics. 
- Collect and share information about differences in outcomes for the identified groups. 
- Consider mediations to mitigate differences, especially those that can be traced to past patterns of unfair or biased human decision making.
- Utilize human-centered design practices to generate deeper focus on societal impacts and counter human-cognitive biases within the AI lifecycle.
- Evaluate practices along the lifecycle to identify potential sources of human-cognitive bias such as availability, observational, and confirmation bias, and to make implicit decision making processes more explicit and open to investigation. 
- Work with human factors experts to evaluate biases in the presentation of system output to end users, operators and practitioners.
- Utilize processes to enhance contextual awareness, such as diverse internal staff and stakeholder engagement.","- Include environmental impact indicators in AI system design and development plans, including reducing consumption and improving efficiencies.
- Identify and implement key indicators of AI system energy and water consumption and efficiency, and/or GHG emissions. 
- Establish measurable baselines for sustainable AI system operation in accordance with organizational policies, regulatory compliance, legal frameworks, and environmental protection and sustainability norms.
- Assess tradeoffs between AI system performance and sustainable operations in accordance with organizational principles and policies, regulatory compliance, legal frameworks, and environmental protection and sustainability norms.
- Identify and establish acceptable resource consumption and efficiency, and GHG emissions levels, along with actions to be taken if indicators rise above acceptable levels.
- Estimate AI system emissions levels throughout the AI lifecycle via carbon calculators or similar process.","- Review selected system metrics and associated TEVV processes to determine if they are able to sustain system improvements, including the identification and removal of errors.
- Regularly evaluate system metrics for utility, and consider descriptive approaches in place of overly complex methods.
- Review selected system metrics for acceptability within the end user and impacted community of interest.
- Assess effectiveness of metrics for identifying and measuring risks.","- Compare AI system risks with:
	- simpler or traditional models
	- human baseline performance
	- other manual performance benchmarks
- Compare end user and community feedback about deployed AI systems to internal measures of system performance.
- Assess effectiveness of metrics for identifying and measuring emergent risks.
- Measure error response times and track response quality. 
- Elicit and track feedback from AI actors in user support roles about the type of metrics, explanations and other system information required for fulsome resolution of system issues. Consider:
	- Instances where explanations are insufficient for investigating possible error sources or identifying responses.
	- System metrics, including system logs and explanations, for identifying and diagnosing sources of system error. 
- Elicit and track feedback from AI actors in incident response and support roles about the adequacy of staffing and resources to perform their duties in an effective and timely manner.","- Establish processes for tracking emergent risks that may not be measurable with current approaches. Some processes may include:
	- Recourse mechanisms for faulty AI system outputs.
	- Bug bounties.
	- Human-centered design approaches.
	- User-interaction and experience research.
	- Participatory stakeholder engagement with affected or potentially impacted individuals and communities.
- Identify AI actors responsible for tracking emergent risks and inventory methods. 
- Determine and document the rate of occurrence and severity level for complex or difficult-to-measure risks when:
	- Prioritizing new measurement approaches for deployment tasks. 
	- Allocating AI system risk management resources.
	- Evaluating AI system improvements.
	- Making go/no-go decisions for subsequent system iterations.","- Measure efficacy of end user and operator error reporting processes.
- Categorize and analyze type and rate of end user appeal requests and results.
- Measure feedback activity participation rates and awareness of feedback activity availability.
- Utilize feedback to analyze measurement approaches and determine subsequent courses of action.
- Evaluate measurement approaches to determine efficacy for enhancing organizational understanding of real world impacts. 
- Analyze end user and community feedback in close collaboration with domain experts.","- Support mechanisms for capturing feedback from system end users (including domain experts, operators, and practitioners). Successful approaches are:
	- conducted in settings where end users are able to openly share their doubts and insights about AI system output, and in connection to their specific context of use (including setting and task-specific lines of inquiry)
	- developed and implemented by human-factors and socio-technical domain experts and researchers
	- designed to ensure control of interviewer and end user subjectivity and biases 
- Identify and document approaches
	- for evaluating and integrating elicited feedback from system end users 
	- in collaboration with human-factors and socio-technical domain experts, 
	- to actively inform a process of continual improvement.
- Evaluate feedback from end users alongside evaluated feedback from impacted communities (MEASURE 3.3). 
- Utilize end user feedback to investigate how selected metrics and measurement approaches interact with organizational and operational contexts.
- Analyze and document system-internal measurement processes in comparison to collected end user feedback.
- Identify and implement approaches to measure effectiveness and satisfaction with end user elicitation techniques, and document results.","- Integrate feedback from end users, operators, and affected individuals and communities from Map function as inputs to assess AI system trustworthiness characteristics. Ensure both positive and negative feedback is being assessed.
- Evaluate feedback in connection with AI system trustworthiness characteristics from Measure 2.5 to 2.11.
- Evaluate feedback regarding end user satisfaction with, and confidence in, AI system performance including whether output is considered valid and reliable, and explainable and interpretable. 
- Identify mechanisms to confirm/support AI system output (e.g. recommendations), and end user perspectives about that output. 
- Measure frequency of AI systems’ override decisions, evaluate and document results, and feed insights back into continual improvement processes. 
- Consult AI actors in impact assessment, human factors and socio-technical tasks to assist with analysis and interpretation of results.","- Develop baseline quantitative measures for trustworthy characteristics. 
- Delimit and characterize baseline operation values and states. 
- Utilize qualitative approaches to augment and complement quantitative baseline measures, in close coordination with impact assessment, human factors and socio-technical AI actors. 
- Monitor and assess measurements as part of continual improvement to identify potential system adjustments or modifications
- Perform and document sensitivity analysis to characterize actual and expected variance in performance after applying system or procedural updates. 
- Document decisions related to the sensitivity analysis and record expected influence on  system performance and identified risks."
section_doc,"### Organizations can document the following
- To what extent has the entity defined and documented the regulatory environment—including minimum requirements in laws and regulations?
- Has the system been reviewed for its compliance to applicable laws, regulations, standards, and guidance? 
- To what extent has the entity defined and documented the regulatory environment—including applicable requirements in laws and regulations? 
- Has the system been reviewed for its compliance to relevant applicable laws, regulations, standards, and guidance? 

### AI Transparency Resources

GAO-21-519SP: AI Accountability Framework for Federal Agencies & Other Entities. [URL](https://www.gao.gov/products/gao-21-519sp)","### Organizations can document the following
- To what extent do these policies foster public trust and confidence in the use of the AI system?
- What policies has the entity developed to ensure the use of the AI system is consistent with its stated values and principles?
- What policies and documentation has the entity developed to encourage the use of its AI system as intended?
- To what extent are the model outputs consistent with the entity’s values and principles to foster public trust and equity?

### AI Transparency Resources


GAO-21-519SP: AI Accountability Framework for Federal Agencies & Other Entities. [URL](https://www.gao.gov/products/gao-21-519sp)","### Organizations can document the following
- How do system performance metrics inform risk tolerance decisions?
- What policies has the entity developed to ensure the use of the AI system is consistent with organizational risk tolerance?
- How do the entity’s data security and privacy assessments inform risk tolerance decisions?


### AI Transparency Resources
- GAO-21-519SP: AI Accountability Framework for Federal Agencies & Other Entities. [URL](https://www.gao.gov/products/gao-21-519sp)","### Organizations can document the following
- To what extent has the entity clarified the roles, responsibilities, and delegated authorities to relevant stakeholders?
- What are the roles, responsibilities, and delegation of authorities of personnel involved in the design, development, deployment, assessment and monitoring of the AI system?
- How will the appropriate performance metrics, such as accuracy, of the AI be monitored after the AI is deployed? How much distributional shift or model drift from baseline performance is acceptable?

### AI Transparency Resources
- GAO-21-519SP: AI Accountability Framework for Federal Agencies & Other Entities. [URL](https://www.gao.gov/products/gao-21-519sp)
- Intel.gov: AI Ethics Framework for Intelligence Community  - 2020. [URL](https://www.intelligence.gov/artificial-intelligence-ethics-framework-for-the-intelligence-community)","### Organizations can document the following
- To what extent does the system/entity consistently measure progress towards stated goals and objectives?
- Did your organization implement a risk management system to address risks involved in deploying the identified AI solution (e.g. personnel risk or changes to commercial objectives)?
- Did your organization address usability problems and test whether user interfaces served their intended purposes? 

### AI Transparency Resources
- GAO-21-519SP: AI Accountability Framework for Federal Agencies & Other Entities. [URL](https://www.gao.gov/products/gao-21-519sp)
- WEF Model AI Governance Framework Assessment 2020. [URL](https://www.pdpc.gov.sg/-/media/Files/PDPC/PDF-Files/Resource-for-Organisation/AI/SGModelAIGovFramework2.pdf)","### Organizations can document the following
- Who is responsible for documenting and maintaining the AI system inventory details?
- What processes exist for data generation, acquisition/collection, ingestion, staging/storage, transformations, security, maintenance, and dissemination?
- Given the purpose of this AI, what is an appropriate interval for checking whether it is still accurate, unbiased, explainable, etc.? What are the checks for this model?
- What processes exist for data generation, acquisition/collection, ingestion, staging/storage, transformations, security, maintenance, and dissemination?

### AI Transparency Resources
- GAO-21-519SP: AI Accountability Framework for Federal Agencies & Other Entities. [URL](https://www.gao.gov/products/gao-21-519sp)
- Intel.gov: AI Ethics Framework for Intelligence Community - 2020. [URL](https://www.intelligence.gov/artificial-intelligence-ethics-framework-for-the-intelligence-community)","### Organizations can document the following
- What processes exist for data generation, acquisition/collection, ingestion, staging/storage, transformations, security, maintenance, and dissemination?
- To what extent do these policies foster public trust and confidence in the use of the AI system?
- If anyone believes that the AI no longer meets this ethical framework, who will be responsible for receiving the concern and as appropriate investigating and remediating the issue? Do they have authority to modify, limit, or stop the use of the AI?
- If it relates to people, were there any ethical review applications/reviews/approvals? (e.g. Institutional Review Board applications)

### AI Transparency Resources
- GAO-21-519SP: AI Accountability Framework for Federal Agencies & Other Entities. [URL](https://www.gao.gov/products/gao-21-519sp)
- Intel.gov: AI Ethics Framework for Intelligence Community - 2020. [URL](https://www.intelligence.gov/artificial-intelligence-ethics-framework-for-the-intelligence-community)
- Datasheets for Datasets. [URL](http://arxiv.org/abs/1803.09010)","### Organizations can document the following
- To what extent has the entity clarified the roles, responsibilities, and delegated authorities to relevant stakeholders?
- Who is ultimately responsible for the decisions of the AI and is this person aware of the intended uses and limitations of the analytic?
- Are the responsibilities of the personnel involved in the various AI governance processes clearly defined?
- What are the roles, responsibilities, and delegation of authorities of personnel involved in the design, development, deployment, assessment and monitoring of the AI system?
- Did your organization implement accountability-based practices in data management and protection (e.g. the PDPA and OECD Privacy Principles)?

### AI Transparency Resources
- WEF Model AI Governance Framework Assessment 2020. [URL](https://www.pdpc.gov.sg/-/media/Files/PDPC/PDF-Files/Resource-for-Organisation/AI/SGModelAIGovFramework2.pdf)
- WEF Companion to the Model AI Governance Framework- 2020. [URL](https://www.pdpc.gov.sg/-/media/Files/PDPC/PDF-Files/Resource-for-Organisation/AI/SGIsago.pdf)
- GAO-21-519SP: AI Accountability Framework for Federal Agencies & Other Entities. [URL](https://www.gao.gov/products/gao-21-519sp)","### Organizations can document the following
- Are the relevant staff dealing with AI systems properly trained to interpret AI model output and decisions as well as to detect and manage bias in data?
- How does the entity determine the necessary skills and experience needed to design, develop, deploy, assess, and monitor the AI system?
- How does the entity assess whether personnel have the necessary skills, training, resources, and domain knowledge to fulfill their assigned responsibilities?
- What efforts has the entity undertaken to recruit, develop, and retain a workforce with backgrounds, experience, and perspectives that reflect the community impacted by the AI system?

### AI Transparency Resources
- WEF Model AI Governance Framework Assessment 2020. [URL](https://www.pdpc.gov.sg/-/media/Files/PDPC/PDF-Files/Resource-for-Organisation/AI/SGModelAIGovFramework2.pdf)
- WEF Companion to the Model AI Governance Framework- 2020. [URL](https://www.pdpc.gov.sg/-/media/Files/PDPC/PDF-Files/Resource-for-Organisation/AI/SGIsago.pdf)
- GAO-21-519SP: AI Accountability Framework for Federal Agencies & Other Entities. [URL](https://www.gao.gov/products/gao-21-519sp)","### Organizations can document the following
- Did your organization’s board and/or senior management sponsor, support and participate in your organization’s AI governance?
- What are the roles, responsibilities, and delegation of authorities of personnel involved in the design, development, deployment, assessment and monitoring of the AI system?
- Do AI solutions provide sufficient information to assist the personnel to make an informed decision and take actions accordingly?
- To what extent has the entity clarified the roles, responsibilities, and delegated authorities to relevant stakeholders?

### AI Transparency Resources
- WEF Companion to the Model AI Governance Framework- 2020. [URL](https://www.pdpc.gov.sg/-/media/Files/PDPC/PDF-Files/Resource-for-Organisation/AI/SGIsago.pdf)
- GAO-21-519SP: AI Accountability Framework for Federal Agencies & Other Entities. [URL](https://www.gao.gov/products/gao-21-519sp)","### Organizations can document the following
- Are the relevant staff dealing with AI systems properly trained to interpret AI model output and decisions as well as to detect and manage bias in data?
- Entities include diverse perspectives from technical and non-technical communities throughout the AI life cycle to anticipate and mitigate unintended consequences including potential bias and discrimination.
- Stakeholder involvement: Include diverse perspectives from a community of stakeholders throughout the AI life cycle to mitigate risks.
- Strategies to incorporate diverse perspectives include establishing collaborative processes and multidisciplinary teams that involve subject matter experts in data science, software development, civil liberties, privacy and security, legal counsel, and risk management.
- To what extent are the established procedures effective in mitigating bias, inequity, and other concerns resulting from the system?

### AI Transparency Resources
- WEF Model AI Governance Framework Assessment 2020. [URL](https://www.pdpc.gov.sg/-/media/Files/PDPC/PDF-Files/Resource-for-Organisation/AI/SGModelAIGovFramework2.pdf)
- Datasheets for Datasets. [URL](http://arxiv.org/abs/1803.09010)","### Organizations can document the following
- What type of information is accessible on the design, operations, and limitations of the AI system to external stakeholders, including end users, consumers, regulators, and individuals impacted by use of the AI system?
-  To what extent has the entity documented the appropriate level of human involvement in AI-augmented decision-making?
- How will the accountable human(s) address changes in accuracy and precision due to either an adversary’s attempts to disrupt the AI or unrelated changes in operational/business environment, which may impact the accuracy of the AI?
- To what extent has the entity clarified the roles, responsibilities, and delegated authorities to relevant stakeholders?
- How does the entity assess whether personnel have the necessary skills, training, resources, and domain knowledge to fulfill their assigned responsibilities?

### AI Transparency Resources
- GAO-21-519SP: AI Accountability Framework for Federal Agencies & Other Entities. [URL](https://www.gao.gov/products/gao-21-519sp)
- Intel.gov: AI Ethics Framework for Intelligence Community - 2020. [URL](https://www.intelligence.gov/artificial-intelligence-ethics-framework-for-the-intelligence-community)
- WEF Companion to the Model AI Governance Framework- 2020. [URL](https://www.pdpc.gov.sg/-/media/Files/PDPC/PDF-Files/Resource-for-Organisation/AI/SGIsago.pdf)","### Organizations can document the following
- To what extent has the entity documented the AI system’s development, testing methodology, metrics, and performance outcomes?
- Are organizational information sharing practices widely followed and transparent, such that related past failed designs can be avoided? 
- Are training manuals and other resources for carrying out incident response documented and available? 
- Are processes for operator reporting of incidents and near-misses documented and available?


### AI Transparency Resources
- Datasheets for Datasets. [URL](http://arxiv.org/abs/1803.09010)
- GAO-21-519SP: AI Accountability Framework for Federal Agencies & Other Entities. [URL](https://www.gao.gov/products/gao-21-519sp)
- WEF Model AI Governance Framework Assessment 2020. [URL](https://www.pdpc.gov.sg/-/media/Files/PDPC/PDF-Files/Resource-for-Organisation/AI/SGModelAIGovFramework2.pdf)","### Organizations can document the following
- How has the entity identified and mitigated potential impacts of bias in the data, including inequitable or discriminatory outcomes?
- How has the entity documented the AI system’s data provenance, including sources, origins, transformations, augmentations, labels, dependencies, constraints, and metadata?
- To what extent has the entity clearly defined technical specifications and requirements for the AI system?
- To what extent has the entity documented and communicated the AI system’s development, testing methodology, metrics, and performance outcomes?
- Have you documented and explained that machine errors may differ from human errors?

### AI Transparency Resources
- GAO-21-519SP: AI Accountability Framework for Federal Agencies & Other Entities. [URL](https://www.gao.gov/products/gao-21-519sp)
- Datasheets for Datasets. [URL](http://arxiv.org/abs/1803.09010)","### Organizations can document the following
- Did your organization address usability problems and test whether user interfaces served their intended purposes? Consulting the community or end users at the earliest stages of development to ensure there is transparency on the technology used and how it is deployed.
- Did your organization implement a risk management system to address risks involved in deploying the identified AI solution (e.g. personnel risk or changes to commercial objectives)?
- To what extent can users or parties affected by the outputs of the AI system test the AI system and provide feedback?

### AI Transparency Resources
- WEF Model AI Governance Framework Assessment 2020. [URL](https://www.pdpc.gov.sg/-/media/Files/PDPC/PDF-Files/Resource-for-Organisation/AI/SGModelAIGovFramework2.pdf)
- WEF Companion to the Model AI Governance Framework- 2020. [URL](https://www.pdpc.gov.sg/-/media/Files/PDPC/PDF-Files/Resource-for-Organisation/AI/SGIsago.pdf)","### Organizations can document the following 
- What type of information is accessible on the design, operations, and limitations of the AI system to external stakeholders, including end users, consumers, regulators, and individuals impacted by use of the AI system?
- To what extent has the entity clarified the roles, responsibilities, and delegated authorities to relevant stakeholders?
- How easily accessible and current is the information available to external stakeholders?
- What was done to mitigate or reduce the potential for harm?
- Stakeholder involvement: Include diverse perspectives from a community of stakeholders throughout the AI life cycle to mitigate risks.

### AI Transparency Resources
- Datasheets for Datasets. [URL](http://arxiv.org/abs/1803.09010)
- GAO-21-519SP: AI Accountability Framework for Federal Agencies & Other Entities. [URL](https://www.gao.gov/products/gao-21-519sp)
- AI policies and initiatives, in Artificial Intelligence in Society, OECD, 2019. [URL](https://www.oecd.org/publications/artificial-intelligence-in-society-eedfee77-en.htm)
- Stakeholders in Explainable AI, Sep. 2018. [URL](http://arxiv.org/abs/1810.00184)","### Organizations can document the following
- Who is ultimately responsible for the decisions of the AI and is this person aware of the intended uses and limitations of the analytic?
- Who will be responsible for maintaining, re-verifying, monitoring, and updating this AI once deployed?
- Who is accountable for the ethical considerations during all stages of the AI lifecycle?
- To what extent are the established procedures effective in mitigating bias, inequity, and other concerns resulting from the system?
- Does the AI solution provide sufficient information to assist the personnel to make an informed decision and take actions accordingly?

### AI Transparency Resources
- WEF Model AI Governance Framework Assessment 2020. [URL](https://www.pdpc.gov.sg/-/media/Files/PDPC/PDF-Files/Resource-for-Organisation/AI/SGModelAIGovFramework2.pdf)
- WEF Companion to the Model AI Governance Framework- 2020. [URL](https://www.pdpc.gov.sg/-/media/Files/PDPC/PDF-Files/Resource-for-Organisation/AI/SGIsago.pdf)
- Stakeholders in Explainable AI, Sep. 2018. [URL](http://arxiv.org/abs/1810.00184)
- AI policies and initiatives, in Artificial Intelligence in Society, OECD, 2019. [URL](https://www.oecd.org/publications/artificial-intelligence-in-society-eedfee77-en.htm)","### Organizations can document the following
- Did you establish mechanisms that facilitate the AI system’s auditability (e.g. traceability of the development process, the sourcing of training data and the logging of the AI system’s processes, outcomes, positive and negative impact)?
- If a third party created the AI, how will you ensure a level of explainability or interpretability?
- Did you ensure that the AI system can be audited by independent third parties?
- Did you establish a process for third parties (e.g. suppliers, end users, subjects, distributors/vendors or workers) to report potential vulnerabilities, risks or biases in the AI system?
- To what extent does the plan specifically address risks associated with acquisition, procurement of packaged software from vendors, cybersecurity controls, computational infrastructure, data, data science, deployment mechanics, and system failure?

### AI Transparency Resources
- GAO-21-519SP: AI Accountability Framework for Federal Agencies & Other Entities. [URL](https://www.gao.gov/products/gao-21-519sp)
- Intel.gov: AI Ethics Framework for Intelligence Community  - 2020. [URL](https://www.intelligence.gov/artificial-intelligence-ethics-framework-for-the-intelligence-community)
- WEF Model AI Governance Framework Assessment 2020. [URL](https://www.pdpc.gov.sg/-/media/Files/PDPC/PDF-Files/Resource-for-Organisation/AI/SGModelAIGovFramework2.pdf)
- WEF Companion to the Model AI Governance Framework- 2020. [URL](https://www.pdpc.gov.sg/-/media/Files/PDPC/PDF-Files/Resource-for-Organisation/AI/SGIsago.pdf)
- AI policies and initiatives, in Artificial Intelligence in Society, OECD, 2019. [URL](https://www.oecd.org/publications/artificial-intelligence-in-society-eedfee77-en.htm)
- Assessment List for Trustworthy AI (ALTAI) - The High-Level Expert Group on AI - 2019. [URL](https://digital-strategy.ec.europa.eu/en/library/assessment-list-trustworthy-artificial-intelligence-altai-self-assessment)","### Organizations can document the following
- To what extent does the plan specifically address risks associated with acquisition, procurement of packaged software from vendors, cybersecurity controls, computational infrastructure, data, data science, deployment mechanics, and system failure?
- Did you establish a process for third parties (e.g. suppliers, end users, subjects, distributors/vendors or workers) to report potential vulnerabilities, risks or biases in the AI system?
- If your organization obtained datasets from a third party, did your organization assess and manage the risks of using such datasets?

### AI Transparency Resources
- GAO-21-519SP: AI Accountability Framework for Federal Agencies & Other Entities. [URL](https://www.gao.gov/products/gao-21-519sp)
- WEF Model AI Governance Framework Assessment 2020. [URL](https://www.pdpc.gov.sg/-/media/Files/PDPC/PDF-Files/Resource-for-Organisation/AI/SGModelAIGovFramework2.pdf)
- WEF Companion to the Model AI Governance Framework- 2020. [URL](https://www.pdpc.gov.sg/-/media/Files/PDPC/PDF-Files/Resource-for-Organisation/AI/SGIsago.pdf)
- AI policies and initiatives, in Artificial Intelligence in Society, OECD, 2019. [URL](https://www.oecd.org/publications/artificial-intelligence-in-society-eedfee77-en.htm)","### Organizations can document the following

- How do the technical specifications and requirements align with the AI system’s goals and objectives?
- To what extent are the metrics consistent with system goals, objectives, and constraints, including ethical and compliance considerations?
- What goals and objectives does the entity expect to achieve by designing, developing, and/or deploying the AI system?

### AI Transparency Resources

- GAO-21-519SP - Artificial Intelligence: An Accountability Framework for Federal Agencies & Other Entities. [URL](https://www.gao.gov/products/gao-21-519sp)
- Artificial Intelligence Ethics Framework For The Intelligence Community. [URL](https://www.intelligence.gov/artificial-intelligence-ethics-framework-for-the-intelligence-community) 
- WEF Companion to the Model AI Governance Framework – Implementation and Self-Assessment Guide for Organizations [URL](https://www.pdpc.gov.sg/-/media/files/pdpc/pdf-files/resource-for-organisation/ai/sgisago.ashx)","### Organizations can document the following

- Did your organization implement a risk management system to address risks involved in deploying the identified AI solution (e.g. personnel risk or changes to commercial objectives)?
- What assessments has the entity conducted on data security and privacy impacts associated with the AI system?
- Does your organization have an existing governance structure that can be leveraged to oversee the organization’s use of AI?

### AI Transparency Resources

- WEF Companion to the Model AI Governance Framework – Implementation and Self-Assessment Guide for Organizations [URL](https://www.pdpc.gov.sg/-/media/files/pdpc/pdf-files/resource-for-organisation/ai/sgisago.ashx)
- GAO-21-519SP - Artificial Intelligence: An Accountability Framework for Federal Agencies & Other Entities. [URL](https://www.gao.gov/products/gao-21-519sp)","### Organizations can document the following

- Has the system been reviewed to ensure the AI system complies with relevant laws, regulations, standards, and guidance?
- To what extent has the entity defined and documented the regulatory environment—including minimum requirements in laws and regulations?
- Did your organization implement a risk management system to address risks involved in deploying the identified AI solution (e.g. personnel risk or changes to commercial objectives)?

### AI Transparency Resources

- GAO-21-519SP - Artificial Intelligence: An Accountability Framework for Federal Agencies & Other Entities. [URL](https://www.gao.gov/products/gao-21-519sp)
- Datasheets for Datasets. [URL](https://arxiv.org/abs/1803.09010)","### Organizations can document the following

- What are the roles, responsibilities, and delegation of authorities of personnel involved in the design, development, deployment, assessment and monitoring of the AI system?
- Who will be responsible for maintaining, re-verifying, monitoring, and updating this AI once deployed?
- How will updates/revisions be documented and communicated? How often and by whom?
- How easily accessible and current is the information available to external stakeholders?

### AI Transparency Resources

- GAO-21-519SP - Artificial Intelligence: An Accountability Framework for Federal Agencies & Other Entities. [URL](https://www.gao.gov/products/gao-21-519sp)
- Artificial Intelligence Ethics Framework For The Intelligence Community. [URL](https://www.intelligence.gov/artificial-intelligence-ethics-framework-for-the-intelligence-community) 
- Datasheets for Datasets. [URL](https://arxiv.org/abs/1803.09010)","### Organizations can document the following

- Are mechanisms in place to evaluate whether internal teams are empowered and resourced to effectively carry out risk management functions?
- How will user and other forms of stakeholder engagement be integrated into risk management processes?

### AI Transparency Resources

- Artificial Intelligence Ethics Framework For The Intelligence Community. [URL](https://www.intelligence.gov/artificial-intelligence-ethics-framework-for-the-intelligence-community) 
- Datasheets for Datasets. [URL](https://arxiv.org/abs/1803.09010)
- GAO-21-519SP - Artificial Intelligence: An Accountability Framework for Federal Agencies & Other Entities. [URL](https://www.gao.gov/products/gao-21-519sp)","### Organizations can document the following

- To what extent can users or parties affected by the outputs of the AI system test the AI system and provide feedback?
- Could the AI system expose people to harm or negative impacts? What was done to mitigate or reduce the potential for harm?
- How will the accountable human(s) address changes in accuracy and precision due to either an adversary’s attempts to disrupt the AI or unrelated changes in the operational or business environment?

### AI Transparency Resources

- GAO-21-519SP - Artificial Intelligence: An Accountability Framework for Federal Agencies & Other Entities. [URL](https://www.gao.gov/products/gao-21-519sp)
- Artificial Intelligence Ethics Framework For The Intelligence Community. [URL](https://www.intelligence.gov/artificial-intelligence-ethics-framework-for-the-intelligence-community)","### Organizations can document the following

- Who will be responsible for maintaining, re-verifying, monitoring, and updating this AI once deployed?
- Are the responsibilities of the personnel involved in the various AI governance processes clearly defined? (Including responsibilities to decommission the AI system.)
- What processes exist for data generation, acquisition/collection, ingestion, staging/storage, transformations, security, maintenance, and dissemination?
- How will the appropriate performance metrics, such as accuracy, of the AI be monitored after the AI is deployed? 

### AI Transparency Resources

- Artificial Intelligence Ethics Framework For The Intelligence Community. [URL](https://www.intelligence.gov/artificial-intelligence-ethics-framework-for-the-intelligence-community) 
- WEF - Companion to the Model AI Governance Framework – Implementation and Self-Assessment Guide for Organizations. [URL](https://www.pdpc.gov.sg/-/media/files/pdpc/pdf-files/resource-for-organisation/ai/sgisago.ashx)
- GAO-21-519SP - Artificial Intelligence: An Accountability Framework for Federal Agencies & Other Entities. [URL](https://www.gao.gov/products/gao-21-519sp)","### Organizations can document the following

- What are the roles, responsibilities, and delegation of authorities of personnel involved in the design, development, deployment, assessment and monitoring of the AI system?
- Did your organization implement a risk management system to address risks involved in deploying the identified AI solution (e.g. personnel risk or changes to commercial objectives)?
- What testing, if any, has the entity conducted on the AI system to identify errors and limitations (i.e. adversarial or stress testing)?
- To what extent does the entity have established procedures for retiring the AI system, if it is no longer needed?
- How did the entity use assessments and/or evaluations to determine if the system can be scaled up, continue, or be decommissioned?

### AI Transparency Resources

- GAO-21-519SP - Artificial Intelligence: An Accountability Framework for Federal Agencies & Other Entities. [URL](https://www.gao.gov/products/gao-21-519sp)","### Organizations can document the following

- If a third party created the AI system or some of its components, how will you ensure a level of explainability or interpretability? Is there documentation?
- If your organization obtained datasets from a third party, did your organization assess and manage the risks of using such datasets?
- Did you establish a process for third parties (e.g. suppliers, end users, subjects, distributors/vendors or workers) to report potential vulnerabilities, risks or biases in the AI system?
- Have legal requirements been addressed?

### AI Transparency Resources

- Artificial Intelligence Ethics Framework For The Intelligence Community. [URL](https://www.intelligence.gov/artificial-intelligence-ethics-framework-for-the-intelligence-community)
- WEF - Companion to the Model AI Governance Framework – Implementation and Self-Assessment Guide for Organizations. [URL](https://www.pdpc.gov.sg/-/media/files/pdpc/pdf-files/resource-for-organisation/ai/sgisago.ashx)
- Datasheets for Datasets. [URL](https://arxiv.org/abs/1803.09010)","### Organizations can document the following

- How has the entity documented the AI system’s data provenance, including sources, origins, transformations, augmentations, labels, dependencies, constraints, and metadata?
- Does this dataset collection/processing procedure achieve the motivation for creating the dataset stated in the first section of this datasheet?
- How does the entity ensure that the data collected are adequate, relevant, and not excessive in relation to the intended purpose?
- If the dataset becomes obsolete how will this be communicated?

### AI Transparency Resources

- Artificial Intelligence Ethics Framework For The Intelligence Community. [URL](https://www.intelligence.gov/artificial-intelligence-ethics-framework-for-the-intelligence-community)
- WEF - Companion to the Model AI Governance Framework – Implementation and Self-Assessment Guide for Organizations. [URL](https://www.pdpc.gov.sg/-/media/files/pdpc/pdf-files/resource-for-organisation/ai/sgisago.ashx)
- Datasheets for Datasets. [URL](https://arxiv.org/abs/1803.09010)","### Organizations can document the following

- To what extent has the entity documented the post-deployment AI system’s testing methodology, metrics, and performance outcomes?
- How easily accessible and current is the information available to external stakeholders?

### AI Transparency Resources

- GAO-21-519SP - Artificial Intelligence: An Accountability Framework for Federal Agencies & Other Entities, [URL](https://www.gao.gov/products/gao-21-519sp)
- Datasheets for Datasets. [URL](https://arxiv.org/abs/1803.09010)","### Organizations can document the following

- How will user and other forms of stakeholder engagement be integrated into the model development process and regular performance review once deployed?
- To what extent can users or parties affected by the outputs of the AI system test the AI system and provide feedback?
- To what extent has the entity defined and documented the regulatory environment—including minimum requirements in laws and regulations?

### AI Transparency Resources

- GAO-21-519SP - Artificial Intelligence: An Accountability Framework for Federal Agencies & Other Entities, [URL](https://www.gao.gov/products/gao-21-519sp)
- Artificial Intelligence Ethics Framework For The Intelligence Community. [URL](https://www.intelligence.gov/artificial-intelligence-ethics-framework-for-the-intelligence-community)","### Organizations can document the following

- What corrective actions has the entity taken to enhance the quality, accuracy, reliability, and representativeness of the data?
- To what extent does the entity communicate its AI strategic goals and objectives to the community of stakeholders? How easily accessible and current is the information available to external stakeholders?
- What type of information is accessible on the design, operations, and limitations of the AI system to external stakeholders, including end users, consumers, regulators, and individuals impacted by use of the AI system?

### AI Transparency Resources

- GAO-21-519SP: Artificial Intelligence: An Accountability Framework for Federal Agencies & Other Entities, [URL](https://www.gao.gov/products/gao-21-519sp)","### Organizations can document the following
- To what extent is the output of each component appropriate for the operational context?
- Which AI actors are responsible for the decisions of the AI and is this person aware of the intended uses and limitations of the analytic?
- Which AI actors are responsible for maintaining, re-verifying, monitoring, and updating this AI once deployed?
- Who is the person(s) accountable for the ethical considerations across the AI lifecycle?

### AI Transparency Resources
- GAO-21-519SP: AI Accountability Framework for Federal Agencies & Other Entities, [URL](https://www.gao.gov/products/gao-21-519sp)
- “Stakeholders in Explainable AI,” Sep. 2018. [URL](http://arxiv.org/abs/1810.00184)
- ""Microsoft Responsible AI Standard, v2"". [URL](https://query.prod.cms.rt.microsoft.com/cms/api/am/binary/RE4ZPmV)","### Organizations can document the following
- To what extent do the teams responsible for developing and maintaining the AI system reflect diverse opinions, backgrounds, experiences, and perspectives?
- Did the entity document the demographics of those involved in the design and development of the AI system to capture and communicate potential biases inherent to the development process, according to forum participants?
- What specific perspectives did stakeholders share, and how were they integrated across the design, development, deployment, assessment, and monitoring of the AI system?
- To what extent has the entity addressed stakeholder perspectives on the potential negative impacts of the AI system on end users and impacted populations?
- What type of information is accessible on the design, operations, and limitations of the AI system to external stakeholders, including end users, consumers, regulators, and individuals impacted by use of the AI system?
- Did your organization address usability problems and test whether user interfaces served their intended purposes? Consulting the community or end users at the earliest stages of development to ensure there is transparency on the technology used and how it is deployed.

### AI Transparency Resources
- GAO-21-519SP: AI Accountability Framework for Federal Agencies & Other Entities. [URL](https://www.gao.gov/products/gao-21-519sp)
- WEF Model AI Governance Framework Assessment 2020. [URL](https://www.pdpc.gov.sg/-/media/Files/PDPC/PDF-Files/Resource-for-Organisation/AI/SGModelAIGovFramework2.pdf)
- WEF Companion to the Model AI Governance Framework- 2020. [URL](https://www.pdpc.gov.sg/-/media/Files/PDPC/PDF-Files/Resource-for-Organisation/AI/SGIsago.pdf)
- AI policies and initiatives, in Artificial Intelligence in Society, OECD, 2019. [URL](https://www.oecd.org/publications/artificial-intelligence-in-society-eedfee77-en.htm)","### Organizations can document the following
- How does the AI system help the entity meet its goals and objectives?
- How do the technical specifications and requirements align with the AI system’s goals and objectives?
- To what extent is the output appropriate for the operational context?

### AI Transparency Resources
- Assessment List for Trustworthy AI (ALTAI) - The High-Level Expert Group on AI – 2019, [LINK](https://altai.insight-centre.org/), [URL](https://digital-strategy.ec.europa.eu/en/library/assessment-list-trustworthy-artificial-intelligence-altai-self-assessment).
- Including Insights from the Comptroller General’s Forum on the Oversight of Artificial Intelligence An Accountability Framework for Federal Agencies and Other Entities, 2021, [URL](https://www.gao.gov/products/gao-21-519sp), [PDF](https://www.gao.gov/assets/gao-21-519sp-highlights.pdf).","### Organizations can document the following
- What goals and objectives does the entity expect to achieve by designing, developing, and/or deploying the AI system?
- To what extent are the system outputs consistent with the entity’s values and principles to foster public trust and equity?
- To what extent are the metrics consistent with system goals, objectives, and constraints, including ethical and compliance considerations?

### AI Transparency Resources
- GAO-21-519SP: AI Accountability Framework for Federal Agencies & Other Entities. [URL](https://www.gao.gov/products/gao-21-519sp)
- Intel.gov: AI Ethics Framework for Intelligence Community  - 2020. [URL](https://www.intelligence.gov/artificial-intelligence-ethics-framework-for-the-intelligence-community)
- WEF Model AI Governance Framework Assessment 2020. [URL](https://www.pdpc.gov.sg/-/media/Files/PDPC/PDF-Files/Resource-for-Organisation/AI/SGModelAIGovFramework2.pdf)","### Organizations can document the following
- Which existing regulations and guidelines apply, and the entity has followed, in the development of system risk tolerances?
- What criteria and assumptions has the entity utilized when developing system risk tolerances? 
- How has the entity identified maximum allowable risk tolerance?
- What conditions and purposes are considered “off-label” for system use?

### AI Transparency Resources
- GAO-21-519SP: AI Accountability Framework for Federal Agencies & Other Entities. [URL](https://www.gao.gov/products/gao-21-519sp)
- WEF Model AI Governance Framework Assessment 2020. [URL](https://www.pdpc.gov.sg/-/media/Files/PDPC/PDF-Files/Resource-for-Organisation/AI/SGModelAIGovFramework2.pdf)
- WEF Companion to the Model AI Governance Framework- 2020. [URL](https://www.pdpc.gov.sg/-/media/Files/PDPC/PDF-Files/Resource-for-Organisation/AI/SGIsago.pdf)","### Organizations can document the following
- What type of information is accessible on the design, operations, and limitations of the AI system to external stakeholders, including end users, consumers, regulators, and individuals impacted by use of the AI system?
- To what extent is this information sufficient and appropriate to promote transparency? Promote transparency by enabling external stakeholders to access information on the design, operation, and limitations of the AI system.
- To what extent has relevant information been disclosed regarding the use of AI systems, such as (a) what the system is for, (b) what it is not for, (c) how it was designed, and (d) what its limitations are? (Documentation and external communication can offer a way for entities to provide transparency.)
- How will the relevant AI actor(s) address changes in accuracy and precision due to either an adversary’s attempts to disrupt the AI system or unrelated changes in the operational/business environment, which may impact the accuracy of the AI system?
- What metrics has the entity developed to measure performance of the AI system?
- What justifications, if any, has the entity provided for the assumptions, boundaries, and limitations of the AI system?

### AI Transparency Resources
- GAO-21-519SP: AI Accountability Framework for Federal Agencies & Other Entities. [URL](https://www.gao.gov/products/gao-21-519sp)
- Stakeholders in Explainable AI, Sep. 2018. [URL]( http://arxiv.org/abs/1810.00184)
- High-Level Expert Group on Artificial Intelligence set up by the European Commission, Ethics Guidelines for Trustworthy AI. [URL](https://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai), [PDF](https://www.aepd.es/sites/default/files/2019-12/ai-ethics-guidelines.pdf)","### Organizations can document the following

- To what extent has the entity clearly defined technical specifications and requirements for the AI system?
- To what extent has the entity documented the AI system’s development, testing methodology, metrics, and performance outcomes?
- How do the technical specifications and requirements align with the AI system’s goals and objectives?
- Did your organization implement accountability-based practices in data management and protection (e.g. the PDPA and OECD Privacy Principles)?
- How are outputs marked to clearly show that they came from an AI?

### AI Transparency Resources

- Datasheets for Datasets. [URL](http://arxiv.org/abs/1803.09010)
- WEF Model AI Governance Framework Assessment 2020. [URL](https://www.pdpc.gov.sg/-/media/Files/PDPC/PDF-Files/Resource-for-Organisation/AI/SGModelAIGovFramework2.pdf)
- WEF Companion to the Model AI Governance Framework- 2020. [URL](https://www.pdpc.gov.sg/-/media/Files/PDPC/PDF-Files/Resource-for-Organisation/AI/SGIsago.pdf)
- ATARC Model Transparency Assessment (WD) – 2020. [URL](https://atarc.org/wp-content/uploads/2020/10/atarc_model_transparency_assessment-FINAL-092020-2.docx)
- Transparency in Artificial Intelligence - S. Larsson and F. Heintz – 2020. [URL](https://lucris.lub.lu.se/ws/files/79208055/Larsson_Heintz_2020_Transparency_in_artificial_intelligence_2020_05_05.pdf)","### Organizations can document the following
- Does the AI system provide sufficient information to assist the personnel to make an informed decision and take actions accordingly?
- What type of information is accessible on the design, operations, and limitations of the AI system to external stakeholders, including end users, consumers, regulators, and individuals impacted by use of the AI system?
- Based on the assessment, did your organization implement the appropriate level of human involvement in AI-augmented decision-making? 

### AI Transparency Resources
- Datasheets for Datasets. [URL](http://arxiv.org/abs/1803.09010)
- WEF Model AI Governance Framework Assessment 2020. [URL](https://www.pdpc.gov.sg/-/media/Files/PDPC/PDF-Files/Resource-for-Organisation/AI/SGModelAIGovFramework2.pdf)
- WEF Companion to the Model AI Governance Framework- 2020. [URL](https://www.pdpc.gov.sg/-/media/Files/PDPC/PDF-Files/Resource-for-Organisation/AI/SGIsago.pdf)
- ATARC Model Transparency Assessment (WD) – 2020. [URL](https://atarc.org/wp-content/uploads/2020/10/atarc_model_transparency_assessment-FINAL-092020-2.docx)
- Transparency in Artificial Intelligence - S. Larsson and F. Heintz – 2020. [URL](https://lucris.lub.lu.se/ws/files/79208055/Larsson_Heintz_2020_Transparency_in_artificial_intelligence_2020_05_05.pdf)","### Organizations can document the following
- Are there any known errors, sources of noise, or redundancies in the data?
- Over what time-frame was the data collected? Does the collection time-frame match the creation time-frame
- What is the variable selection and evaluation process?
- How was the data collected? Who was involved in the data collection process? If the dataset relates to people (e.g., their attributes) or was generated by people, were they informed about the data collection? (e.g., datasets that collect writing, photos, interactions, transactions, etc.)
- As time passes and conditions change, is the training data still representative of the operational environment?
- Why was the dataset created? (e.g., were there specific tasks in mind, or a specific gap that needed to be filled?)
- How does the entity ensure that the data collected are adequate, relevant, and not excessive in relation to the intended purpose?

### AI Transparency Resources
- Datasheets for Datasets. [URL](http://arxiv.org/abs/1803.09010)
- WEF Model AI Governance Framework Assessment 2020. [URL](https://www.pdpc.gov.sg/-/media/Files/PDPC/PDF-Files/Resource-for-Organisation/AI/SGModelAIGovFramework2.pdf)
- WEF Companion to the Model AI Governance Framework- 2020. [URL](https://www.pdpc.gov.sg/-/media/Files/PDPC/PDF-Files/Resource-for-Organisation/AI/SGIsago.pdf)
- GAO-21-519SP: AI Accountability Framework for Federal Agencies & Other Entities. [URL](https://www.gao.gov/products/gao-21-519sp)
- ATARC Model Transparency Assessment (WD) – 2020. [URL](https://atarc.org/wp-content/uploads/2020/10/atarc_model_transparency_assessment-FINAL-092020-2.docx)
- Transparency in Artificial Intelligence - S. Larsson and F. Heintz – 2020. [URL](https://lucris.lub.lu.se/ws/files/79208055/Larsson_Heintz_2020_Transparency_in_artificial_intelligence_2020_05_05.pdf)","### Organizations can document the following
- Have the benefits of the AI system been communicated to end users?
- Have the appropriate training material and disclaimers about how to adequately use the AI system been provided to end users?
- Has your organization implemented a risk management system to address risks involved in deploying the identified AI system (e.g. personnel risk or changes to commercial objectives)?

### AI Transparency Resources
- Intel.gov: AI Ethics Framework for Intelligence Community  - 2020. [URL](https://www.intelligence.gov/artificial-intelligence-ethics-framework-for-the-intelligence-community)
- GAO-21-519SP: AI Accountability Framework for Federal Agencies & Other Entities. [URL](https://www.gao.gov/products/gao-21-519sp)
- Assessment List for Trustworthy AI (ALTAI) - The High-Level Expert Group on AI – 2019. [LINK](https://altai.insight-centre.org/), [URL](https://digital-strategy.ec.europa.eu/en/library/assessment-list-trustworthy-artificial-intelligence-altai-self-assessment)","### Organizations can document the following
- To what extent does the system/entity consistently measure progress towards stated goals and objectives?
- To what extent can users or parties affected by the outputs of the AI system test the AI system and provide feedback?
- Have you documented and explained that machine errors may differ from human errors?

### AI Transparency Resources
- Intel.gov: AI Ethics Framework for Intelligence Community  - 2020. [URL](https://www.intelligence.gov/artificial-intelligence-ethics-framework-for-the-intelligence-community)
- GAO-21-519SP: AI Accountability Framework for Federal Agencies & Other Entities. [URL](https://www.gao.gov/products/gao-21-519sp)
- Assessment List for Trustworthy AI (ALTAI) - The High-Level Expert Group on AI – 2019. [LINK](https://altai.insight-centre.org/), [URL](https://digital-strategy.ec.europa.eu/en/library/assessment-list-trustworthy-artificial-intelligence-altai-self-assessment)","### Organizations can document the following
- To what extent has the entity clearly defined technical specifications and requirements for the AI system?
- How do the technical specifications and requirements align with the AI system’s goals and objectives?

### AI Transparency Resources
- GAO-21-519SP: AI Accountability Framework for Federal Agencies & Other Entities. [URL](https://www.gao.gov/products/gao-21-519sp)
- Assessment List for Trustworthy AI (ALTAI) - The High-Level Expert Group on AI – 2019. [LINK](https://altai.insight-centre.org/), [URL](https://digital-strategy.ec.europa.eu/en/library/assessment-list-trustworthy-artificial-intelligence-altai-self-assessment)","### Organizations can document the following
- What policies has the entity developed to ensure the use of the AI system is consistent with its stated values and principles?
- How will the accountable human(s) address changes in accuracy and precision due to either an adversary’s attempts to disrupt the AI or unrelated changes in operational/business environment, which may impact the accuracy of the AI?
- How does the entity assess whether personnel have the necessary skills, training, resources, and domain knowledge to fulfill their assigned responsibilities? 
- Are the relevant staff dealing with AI systems properly trained to interpret AI model output and decisions as well as to detect and manage bias in data?
- What metrics has the entity developed to measure performance of various components?

### AI Transparency Resources
- GAO-21-519SP: AI Accountability Framework for Federal Agencies & Other Entities. [URL](https://www.gao.gov/products/gao-21-519sp)
- WEF Companion to the Model AI Governance Framework- 2020. [URL](https://www.pdpc.gov.sg/-/media/Files/PDPC/PDF-Files/Resource-for-Organisation/AI/SGIsago.pdf)","### Organizations can document the following
- What are the roles, responsibilities, and delegation of authorities of personnel involved in the design, development, deployment, assessment and monitoring of the AI system?
- How does the entity assess whether personnel have the necessary skills, training, resources, and domain knowledge to fulfill their assigned responsibilities? 
- Are the relevant staff dealing with AI systems properly trained to interpret AI model output and decisions as well as to detect and manage bias in data?
- To what extent has the entity documented the AI system’s development, testing methodology, metrics, and performance outcomes?

### AI Transparency Resources
- GAO-21-519SP: AI Accountability Framework for Federal Agencies & Other Entities. [URL](https://www.gao.gov/products/gao-21-519sp)","### Organizations can document the following
- Did you establish a process for third parties (e.g. suppliers, end users, subjects, distributors/vendors or workers) to report potential vulnerabilities, risks or biases in the AI system?
- If your organization obtained datasets from a third party, did your organization assess and manage the risks of using such datasets?
- How will the results be independently verified?

### AI Transparency Resources
- GAO-21-519SP: AI Accountability Framework for Federal Agencies & Other Entities. [URL](https://www.gao.gov/products/gao-21-519sp)
- Intel.gov: AI Ethics Framework for Intelligence Community  - 2020. [URL](https://www.intelligence.gov/artificial-intelligence-ethics-framework-for-the-intelligence-community)
- WEF Model AI Governance Framework Assessment 2020. [URL](https://www.pdpc.gov.sg/-/media/Files/PDPC/PDF-Files/Resource-for-Organisation/AI/SGModelAIGovFramework2.pdf)","### Organizations can document the following
- Can the AI system be audited by independent third parties?
- To what extent do these policies foster public trust and confidence in the use of the AI system?
- Are mechanisms established to facilitate the AI system’s auditability (e.g. traceability of the development process, the sourcing of training data and the logging of the AI system’s processes, outcomes, positive and negative impact)?

### AI Transparency Resources
- GAO-21-519SP: AI Accountability Framework for Federal Agencies & Other Entities. [URL](https://www.gao.gov/products/gao-21-519sp)
- Intel.gov: AI Ethics Framework for Intelligence Community  - 2020. [URL](https://www.intelligence.gov/artificial-intelligence-ethics-framework-for-the-intelligence-community)
- WEF Model AI Governance Framework Assessment 2020. [URL](https://www.pdpc.gov.sg/-/media/Files/PDPC/PDF-Files/Resource-for-Organisation/AI/SGModelAIGovFramework2.pdf)
- Assessment List for Trustworthy AI (ALTAI) - The High-Level Expert Group on AI - 2019. [LINK](https://altai.insight-centre.org/), [URL](https://digital-strategy.ec.europa.eu/en/library/assessment-list-trustworthy-artificial-intelligence-altai-self-assessment).","### Organizations can document the following
- Which population(s) does the AI system impact?
- What assessments has the entity conducted on trustworthiness characteristics for example data security and privacy impacts associated with the AI system?
- Can the AI system be tested by independent third parties?

### AI Transparency Resources
- Datasheets for Datasets. [URL](http://arxiv.org/abs/1803.09010)
- GAO-21-519SP: AI Accountability Framework for Federal Agencies & Other Entities. [URL](https://www.gao.gov/products/gao-21-519sp)
- AI policies and initiatives, in Artificial Intelligence in Society, OECD, 2019. [URL](https://www.oecd.org/publications/artificial-intelligence-in-society-eedfee77-en.htm)
- Intel.gov: AI Ethics Framework for Intelligence Community  - 2020. [URL](https://www.intelligence.gov/artificial-intelligence-ethics-framework-for-the-intelligence-community)
- Assessment List for Trustworthy AI (ALTAI) - The High-Level Expert Group on AI - 2019. [LINK](https://altai.insight-centre.org/), [URL](https://digital-strategy.ec.europa.eu/en/library/assessment-list-trustworthy-artificial-intelligence-altai-self-assessment)","### Organizations can document the following
- If the AI system relates to people, does it unfairly advantage or disadvantage a particular social group? In what ways? How was this managed?
- If the AI system relates to other ethically protected groups, have appropriate obligations been met? (e.g., medical data might include information collected from animals)
- If the AI system relates to people, could this dataset expose people to harm or legal action? (e.g., financial social or otherwise) What was done to mitigate or reduce the potential for harm?

### AI Transparency Resources
- Datasheets for Datasets. [URL](http://arxiv.org/abs/1803.09010)
- GAO-21-519SP: AI Accountability Framework for Federal Agencies & Other Entities. [URL](https://www.gao.gov/products/gao-21-519sp)
- AI policies and initiatives, in Artificial Intelligence in Society, OECD, 2019. [URL](https://www.oecd.org/publications/artificial-intelligence-in-society-eedfee77-en.htm)
- Intel.gov: AI Ethics Framework for Intelligence Community  - 2020. [URL](https://www.intelligence.gov/artificial-intelligence-ethics-framework-for-the-intelligence-community)
- Assessment List for Trustworthy AI (ALTAI) - The High-Level Expert Group on AI - 2019. [LINK](https://altai.insight-centre.org/), [URL](https://digital-strategy.ec.europa.eu/en/library/assessment-list-trustworthy-artificial-intelligence-altai-self-assessment)","### Organizations can document the following

- How will the appropriate performance metrics, such as accuracy, of the AI be monitored after the AI is deployed? 
- What corrective actions has the entity taken to enhance the quality, accuracy, reliability, and representativeness of the data?
- Are there recommended data splits or evaluation measures? (e.g., training, development, testing; accuracy/AUC)
- Did your organization address usability problems and test whether user interfaces served their intended purposes?
- What testing, if any, has the entity conducted on the AI system to identify errors and limitations (i.e. manual vs automated, adversarial and stress testing)?


### AI Transparency Resources

- GAO-21-519SP - Artificial Intelligence: An Accountability Framework for Federal Agencies & Other Entities. [URL](https://www.gao.gov/products/gao-21-519sp)
- Artificial Intelligence Ethics Framework For The Intelligence Community. [URL](https://www.intelligence.gov/artificial-intelligence-ethics-framework-for-the-intelligence-community) 
- Datasheets for Datasets. [URL](https://arxiv.org/abs/1803.09010)","### Organizations can document the following

- What metrics has the entity developed to measure performance of the AI system?
- To what extent do the metrics provide accurate and useful measure of performance?
- What corrective actions has the entity taken to enhance the quality, accuracy, reliability, and representativeness of the data?
- How will the accuracy or appropriate performance metrics be assessed?
- What is the justification for the metrics selected?

### AI Transparency Resources

- GAO-21-519SP - Artificial Intelligence: An Accountability Framework for Federal Agencies & Other Entities. [URL](https://www.gao.gov/products/gao-21-519sp)
- Artificial Intelligence Ethics Framework For The Intelligence Community. [URL](https://www.intelligence.gov/artificial-intelligence-ethics-framework-for-the-intelligence-community)","### Organizations can document the following

- What are the roles, responsibilities, and delegation of authorities of personnel involved in the design, development, deployment, assessment and monitoring of the AI system?
- How easily accessible and current is the information available to external stakeholders?
- To what extent does the entity communicate its AI strategic goals and objectives to the community of stakeholders?
- To what extent can users or parties affected by the outputs of the AI system test the AI system and provide feedback?
- To what extent is this information sufficient and appropriate to promote transparency? Do external stakeholders have access to information on the design, operation, and limitations of the AI system?
- What type of information is accessible on the design, operations, and limitations of the AI system to external stakeholders, including end users, consumers, regulators, and individuals impacted by use of the AI system?

### AI Transparency Resources

- GAO-21-519SP - Artificial Intelligence: An Accountability Framework for Federal Agencies & Other Entities. [URL](https://www.gao.gov/products/gao-21-519sp)
- Artificial Intelligence Ethics Framework For The Intelligence Community. [URL](https://www.intelligence.gov/artificial-intelligence-ethics-framework-for-the-intelligence-community)","### Organizations can document the following

- Given the purpose of this AI, what is an appropriate interval for checking whether it is still accurate, unbiased, explainable, etc.? What are the checks for this model?
- To what extent has the entity documented the AI system’s development, testing methodology, metrics, and performance outcomes?

### AI Transparency Resources

- GAO-21-519SP - Artificial Intelligence: An Accountability Framework for Federal Agencies & Other Entities. [URL](https://www.gao.gov/products/gao-21-519sp)
- Artificial Intelligence Ethics Framework For The Intelligence Community. [URL](https://www.intelligence.gov/artificial-intelligence-ethics-framework-for-the-intelligence-community) 
- WEF Companion to the Model AI Governance Framework- WEF - Companion to the Model AI Governance Framework, 2020. [URL](https://www.pdpc.gov.sg/-/media/Files/PDPC/PDF-Files/Resource-for-Organisation/AI/SGIsago.pdf)","### Organizations can document the following

- Given the purpose of this AI, what is an appropriate interval for checking whether it is still accurate, unbiased, explainable, etc.? What are the checks for this model?
- How has the entity identified and mitigated potential impacts of bias in the data, including inequitable or discriminatory outcomes?
- To what extent are the established procedures effective in mitigating bias, inequity, and other concerns resulting from the system?
- To what extent has the entity identified and mitigated potential bias—statistical, contextual, and historical—in the data?
- If it relates to people, were they told what the dataset would be used for and did they consent? What community norms exist for data collected from human communications? If consent was obtained, how? Were the people provided with any mechanism to revoke their consent in the future or for certain uses?
- If human subjects were used in the development or testing of the AI system, what protections were put in place to promote their safety and wellbeing?.

### AI Transparency Resources

- GAO-21-519SP - Artificial Intelligence: An Accountability Framework for Federal Agencies & Other Entities. [URL](https://www.gao.gov/products/gao-21-519sp)
- Artificial Intelligence Ethics Framework For The Intelligence Community. [URL](https://www.intelligence.gov/artificial-intelligence-ethics-framework-for-the-intelligence-community) 
- WEF Companion to the Model AI Governance Framework- WEF - Companion to the Model AI Governance Framework, 2020. [URL](https://www.pdpc.gov.sg/-/media/Files/PDPC/PDF-Files/Resource-for-Organisation/AI/SGIsago.pdf)
- Datasheets for Datasets. [URL](https://arxiv.org/abs/1803.09010)","### Organizations can document the following

- What experiments were initially run on this dataset? To what extent have experiments on the AI system been documented?
- To what extent does the system/entity consistently measure progress towards stated goals and objectives?
- How will the appropriate performance metrics, such as accuracy, of the AI be monitored after the AI is deployed? How much distributional shift or model drift from baseline performance is acceptable?
- As time passes and conditions change, is the training data still representative of the operational environment?
- What testing, if any, has the entity conducted on theAI system to identify errors and limitations (i.e.adversarial or stress testing)?

### AI Transparency Resources

- Artificial Intelligence Ethics Framework For The Intelligence Community. [URL](https://www.intelligence.gov/artificial-intelligence-ethics-framework-for-the-intelligence-community) 
- WEF Companion to the Model AI Governance Framework- WEF - Companion to the Model AI Governance Framework, 2020. [URL](https://www.pdpc.gov.sg/-/media/Files/PDPC/PDF-Files/Resource-for-Organisation/AI/SGIsago.pdf)
- Datasheets for Datasets. [URL](https://arxiv.org/abs/1803.09010)","### Organizations can document the following

- To what extent is the output of each component appropriate for the operational context?
- What justifications, if any, has the entity provided for the assumptions, boundaries, and limitations of the AI system?
- How will the appropriate performance metrics, such as accuracy, of the AI be monitored after the AI is deployed?
- As time passes and conditions change, is the training data still representative of the operational environment?

### AI Transparency Resources

- GAO-21-519SP - Artificial Intelligence: An Accountability Framework for Federal Agencies & Other Entities. [URL](https://www.gao.gov/products/gao-21-519sp)
- Artificial Intelligence Ethics Framework For The Intelligence Community. [URL](https://www.intelligence.gov/artificial-intelligence-ethics-framework-for-the-intelligence-community)","### Organizations can document the following
 
- What testing, if any, has the entity conducted on theAI system to identify errors and limitations (i.e.adversarial or stress testing)?
- Given the purpose of this AI, what is an appropriate interval for checking whether it is still accurate, unbiased, explainable, etc.? What are the checks for this model?
- How has the entity identified and mitigated potential impacts of bias in the data, including inequitable or discriminatory outcomes?
- To what extent are the established procedures effective in mitigating bias, inequity, and other concerns resulting from the system?
- What goals and objectives does the entity expect to achieve by designing, developing, and/or deploying the AI system?

### AI Transparency Resources

- GAO-21-519SP - Artificial Intelligence: An Accountability Framework for Federal Agencies & Other Entities. [URL](https://www.gao.gov/products/gao-21-519sp)","### Organizations can document the following

- What testing, if any, has the entity conducted on the AI system to identify errors and limitations (i.e.adversarial or stress testing)?
- To what extent has the entity documented the AI system’s development, testing methodology, metrics, and performance outcomes?
- Did you establish mechanisms that facilitate the AI system’s auditability (e.g. traceability of the development process, the sourcing of training data and the logging of the AI system’s processes, outcomes, positive and negative impact)?
- Did you ensure that the AI system can be audited by independent third parties?
- Did you establish a process for third parties (e.g. suppliers, end-users, subjects, distributors/vendors or workers) to report potential vulnerabilities, risks or biases in the AI system?

### AI Transparency Resources

- GAO-21-519SP - Artificial Intelligence: An Accountability Framework for Federal Agencies & Other Entities. [URL](https://www.gao.gov/products/gao-21-519sp)
- Artificial Intelligence Ethics Framework For The Intelligence Community. [URL](https://www.intelligence.gov/artificial-intelligence-ethics-framework-for-the-intelligence-community)","### Organizations can document the following

- To what extent does the plan specifically address risks associated with acquisition, procurement of packaged software from vendors, cybersecurity controls, computational infrastructure, data, data science, deployment mechanics, and system failure?
- What assessments has the entity conducted on data security and privacy impacts associated with the AI system?
- What processes exist for data generation, acquisition/collection, security, maintenance, and dissemination?
- What testing, if any, has the entity conducted on the AI system to identify errors and limitations (i.e. adversarial or stress testing)?
- If a third party created the AI, how will you ensure a level of explainability or interpretability?

### AI Transparency Resources

- GAO-21-519SP - Artificial Intelligence: An Accountability Framework for Federal Agencies & Other Entities. [URL](https://www.gao.gov/products/gao-21-519sp)
- Artificial Intelligence Ethics Framework For The Intelligence Community. [URL](https://www.intelligence.gov/artificial-intelligence-ethics-framework-for-the-intelligence-community)","### Organizations can document the following

- To what extent has the entity clarified the roles, responsibilities, and delegated authorities to relevant stakeholders?
- What are the roles, responsibilities, and delegation of authorities of personnel involved in the design, development, deployment, assessment and monitoring of the AI system?
- Who is accountable for the ethical considerations during all stages of the AI lifecycle?
- Who will be responsible for maintaining, re-verifying, monitoring, and updating this AI once deployed?
- Are the responsibilities of the personnel involved in the various AI governance processes clearly defined?

### AI Transparency Resources

- GAO-21-519SP - Artificial Intelligence: An Accountability Framework for Federal Agencies & Other Entities. [URL](https://www.gao.gov/products/gao-21-519sp)
- Artificial Intelligence Ethics Framework For The Intelligence Community. [URL](https://www.intelligence.gov/artificial-intelligence-ethics-framework-for-the-intelligence-community)","### Organizations can document the following

- Given the purpose of the AI, what level of explainability or interpretability is required for how the AI made its determination?
- Given the purpose of this AI, what is an appropriate interval for checking whether it is still accurate, unbiased, explainable, etc.? What are the checks for this model?
- How has the entity documented the AI system’s data provenance, including sources, origins, transformations, augmentations, labels, dependencies, constraints, and metadata?
- What type of information is accessible on the design, operations, and limitations of the AI system to external stakeholders, including end users, consumers, regulators, and individuals impacted by use of the AI system?

### AI Transparency Resources

- GAO-21-519SP - Artificial Intelligence: An Accountability Framework for Federal Agencies & Other Entities. [URL](https://www.gao.gov/products/gao-21-519sp)
- Artificial Intelligence Ethics Framework For The Intelligence Community. [URL](https://www.intelligence.gov/artificial-intelligence-ethics-framework-for-the-intelligence-community) 
- WEF Companion to the Model AI Governance Framework- WEF - Companion to the Model AI Governance Framework, 2020. [URL](https://www.pdpc.gov.sg/-/media/Files/PDPC/PDF-Files/Resource-for-Organisation/AI/SGIsago.pdf)","### Organizations can document the following

- Did your organization implement accountability-based practices in data management and protection (e.g. the PDPA and OECD Privacy Principles)?
- What assessments has the entity conducted on data security and privacy impacts associated with the AI system?
- Did your organization implement a risk management system to address risks involved in deploying the identified AI solution (e.g. personnel risk or changes to commercial objectives)?
- Does the dataset contain information that might be considered sensitive or confidential? (e.g., personally identifying information)
- If it relates to people, could this dataset expose people to harm or legal action? (e.g., financial, social or otherwise) What was done to mitigate or reduce the potential for harm?

### AI Transparency Resources

- WEF Companion to the Model AI Governance Framework- WEF - Companion to the Model AI Governance Framework, 2020. ([URL](https://www.pdpc.gov.sg/-/media/Files/PDPC/PDF-Files/Resource-for-Organisation/AI/SGIsago.pdf)
- Datasheets for Datasets. [URL](https://arxiv.org/abs/1803.09010)","### Organizations can document the following

- To what extent are the established procedures effective in mitigating bias, inequity, and other concerns resulting from the system?
- If it relates to people, does it unfairly advantage or disadvantage a particular social group? In what ways? How was this mitigated?
- Given the purpose of this AI, what is an appropriate interval for checking whether it is still accurate, unbiased, explainable, etc.? What are the checks for this model?
- How has the entity identified and mitigated potential impacts of bias in the data, including inequitable or discriminatory outcomes?
- To what extent has the entity identified and mitigated potential bias—statistical, contextual, and historical—in the data?
- Were adversarial machine learning approaches considered or used for measuring bias (e.g.: prompt engineering, adversarial models) 

### AI Transparency Resources

- GAO-21-519SP - Artificial Intelligence: An Accountability Framework for Federal Agencies & Other Entities. [URL](https://www.gao.gov/products/gao-21-519sp)
- Artificial Intelligence Ethics Framework For The Intelligence Community. [URL](https://www.intelligence.gov/artificial-intelligence-ethics-framework-for-the-intelligence-community) 
- WEF Companion to the Model AI Governance Framework- WEF - Companion to the Model AI Governance Framework, 2020. [URL](https://www.pdpc.gov.sg/-/media/Files/PDPC/PDF-Files/Resource-for-Organisation/AI/SGIsago.pdf)
- Datasheets for Datasets. [URL](https://arxiv.org/abs/1803.09010)","### Organizations can document the following

- Are greenhouse gas emissions, and energy and water consumption and efficiency tracked within the organization?
- Are deployed AI systems evaluated for potential upstream and downstream environmental impacts (e.g., increased consumption, increased emissions, etc.)?
- Could deployed AI systems cause environmental incidents, e.g., air or water pollution incidents, toxic spills, fires or explosions?


### AI Transparency Resources

- GAO-21-519SP - Artificial Intelligence: An Accountability Framework for Federal Agencies & Other Entities. [URL](https://www.gao.gov/products/gao-21-519sp)
- Artificial Intelligence Ethics Framework For The Intelligence Community. [URL](https://www.intelligence.gov/artificial-intelligence-ethics-framework-for-the-intelligence-community) 
- Datasheets for Datasets. [URL](https://arxiv.org/abs/1803.09010)","### Organizations can document the following

- To what extent does the system/entity consistently measure progress towards stated goals and objectives?
- Given the purpose of this AI, what is an appropriate interval for checking whether it is still accurate, unbiased, explainable, etc.? What are the checks for this model?
- What corrective actions has the entity taken to enhance the quality, accuracy, reliability, and representativeness of the data?
- To what extent are the model outputs consistent with the entity’s values and principles to foster public trust and equity?
- How will the accuracy or appropriate performance metrics be assessed?

### AI Transparency Resources

- GAO-21-519SP - Artificial Intelligence: An Accountability Framework for Federal Agencies & Other Entities. [URL](https://www.gao.gov/products/gao-21-519sp)
- Artificial Intelligence Ethics Framework For The Intelligence Community. [URL](https://www.intelligence.gov/artificial-intelligence-ethics-framework-for-the-intelligence-community)","### Organizations can document the following

- Did your organization implement a risk management system to address risks involved in deploying the identified AI solution (e.g. personnel risk or changes to commercial objectives)?
- To what extent can users or parties affected by the outputs of the AI system test the AI system and provide feedback?
- What metrics has the entity developed to measure performance of the AI system, including error logging?
- To what extent do the metrics provide accurate and useful measure of performance?

### AI Transparency Resources

- GAO-21-519SP - Artificial Intelligence: An Accountability Framework for Federal Agencies & Other Entities. [URL](https://www.gao.gov/products/gao-21-519sp)
- Artificial Intelligence Ethics Framework For The Intelligence Community. [URL](https://www.intelligence.gov/artificial-intelligence-ethics-framework-for-the-intelligence-community) 
- WEF Companion to the Model AI Governance Framework – Implementation and Self-Assessment Guide for Organizations [URL](https://www.pdpc.gov.sg/-/media/files/pdpc/pdf-files/resource-for-organisation/ai/sgisago.ashx)","### Organizations can document the following

- Who is ultimately responsible for the decisions of the AI and is this person aware of the intended uses and limitations of the analytic?
- Who will be responsible for maintaining, re-verifying, monitoring, and updating this AI once deployed?
- To what extent does the entity communicate its AI strategic goals and objectives to the community of stakeholders?
- Given the purpose of this AI, what is an appropriate interval for checking whether it is still accurate, unbiased, explainable, etc.? What are the checks for this model?
- If anyone believes that the AI no longer meets this ethical framework, who will be responsible for receiving the concern and as appropriate investigating and remediating the issue? Do they have authority to modify, limit, or stop the use of the AI?

### AI Transparency Resources

- GAO-21-519SP - Artificial Intelligence: An Accountability Framework for Federal Agencies & Other Entities. [URL](https://www.gao.gov/products/gao-21-519sp)
- Artificial Intelligence Ethics Framework For The Intelligence Community. [URL](https://www.intelligence.gov/artificial-intelligence-ethics-framework-for-the-intelligence-community)","### Organizations can document the following

- To what extent can users or parties affected by the outputs of the AI system test the AI system and provide feedback?
- Did your organization address usability problems and test whether user interfaces served their intended purposes?
- How easily accessible and current is the information available to external stakeholders?
- What type of information is accessible on the design, operations, and limitations of the AI system to external stakeholders, including end users, consumers, regulators, and individuals impacted by use of the AI system?

### AI Transparency Resources

- GAO-21-519SP - Artificial Intelligence: An Accountability Framework for Federal Agencies & Other Entities. [URL](https://www.gao.gov/products/gao-21-519sp)
- WEF Companion to the Model AI Governance Framework – Implementation and Self-Assessment Guide for Organizations [URL](https://www.pdpc.gov.sg/-/media/files/pdpc/pdf-files/resource-for-organisation/ai/sgisago.ashx)","### Organizations can document the following

- Did your organization address usability problems and test whether user interfaces served their intended purposes?
- How will user and peer engagement be integrated into the model development process and periodic performance review once deployed?
- To what extent can users or parties affected by the outputs of the AI system test the AI system and provide feedback?
- To what extent are the established procedures effective in mitigating bias, inequity, and other concerns resulting from the system?

### AI Transparency Resources

- GAO-21-519SP - Artificial Intelligence: An Accountability Framework for Federal Agencies & Other Entities. [URL](https://www.gao.gov/products/gao-21-519sp)
- Artificial Intelligence Ethics Framework For The Intelligence Community. [URL](https://www.intelligence.gov/artificial-intelligence-ethics-framework-for-the-intelligence-community) 
- WEF Companion to the Model AI Governance Framework – Implementation and Self-Assessment Guide for Organizations [URL](https://www.pdpc.gov.sg/-/media/files/pdpc/pdf-files/resource-for-organisation/ai/sgisago.ashx)","### Organizations can document the following

- To what extent does the system/entity consistently measure progress towards stated goals and objectives?
- What policies has the entity developed to ensure the use of the AI system is consistent with its stated values and principles?
- To what extent are the model outputs consistent with the entity’s values and principles to foster public trust and equity?
- Given the purpose of the AI, what level of explainability or interpretability is required for how the AI made its determination?
- To what extent can users or parties affected by the outputs of the AI system test the AI system and provide feedback?

### AI Transparency Resources

- GAO-21-519SP - Artificial Intelligence: An Accountability Framework for Federal Agencies & Other Entities. [URL](https://www.gao.gov/products/gao-21-519sp)
- Artificial Intelligence Ethics Framework For The Intelligence Community. [URL](https://www.intelligence.gov/artificial-intelligence-ethics-framework-for-the-intelligence-community)","### Organizations can document the following

- To what extent are the model outputs consistent with the entity’s values and principles to foster public trust and equity?
- How were sensitive variables (e.g., demographic and socioeconomic categories) that may be subject to regulatory compliance specifically selected or not selected for modeling purposes?
- Did your organization implement a risk management system to address risks involved in deploying the identified AI solution (e.g. personnel risk or changes to commercial objectives)?
- How will the accountable human(s) address changes in accuracy and precision due to either an adversary’s attempts to disrupt the AI or unrelated changes in the operational/business environment?
- How will user and peer engagement be integrated into the model development process and periodic performance review once deployed?

### AI Transparency Resources

- GAO-21-519SP - Artificial Intelligence: An Accountability Framework for Federal Agencies & Other Entities. [URL](https://www.gao.gov/products/gao-21-519sp)
- Artificial Intelligence Ethics Framework For The Intelligence Community. [URL](https://www.intelligence.gov/artificial-intelligence-ethics-framework-for-the-intelligence-community)"
section_ref,"Andrew Smith, ""Using Artificial Intelligence and Algorithms,"" FTC Business Blog (2020). [URL](https://www.ftc.gov/business-guidance/blog/2020/04/using-artificial-intelligence-and-algorithms)
 
Rebecca Kelly Slaughter, ""Algorithms and Economic Justice,"" ISP Digital Future Whitepaper & YJoLT Special Publication (2021). [URL](https://law.yale.edu/sites/default/files/area/center/isp/documents/algorithms_and_economic_justice_master_final.pdf)
 
Patrick Hall, Benjamin Cox, Steven Dickerson, Arjun Ravi Kannan, Raghu Kulkarni, and Nicholas Schmidt, ""A United States fair lending perspective on machine learning,"" Frontiers in Artificial Intelligence 4 (2021). [URL](https://www.frontiersin.org/articles/10.3389/frai.2021.695301/full)

AI Hiring Tools and the Law, Partnership on Employment & Accessible Technology (PEAT, peatworks.org). [URL](https://www.peatworks.org/ai-disability-inclusion-toolkit/ai-hiring-tools-and-the-law/)","Off. Comptroller Currency, Comptroller’s Handbook: Model Risk Management (Aug. 2021). [URL](https://www.occ.gov/publications-and-resources/publications/comptrollers-handbook/files/model-risk-management/index-model-risk-management.html)

GAO, “Artificial Intelligence: An Accountability Framework for Federal Agencies and Other Entities,” GAO@100 (GAO-21-519SP), June 2021. [URL](https://www.gao.gov/assets/gao-21-519sp.pdf)

NIST, ""U.S. Leadership in AI: A Plan for Federal Engagement in Developing Technical Standards and Related Tools"". [URL](https://www.nist.gov/system/files/documents/2019/08/10/ai_standards_fedengagement_plan_9aug2019.pdf)

Lipton, Zachary and McAuley, Julian and Chouldechova, Alexandra, Does mitigating ML’s impact disparity require treatment disparity? Advances in Neural Information Processing Systems, 2018. [URL](https://proceedings.neurips.cc/paper/2018/file/8e0384779e58ce2af40eb365b318cc32-Paper.pdf)

Jessica Newman (2023) “A Taxonomy of Trustworthiness for Artificial Intelligence: Connecting Properties of Trustworthiness with Risk Management and the AI Lifecycle,” UC Berkeley Center for Long-Term Cybersecurity. [URL](https://cltc.berkeley.edu/wp-content/uploads/2023/01/Taxonomy_of_AI_Trustworthiness.pdf)

Emily Hadley (2022). Prioritizing Policies for Furthering Responsible Artificial Intelligence in the United States. 2022 IEEE International Conference on Big Data (Big Data), 5029-5038. [URL](https://arxiv.org/abs/2212.00740) 

SAS Institute, “The SAS® Data Governance Framework: A Blueprint for Success”. [URL](https://www.sas.com/content/dam/SAS/en_us/doc/whitepaper1/sas-data-governance-framework-107325.pdf)

ISO, “Information technology — Reference Model of Data Management, “ ISO/IEC TR 10032:200. [URL](https://www.iso.org/standard/38607.html)

“Play 5: Create a formal policy,” Partnership on Employment & Accessible Technology (PEAT, peatworks.org). [URL](https://www.peatworks.org/ai-disability-inclusion-toolkit/the-equitable-ai-playbook/play-5-create-a-formal-equitable-ai-policy/) 

""National Institute of Standards and Technology. (2018). Framework for improving critical infrastructure cybersecurity. [URL](https://nvlpubs.nist.gov/nistpubs/cswp/nist.cswp.04162018.pdf)

Kaitlin R. Boeckl and Naomi B. Lefkovitz. ""NIST Privacy Framework: A Tool for Improving Privacy Through Enterprise Risk Management, Version 1.0."" National Institute of Standards and Technology (NIST), January 16, 2020. [URL](https://www.nist.gov/publications/nist-privacy-framework-tool-improving-privacy-through-enterprise-risk-management.)

“plainlanguage.gov – Home,” The U.S. Government. [URL](https://www.plainlanguage.gov/)","Board of Governors of the Federal Reserve System. SR 11-7: Guidance on Model Risk Management. (April 4, 2011). [URL](https://www.federalreserve.gov/supervisionreg/srletters/sr1107.htm)

The Office of the Comptroller of the Currency. Enterprise Risk Appetite Statement. (Nov. 20, 2019). [URL](https://www.occ.treas.gov/publications-and-resources/publications/banker-education/files/pub-risk-appetite-statement.pdf)

Brenda Boultwood, How to Develop an Enterprise Risk-Rating Approach (Aug. 26, 2021). Global Association of Risk Professionals (garp.org). Accessed Jan. 4, 2023. [URL](https://www.garp.org/risk-intelligence/culture-governance/how-to-develop-an-enterprise-risk-rating-approach)

GAO-17-63: Enterprise Risk Management: Selected Agencies’ Experiences Illustrate Good Practices in Managing Risk. [URL](https://www.gao.gov/assets/gao-17-63.pdf)","Bd. Governors Fed. Rsrv. Sys., Supervisory Guidance on Model Risk Management, SR Letter 11-7 (Apr. 4, 2011).

Off. Comptroller Currency, Comptroller’s Handbook: Model Risk Management (Aug. 2021). [URL](https://www.occ.gov/publications-and-resources/publications/comptrollers-handbook/files/model-risk-management/index-model-risk-management.html)

Margaret Mitchell et al., “Model Cards for Model Reporting.” Proceedings of 2019 FATML Conference. [URL](https://arxiv.org/pdf/1810.03993.pdf)

Timnit Gebru et al., “Datasheets for Datasets,” Communications of the ACM 64, No. 12, 2021. [URL](https://arxiv.org/pdf/1803.09010.pdf)

Emily M. Bender, Batya Friedman,  Angelina McMillan-Major (2022). A Guide for Writing Data Statements for Natural Language Processing. University of Washington. Accessed July 14, 2022. [URL](https://techpolicylab.uw.edu/wp-content/uploads/2021/11/Data_Statements_Guide_V2.pdf)

M. Arnold, R. K. E. Bellamy, M. Hind, et al. FactSheets: Increasing trust in AI services through supplier’s declarations of conformity. IBM Journal of Research and Development 63, 4/5 (July-September 2019), 6:1-6:13. [URL](https://techpolicylab.uw.edu/wp-content/uploads/2021/11/Data_Statements_Guide_V2.pdf)

Navdeep Gill, Abhishek Mathur, Marcos V. Conde (2022). A Brief Overview of AI Governance for Responsible Machine Learning Systems. ArXiv, abs/2211.13130. [URL](https://arxiv.org/pdf/2211.13130.pdf)

John Richards, David Piorkowski, Michael Hind, et al. A Human-Centered Methodology for Creating AI FactSheets. Bulletin of the IEEE Computer Society Technical Committee on Data Engineering. [URL](http://sites.computer.org/debull/A21dec/p47.pdf)

Christoph Molnar, Interpretable Machine Learning, lulu.com. [URL](https://christophm.github.io/interpretable-ml-book/)

David A. Broniatowski. 2021. Psychological Foundations of Explainability and Interpretability in Artificial Intelligence. National Institute of Standards and Technology (NIST) IR 8367. National Institute of Standards and Technology, Gaithersburg, MD. [URL](https://doi.org/10.6028/NIST.IR.8367)

OECD (2022), “OECD Framework for the Classification of AI systems”, OECD Digital Economy Papers, No. 323, OECD Publishing, Paris. [URL](https://doi.org/10.1787/cb6d9eca-en)","National Institute of Standards and Technology. (2018). Framework for improving critical infrastructure cybersecurity. [URL](https://nvlpubs.nist.gov/nistpubs/cswp/nist.cswp.04162018.pdf)

National Institute of Standards and Technology. (2012). Computer Security Incident Handling Guide. NIST Special Publication 800-61 Revision 2. [URL](https://nvlpubs.nist.gov/nistpubs/specialpublications/nist.sp.800-61r2.pdf)","“A risk-based integrity level schema”, in IEEE 1012, IEEE Standard for System, Software, and Hardware Verification and Validation. See Annex B. [URL](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1488512)

Off. Comptroller Currency, Comptroller’s Handbook: Model Risk Management (Aug. 2021). See “Model Inventory,” pg. 26. [URL](https://www.occ.gov/publications-and-resources/publications/comptrollers-handbook/files/model-risk-management/index-model-risk-management.html) 

VertaAI, “ModelDB: An open-source system for Machine Learning model versioning, metadata, and experiment management.” Accessed Jan. 5, 2023. [URL](https://github.com/VertaAI/modeldb)","Michelle De Mooy, Joseph Jerome and Vijay Kasschau, “Should It Stay or Should It Go? The Legal, Policy and Technical Landscape Around Data Deletion,” Center for Democracy and Technology, 2017. [URL](https://cdt.org/wp-content/uploads/2017/02/2017-02-23-Data-Deletion-FNL2.pdf)

Burcu Baykurt, ""Algorithmic accountability in US cities: Transparency, impact, and political economy."" Big Data & Society 9, no. 2 (2022): 20539517221115426. [URL](https://journals.sagepub.com/doi/full/10.1177/20539517221115426)

“Information System Decommissioning Guide,” Bureau of Land Management, 2011. [URL](https://www.blm.gov/sites/blm.gov/files/uploads/IM2011-174_att1.pdf)","Andrew Smith, “Using Artificial Intelligence and Algorithms,” FTC Business Blog (Apr. 8, 2020). [URL](https://www.ftc.gov/news-events/blogs/business-blog/2020/04/using-artificial-intelligence-algorithms)

Off. Superintendent Fin. Inst. Canada, Enterprise-Wide Model Risk Management for Deposit-Taking Institutions, E-23 (Sept. 2017).

Bd. Governors Fed. Rsrv. Sys., Supervisory Guidance on Model Risk Management, SR Letter 11-7 (Apr. 4, 2011).

Off. Comptroller Currency, Comptroller’s Handbook: Model Risk Management (Aug. 2021). [URL](https://www.occ.gov/publications-and-resources/publications/comptrollers-handbook/files/model-risk-management/index-model-risk-management.html)

ISO, “Information Technology — Artificial Intelligence — Guidelines for AI applications,” ISO/IEC CD 5339. See Section 6, “Stakeholders’ perspectives and AI application framework.” [URL](https://www.iso.org/standard/81120.html)","Off. Comptroller Currency, Comptroller’s Handbook: Model Risk Management (Aug. 2021). [URL](https://www.occ.gov/publications-and-resources/publications/comptrollers-handbook/files/model-risk-management/index-model-risk-management.html)

“Developing Staff Trainings for Equitable AI,” Partnership on Employment & Accessible Technology (PEAT, peatworks.org). [URL](https://www.peatworks.org/ai-disability-inclusion-toolkit/ai-disability-inclusion-resources/developing-staff-trainings-for-equitable-ai/)","Bd. Governors Fed. Rsrv. Sys., Supervisory Guidance on Model Risk Management, SR Letter 11-7 (Apr. 4, 2011)

Off. Superintendent Fin. Inst. Canada, Enterprise-Wide Model Risk Management for Deposit-Taking Institutions, E-23 (Sept. 2017).","Dylan Walsh, “How can human-centered AI fight bias in machines and people?” MIT Sloan Mgmt. Rev., 2021. [URL](https://mitsloan.mit.edu/ideas-made-to-matter/how-can-human-centered-ai-fight-bias-machines-and-people)

Michael Li, “To Build Less-Biased AI, Hire a More Diverse Team,” Harvard Bus. Rev., 2020. [URL](https://hbr.org/2020/10/to-build-less-biased-ai-hire-a-more-diverse-team)

Bo Cowgill et al., “Biased Programmers? Or Biased Data? A Field Experiment in Operationalizing AI Ethics,” 2020. [URL](https://arxiv.org/pdf/2012.02394.pdf)

Naomi Ellemers, Floortje Rink, “Diversity in work groups,” Current opinion in psychology, vol. 11, pp. 49–53, 2016.

Katrin Talke, Søren Salomo, Alexander Kock, “Top management team diversity and strategic innovation orientation: The relationship and consequences for innovativeness and performance,” Journal of Product Innovation Management, vol. 28, pp. 819–832, 2011.

Sarah Myers West, Meredith Whittaker, and Kate Crawford,, “Discriminating Systems: Gender, Race, and Power in AI,” AI Now Institute, Tech. Rep., 2019. [URL](https://ainowinstitute.org/discriminatingsystems.pdf)

Sina Fazelpour, Maria De-Arteaga, Diversity in sociotechnical machine learning systems. Big Data & Society. January 2022. doi:10.1177/20539517221082027

Mary L. Cummings and Songpo Li, 2021a. Sources of subjectivity in machine learning models. ACM Journal of Data and Information Quality, 13(2), 1–9

“Staffing for Equitable AI: Roles & Responsibilities,” Partnership on Employment & Accessible  Technology (PEAT, peatworks.org). Accessed Jan. 6, 2023. [URL](https://www.peatworks.org/ai-disability-inclusion-toolkit/ai-disability-inclusion-resources/staffing-for-equitable-ai-roles-responsibilities/)","Madeleine Clare Elish, ""Moral Crumple Zones: Cautionary tales in human-robot interaction,"" Engaging Science, Technology, and Society, Vol. 5, 2019. [URL](https://estsjournal.org/index.php/ests/article/view/260)

“Human-AI Teaming: State-Of-The-Art and Research Needs,” National Academies of Sciences, Engineering, and Medicine, 2022. [URL](https://doi.org/10.17226/26355)

Ben Green, ""The Flaws Of Policies Requiring Human Oversight Of Government Algorithms,"" Computer Law & Security Review 45 (2022). [URL](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3921216)

David A. Broniatowski. 2021. Psychological Foundations of Explainability and Interpretability in Artificial Intelligence. National Institute of Standards and Technology (NIST) IR 8367. National Institute of Standards and Technology, Gaithersburg, MD. [URL](https://doi.org/10.6028/NIST.IR.8367)

Off. Comptroller Currency, Comptroller’s Handbook: Model Risk Management (Aug. 2021). [URL](https://www.occ.gov/publications-and-resources/publications/comptrollers-handbook/files/model-risk-management/index-model-risk-management.html)","Bd. Governors Fed. Rsrv. Sys., Supervisory Guidance on Model Risk Management, SR Letter 11-7 (Apr. 4, 2011)

Patrick Hall, Navdeep Gill, and Benjamin Cox, “Responsible Machine Learning,” O’Reilly Media, 2020. [URL](https://www.oreilly.com/library/view/responsible-machine-learning/9781492090878/)

Off. Superintendent Fin. Inst. Canada, Enterprise-Wide Model Risk Management for Deposit-Taking Institutions, E-23 (Sept. 2017).

GAO, “Artificial Intelligence: An Accountability Framework for Federal Agencies and Other Entities,” GAO@100 (GAO-21-519SP), June 2021. [URL](https://www.gao.gov/assets/gao-21-519sp.pdf)

Donald Sull, Stefano Turconi, and Charles Sull, “When It Comes to Culture, Does Your Company Walk the Talk?” MIT Sloan Mgmt. Rev., 2020. [URL](https://sloanreview.mit.edu/article/when-it-comes-to-culture-does-your-company-walk-the-talk)

Kathy Baxter, AI Ethics Maturity Model, Salesforce. [URL](https://www.salesforceairesearch.com/static/ethics/EthicalAIMaturityModel.pdf)","Dillon Reisman, Jason Schultz, Kate Crawford, Meredith Whittaker, “Algorithmic Impact Assessments: A Practical Framework For Public Agency Accountability,” AI Now Institute, 2018. [URL](https://ainowinstitute.org/aiareport2018.pdf)

H.R. 2231, 116th Cong. (2019). [URL](https://www.congress.gov/bill/116th-congress/house-bill/2231/text)

BSA The Software Alliance (2021) Confronting Bias: BSA’s Framework to Build Trust in AI. [URL](https://www.bsa.org/reports/confronting-bias-bsas-framework-to-build-trust-in-ai)

Anthony M. Barrett, Dan Hendrycks, Jessica Newman and Brandie Nonnecke. Actionable Guidance for High-Consequence AI Risk Management: Towards Standards Addressing AI Catastrophic Risks. ArXiv abs/2206.08966 (2022) https://arxiv.org/abs/2206.08966

David Wright, “Making Privacy Impact Assessments More Effective."" The Information Society 29, 2013. [URL](https://iapp.org/media/pdf/knowledge_center/Making_PIA__more_effective.pdf)

Konstantinia Charitoudi and Andrew Blyth. A Socio-Technical Approach to Cyber Risk Management and Impact Assessment. Journal of Information Security 4, 1 (2013), 33-41. [URL](https://www.scirp.org/pdf/JIS_2013013014352043.pdf)

Emanuel Moss, Elizabeth Anne Watkins, Ranjit Singh, Madeleine Clare Elish, & Jacob Metcalf. 2021. “Assembling Accountability: Algorithmic Impact Assessment for the Public Interest”. [URL](https://datasociety.net/library/assembling-accountability-algorithmic-impact-assessment-for-the-public-interest/)

Microsoft. Responsible AI Impact Assessment Template. 2022. [URL](https://blogs.microsoft.com/wp-content/uploads/prod/sites/5/2022/06/Microsoft-RAI-Impact-Assessment-Template.pdf)

Microsoft. Responsible AI Impact Assessment Guide. 2022. [URL](https://blogs.microsoft.com/wp-content/uploads/prod/sites/5/2022/06/Microsoft-RAI-Impact-Assessment-Guide.pdf)

Microsoft. Foundations of assessing harm. 2022. [URL](https://opdhsblobprod04.blob.core.windows.net/contents/f4438a49b5d04a4b93b0fa1f989369cf/8db74d210fb2fc34b7d6981ed0545adc?skoid=2d004ef0-5468-4cd8-a5b7-14c04c6415bc&sktid=975f013f-7f24-47e8-a7d3-abc4752bf346&skt=2023-01-15T14%3A46%3A07Z&ske=2023-01-22T14%3A51%3A07Z&sks=b&skv=2021-10-04&sv=2021-10-04&se=2023-01-21T05%3A44%3A16Z&sr=b&sp=r&sig=zr00zgBC8dJFXCJB%2BrZkY%2BHse1Y2g886cE9zqO7yvMg%3D)

Mauritz Kop, “AI Impact Assessment & Code of Conduct,” Futurium, May 2019. [URL](https://futurium.ec.europa.eu/en/european-ai-alliance/best-practices/ai-impact-assessment-code-conduct)

Dillon Reisman, Jason Schultz, Kate Crawford, and Meredith Whittaker, “Algorithmic Impact Assessments: A Practical Framework For Public Agency Accountability,” AI Now, Apr. 2018. [URL](https://ainowinstitute.org/aiareport2018.pdf)

Andrew D. Selbst, “An Institutional View Of Algorithmic Impact Assessments,” Harvard Journal of Law & Technology, vol. 35, no. 1, 2021

Ada Lovelace Institute. 2022. Algorithmic Impact Assessment: A Case Study in Healthcare. Accessed July 14, 2022. [URL](https://www.adalovelaceinstitute.org/report/algorithmic-impact-assessment-case-study-healthcare/)

Kathy Baxter, AI Ethics Maturity Model, Salesforce [URL](https://www.salesforceairesearch.com/static/ethics/EthicalAIMaturityModel.pdf)","Sean McGregor, “Preventing Repeated Real World AI Failures by Cataloging Incidents: The AI Incident Database,” arXiv:2011.08512 [cs], Nov. 2020, arXiv:2011.08512. [URL](http://arxiv.org/abs/2011.08512)

Christopher Johnson, Mark Badger, David Waltermire, Julie Snyder, and Clem Skorupka,  “Guide to cyber threat information sharing,” National Institute of Standards and Technology, NIST Special Publication 800-150, Nov 2016. [URL](https://doi.org/10.6028/NIST.SP.800-150)

Mengyi Wei, Zhixuan Zhou (2022). AI Ethics Issues in Real World: Evidence from AI Incident Database. ArXiv, abs/2206.07635. [URL](https://arxiv.org/pdf/2206.07635.pdf)

BSA The Software Alliance (2021) Confronting Bias: BSA’s Framework to Build Trust in AI. [URL](https://www.bsa.org/reports/confronting-bias-bsas-framework-to-build-trust-in-ai)

“Using Combined Expertise to Evaluate Web Accessibility,” W3C Web Accessibility Initiative. [URL](https://www.w3.org/WAI/test-evaluate/combined-expertise/)","ISO, “Ergonomics of human-system interaction — Part 210: Human-centered design for interactive systems,” ISO 9241-210:2019 (2nd ed.), July 2019. [URL](https://www.iso.org/standard/77520.html)

Rumman Chowdhury and Jutta Williams, ""Introducing Twitter’s first algorithmic bias bounty challenge,"" [URL](https://blog.twitter.com/engineering/en_us/topics/insights/2021/algorithmic-bias-bounty-challenge)

Leonard Haas and Sebastian Gießler, “In the realm of paper tigers – exploring the failings of AI ethics guidelines,” AlgorithmWatch, 2020. [URL](https://algorithmwatch.org/en/ai-ethics-guidelines-inventory-upgrade-2020/)

Josh Kenway, Camille Francois, Dr. Sasha Costanza-Chock, Inioluwa Deborah Raji, & Dr. Joy Buolamwini. 2022. Bug Bounties for Algorithmic Harms? Algorithmic Justice League. Accessed July 14, 2022. [URL](https://www.ajl.org/bugs)

Microsoft Community Jury , Azure Application Architecture Guide. [URL](https://docs.microsoft.com/en-us/azure/architecture/guide/responsible-innovation/community-jury/)

“Definition of independent verification and validation (IV&V)”, in IEEE 1012, IEEE Standard for System, Software, and Hardware Verification and Validation. Annex C, [URL](https://people.eecs.ku.edu/~hossein/Teaching/Stds/1012.pdf)","Bd. Governors Fed. Rsrv. Sys., Supervisory Guidance on Model Risk Management, SR Letter 11-7 (Apr. 4, 2011)

Off. Comptroller Currency, Comptroller’s Handbook: Model Risk Management (Aug. 2021). [URL](https://www.occ.gov/publications-and-resources/publications/comptrollers-handbook/files/model-risk-management/index-model-risk-management.html)

The Office of the Comptroller of the Currency. Enterprise Risk Appetite Statement. (Nov. 20, 2019). Retrieved on July 12, 2022. [URL](https://www.occ.treas.gov/publications-and-resources/publications/banker-education/files/pub-risk-appetite-statement.pdf)","Bd. Governors Fed. Rsrv. Sys., Supervisory Guidance on Model Risk Management, SR Letter 11-7 (Apr. 4, 2011)

“Proposed Interagency Guidance on Third-Party Relationships: Risk Management,” 2021. [URL](https://www.occ.gov/news-issuances/news-releases/2021/nr-occ-2021-74a.pdf)

Off. Comptroller Currency, Comptroller’s Handbook: Model Risk Management (Aug. 2021). [URL](https://www.occ.gov/publications-and-resources/publications/comptrollers-handbook/files/model-risk-management/index-model-risk-management.html)","Bd. Governors Fed. Rsrv. Sys., Supervisory Guidance on Model Risk Management, SR Letter 11-7 (Apr. 4, 2011)

“Proposed Interagency Guidance on Third-Party Relationships: Risk Management,” 2021. [URL](https://www.occ.gov/news-issuances/news-releases/2021/nr-occ-2021-74a.pdf)

Off. Comptroller Currency, Comptroller’s Handbook: Model Risk Management (Aug. 2021). [URL](https://www.occ.gov/publications-and-resources/publications/comptrollers-handbook/files/model-risk-management/index-model-risk-management.html)","Arvind Narayanan. How to recognize AI snake oil. Retrieved October 15, 2022. [URL](https://www.cs.princeton.edu/~arvindn/talks/MIT-STS-AI-snakeoil.pdf)

Board of Governors of the Federal Reserve System. SR 11-7: Guidance on Model Risk Management. (April 4, 2011). [URL](https://www.federalreserve.gov/supervisionreg/srletters/sr1107.htm)

Emanuel Moss, Elizabeth Watkins, Ranjit Singh, Madeleine Clare Elish, Jacob Metcalf. 2021. Assembling Accountability: Algorithmic Impact Assessment for the Public Interest. (June 29, 2021). [URL](https://ssrn.com/abstract=3877437 or http://dx.doi.org/10.2139/ssrn.3877437)

Fraser, Henry L and Bello y Villarino, Jose-Miguel, Where Residual Risks Reside: A Comparative Approach to Art 9(4) of the European Union's Proposed AI Regulation (September 30, 2021). [LINK](https://ssrn.com/abstract=3960461), [URL](http://dx.doi.org/10.2139/ssrn.3960461)

Microsoft. 2022. Microsoft Responsible AI Impact Assessment Template. (June 2022). [URL](https://blogs.microsoft.com/wp-content/uploads/prod/sites/5/2022/06/Microsoft-RAI-Impact-Assessment-Template.pdf)

Office of the Comptroller of the Currency. 2021. Comptroller's Handbook: Model Risk Management, Version 1.0, August 2021. [URL](https://www.occ.gov/publications-and-resources/publications/comptrollers-handbook/files/model-risk-management/index-model-risk-management.html)

Solon Barocas, Asia J. Biega, Benjamin Fish, et al. 2020. When not to design, build, or deploy. In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency (FAT* '20). Association for Computing Machinery, New York, NY, USA, 695. [URL](https://doi.org/10.1145/3351095.3375691)","Arvind Narayanan. How to recognize AI snake oil. Retrieved October 15, 2022. [URL](https://www.cs.princeton.edu/~arvindn/talks/MIT-STS-AI-snakeoil.pdf)

Board of Governors of the Federal Reserve System. SR 11-7: Guidance on Model Risk Management. (April 4, 2011). [URL](https://www.federalreserve.gov/supervisionreg/srletters/sr1107.htm)

Emanuel Moss, Elizabeth Watkins, Ranjit Singh, Madeleine Clare Elish, Jacob Metcalf. 2021. Assembling Accountability: Algorithmic Impact Assessment for the Public Interest. (June 29, 2021). [URL](https://ssrn.com/abstract=3877437 or http://dx.doi.org/10.2139/ssrn.3877437)

Fraser, Henry L and Bello y Villarino, Jose-Miguel, Where Residual Risks Reside: A Comparative Approach to Art 9(4) of the European Union's Proposed AI Regulation (September 30, 2021). [LINK](https://ssrn.com/abstract=3960461), [URL](http://dx.doi.org/10.2139/ssrn.3960461)

Microsoft. 2022. Microsoft Responsible AI Impact Assessment Template. (June 2022). [URL](https://blogs.microsoft.com/wp-content/uploads/prod/sites/5/2022/06/Microsoft-RAI-Impact-Assessment-Template.pdf)

Office of the Comptroller of the Currency. 2021. Comptroller's Handbook: Model Risk Management, Version 1.0, August 2021. [URL](https://www.occ.gov/publications-and-resources/publications/comptrollers-handbook/files/model-risk-management/index-model-risk-management.html)

Solon Barocas, Asia J. Biega, Benjamin Fish, et al. 2020. When not to design, build, or deploy. In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency (FAT* '20). Association for Computing Machinery, New York, NY, USA, 695. [URL](https://doi.org/10.1145/3351095.3375691)","Arvind Narayanan. How to recognize AI snake oil. Retrieved October 15, 2022. [URL](https://www.cs.princeton.edu/~arvindn/talks/MIT-STS-AI-snakeoil.pdf)

Board of Governors of the Federal Reserve System. SR 11-7: Guidance on Model Risk Management. (April 4, 2011). [URL](https://www.federalreserve.gov/supervisionreg/srletters/sr1107.htm)

Emanuel Moss, Elizabeth Watkins, Ranjit Singh, Madeleine Clare Elish, Jacob Metcalf. 2021. Assembling Accountability: Algorithmic Impact Assessment for the Public Interest. (June 29, 2021). [URL](https://ssrn.com/abstract=3877437 or http://dx.doi.org/10.2139/ssrn.3877437)

Fraser, Henry L and Bello y Villarino, Jose-Miguel, Where Residual Risks Reside: A Comparative Approach to Art 9(4) of the European Union's Proposed AI Regulation (September 30, 2021). [LINK](https://ssrn.com/abstract=3960461), [URL](http://dx.doi.org/10.2139/ssrn.3960461)

Microsoft. 2022. Microsoft Responsible AI Impact Assessment Template. (June 2022). [URL](https://blogs.microsoft.com/wp-content/uploads/prod/sites/5/2022/06/Microsoft-RAI-Impact-Assessment-Template.pdf)

Office of the Comptroller of the Currency. 2021. Comptroller's Handbook: Model Risk Management, Version 1.0, August 2021. [URL](https://www.occ.gov/publications-and-resources/publications/comptrollers-handbook/files/model-risk-management/index-model-risk-management.html)

Solon Barocas, Asia J. Biega, Benjamin Fish, et al. 2020. When not to design, build, or deploy. In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency (FAT* '20). Association for Computing Machinery, New York, NY, USA, 695. [URL](https://doi.org/10.1145/3351095.3375691)","Arvind Narayanan. How to recognize AI snake oil. Retrieved October 15, 2022. [URL](https://www.cs.princeton.edu/~arvindn/talks/MIT-STS-AI-snakeoil.pdf)

Board of Governors of the Federal Reserve System. SR 11-7: Guidance on Model Risk Management. (April 4, 2011). [URL](https://www.federalreserve.gov/supervisionreg/srletters/sr1107.htm)

Emanuel Moss, Elizabeth Watkins, Ranjit Singh, Madeleine Clare Elish, Jacob Metcalf. 2021. Assembling Accountability: Algorithmic Impact Assessment for the Public Interest. (June 29, 2021). [URL](https://ssrn.com/abstract=3877437 or http://dx.doi.org/10.2139/ssrn.3877437)

Fraser, Henry L and Bello y Villarino, Jose-Miguel, Where Residual Risks Reside: A Comparative Approach to Art 9(4) of the European Union's Proposed AI Regulation (September 30, 2021). [LINK](https://ssrn.com/abstract=3960461), [URL](http://dx.doi.org/10.2139/ssrn.3960461)

Microsoft. 2022. Microsoft Responsible AI Impact Assessment Template. (June 2022). [URL](https://blogs.microsoft.com/wp-content/uploads/prod/sites/5/2022/06/Microsoft-RAI-Impact-Assessment-Template.pdf)

Office of the Comptroller of the Currency. 2021. Comptroller's Handbook: Model Risk Management, Version 1.0, August 2021. [URL](https://www.occ.gov/publications-and-resources/publications/comptrollers-handbook/files/model-risk-management/index-model-risk-management.html)

Solon Barocas, Asia J. Biega, Benjamin Fish, et al. 2020. When not to design, build, or deploy. In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency (FAT* '20). Association for Computing Machinery, New York, NY, USA, 695. [URL](https://doi.org/10.1145/3351095.3375691)","Board of Governors of the Federal Reserve System. SR 11-7: Guidance on Model Risk Management. (April 4, 2011). [URL](https://www.federalreserve.gov/supervisionreg/srletters/sr1107.htm)

David Wright. 2013. Making Privacy Impact Assessments More Effective. The Information Society, 29 (Oct 2013), 307-315. [URL](https://doi-org.proxygw.wrlc.org/10.1080/01972243.2013.825687)

Margaret Mitchell, Simone Wu, Andrew Zaldivar, et al. 2019. Model Cards for Model Reporting. In Proceedings of the Conference on Fairness, Accountability, and Transparency (FAT* '19). Association for Computing Machinery, New York, NY, USA, 220–229. [URL](https://doi.org/10.1145/3287560.3287596)

Office of the Comptroller of the Currency. 2021. Comptroller's Handbook: Model Risk Management, Version 1.0, August 2021. [URL](https://www.occ.gov/publications-and-resources/publications/comptrollers-handbook/files/model-risk-management/index-model-risk-management.html)

Timnit Gebru, Jamie Morgenstern, Briana Vecchione, et al. 2021. Datasheets for Datasets. arXiv:1803.09010. [URL](https://arxiv.org/abs/1803.09010)","### Safety, Validity and Reliability Risk Management Approaches and Resources

AI Incident Database. 2022. AI Incident Database. [URL](https://incidentdatabase.ai/)

AIAAIC Repository. 2022. AI, algorithmic and automation incidents collected, dissected, examined, and divulged. [URL](https://www.aiaaic.org/aiaaic-repository)

Alexander D'Amour, Katherine Heller, Dan Moldovan, et al. 2020. Underspecification Presents Challenges for Credibility in Modern Machine Learning. arXiv:2011.03395. [URL](https://arxiv.org/abs/2011.03395)

Andrew L. Beam, Arjun K. Manrai, Marzyeh Ghassemi. 2020. Challenges to the Reproducibility of Machine Learning Models in Health Care. Jama 323, 4 (January 6, 2020), 305-306. [URL](https://doi.org/10.1001/jama.2019.20866)

Anthony M. Barrett, Dan Hendrycks, Jessica Newman et al. 2022. Actionable Guidance for High-Consequence AI Risk Management: Towards Standards Addressing AI Catastrophic Risks. arXiv:2206.08966. [URL](https://doi.org/10.48550/arXiv.2206.08966)

Debugging Machine Learning Models, In Proceedings of ICLR 2019 Workshop, May 6, 2019, New Orleans, Louisiana. [URL](https://debug-ml-iclr2019.github.io/)

Jessie J. Smith, Saleema Amershi, Solon Barocas, et al. 2022. REAL ML: Recognizing, Exploring, and Articulating Limitations of Machine Learning Research. arXiv:2205.08363. [URL](https://arxiv.org/abs/2205.08363)

Joelle Pineau, Philippe Vincent-Lamarre, Koustuv Sinha, et al. 2020. Improving Reproducibility in Machine Learning Research (A Report from the NeurIPS 2019 Reproducibility Program) arXiv:2003.12206. [URL](https://doi.org/10.48550/arXiv.2003.12206)

Kirstie Whitaker. 2017. Showing your working: a how to guide to reproducible research. (August 2017). [LINK](https://github.com/WhitakerLab/ReproducibleResearch/blob/master/PRESENTATIONS/Whitaker_ICON_August2017.pdf), [URL](https://doi.org/10.6084/m9.figshare.4244996.v2)

Netflix. Chaos Monkey. [URL](https://netflix.github.io/chaosmonkey/)

Peter Henderson, Riashat Islam, Philip Bachman, et al. 2018. Deep reinforcement learning that matters. Proceedings of the AAAI Conference on Artificial Intelligence. 32, 1 (Apr. 2018). [URL](https://doi.org/10.1609/aaai.v32i1.11694)

Suchi Saria, Adarsh Subbaswamy. 2019. Tutorial: Safe and Reliable Machine Learning. arXiv:1904.07204. [URL](https://doi.org/10.48550/arXiv.1904.07204)

Kang, Daniel, Deepti Raghavan, Peter Bailis, and Matei Zaharia. ""Model assertions for monitoring and improving ML models."" Proceedings of Machine Learning and Systems 2 (2020): 481-496. [URL](https://proceedings.mlsys.org/paper/2020/file/a2557a7b2e94197ff767970b67041697-Paper.pdf)

### Managing Risk Bias

National Institute of Standards and Technology (NIST), Reva Schwartz, Apostol Vassilev, et al. 2022. NIST Special Publication 1270 Towards a Standard for Identifying and Managing Bias in Artificial Intelligence. [URL](https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.1270.pdf)

### Bias Testing and Remediation Approaches 

Alekh Agarwal, Alina Beygelzimer, Miroslav Dudík, et al. 2018. A Reductions Approach to Fair Classification. arXiv:1803.02453. [URL](https://doi.org/10.48550/arXiv.1803.02453)

Brian Hu Zhang, Blake Lemoine, Margaret Mitchell. 2018. Mitigating Unwanted Biases with Adversarial Learning. arXiv:1801.07593. [URL](https://doi.org/10.48550/arXiv.1801.07593)

Drago Plecko, Nicolas Bennett, Nicolai Meinshausen. 2021. Fairadapt: Causal Reasoning for Fair Data Pre-processing. arXiv:2110.10200. [URL](https://doi.org/10.48550/arXiv.2110.10200)

Faisal Kamiran, Toon Calders. 2012. Data Preprocessing Techniques for Classification without Discrimination. Knowledge and Information Systems 33 (2012), 1–33. [URL](https://doi.org/10.1007/s10115-011-0463-8)

Faisal Kamiran; Asim Karim; Xiangliang Zhang. 2012. Decision Theory for Discrimination-Aware Classification. In Proceedings of the 2012 IEEE 12th International Conference on Data Mining, December 10-13, 2012, Brussels, Belgium. IEEE, 924-929. [URL](https://doi.org/10.1109/ICDM.2012.45)

Flavio P. Calmon, Dennis Wei, Karthikeyan Natesan Ramamurthy, et al. 2017. Optimized Data Pre-Processing for Discrimination Prevention. arXiv:1704.03354. [URL](https://doi.org/10.48550/arXiv.1704.03354)

Geoff Pleiss, Manish Raghavan, Felix Wu, et al. 2017. On Fairness and Calibration. arXiv:1709.02012. [URL](https://doi.org/10.48550/arXiv.1709.02012)

L. Elisa Celis, Lingxiao Huang, Vijay Keswani, et al. 2020. Classification with Fairness Constraints: A Meta-Algorithm with Provable Guarantees. arXiv:1806.06055. [URL](https://doi.org/10.48550/arXiv.1806.06055)

Michael Feldman, Sorelle Friedler, John Moeller, et al. 2014. Certifying and Removing Disparate Impact. arXiv:1412.3756. [URL](https://doi.org/10.48550/arXiv.1412.3756)

Michael Kearns, Seth Neel, Aaron Roth, et al. 2017. Preventing Fairness Gerrymandering: Auditing and Learning for Subgroup Fairness. arXiv:1711.05144. [URL](https://doi.org/10.48550/arXiv.1711.05144)

Michael Kearns, Seth Neel, Aaron Roth, et al. 2018. An Empirical Study of Rich Subgroup Fairness for Machine Learning. arXiv:1808.08166. [URL](https://doi.org/10.48550/arXiv.1808.08166)

Moritz Hardt, Eric Price, and Nathan Srebro. 2016. Equality of Opportunity in Supervised Learning. In Proceedings of the 30th Conference on Neural Information Processing Systems (NIPS 2016), 2016, Barcelona, Spain. [URL](https://papers.nips.cc/paper/2016/file/9d2682367c3935defcb1f9e247a97c0d-Paper.pdf)

Rich Zemel, Yu Wu, Kevin Swersky, et al. 2013. Learning Fair Representations. In Proceedings of the 30th International Conference on Machine Learning 2013, PMLR 28, 3, 325-333. [URL](http://proceedings.mlr.press/v28/zemel13.html)

Toshihiro Kamishima, Shotaro Akaho, Hideki Asoh & Jun Sakuma. 2012. Fairness-Aware Classifier with Prejudice Remover Regularizer. In Peter A. Flach, Tijl De Bie, Nello Cristianini (eds) Machine Learning and Knowledge Discovery in Databases. European Conference ECML PKDD 2012, Proceedings Part II, September 24-28, 2012, Bristol, UK. Lecture Notes in Computer Science 7524. Springer, Berlin, Heidelberg. [URL](https://doi.org/10.1007/978-3-642-33486-3_3)

### Security and Resilience Resources

FTC Start With Security Guidelines. 2015. [URL](https://www.ftc.gov/system/files/documents/plain-language/pdf0205-startwithsecurity.pdf) 

Gary McGraw et al. 2022. BIML Interactive Machine Learning Risk Framework. Berryville Institute for Machine Learning. [URL](https://berryvilleiml.com/interactive/)

Ilia Shumailov, Yiren Zhao, Daniel Bates, et al. 2021. Sponge Examples: Energy-Latency Attacks on Neural Networks. arXiv:2006.03463. [URL](https://doi.org/10.48550/arXiv.2006.03463)

Marco Barreno, Blaine Nelson, Anthony D. Joseph, et al. 2010. The Security of Machine Learning. Machine Learning 81 (2010), 121-148. [URL](https://doi.org/10.1007/s10994-010-5188-5)

Matt Fredrikson, Somesh Jha, Thomas Ristenpart. 2015. Model Inversion Attacks that Exploit Confidence Information and Basic Countermeasures. In Proceedings of the 22nd ACM SIGSAC Conference on Computer and Communications Security (CCS '15), October 2015. Association for Computing Machinery, New York, NY, USA, 1322–1333. [URL](https://doi.org/10.1145/2810103.2813677)

National Institute for Standards and Technology (NIST). 2022. Cybersecurity Framework. [URL](https://www.nist.gov/cyberframework)

Nicolas Papernot. 2018. A Marauder's Map of Security and Privacy in Machine Learning. arXiv:1811.01134. [URL](https://doi.org/10.48550/arXiv.1811.01134)

Reza Shokri, Marco Stronati, Congzheng Song, et al. 2017. Membership Inference Attacks against Machine Learning Models. arXiv:1610.05820. [URL](https://doi.org/10.48550/arXiv.1610.05820)

Adversarial Threat Matrix (MITRE). 2021. [URL](https://github.com/mitre/advmlthreatmatrix)

### Interpretability and Explainability Approaches

Chaofan Chen, Oscar Li, Chaofan Tao, et al. 2019. This Looks Like That: Deep Learning for Interpretable Image Recognition. arXiv:1806.10574. [URL](https://doi.org/10.48550/arXiv.1806.10574)

Cynthia Rudin. 2019. Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead. arXiv:1811.10154. [URL](https://doi.org/10.48550/arXiv.1811.10154)

Daniel W. Apley, Jingyu Zhu. 2019. Visualizing the Effects of Predictor Variables in Black Box Supervised Learning Models. arXiv:1612.08468. [URL](https://doi.org/10.48550/arXiv.1612.08468)

David A. Broniatowski. 2021. Psychological Foundations of Explainability and Interpretability in Artificial Intelligence. National Institute of Standards and Technology (NIST) IR 8367. National Institute of Standards and Technology, Gaithersburg, MD.  [URL](https://doi.org/10.6028/NIST.IR.8367)

Forough Poursabzi-Sangdeh, Daniel G. Goldstein, Jake M. Hofman, et al. 2021. Manipulating and Measuring Model Interpretability. arXiv:1802.07810. [URL](https://doi.org/10.48550/arXiv.1802.07810)

Hongyu Yang, Cynthia Rudin, Margo Seltzer. 2017. Scalable Bayesian Rule Lists. arXiv:1602.08610. [URL](https://doi.org/10.48550/arXiv.1602.08610)

P. Jonathon Phillips, Carina A. Hahn, Peter C. Fontana, et al. 2021. Four Principles of Explainable Artificial Intelligence. National Institute of Standards and Technology (NIST) IR 8312. National Institute of Standards and Technology, Gaithersburg, MD. [URL](https://doi.org/10.6028/NIST.IR.8312)

Scott Lundberg, Su-In Lee. 2017. A Unified Approach to Interpreting Model Predictions. arXiv:1705.07874. [URL](https://doi.org/10.48550/arXiv.1705.07874)

Susanne Gaube, Harini Suresh, Martina Raue, et al. 2021. Do as AI say: susceptibility in deployment of clinical decision-aids. npj Digital Medicine 4, Article 31 (2021). [URL](https://doi.org/10.1038/s41746-021-00385-9)

Yin Lou, Rich Caruana, Johannes Gehrke, et al. 2013. Accurate intelligible models with pairwise interactions. In Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining (KDD '13), August 2013. Association for Computing Machinery, New York, NY, USA, 623–631. [URL](https://doi.org/10.1145/2487575.2487579)

### Privacy Resources

National Institute for Standards and Technology (NIST). 2022. Privacy Framework. [URL](https://www.nist.gov/privacy-framework)

### Data Governance

Marijn Janssen, Paul Brous, Elsa Estevez, Luis S. Barbosa, Tomasz Janowski, Data governance: Organizing data for trustworthy Artificial Intelligence, Government Information Quarterly, Volume 37, Issue 3, 2020, 101493, ISSN 0740-624X. [URL](https://doi.org/10.1016/j.giq.2020.101493)

### Software Resources

- [PiML](https://github.com/SelfExplainML/PiML-Toolbox) (explainable models, performance assessment)
- [Interpret](https://github.com/interpretml/interpret) (explainable models)
- [Iml](https://cran.r-project.org/web/packages/iml/index.html) (explainable models)
- [Drifter](https://github.com/ModelOriented/drifter) library (performance assessment)
- [Manifold](https://github.com/uber/manifold) library (performance assessment)
- [SALib](https://github.com/SALib/SALib) library (performance assessment)
- [What-If Tool](https://pair-code.github.io/what-if-tool/index.html#about) (performance assessment)
- [MLextend](http://rasbt.github.io/mlxtend/) (performance assessment)
- AI Fairness 360: 
    - [Python](https://github.com/Trusted-AI/AIF360) (bias testing and mitigation)
    - [R](https://github.com/Trusted-AI/AIF360/tree/master/aif360/aif360-r) (bias testing and mitigation)
- [Adversarial-robustness-toolbox](https://github.com/Trusted-AI/adversarial-robustness-toolbox) (ML security)
- [Robustness](https://github.com/MadryLab/robustness) (ML security)
- [tensorflow/privacy](https://github.com/tensorflow/privacy) (ML security)
- [NIST De-identification Tools](https://www.nist.gov/itl/applied-cybersecurity/privacy-engineering/collaboration-space/focus-areas/de-id/tools) (Privacy and ML security)
- [Dvc](https://dvc.org/) (MLops, deployment)
- [Gigantum](https://github.com/gigantum) (MLops, deployment)
- [Mlflow](https://mlflow.org/) (MLops, deployment)
- [Mlmd](https://github.com/google/ml-metadata) (MLops, deployment)
- [Modeldb](https://github.com/VertaAI/modeldb) (MLops, deployment)","AI Incident Database. 2022. AI Incident Database. [URL](https://incidentdatabase.ai/)

AIAAIC Repository. 2022. AI, algorithmic and automation incidents collected, dissected, examined, and divulged. [URL](https://www.aiaaic.org/aiaaic-repository)

Andrew Burt and Patrick Hall. 2018. What to Do When AI Fails. O’Reilly Media, Inc. (May 18, 2020). Retrieved October 17, 2022. [URL](https://www.oreilly.com/radar/what-to-do-when-ai-fails/)

National Institute for Standards and Technology (NIST). 2022. Cybersecurity Framework. [URL](https://www.nist.gov/cyberframework)

SANS Institute. 2022. Security Consensus Operational Readiness Evaluation (SCORE) Security Checklist [or Advanced Persistent Threat (APT) Handling Checklist]. [URL](https://www.sans.org/media/score/checklists/APT-IncidentHandling-Checklist.pdf)

Suchi Saria, Adarsh Subbaswamy. 2019. Tutorial: Safe and Reliable Machine Learning. arXiv:1904.07204. [URL](https://doi.org/10.48550/arXiv.1904.07204)","Decommissioning Template. Application Lifecycle And Supporting Docs. Cloud and Infrastructure Community of Practice. [URL](https://www.cio.gov/policies-and-priorities/application-lifecycle/)

Develop a Decommission Plan. M3 Playbook. Office of Shared Services and Solutions and Performance Improvement. General Services Administration. [URL](https://ussm.gsa.gov/2.8/)","Office of the Comptroller of the Currency. 2021. Proposed Interagency Guidance on Third-Party Relationships: Risk Management. July 12, 2021. [URL](https://www.occ.gov/news-issuances/news-releases/2021/nr-occ-2021-74a.pdf)","Larysa Visengeriyeva et al. “Awesome MLOps,“ GitHub. Accessed January 9, 2023. [URL](https://github.com/visenger)","Navdeep Gill, Patrick Hall, Kim Montgomery, and Nicholas Schmidt. ""A Responsible Machine Learning Workflow with Focus on Interpretable Models, Post-hoc Explanation, and Discrimination Testing."" Information 11, no. 3 (2020): 137. [URL](https://www.mdpi.com/2078-2489/11/3/137)","Yen, Po-Yin, et al. ""Development and Evaluation of Socio-Technical Metrics to Inform HIT Adaptation."" [URL](https://digital.ahrq.gov/sites/default/files/docs/citation/r21hs024767-yen-final-report-2019.pdf)

Carayon, Pascale, and Megan E. Salwei. ""Moving toward a sociotechnical systems approach to continuous health information technology design: the path forward for improving electronic health record usability and reducing clinician burnout."" Journal of the American Medical Informatics Association 28.5 (2021): 1026-1028. [URL](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8068435/pdf/ocab002.pdf)

Mishra, Deepa, et al. ""Organizational capabilities that enable big data and predictive analytics diffusion and organizational performance: A resource-based perspective."" Management Decision (2018).","Wei, M., & Zhou, Z. (2022). AI Ethics Issues in Real World: Evidence from AI Incident Database. ArXiv, abs/2206.07635. [URL](https://arxiv.org/pdf/2206.07635.pdf)

McGregor, Sean. ""Preventing repeated real world AI failures by cataloging incidents: The AI incident database."" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 35. No. 17. 2021. [URL](https://arxiv.org/pdf/2011.08512.pdf)

Macrae, Carl. ""Learning from the failure of autonomous and intelligent systems: Accidents, safety, and sociotechnical sources of risk."" Risk analysis 42.9 (2022): 1999-2025. [URL](https://onlinelibrary.wiley.com/doi/epdf/10.1111/risa.13850)","### Socio-technical systems

Andrew D. Selbst, danah boyd, Sorelle A. Friedler, et al. 2019. Fairness and Abstraction in Sociotechnical Systems. In Proceedings of the Conference on Fairness, Accountability, and Transparency (FAccT'19). Association for Computing Machinery, New York, NY, USA, 59–68. [URL](https://doi.org/10.1145/3287560.3287598)

### Problem formulation

Roel Dobbe, Thomas Krendl Gilbert, and Yonatan Mintz. 2021. Hard choices in artificial intelligence. Artificial Intelligence 300 (14 July 2021), 103555, ISSN 0004-3702. [URL](https://doi.org/10.1016/j.artint.2021.103555)

Samir Passi and Solon Barocas. 2019. Problem Formulation and Fairness. In Proceedings of the Conference on Fairness, Accountability, and Transparency (FAccT'19). Association for Computing Machinery, New York, NY, USA, 39–48. [URL](https://doi.org/10.1145/3287560.3287567)

### Context mapping

Emilio Gómez-González and Emilia Gómez. 2020. Artificial intelligence in medicine and healthcare. Joint Research Centre (European Commission). [URL](https://op.europa.eu/en/publication-detail/-/publication/b4b5db47-94c0-11ea-aac4-01aa75ed71a1/language-en)

Sarah Spiekermann and Till Winkler. 2020. Value-based Engineering for Ethics by Design. arXiv:2004.13676. [URL](https://arxiv.org/abs/2004.13676)

Social Impact Lab. 2017. Framework for Context Analysis of Technologies in Social Change Projects (Draft v2.0). [URL](https://www.alnap.org/system/files/content/resource/files/main/Draft%20SIMLab%20Context%20Analysis%20Framework%20v2.0.pdf)

Solon Barocas, Asia J. Biega, Margarita Boyarskaya, et al. 2021. Responsible computing during COVID-19 and beyond. Commun. ACM 64, 7 (July 2021), 30–32. [URL](https://doi.org/10.1145/3466612)

### Identification of harms

Harini Suresh and John V. Guttag. 2020. A Framework for Understanding Sources of Harm throughout the Machine Learning Life Cycle. arXiv:1901.10002. [URL](https://arxiv.org/abs/1901.10002)

Margarita Boyarskaya, Alexandra Olteanu, and Kate Crawford. 2020. Overcoming Failures of Imagination in AI Infused System Development and Deployment. arXiv:2011.13416. [URL](https://arxiv.org/abs/2011.13416)

Microsoft. Foundations of assessing harm. 2022. [URL](https://docs.microsoft.com/en-us/azure/architecture/guide/responsible-innovation/harms-modeling/)

### Understanding and documenting limitations in ML

Alexander D'Amour, Katherine Heller, Dan Moldovan, et al. 2020. Underspecification Presents Challenges for Credibility in Modern Machine Learning. arXiv:2011.03395. [URL](https://arxiv.org/abs/2011.03395)

Arvind Narayanan. ""How to Recognize AI Snake Oil."" Arthur Miller Lecture on Science and Ethics (2019). [URL](https://www.cs.princeton.edu/~arvindn/talks/MIT-STS-AI-snakeoil.pdf)

Jessie J. Smith, Saleema Amershi, Solon Barocas, et al. 2022. REAL ML: Recognizing, Exploring, and Articulating Limitations of Machine Learning Research. arXiv:2205.08363. [URL](https://arxiv.org/abs/2205.08363)

Margaret Mitchell, Simone Wu, Andrew Zaldivar, et al. 2019. Model Cards for Model Reporting. In Proceedings of the Conference on Fairness, Accountability, and Transparency (FAT* '19). Association for Computing Machinery, New York, NY, USA, 220–229. [URL](https://doi.org/10.1145/3287560.3287596)

Matthew Arnold, Rachel K. E. Bellamy, Michael Hind, et al. 2019. FactSheets: Increasing Trust in AI Services through Supplier's Declarations of Conformity. arXiv:1808.07261. [URL](https://arxiv.org/abs/1808.07261)

Matthew J. Salganik, Ian Lundberg, Alexander T. Kindel, Caitlin E. Ahearn, Khaled Al-Ghoneim, Abdullah Almaatouq, Drew M. Altschul et al. ""Measuring the Predictability of Life Outcomes with a Scientific Mass Collaboration."" Proceedings of the National Academy of Sciences 117, No. 15 (2020): 8398-8403. [URL](https://www.pnas.org/doi/10.1073/pnas.1915006117)

Michael A. Madaio, Luke Stark, Jennifer Wortman Vaughan, and Hanna Wallach. 2020. Co-Designing Checklists to Understand Organizational Challenges and Opportunities around Fairness in AI. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems (CHI ‘20). Association for Computing Machinery, New York, NY, USA, 1–14. [URL](https://doi.org/10.1145/3313831.3376445)

Timnit Gebru, Jamie Morgenstern, Briana Vecchione, et al. 2021. Datasheets for Datasets. arXiv:1803.09010. [URL](https://arxiv.org/abs/1803.09010)

Bender, E. M., Friedman, B. & McMillan-Major, A.,  (2022). A Guide for Writing Data Statements for Natural Language Processing. University of Washington.  Accessed July 14, 2022. [URL](https://techpolicylab.uw.edu/wp-content/uploads/2021/11/Data_Statements_Guide_V2.pdf)

Meta AI. System Cards, a new resource for understanding how AI systems work, 2021. [URL](https://ai.facebook.com/blog/system-cards-a-new-resource-for-understanding-how-ai-systems-work/)

### When not to deploy

Solon Barocas, Asia J. Biega, Benjamin Fish, et al. 2020. When not to design, build, or deploy. In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency (FAT* '20). Association for Computing Machinery, New York, NY, USA, 695. [URL](https://doi.org/10.1145/3351095.3375691)

### Statistical balance

Ziad Obermeyer, Brian Powers, Christine Vogeli, and Sendhil Mullainathan. 2019. Dissecting racial bias in an algorithm used to manage the health of populations. Science 366, 6464 (25 Oct. 2019), 447-453. [URL](https://doi.org/10.1126/science.aax2342)

### Assessment of science in AI

Arvind Narayanan. How to recognize AI snake oil. [URL](https://www.cs.princeton.edu/~arvindn/talks/MIT-STS-AI-snakeoil.pdf)

Emily M. Bender. 2022. On NYT Magazine on AI: Resist the Urge to be Impressed. (April 17, 2022). [URL](https://medium.com/@emilymenonbender/on-nyt-magazine-on-ai-resist-the-urge-to-be-impressed-3d92fd9a0edd)","Sina Fazelpour and Maria De-Arteaga. 2022. Diversity in sociotechnical machine learning systems. Big Data & Society 9, 1 (Jan. 2022). [URL](https://doi.org/10.1177%2F20539517221082027)

Microsoft Community Jury , Azure Application Architecture Guide. [URL](https://docs.microsoft.com/en-us/azure/architecture/guide/responsible-innovation/community-jury/)

Fernando Delgado, Stephen Yang, Michael Madaio, Qian Yang. (2021). Stakeholder Participation in AI: Beyond ""Add Diverse Stakeholders and Stir"". [URL](https://deepai.org/publication/stakeholder-participation-in-ai-beyond-add-diverse-stakeholders-and-stir)

Kush Varshney, Tina Park, Inioluwa Deborah Raji, Gaurush Hiranandani, Narasimhan Harikrishna, Oluwasanmi Koyejo, Brianna Richardson, and Min Kyung Lee. Participatory specification of trustworthy machine learning, 2021.

Donald Martin, Vinodkumar Prabhakaran, Jill A. Kuhlberg, Andrew Smart and William S. Isaac. “Participatory Problem Formulation for Fairer Machine Learning Through Community Based System Dynamics”, ArXiv abs/2005.07572 (2020). [URL](https://arxiv.org/pdf/2005.07572.pdf)","M.S. Ackerman (2000). The Intellectual Challenge of CSCW: The Gap Between Social Requirements and Technical Feasibility. Human–Computer Interaction, 15, 179 - 203. [URL](https://socialworldsresearch.org/sites/default/files/hci.final_.pdf)

McKane Andrus, Sarah Dean, Thomas Gilbert,  Nathan Lambert, Tom Zick (2021). AI Development for the Public Interest: From Abstraction Traps to Sociotechnical Risks. [URL](https://arxiv.org/pdf/2102.04255.pdf)

Abeba Birhane, Pratyusha Kalluri, Dallas Card, et al. 2022. The Values Encoded in Machine Learning Research. arXiv:2106.15590. [URL](https://arxiv.org/abs/2106.15590)

Board of Governors of the Federal Reserve System. SR 11-7: Guidance on Model Risk Management. (April 4, 2011). [URL](https://www.federalreserve.gov/supervisionreg/srletters/sr1107.htm)

Iason Gabriel, Artificial Intelligence, Values, and Alignment. Minds & Machines 30, 411–437 (2020). [URL](https://doi.org/10.1007/s11023-020-09539-2)

PEAT “Business Case for Equitable AI”. [URL](https://www.peatworks.org/ai-disability-inclusion-toolkit/business-case-for-equitable-ai/)","Algorithm Watch. AI Ethics Guidelines Global Inventory. [URL](https://inventory.algorithmwatch.org/)

Ethical OS toolkit. [URL](https://ethicalos.org/)

Emanuel Moss and Jacob Metcalf. 2020. Ethics Owners: A New Model of Organizational Responsibility in Data-Driven Technology Companies. Data & Society Research Institute. [URL](https://datasociety.net/pubs/Ethics-Owners.pdf)

Future of Life Institute. Asilomar AI Principles. [URL](https://futureoflife.org/2017/08/11/ai-principles/)

Leonard Haas, Sebastian Gießler, and Veronika Thiel. 2020. In the realm of paper tigers – exploring the failings of AI ethics guidelines. (April 28, 2020). [URL](https://algorithmwatch.org/en/ai-ethics-guidelines-inventory-upgrade-2020/)","Board of Governors of the Federal Reserve System. SR 11-7: Guidance on Model Risk Management. (April 4, 2011). [URL](https://www.federalreserve.gov/supervisionreg/srletters/sr1107.htm)

The Office of the Comptroller of the Currency. Enterprise Risk Appetite Statement. (Nov. 20, 2019). [URL](https://www.occ.treas.gov/publications-and-resources/publications/banker-education/files/pub-risk-appetite-statement.pdf)

Brenda Boultwood, How to Develop an Enterprise Risk-Rating Approach (Aug. 26, 2021). Global Association of Risk Professionals (garp.org). Accessed Jan. 4, 2023. [URL](https://www.garp.org/risk-intelligence/culture-governance/how-to-develop-an-enterprise-risk-rating-approach)

Virginia Eubanks, 1972-, Automating Inequality: How High-tech Tools Profile, Police, and Punish the Poor. New York, NY, St. Martin's Press, 2018.

GAO-17-63: Enterprise Risk Management: Selected Agencies’ Experiences Illustrate Good Practices in Managing Risk. [URL](https://www.gao.gov/assets/gao-17-63.pdf) See Table 3.

NIST Risk Management Framework. [URL](https://csrc.nist.gov/projects/risk-management/about-rmf)","National Academies of Sciences, Engineering, and Medicine 2022. Fostering Responsible Computing Research: Foundations and Practices. Washington, DC: The National Academies Press. [URL](https://doi.org/10.17226/26507)

Abeba Birhane,  William S. Isaac, Vinodkumar Prabhakaran, Mark Diaz, Madeleine Clare Elish, Iason Gabriel and Shakir Mohamed. “Power to the People? Opportunities and Challenges for Participatory AI.” Equity and Access in Algorithms, Mechanisms, and Optimization (2022). [URL](https://arxiv.org/pdf/2209.07572.pdf)

Amit K. Chopra, Fabiano Dalpiaz, F. Basak Aydemir, et al. 2014. Protos: Foundations for engineering innovative sociotechnical systems. In 2014 IEEE 22nd International Requirements Engineering Conference (RE) (2014), 53-62. [URL](https://doi.org/10.1109/RE.2014.6912247)

Andrew D. Selbst, danah boyd, Sorelle A. Friedler, et al. 2019. Fairness and Abstraction in Sociotechnical Systems. In Proceedings of the Conference on Fairness, Accountability, and Transparency (FAT* '19). Association for Computing Machinery, New York, NY, USA, 59–68. [URL](https://doi.org/10.1145/3287560.3287598)

Gordon Baxter and Ian Sommerville. 2011. Socio-technical systems: From design methods to systems engineering. Interacting with Computers, 23, 1 (Jan. 2011), 4–17. [URL](https://doi.org/10.1016/j.intcom.2010.07.003)

Roel Dobbe, Thomas Krendl Gilbert, and Yonatan Mintz. 2021. Hard choices in artificial intelligence. Artificial Intelligence 300 (14 July 2021), 103555, ISSN 0004-3702. [URL](https://doi.org/10.1016/j.artint.2021.103555)

Yilin Huang, Giacomo Poderi, Sanja Šcepanovic, et al. 2019. Embedding Internet-of-Things in Large-Scale Socio-technical Systems: A Community-Oriented Design in Future Smart Grids. In The Internet of Things for Smart Urban Ecosystems (2019), 125-150. Springer, Cham. [URL](https://doi.org/10.1007/978-3-319-96550-5_6)

Victor Udoewa, (2022). An introduction to radical participatory design: decolonising participatory design processes. Design Science. 8. 10.1017/dsj.2022.24. [URL](https://www.cambridge.org/core/journals/design-science/article/an-introduction-to-radical-participatory-design-decolonising-participatory-design-processes/63F70ECC408844D3CD6C1A5AC7D35F4D)","Leong, Brenda (2020). The Spectrum of Artificial Intelligence - An Infographic Tool. Future of Privacy Forum. [URL](https://fpf.org/blog/the-spectrum-of-artificial-intelligence-an-infographic-tool/)

Brownlee, Jason (2020). A Tour of Machine Learning Algorithms. Machine Learning Mastery. [URL](https://machinelearningmastery.com/a-tour-of-machine-learning-algorithms/)","### Context of use

International Standards Organization (ISO). 2019. ISO 9241-210:2019 Ergonomics of human-system interaction — Part 210: Human-centred design for interactive systems. [URL](https://www.iso.org/standard/77520.html)

National Institute of Standards and Technology (NIST), Mary Theofanos, Yee-Yin Choong, et al. 2017. NIST Handbook 161 Usability Handbook for Public Safety Communications: Ensuring Successful Systems for First Responders. [URL](https://doi.org/10.6028/NIST.HB.161)

### Human-AI interaction

Committee on Human-System Integration Research Topics for the 711th Human Performance Wing of the Air Force Research Laboratory and the National Academies of Sciences, Engineering, and Medicine. 2022. Human-AI Teaming: State-of-the-Art and Research Needs. Washington, D.C. National Academies Press. [URL](https://nap.nationalacademies.org/catalog/26355/human-ai-teaming-state-of-the-art-and-research-needs)

Human Readiness Level Scale in the System Development Process, American National Standards Institute and Human Factors and Ergonomics Society, ANSI/HFES 400-2021

Microsoft Responsible AI Standard, v2. [URL](https://query.prod.cms.rt.microsoft.com/cms/api/am/binary/RE4ZPmV)

Saar Alon-Barkat, Madalina Busuioc, Human–AI Interactions in Public Sector Decision Making: “Automation Bias” and “Selective Adherence” to Algorithmic Advice, Journal of Public Administration Research and Theory, 2022;, muac007. [URL](https://doi.org/10.1093/jopart/muac007)

Zana Buçinca, Maja Barbara Malaya, and Krzysztof Z. Gajos. 2021. To Trust or to Think: Cognitive Forcing Functions Can Reduce Overreliance on AI in AI-assisted Decision-making. Proc. ACM Hum.-Comput. Interact. 5, CSCW1, Article 188 (April 2021), 21 pages. [URL](https://doi.org/10.1145/3449287)

Mary L. Cummings. 2006 Automation and accountability in decision support system interface design.The Journal of Technology Studies 32(1): 23–31. [URL](https://scholar.lib.vt.edu/ejournals/JOTS/v32/v32n1/pdf/cummings.pdf)

Engstrom, D. F., Ho, D. E., Sharkey, C. M., & Cuéllar, M. F. (2020). Government by algorithm: Artificial intelligence in federal administrative agencies. NYU School of Law, Public Law Research Paper, (20-54). [URL](https://www.acus.gov/report/government-algorithm-artificial-intelligence-federal-administrative-agencies) 

Susanne Gaube, Harini Suresh, Martina Raue, et al. 2021. Do as AI say: susceptibility in deployment of clinical decision-aids. npj Digital Medicine 4, Article 31 (2021). [URL](https://doi.org/10.1038/s41746-021-00385-9)

Ben Green. 2021. The Flaws of Policies Requiring Human Oversight of Government Algorithms. Computer Law & Security Review 45 (26 Apr. 2021). [URL](https://dx.doi.org/10.2139/ssrn.3921216)

Ben Green and Amba Kak. 2021. The False Comfort of Human Oversight as an Antidote to A.I. Harm. (June 15, 2021). [URL](https://slate.com/technology/2021/06/human-oversight-artificial-intelligence-laws.html)

Grgic-Hlaca, N., Engel, C., & Gummadi, K. P. (2019). Human decision making with machine assistance: An experiment on bailing and jailing. Proceedings of the ACM on Human-Computer Interaction, 3(CSCW), 1-25. [URL](https://dl.acm.org/doi/pdf/10.1145/3359280)

Forough Poursabzi-Sangdeh, Daniel G Goldstein, Jake M Hofman, et al. 2021. Manipulating and Measuring Model Interpretability. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems (CHI '21). Association for Computing Machinery, New York, NY, USA, Article 237, 1–52. [URL](https://doi.org/10.1145/3411764.3445315)

C. J. Smith (2019). Designing trustworthy AI: A human-machine teaming framework to guide development. arXiv preprint arXiv:1910.03515. [URL](https://kilthub.cmu.edu/articles/conference_contribution/Designing_Trustworthy_AI_A_Human-Machine_Teaming_Framework_to_Guide_Development/12119847/1)

T. Warden, P. Carayon, EM  et al. The National Academies Board on Human System Integration (BOHSI) Panel: Explainable AI, System Transparency, and Human Machine Teaming. Proceedings of the Human Factors and Ergonomics Society Annual Meeting. 2019;63(1):631-635. doi:10.1177/1071181319631100. [URL](https://sites.nationalacademies.org/cs/groups/dbassesite/documents/webpage/dbasse_196735.pdf)","### Challenges with dataset selection

Alexandra Olteanu, Carlos Castillo, Fernando Diaz, and Emre Kiciman. 2019. Social Data: Biases, Methodological Pitfalls, and Ethical Boundaries. Front. Big Data 2, 13 (11 July 2019). [URL](https://doi.org/10.3389/fdata.2019.00013)

Amandalynne Paullada, Inioluwa Deborah Raji, Emily M. Bender, et al. 2020. Data and its (dis)contents: A survey of dataset development and use in machine learning research. arXiv:2012.05345. [URL](https://arxiv.org/abs/2012.05345)

Catherine D'Ignazio and Lauren F. Klein. 2020. Data Feminism. The MIT Press, Cambridge, MA. [URL](https://data-feminism.mitpress.mit.edu/)

Miceli, M., & Posada, J. (2022). The Data-Production Dispositif. ArXiv, abs/2205.11963.

Barbara Plank. 2016. What to do about non-standard (or non-canonical) language in NLP. arXiv:1608.07836. [URL](https://arxiv.org/abs/1608.07836)

### Dataset and test, evaluation, validation and verification (TEVV) processes in AI system development

National Institute of Standards and Technology (NIST), Reva Schwartz, Apostol Vassilev, et al. 2022. NIST Special Publication 1270 Towards a Standard for Identifying and Managing Bias in Artificial Intelligence. [URL](https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.1270.pdf)

Inioluwa Deborah Raji, Emily M. Bender, Amandalynne Paullada, et al. 2021. AI and the Everything in the Whole Wide World Benchmark. arXiv:2111.15366. [URL](https://arxiv.org/abs/2111.15366)

### Statistical balance

Ziad Obermeyer, Brian Powers, Christine Vogeli, and Sendhil Mullainathan. 2019. Dissecting racial bias in an algorithm used to manage the health of populations. Science 366, 6464 (25 Oct. 2019), 447-453. [URL](https://doi.org/10.1126/science.aax2342)

Amandalynne Paullada, Inioluwa Deborah Raji, Emily M. Bender, et al. 2020. Data and its (dis)contents: A survey of dataset development and use in machine learning research. arXiv:2012.05345. [URL](https://arxiv.org/abs/2012.05345)

Solon Barocas, Anhong Guo, Ece Kamar, et al. 2021. Designing Disaggregated Evaluations of AI Systems: Choices, Considerations, and Tradeoffs. Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society. Association for Computing Machinery, New York, NY, USA, 368–378. [URL](https://doi.org/10.1145/3461702.3462610)

### Measurement and evaluation

Abigail Z. Jacobs and Hanna Wallach. 2021. Measurement and Fairness. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency (FAccT ‘21). Association for Computing Machinery, New York, NY, USA, 375–385. [URL](https://doi.org/10.1145/3442188.3445901)

Ben Hutchinson, Negar Rostamzadeh, Christina Greer, et al. 2022. Evaluation Gaps in Machine Learning Practice. arXiv:2205.05256. [URL](https://arxiv.org/abs/2205.05256)

Laura Freeman, ""Test and evaluation for artificial intelligence."" Insight 23.1 (2020): 27-30. [URL](https://doi.org/10.1002/inst.12281)

### Existing frameworks

National Institute of Standards and Technology. (2018). Framework for improving critical infrastructure cybersecurity. [URL](https://nvlpubs.nist.gov/nistpubs/cswp/nist.cswp.04162018.pdf)

Kaitlin R. Boeckl and Naomi B. Lefkovitz. ""NIST Privacy Framework: A Tool for Improving Privacy Through Enterprise Risk Management, Version 1.0."" National Institute of Standards and Technology (NIST), January 16, 2020. [URL](https://www.nist.gov/publications/nist-privacy-framework-tool-improving-privacy-through-enterprise-risk-management.)","Roel Dobbe, Thomas Krendl Gilbert, and Yonatan Mintz. 2021. Hard choices in artificial intelligence. Artificial Intelligence 300 (14 July 2021), 103555, ISSN 0004-3702. [URL](https://doi.org/10.1016/j.artint.2021.103555)

Samir Passi and Solon Barocas. 2019. Problem Formulation and Fairness. In Proceedings of the Conference on Fairness, Accountability, and Transparency (FAT* '19). Association for Computing Machinery, New York, NY, USA, 39–48. [URL](https://doi.org/10.1145/3287560.3287567)

Vincent T. Covello. 2021. Stakeholder Engagement and Empowerment. In Communicating in Risk, Crisis, and High Stress Situations (Vincent T. Covello, ed.), 87-109. [URL](https://ieeexplore.ieee.org/document/9648995)

Yilin Huang, Giacomo Poderi, Sanja Šcepanovic, et al. 2019. Embedding Internet-of-Things in Large-Scale Socio-technical Systems: A Community-Oriented Design in Future Smart Grids. In The Internet of Things for Smart Urban Ecosystems (2019), 125-150. Springer, Cham. [URL](https://link.springer.com/chapter/10.1007/978-3-319-96550-5_6)

Eloise Taysom and Nathan Crilly. 2017. Resilience in Sociotechnical Systems: The Perspectives of Multiple Stakeholders. She Ji: The Journal of Design, Economics, and Innovation, 3, 3 (2017), 165-182, ISSN 2405-8726. [URL](https://www.sciencedirect.com/science/article/pii/S2405872617300758)","Abagayle Lee Blank. 2019. Computer vision machine learning and future-oriented ethics. Honors Project. Seattle Pacific University (SPU), Seattle, WA. [URL](https://digitalcommons.spu.edu/cgi/viewcontent.cgi?article=1100&context=honorsprojects)

Margarita Boyarskaya, Alexandra Olteanu, and Kate Crawford. 2020. Overcoming Failures of Imagination in AI Infused System Development and Deployment. arXiv:2011.13416. [URL](https://arxiv.org/abs/2011.13416)

Jeff Patton. 2014. User Story Mapping. O'Reilly, Sebastopol, CA. [URL](https://www.jpattonassociates.com/story-mapping/)

Margarita Boenig-Liptsin, Anissa Tanweer & Ari Edmundson (2022) Data Science Ethos Lifecycle: Interplay of ethical thinking and data science practice, Journal of Statistics and Data Science Education, DOI: 10.1080/26939169.2022.2089411

J. Cohen, D. S. Katz, M. Barker, N. Chue Hong, R. Haines and C. Jay, ""The Four Pillars of Research Software Engineering,"" in IEEE Software, vol. 38, no. 1, pp. 97-105, Jan.-Feb. 2021, doi: 10.1109/MS.2020.2973362.

National Academies of Sciences, Engineering, and Medicine 2022. Fostering Responsible Computing Research: Foundations and Practices. Washington, DC: The National Academies Press. [URL](https://doi.org/10.17226/26507)","Mark J. Van der Laan and Sherri Rose (2018). Targeted Learning in Data Science. Cham: Springer International Publishing, 2018.

Alice Zheng. 2015. Evaluating Machine Learning Models (2015). O'Reilly. [URL](https://www.oreilly.com/library/view/evaluating-machine-learning/9781492048756/)

Brenda Leong and Patrick Hall (2021). 5 things lawyers should know about artificial intelligence. ABA Journal. [URL](https://www.abajournal.com/columns/article/5-things-lawyers-should-know-about-artificial-intelligence)

UK Centre for Data Ethics and Innovation, “The roadmap to an effective AI assurance ecosystem”. [URL](https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/1039146/The_roadmap_to_an_effective_AI_assurance_ecosystem.pdf)","National Academies of Sciences, Engineering, and Medicine. 2022. Human-AI Teaming:
State-of-the-Art and Research Needs. Washington, DC: The National Academies Press. [URL](https://doi.org/10.17226/26355)

Human Readiness Level Scale in the System Development Process, American National Standards Institute and Human Factors and Ergonomics Society, ANSI/HFES 400-2021.

Human-Machine Teaming Systems Engineering Guide. P McDermott, C Dominguez, N Kasdaglis, M Ryan, I Trahan, A Nelson. MITRE Corporation, 2018.

Saar Alon-Barkat, Madalina Busuioc, Human–AI Interactions in Public Sector Decision Making: “Automation Bias” and “Selective Adherence” to Algorithmic Advice, Journal of Public Administration Research and Theory, 2022;, muac007. [URL](https://doi.org/10.1093/jopart/muac007)

Breana M. Carter-Browne, Susannah B. F. Paletz, Susan G. Campbell , Melissa J. Carraway, Sarah H. Vahlkamp, Jana Schwartz , Polly O’Rourke, “There is No “AI” in Teams: A Multidisciplinary Framework for AIs to Work in Human Teams; Applied Research Laboratory for Intelligence and Security (ARLIS) Report, June 2021. [URL](https://www.arlis.umd.edu/sites/default/files/2022-03/No_AI_In_Teams_FinalReport%20(1).pdf)

R Crootof, ME Kaminski, and WN Price II.  Humans in the Loop (March 25, 2022). Vanderbilt Law Review, Forthcoming 2023, U of Colorado Law Legal Studies Research Paper No. 22-10, U of Michigan Public Law Research Paper No. 22-011. [URL](https://ssrn.com/abstract=4066781 or http://dx.doi.org/10.2139/ssrn.4066781)

S Mo Jones-Jang, Yong Jin Park, How do people react to AI failure? Automation bias, algorithmic aversion, and perceived controllability, Journal of Computer-Mediated Communication, Volume 28, Issue 1, January 2023, zmac029. [URL](https://doi.org/10.1093/jcmc/zmac029)

A Knack, R Carter and A Babuta, ""Human-Machine Teaming in Intelligence Analysis: Requirements for developing trust in machine learning systems,"" CETaS Research Reports (December 2022). [URL](https://cetas.turing.ac.uk/sites/default/files/2022-12/cetas_research_report_-_hmt_and_intelligence_analysis_vfinal.pdf)

SD Ramchurn, S Stein , NR Jennings. Trustworthy human-AI partnerships. iScience. 2021;24(8):102891. Published 2021 Jul 24. doi:10.1016/j.isci.2021.102891. [URL](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8365362/pdf/main.pdf)

M. Veale, M. Van Kleek, and R. Binns, “Fairness and Accountability Design Needs for Algorithmic Support in High-Stakes Public Sector Decision-Making,” in Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems - CHI ’18. Montreal QC, Canada: ACM Press, 2018, pp. 1–14. [URL](http://dl.acm.org/citation.cfm?doid=3173574.3174014)","Ben Green, “The Flaws of Policies Requiring Human Oversight of Government Algorithms,” SSRN Journal, 2021. [URL](https://www.ssrn.com/abstract=3921216)

Luciano Cavalcante Siebert, Maria Luce Lupetti,  Evgeni Aizenberg, Niek Beckers, Arkady Zgonnikov, Herman Veluwenkamp, David Abbink, Elisa Giaccardi, Geert-Jan Houben, Catholijn Jonker, Jeroen van den Hoven, Deborah Forster, & Reginald Lagendijk (2021). Meaningful human control: actionable properties for AI system development. AI and Ethics. [URL](https://link.springer.com/article/10.1007/s43681-022-00167-3)

Mary Cummings, (2014). Automation and Accountability in Decision Support System Interface Design. The Journal of Technology Studies. 32. 10.21061/jots.v32i1.a.4. [URL](https://scholar.lib.vt.edu/ejournals/JOTS/v32/v32n1/pdf/cummings.pdf)

Madeleine Elish, M. (2016). Moral Crumple Zones: Cautionary Tales in Human-Robot Interaction (WeRobot 2016). SSRN Electronic Journal. 10.2139/ssrn.2757236. [URL](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2757236)

R Crootof, ME Kaminski, and WN Price II.  Humans in the Loop (March 25, 2022). Vanderbilt Law Review, Forthcoming 2023, U of Colorado Law Legal Studies Research Paper No. 22-10, U of Michigan Public Law Research Paper No. 22-011. [LINK](https://ssrn.com/abstract=4066781), [URL](http://dx.doi.org/10.2139/ssrn.4066781)

Bogdana Rakova, Jingying Yang, Henriette Cramer, & Rumman Chowdhury (2020). Where Responsible AI meets Reality. Proceedings of the ACM on Human-Computer Interaction, 5, 1 - 23. [URL](https://arxiv.org/pdf/2006.12358.pdf)","### Language  models

Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 2021. On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? ??. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency (FAccT '21). Association for Computing Machinery, New York, NY, USA, 610–623. [URL](https://doi.org/10.1145/3442188.3445922)

Julia Kreutzer, Isaac Caswell, Lisa Wang, et al. 2022. Quality at a Glance: An Audit of Web-Crawled Multilingual Datasets. Transactions of the Association for Computational Linguistics 10 (2022), 50–72.  [URL](https://doi.org/10.1162/tacl_a_00447)

Laura Weidinger, Jonathan Uesato, Maribeth Rauh, et al. 2022. Taxonomy of Risks posed by Language Models. In 2022 ACM Conference on Fairness, Accountability, and Transparency (FAccT '22). Association for Computing Machinery, New York, NY, USA, 214–229. [URL](https://doi.org/10.1145/3531146.3533088)

Office of the Comptroller of the Currency. 2021. Comptroller's Handbook: Model Risk Management, Version 1.0, August 2021. [URL](https://www.occ.gov/publications-and-resources/publications/comptrollers-handbook/files/model-risk-management/index-model-risk-management.html)

Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, et al. 2021. On the Opportunities and Risks of Foundation Models. arXiv:2108.07258. [URL](https://arxiv.org/abs/2108.07258)

Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, William Fedus. “Emergent Abilities of Large Language Models.” ArXiv abs/2206.07682 (2022). [URL](https://arxiv.org/pdf/2206.07682.pdf)","Office of the Comptroller of the Currency. 2021. Comptroller's Handbook: Model Risk Management, Version 1.0, August 2021. Retrieved on July 7, 2022. [URL](https://www.occ.gov/publications-and-resources/publications/comptrollers-handbook/files/model-risk-management/index-model-risk-management.html)

Proposed Interagency Guidance on Third-Party Relationships: Risk Management, 2021. [URL](https://www.occ.gov/news-issuances/news-releases/2021/nr-occ-2021-74a.pdf)

Kang, D., Raghavan, D., Bailis, P.D., & Zaharia, M.A. (2020). Model Assertions for Monitoring and Improving ML Models. ArXiv, abs/2003.01668. [URL](https://proceedings.mlsys.org/paper/2020/file/a2557a7b2e94197ff767970b67041697-Paper.pdf)","Emilio Gómez-González and Emilia Gómez. 2020. Artificial intelligence in medicine and healthcare. Joint Research Centre (European Commission). [URL](https://op.europa.eu/en/publication-detail/-/publication/b4b5db47-94c0-11ea-aac4-01aa75ed71a1/language-en)

Artificial Intelligence Incident Database. 2022. [URL](https://incidentdatabase.ai/?lang=en)

Anthony M. Barrett, Dan Hendrycks, Jessica Newman and Brandie Nonnecke. “Actionable Guidance for High-Consequence AI Risk Management: Towards Standards Addressing AI Catastrophic Risks"". ArXiv abs/2206.08966 (2022) [URL](https://arxiv.org/abs/2206.08966)

Ganguli, D., et al. (2022). Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned. arXiv. https://arxiv.org/abs/2209.07858","Susanne Vernim, Harald Bauer, Erwin Rauch, et al. 2022. A value sensitive design approach for designing AI-based worker assistance systems in manufacturing. Procedia Comput. Sci. 200, C (2022), 505–516. [URL](https://doi.org/10.1016/j.procs.2022.01.248)

Harini Suresh and John V. Guttag. 2020. A Framework for Understanding Sources of Harm throughout the Machine Learning Life Cycle. arXiv:1901.10002. Retrieved from [URL](https://arxiv.org/abs/1901.10002)

Margarita Boyarskaya, Alexandra Olteanu, and Kate Crawford. 2020. Overcoming Failures of Imagination in AI Infused System Development and Deployment. arXiv:2011.13416. [URL](https://arxiv.org/abs/2011.13416)

Konstantinia Charitoudi and Andrew Blyth. A Socio-Technical Approach to Cyber Risk Management and Impact Assessment. Journal of Information Security 4, 1 (2013), 33-41. [URL](http://dx.doi.org/10.4236/jis.2013.41005)

Raji, I.D., Smart, A., White, R.N., Mitchell, M., Gebru, T., Hutchinson, B., Smith-Loud, J., Theron, D., & Barnes, P. (2020). Closing the AI accountability gap: defining an end-to-end framework for internal algorithmic auditing. Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency.

Emanuel Moss, Elizabeth Anne Watkins, Ranjit Singh, Madeleine Clare Elish, & Jacob Metcalf. 2021. Assemlbing Accountability: Algorithmic Impact Assessment for the Public Interest.  Data & Society. Accessed 7/14/2022 at [URL](https://datasociety.net/library/assembling-accountability-algorithmic-impact-assessment-for-the-public-interest/)

Shari Trewin (2018). AI Fairness for People with Disabilities: Point of View. ArXiv, abs/1811.10670. [URL](https://arxiv.org/pdf/1811.10670.pdf)

Ada Lovelace Institute. 2022. Algorithmic Impact Assessment: A Case Study in Healthcare. Accessed July 14, 2022. [URL](https://www.adalovelaceinstitute.org/report/algorithmic-impact-assessment-case-study-healthcare/)

Microsoft Responsible AI Impact Assessment Template. 2022. Accessed July 14, 2022. [URL](https://blogs.microsoft.com/wp-content/uploads/prod/sites/5/2022/06/Microsoft-RAI-Impact-Assessment-Template.pdf)

Microsoft Responsible AI Impact Assessment Guide. 2022. Accessed July 14, 2022. [URL](https://blogs.microsoft.com/wp-content/uploads/prod/sites/5/2022/06/Microsoft-RAI-Impact-Assessment-Guide.pdf)

Microsoft Responsible AI Standard, v2. [URL](https://query.prod.cms.rt.microsoft.com/cms/api/am/binary/RE4ZPmV)

Microsoft Research AI Fairness Checklist. [URL](https://www.microsoft.com/en-us/research/project/ai-fairness-checklist/)

PEAT AI & Disability Inclusion Toolkit – Risks of Bias and Discrimination in AI Hiring Tools. [URL](https://www.peatworks.org/ai-disability-inclusion-toolkit/risks-of-bias-and-discrimination-in-ai-hiring-tools/)","Sara R. Jordan. “Designing Artificial Intelligence Review Boards: Creating Risk Metrics for Review of AI.” 2019 IEEE International Symposium on Technology and Society (ISTAS), 2019. [URL](https://doi.org/10.1109/istas48451.2019.8937942)

IEEE. “IEEE-1012-2016: IEEE Standard for System, Software, and Hardware Verification and Validation.” IEEE Standards Association. [URL](https://standards.ieee.org/ieee/1012/5609/)

ACM Technology Policy Council. “Statement on Principles for Responsible Algorithmic Systems.” Association for Computing Machinery (ACM), October 26, 2022. [URL](https://www.acm.org/binaries/content/assets/public-policy/final-joint-ai-statement-update.pdf)

Perez, E., et al. (2022). Discovering Language Model Behaviors with Model-Written Evaluations. arXiv. https://arxiv.org/abs/2212.09251

Ganguli, D., et al. (2022). Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned. arXiv. https://arxiv.org/abs/2209.07858

David Piorkowski, Michael Hind, and John Richards. ""Quantitative AI Risk Assessments: Opportunities and Challenges."" arXiv preprint, submitted January 11, 2023. [URL](https://arxiv.org/abs/2209.06317)

Daniel Schiff, Aladdin Ayesh, Laura Musikanski, and John C. Havens. “IEEE 7010: A New Standard for Assessing the Well-Being Implications of Artificial Intelligence.” 2020 IEEE International Conference on Systems, Man, and Cybernetics (SMC), 2020. [URL](https://doi.org/10.1109/smc42975.2020.9283454)","ACM Technology Policy Council. “Statement on Principles for Responsible Algorithmic Systems.” Association for Computing Machinery (ACM), October 26, 2022. [URL](https://www.acm.org/binaries/content/assets/public-policy/final-joint-ai-statement-update.pdf)

Trevor Hastie, Robert Tibshirani, and Jerome Friedman. The Elements of Statistical Learning: Data Mining, Inference, and Prediction. 2nd ed. Springer-Verlag, 2009. [URL](https://hastie.su.domains/ElemStatLearn/)

Harini Suresh and John Guttag. “A Framework for Understanding Sources of Harm Throughout the Machine Learning Life Cycle.” Equity and Access in Algorithms, Mechanisms, and Optimization, October 2021. [URL](https://doi.org/10.1145/3465416.3483305)

Christopher M. Bishop. Pattern Recognition and Machine Learning. New York: Springer, 2006. [URL](https://cis.temple.edu/~latecki/Courses/RobotFall08/BishopBook/Pages_from_PatternRecognitionAndMachineLearning1.pdf)

Solon Barocas, Anhong Guo, Ece Kamar, Jacquelyn Krones, Meredith Ringel Morris, Jennifer Wortman Vaughan, W. Duncan Wadsworth, and Hanna Wallach. “Designing Disaggregated Evaluations of AI Systems: Choices, Considerations, and Tradeoffs.” Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society, July 2021, 368–78. [URL](https://doi.org/10.1145/3461702.3462610)","Board of Governors of the Federal Reserve System. “SR 11-7: Guidance on Model Risk Management.” April 4, 2011. [URL](https://www.federalreserve.gov/supervisionreg/srletters/sr1107.htm) 

“Definition of independent verification and validation (IV&V)”, in IEEE 1012, IEEE Standard for System, Software, and Hardware Verification and Validation. Annex C, [URL](https://people.eecs.ku.edu/~hossein/Teaching/Stds/1012.pdf)

Mona Sloane, Emanuel Moss, Olaitan Awomolo, and Laura Forlano. “Participation Is Not a Design Fix for Machine Learning.” Equity and Access in Algorithms, Mechanisms, and Optimization, October 2022. [URL](https://doi.org/10.1145/3551624.3555285)

Rediet Abebe and Kira Goldner. “Mechanism Design for Social Good.” AI Matters 4, no. 3 (October 2018): 27–34. [URL](https://doi.org/10.1145/3284751.3284761)","Emily M. Bender and Batya Friedman. “Data Statements for Natural Language Processing: Toward Mitigating System Bias and Enabling Better Science.” Transactions of the Association for Computational Linguistics 6 (2018): 587–604. [URL](https://doi.org/10.1162/tacl_a_00041)

Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru. “Model Cards for Model Reporting.” FAT *19: Proceedings of the Conference on Fairness, Accountability, and Transparency, January 2019, 220–29. [URL](https://doi.org/10.1145/3287560.3287596)

IEEE Computer Society. “Software Engineering Body of Knowledge Version 3: IEEE Computer Society.” IEEE Computer Society. [URL](https://www.computer.org/education/bodies-of-knowledge/software-engineering/v3)

IEEE. “IEEE-1012-2016: IEEE Standard for System, Software, and Hardware Verification and Validation.” IEEE Standards Association. [URL](https://standards.ieee.org/ieee/1012/5609/)

Board of Governors of the Federal Reserve System. “SR 11-7: Guidance on Model Risk Management.” April 4, 2011. [URL](https://www.federalreserve.gov/supervisionreg/srletters/sr1107.htm)

Abigail Z. Jacobs and Hanna Wallach. “Measurement and Fairness.” FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, March 2021, 375–85. [URL](https://doi.org/10.1145/3442188.3445901)

Jeanna Matthews, Bruce Hedin, Marc Canellas. Trustworthy Evidence for Trustworthy Technology: An Overview of Evidence for Assessing the Trustworthiness of Autonomous and Intelligent Systems. IEEE-USA, September 29 2022. [URL](https://ieeeusa.org/assets/public-policy/committees/aipc/IEEE_Trustworthy-Evidence-for-Trustworthy-Technology_Sept22.pdf)

Roel Dobbe, Thomas Krendl Gilbert, and Yonatan Mintz. “Hard Choices in Artificial Intelligence.” Artificial Intelligence 300 (November 2021). [URL](https://doi.org/10.1016/j.artint.2021.103555)","United States Department of Health, Education, and Welfare's National Commission for the Protection of Human Subjects of Biomedical and Behavioral Research. The Belmont Report: Ethical Principles and Guidelines for the Protection of Human Subjects of Research. Volume II. United States Department of Health and Human Services Office for Human Research Protections. April 18, 1979. [URL](https://www.hhs.gov/ohrp/regulations-and-policy/belmont-report/read-the-belmont-report/index.html)

Office for Human Research Protections (OHRP). “45 CFR 46.” United States Department of Health and Human Services Office for Human Research Protections, March 10, 2021. [URL](https://www.hhs.gov/ohrp/regulations-and-policy/regulations/45-cfr-46/index.html) Note: Federal Policy for Protection of Human Subjects (Common Rule). 45 CFR 46 (2018)

Office for Human Research Protections (OHRP). “Human Subject Regulations Decision Chart.” United States Department of Health and Human Services Office for Human Research Protections, June 30, 2020. [URL](https://www.hhs.gov/ohrp/regulations-and-policy/decision-charts/index.html)

Jacob Metcalf and Kate Crawford. “Where Are Human Subjects in Big Data Research? The Emerging Ethics Divide.” Big Data and Society 3, no. 1 (2016). [URL](https://doi.org/10.1177/2053951716650211)

Boaz Shmueli, Jan Fell, Soumya Ray, and Lun-Wei Ku. ""Beyond Fair Pay: Ethical Implications of NLP Crowdsourcing."" arXiv preprint, submitted April 20, 2021. [URL](https://arxiv.org/abs/2104.10097)

Divyansh Kaushik, Zachary C. Lipton, and Alex John London. ""Resolving the Human Subjects Status of Machine Learning's Crowdworkers."" arXiv preprint, submitted June 8, 2022. [URL](https://arxiv.org/abs/2206.04039)

Office for Human Research Protections (OHRP). “International Compilation of Human Research Standards.” United States Department of Health and Human Services Office for Human Research Protections, February 7, 2022. [URL](https://www.hhs.gov/ohrp/international/compilation-human-research-standards/index.html)

National Institutes of Health. “Definition of Human Subjects Research.” NIH Central Resource for Grants and Funding Information, January 13, 2020. [URL](https://grants.nih.gov/policy/humansubjects/research.htm)

Joy Buolamwini and Timnit Gebru. “Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification.” Proceedings of the 1st Conference on Fairness, Accountability and Transparency in PMLR 81 (2018): 77–91. [URL](https://proceedings.mlr.press/v81/buolamwini18a.html)

Eun Seo Jo and Timnit Gebru. “Lessons from Archives: Strategies for Collecting Sociocultural Data in Machine Learning.” FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency, January 2020, 306–16. [URL](https://doi.org/10.1145/3351095.3372829)

Marco Gerardi, Katarzyna Barud, Marie-Catherine Wagner, Nikolaus Forgo, Francesca Fallucchi, Noemi Scarpato, Fiorella Guadagni, and Fabio Massimo Zanzotto. ""Active Informed Consent to Boost the Application of Machine Learning in Medicine."" arXiv preprint, submitted September 27, 2022. [URL](https://arxiv.org/abs/2210.08987)

Shari Trewin. ""AI Fairness for People with Disabilities: Point of View."" arXiv preprint, submitted November 26, 2018. [URL](https://arxiv.org/abs/1811.10670)

Andrea Brennen, Ryan Ashley, Ricardo Calix, JJ Ben-Joseph, George Sieniawski, Mona Gogia, and BNH.AI. AI Assurance Audit of RoBERTa, an Open source, Pretrained Large Language Model. IQT Labs, December 2022. [URL](https://assets.iqt.org/pdfs/IQTLabs_RoBERTaAudit_Dec2022_final.pdf/web/viewer.html)","Trevor Hastie, Robert Tibshirani, and Jerome Friedman. The Elements of Statistical Learning: Data Mining, Inference, and Prediction. 2nd ed. Springer-Verlag, 2009. [URL](https://hastie.su.domains/ElemStatLearn/)

Jessica Zosa Forde, A. Feder Cooper, Kweku Kwegyir-Aggrey, Chris De Sa, and Michael Littman. ""Model Selection's Disparate Impact in Real-World Deep Learning Applications."" arXiv preprint, submitted September 7, 2021. [URL](https://arxiv.org/abs/2104.00606)

Inioluwa Deborah Raji, I. Elizabeth Kumar, Aaron Horowitz, and Andrew Selbst. “The Fallacy of AI Functionality.” FAccT '22: 2022 ACM Conference on Fairness, Accountability, and Transparency, June 2022, 959–72. [URL](https://doi.org/10.1145/3531146.3533158)

Amandalynne Paullada, Inioluwa Deborah Raji, Emily M. Bender, Emily Denton, and Alex Hanna. “Data and Its (Dis)Contents: A Survey of Dataset Development and Use in Machine Learning Research.” Patterns 2, no. 11 (2021): 100336. [URL](https://doi.org/10.1016/j.patter.2021.100336)

Christopher M. Bishop. Pattern Recognition and Machine Learning. New York: Springer, 2006. [URL](https://cis.temple.edu/~latecki/Courses/RobotFall08/BishopBook/Pages_from_PatternRecognitionAndMachineLearning1.pdf)

Md Johirul Islam, Giang Nguyen, Rangeet Pan, and Hridesh Rajan. ""A Comprehensive Study on Deep Learning Bug Characteristics."" arXiv preprint, submitted June 3, 2019. [URL](https://arxiv.org/abs/1906.01388)

Swaroop Mishra, Anjana Arunkumar, Bhavdeep Sachdeva, Chris Bryan, and Chitta Baral. ""DQI: Measuring Data Quality in NLP."" arXiv preprint, submitted May 2, 2020. [URL](https://arxiv.org/abs/2005.00816)

Doug Wielenga. ""Paper 073-2007: Identifying and Overcoming Common Data Mining Mistakes."" SAS Global Forum 2007: Data Mining and Predictive Modeling, SAS Institute, 2007. [URL](https://support.sas.com/resources/papers/proceedings/proceedings/forum2007/073-2007.pdf)

### Software Resources

- [Drifter](https://github.com/ModelOriented/drifter) library (performance assessment)
- [Manifold](https://github.com/uber/manifold) library (performance assessment)
- [MLextend](http://rasbt.github.io/mlxtend/) library (performance assessment)
- [PiML](https://github.com/SelfExplainML/PiML-Toolbox) library (explainable models, performance assessment)
- [SALib](https://github.com/SALib/SALib) library (performance assessment)
- [What-If Tool](https://pair-code.github.io/what-if-tool/index.html#about) (performance assessment)","Luca Piano, Fabio Garcea, Valentina Gatteschi, Fabrizio Lamberti, and Lia Morra. “Detecting Drift in Deep Learning: A Methodology Primer.” IT Professional 24, no. 5 (2022): 53–60. [URL](https://doi.org/10.1109/mitp.2022.3191318)

Larysa Visengeriyeva, et al. “Awesome MLOps.“ GitHub. [URL](https://github.com/visenger/awesome-mlops)","Abigail Z. Jacobs and Hanna Wallach. “Measurement and Fairness.” FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, March 2021, 375–85. [URL](https://doi.org/10.1145/3442188.3445901)

Debugging Machine Learning Models. Proceedings of ICLR 2019 Workshop, May 6, 2019, New Orleans, Louisiana. [URL](https://debug-ml-iclr2019.github.io/)

Patrick Hall. “Strategies for Model Debugging.” Towards Data Science, November 8, 2019. [URL](https://towardsdatascience.com/strategies-for-model-debugging-aa822f1097ce)

Suchi Saria and Adarsh Subbaswamy. ""Tutorial: Safe and Reliable Machine Learning."" arXiv preprint, submitted April 15, 2019. [URL](https://arxiv.org/abs/1904.07204)

Google Developers. “Overview of Debugging ML Models.” Google Developers Machine Learning Foundational Courses, n.d. [URL](https://developers.google.com/machine-learning/testing-debugging/common/overview)

R. Mohanani, I. Salman, B. Turhan, P. Rodríguez and P. Ralph, ""Cognitive Biases in Software Engineering: A Systematic Mapping Study,"" in IEEE Transactions on Software Engineering, vol. 46, no. 12, pp. 1318-1339, Dec. 2020,


### Software Resources

- [Drifter](https://github.com/ModelOriented/drifter) library (performance assessment)
- [Manifold](https://github.com/uber/manifold) library (performance assessment)
- [MLextend](http://rasbt.github.io/mlxtend/) library (performance assessment)
- [PiML](https://github.com/SelfExplainML/PiML-Toolbox) library (explainable models, performance assessment)
- [SALib](https://github.com/SALib/SALib) library (performance assessment)
- [What-If Tool](https://pair-code.github.io/what-if-tool/index.html#about) (performance assessment)","AI Incident Database. 2022. [URL](https://incidentdatabase.ai/)

AIAAIC Repository. 2022. [URL](https://www.aiaaic.org/aiaaic-repository)

Netflix. Chaos Monkey. [URL](https://netflix.github.io/chaosmonkey/)

IBM. “IBM's Principles of Chaos Engineering.” IBM, n.d. [URL](https://www.ibm.com/cloud/architecture/architecture/practices/chaos-engineering-principles/)

Suchi Saria and Adarsh Subbaswamy. ""Tutorial: Safe and Reliable Machine Learning."" arXiv preprint, submitted April 15, 2019. [URL](https://arxiv.org/abs/1904.07204)

Daniel Kang, Deepti Raghavan, Peter Bailis, and Matei Zaharia. ""Model assertions for monitoring and improving ML models."" Proceedings of Machine Learning and Systems 2 (2020): 481-496. [URL](https://proceedings.mlsys.org/paper/2020/file/a2557a7b2e94197ff767970b67041697-Paper.pdf)

Larysa Visengeriyeva, et al. “Awesome MLOps.“ GitHub. [URL](https://github.com/visenger/awesome-mlops)

McGregor, S., Paeth, K., & Lam, K.T. (2022). Indexing AI Risks with Incidents, Issues, and Variants. ArXiv, abs/2211.10384.","Matthew P. Barrett. “Framework for Improving Critical Infrastructure Cybersecurity Version 1.1.” National Institute of Standards and Technology (NIST), April 16, 2018. [URL](https://doi.org/10.6028/nist.cswp.04162018)

Nicolas Papernot. ""A Marauder's Map of Security and Privacy in Machine Learning."" arXiv preprint, submitted on November 3, 2018. [URL](https://arxiv.org/abs/1811.01134)

Gary McGraw, Harold Figueroa, Victor Shepardson, and Richie Bonett. “BIML Interactive Machine Learning Risk Framework.” Berryville Institute of Machine Learning (BIML), 2022. [URL](https://berryvilleiml.com/interactive/)

Mitre Corporation. “Mitre/Advmlthreatmatrix: Adversarial Threat Landscape for AI Systems.” GitHub, 2023. [URL](https://github.com/mitre/advmlthreatmatrix)

National Institute of Standards and Technology (NIST). “Cybersecurity Framework.” NIST, 2023. [URL](https://www.nist.gov/cyberframework)

### Software Resources

- [adversarial-robustness-toolbox](https://github.com/Trusted-AI/adversarial-robustness-toolbox)
- [counterfit](https://github.com/Azure/counterfit/)
- [foolbox](https://github.com/bethgelab/foolbox)
- [ml_privacy_meter](https://github.com/privacytrustlab/ml_privacy_meter)
- [robustness](https://github.com/MadryLab/robustness) 
- [tensorflow/privacy](https://github.com/tensorflow/privacy)","National Academies of Sciences, Engineering, and Medicine. Human-AI Teaming: State-of-the-Art and Research Needs. 2022. [URL](https://nap.nationalacademies.org/catalog/26355/human-ai-teaming-state-of-the-art-and-research-needs)

Inioluwa Deborah Raji and Jingying Yang. ""ABOUT ML: Annotation and Benchmarking on Understanding and Transparency of Machine Learning Lifecycles."" arXiv preprint, submitted January 8, 2020. [URL](https://arxiv.org/abs/1912.06166)

Andrew Smith. ""Using Artificial Intelligence and Algorithms."" Federal Trade Commission Business Blog, April 8, 2020. [URL](https://www.ftc.gov/business-guidance/blog/2020/04/using-artificial-intelligence-and-algorithms)

Board of Governors of the Federal Reserve System. “SR 11-7: Guidance on Model Risk Management.” April 4, 2011. [URL](https://www.federalreserve.gov/supervisionreg/srletters/sr1107.htm)

Joshua A. Kroll. “Outlining Traceability: A Principle for Operationalizing Accountability in Computing Systems.” FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, March 1, 2021, 758–71. [URL](https://doi.org/10.1145/3442188.3445937)

Jennifer Cobbe, Michelle Seng Lee, and Jatinder Singh. “Reviewable Automated Decision-Making: A Framework for Accountable Algorithmic Systems.” FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, March 1, 2021, 598–609. [URL](https://doi.org/10.1145/3442188.3445921)","Chaofan Chen, Oscar Li, Chaofan Tao, Alina Jade Barnett, Jonathan Su, and Cynthia Rudin. ""This Looks Like That: Deep Learning for Interpretable Image Recognition."" arXiv preprint, submitted December 28, 2019. [URL](https://arxiv.org/abs/1806.10574)

Cynthia Rudin. ""Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use Interpretable Models Instead."" arXiv preprint, submitted September 22, 2019. [URL](https://arxiv.org/abs/1811.10154)

David A. Broniatowski. ""NISTIR 8367 Psychological Foundations of Explainability and Interpretability in Artificial Intelligence. National Institute of Standards and Technology (NIST), 2021. [URL](https://doi.org/10.6028/NIST.IR.8367)

Alejandro Barredo Arrieta, Natalia Díaz-Rodríguez, Javier Del Ser, Adrien Bennetot, Siham Tabik, Alberto Barbado, Salvador Garcia, et al. “Explainable Artificial Intelligence (XAI): Concepts, Taxonomies, Opportunities, and Challenges Toward Responsible AI.” Information Fusion 58 (June 2020): 82–115. [URL](https://doi.org/10.1016/j.inffus.2019.12.012)

Zana Buçinca, Phoebe Lin, Krzysztof Z. Gajos, and Elena L. Glassman. “Proxy Tasks and Subjective Measures Can Be Misleading in Evaluating Explainable AI Systems.” IUI '20: Proceedings of the 25th International Conference on Intelligent User Interfaces, March 17, 2020, 454–64. [URL](https://doi.org/10.1145/3377325.3377498)

P. Jonathon Phillips, Carina A. Hahn, Peter C. Fontana, Amy N. Yates, Kristen Greene, David A. Broniatowski, and Mark A. Przybocki. ""NISTIR 8312 Four Principles of Explainable Artificial Intelligence."" National Institute of Standards and Technology (NIST), September 2021. [URL](https://nvlpubs.nist.gov/nistpubs/ir/2021/NIST.IR.8312.pdf)

Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru. “Model Cards for Model Reporting.” FAT *19: Proceedings of the Conference on Fairness, Accountability, and Transparency, January 2019, 220–29. [URL](https://doi.org/10.1145/3287560.3287596)

Ke Yang, Julia Stoyanovich, Abolfazl Asudeh, Bill Howe, HV Jagadish, and Gerome Miklau. “A Nutritional Label for Rankings.” SIGMOD '18: Proceedings of the 2018 International Conference on Management of Data, May 27, 2018, 1773–76. [URL](https://doi.org/10.1145/3183713.3193568)

Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. ""'Why Should I Trust You?': Explaining the Predictions of Any Classifier."" arXiv preprint, submitted August 9, 2016. [URL](https://arxiv.org/abs/1602.04938)

Scott M. Lundberg and Su-In Lee. ""A unified approach to interpreting model predictions."" NIPS'17: Proceedings of the 31st International Conference on Neural Information Processing Systems, December 4, 2017, 4768-4777. [URL](https://proceedings.neurips.cc/paper/2017/file/8a20a8621978632d76c43dfd28b67767-Paper.pdf)

Dylan Slack, Sophie Hilgard, Emily Jia, Sameer Singh, and Himabindu Lakkaraju. “Fooling LIME and SHAP: Adversarial Attacks on Post Hoc Explanation Methods.” AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society, February 7, 2020, 180–86. [URL](https://doi.org/10.1145/3375627.3375830)

David Alvarez-Melis and Tommi S. Jaakkola. ""Towards robust interpretability with self-explaining neural networks."" NIPS'18: Proceedings of the 32nd International Conference on Neural Information Processing Systems, December 3, 2018, 7786-7795. [URL](https://dl.acm.org/doi/10.5555/3327757.3327875)

FinRegLab, Laura Biattner, and Jann Spiess. ""Machine Learning Explainability & Fairness: Insights from Consumer Lending."" FinRegLab, April 2022. [URL](https://finreglab.org/ai-machine-learning/explainability-and-fairness-of-machine-learning-in-credit-underwriting/machine-learning-explainability-fairness-insights-from-consumer-lending/)

Miguel Ferreira, Muhammad Bilal Zafar, and Krishna P. Gummadi. ""The Case for Temporal Transparency: Detecting Policy Change Events in Black-Box Decision Making Systems."" arXiv preprint, submitted October 31, 2016. [URL](https://arxiv.org/abs/1610.10064)

Himabindu Lakkaraju, Ece Kamar, Rich Caruana, and Jure Leskovec. ""Interpretable & Explorable Approximations of Black Box Models."" arXiv preprint, July 4, 2017. [URL](https://arxiv.org/abs/1707.01154)

### Software Resources

- [SHAP](https://github.com/slundberg/shap)
- [LIME](https://github.com/marcotcr/lime)
- [Interpret](https://github.com/interpretml/interpret)
- [PiML](https://github.com/SelfExplainML/PiML-Toolbox)
- [Iml](https://cran.r-project.org/web/packages/iml/index.html)
- [Dalex](https://cran.r-project.org/web/packages/DALEX/index.html)","Kaitlin R. Boeckl and Naomi B. Lefkovitz. ""NIST Privacy Framework: A Tool for Improving Privacy Through Enterprise Risk Management, Version 1.0."" National Institute of Standards and Technology (NIST), January 16, 2020. [URL](https://www.nist.gov/publications/nist-privacy-framework-tool-improving-privacy-through-enterprise-risk-management)

Latanya Sweeney. “K-Anonymity: A Model for Protecting Privacy.” International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems 10, no. 5 (2002): 557–70. [URL](https://doi.org/10.1142/s0218488502001648)

Ashwin Machanavajjhala, Johannes Gehrke, Daniel Kifer, and Muthuramakrishnan Venkitasubramaniam. “L-Diversity: Privacy beyond K-Anonymity.” 22nd International Conference on Data Engineering (ICDE'06), 2006. [URL](https://doi.org/10.1109/icde.2006.1)

Ninghui Li, Tiancheng Li, and Suresh Venkatasubramanian. ""CERIAS Tech Report 2007-78 t-Closeness: Privacy Beyond k-Anonymity and -Diversity."" Center for Education and Research, Information Assurance and Security, Purdue University, 2001. [URL](https://www.cerias.purdue.edu/apps/reports_and_papers/view/3356)

J. Domingo-Ferrer and J. Soria-Comas. ""From t-closeness to differential privacy and vice versa in data anonymization."" arXiv preprint, submitted December 21, 2015. [URL](https://arxiv.org/abs/1512.05110)

Joseph Near, David Darais, and Kaitlin Boeckly. ""Differential Privacy for Privacy-Preserving Data Analysis: An Introduction to our Blog Series."" National Institute of Standards and Technology (NIST), July 27, 2020. [URL](https://www.nist.gov/blogs/cybersecurity-insights/differential-privacy-privacy-preserving-data-analysis-introduction-our)

Cynthia Dwork. “Differential Privacy.” Automata, Languages and Programming, 2006, 1–12. [URL](https://doi.org/10.1007/11787006_1)

Zhanglong Ji, Zachary C. Lipton, and Charles Elkan. ""Differential Privacy and Machine Learning: a Survey and Review."" arXiv preprint, submitted December 24,2014. [URL](https://arxiv.org/abs/1412.7584)

Michael B. Hawes. ""Implementing Differential Privacy: Seven Lessons From the 2020 United States Census."" Harvard Data Science Review 2, no. 2 (2020). [URL](https://doi.org/10.1162/99608f92.353c6f99)

Harvard University Privacy Tools Project. “Differential Privacy.” Harvard University, n.d. [URL](https://privacytools.seas.harvard.edu/differential-privacy)

John M. Abowd, Robert Ashmead, Ryan Cumings-Menon, Simson Garfinkel, Micah Heineck, Christine Heiss, Robert Johns, Daniel Kifer, Philip Leclerc, Ashwin Machanavajjhala, Brett Moran, William Matthew Spence Sexton and Pavel Zhuravlev. ""The 2020 Census Disclosure Avoidance System TopDown Algorithm."" United States Census Bureau, April 7, 2022. [URL](https://www2.census.gov/adrm/CED/Papers/CY22/2022-002-AbowdAshmeadCumingMenonGarfinkelEtal.pdf)

Nicolas Papernot and Abhradeep Guha Thakurta. ""How to deploy machine learning with differential privacy."" National Institute of Standards and Technology (NIST), December 21, 2021. [URL](https://www.nist.gov/blogs/cybersecurity-insights/how-deploy-machine-learning-differential-privacy)

Claire McKay Bowen. ""Utility Metrics for Differential Privacy: No One-Size-Fits-All."" National Institute of Standards and Technology (NIST), November 29, 2021. [URL](https://www.nist.gov/blogs/cybersecurity-insights/utility-metrics-differential-privacy-no-one-size-fits-all)

Helen Nissenbaum. ""Contextual Integrity Up and Down the Data Food Chain."" Theoretical Inquiries in Law 20, L. 221 (2019): 221-256. [URL](https://nissenbaum.tech.cornell.edu/papers/Contextual%20Integrity%20Up%20and%20Down.pdf)

Sebastian Benthall, Seda Gürses, and Helen Nissenbaum. “Contextual Integrity through the Lens of Computer Science.” Foundations and Trends in Privacy and Security 2, no. 1 (December 22, 2017): 1–69. [URL](https://doi.org/10.1561/3300000016)

Jenifer Sunrise Winter and Elizabeth Davidson. “Big Data Governance of Personal Health Information and Challenges to Contextual Integrity.” The Information Society: An International Journal 35, no. 1 (2019): 36–51. [URL](https://doi.org/10.1080/01972243.2018.1542648.","Ali Hasan, Shea Brown, Jovana Davidovic, Benjamin Lange, and Mitt Regan. “Algorithmic Bias and Risk Assessments: Lessons from Practice.” Digital Society 1 (2022). [URL](https://doi.org/10.1007/s44206-022-00017-z)

Richard N. Landers and Tara S. Behrend. “Auditing the AI Auditors: A Framework for Evaluating Fairness and Bias in High Stakes AI Predictive Models.” American Psychologist 78, no. 1 (2023): 36–49. [URL](https://doi.org/10.1037/amp0000972) 

Ninareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, and Aram Galstyan. “A Survey on Bias and Fairness in Machine Learning.” ACM Computing Surveys 54, no. 6 (July 2021): 1–35. [URL](https://doi.org/10.1145/3457607)

Michele Loi and Christoph Heitz. “Is Calibration a Fairness Requirement?” FAccT '22: 2022 ACM Conference on Fairness, Accountability, and Transparency, June 2022, 2026–34. [URL](https://doi.org/10.1145/3531146.3533245)

Shea Brown, Ryan Carrier, Merve Hickok, and Adam Leon Smith. “Bias Mitigation in Data Sets.” SocArXiv, July 8, 2021. [URL](https://doi.org/10.31235/osf.io/z8qrb)

Reva Schwartz, Apostol Vassilev, Kristen Greene, Lori Perine, Andrew Burt, and Patrick Hall. ""NIST Special Publication 1270 Towards a Standard for Identifying and Managing Bias in Artificial Intelligence."" National Institute of Standards and Technology (NIST), 2022. [URL](https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.1270.pdf)

Microsoft Research. “AI Fairness Checklist.” Microsoft, February 7, 2022. [URL](https://www.microsoft.com/en-us/research/project/ai-fairness-checklist/)

Samir Passi and Solon Barocas. “Problem Formulation and Fairness.” FAT* '19: Proceedings of the Conference on Fairness, Accountability, and Transparency, January 2019, 39–48. [URL](https://doi.org/10.1145/3287560.3287567)

Jade S. Franklin, Karan Bhanot, Mohamed Ghalwash, Kristin P. Bennett, Jamie McCusker, and Deborah L. McGuinness. “An Ontology for Fairness Metrics.” AIES '22: Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society, July 2022, 265–75. [URL](https://doi.org/10.1145/3514094.3534137)

Zhang, B., Lemoine, B., & Mitchell, M. (2018). Mitigating Unwanted Biases with Adversarial Learning. Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society. https://arxiv.org/pdf/1801.07593.pdf 

Ganguli, D., et al. (2023). The Capacity for Moral Self-Correction in Large Language Models. arXiv. https://arxiv.org/abs/2302.07459

Arvind Narayanan. “Tl;DS - 21 Fairness Definition and Their Politics by Arvind Narayanan.” Dora's world, July 19, 2019. [URL](https://shubhamjain0594.github.io/post/tlds-arvind-fairness-definitions/)

Ben Green. “Escaping the Impossibility of Fairness: From Formal to Substantive Algorithmic Fairness.” Philosophy and Technology 35, no. 90 (October 8, 2022). [URL](https://doi.org/10.1007/s13347-022-00584-6)

Alexandra Chouldechova. “Fair Prediction with Disparate Impact: A Study of Bias in Recidivism Prediction Instruments.” Big Data 5, no. 2 (June 1, 2017): 153–63. [URL](https://doi.org/10.1089/big.2016.0047)

Sina Fazelpour and Zachary C. Lipton. “Algorithmic Fairness from a Non-Ideal Perspective.” AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society, February 7, 2020, 57–63. [URL](https://doi.org/10.1145/3375627.3375828)

Hemank Lamba, Kit T. Rodolfa, and Rayid Ghani. “An Empirical Comparison of Bias Reduction Methods on Real-World Problems in High-Stakes Policy Settings.” ACM SIGKDD Explorations Newsletter 23, no. 1 (May 29, 2021): 69–85. [URL](https://doi.org/10.1145/3468507.3468518)

ISO. “ISO/IEC TR 24027:2021 Information technology — Artificial intelligence (AI) — Bias in AI systems and AI aided decision making.” ISO Standards, November 2021. [URL](https://www.iso.org/standard/77607.html)

Shari Trewin. ""AI Fairness for People with Disabilities: Point of View."" arXiv preprint, submitted November 26, 2018. [URL](https://arxiv.org/abs/1811.10670)

MathWorks. “Explore Fairness Metrics for Credit Scoring Model.” MATLAB & Simulink, 2023. [URL](https://www.mathworks.com/help/risk/explore-fairness-metrics-for-credit-scoring-model.html)

Abigail Z. Jacobs and Hanna Wallach. “Measurement and Fairness.” FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, March 2021, 375–85. [URL](https://doi.org/10.1145/3442188.3445901)

Tolga Bolukbasi, Kai-Wei Chang, James Zou, Venkatesh Saligrama, and Adam Kalai. ""Quantifying and Reducing Stereotypes in Word Embeddings."" arXiv preprint, submitted June 20, 2016. [URL](https://arxiv.org/abs/1606.06121)

Aylin Caliskan, Joanna J. Bryson, and Arvind Narayanan. “Semantics Derived Automatically from Language Corpora Contain Human-Like Biases.” Science 356, no. 6334 (April 14, 2017): 183–86. [URL](https://doi.org/10.1126/sc
ience.aal4230)

Sina Fazelpour and Maria De-Arteaga. “Diversity in Sociotechnical Machine Learning Systems.” Big Data and Society 9, no. 1 (2022). [URL](https://doi.org/10.1177/20539517221082027)

Fairlearn. “Fairness in Machine Learning.” Fairlearn 0.8.0 Documentation, n.d. [URL](https://fairlearn.org/v0.8/user_guide/fairness_in_machine_learning.html#)

Safiya Umoja Noble. Algorithms of Oppression: How Search Engines Reinforce Racism. New York, NY: New York University Press, 2018. [URL](http://algorithmsofoppression.com/)

Ziad Obermeyer, Brian Powers, Christine Vogeli, and Sendhil Mullainathan. “Dissecting Racial Bias in an Algorithm Used to Manage the Health of Populations.” Science 366, no. 6464 (October 25, 2019): 447–53. [URL](https://doi.org/10.1126/science.aax2342)

Alekh Agarwal, Alina Beygelzimer, Miroslav Dudík, John Langford, and Hanna Wallach. ""A Reductions Approach to Fair Classification."" arXiv preprint, submitted July 16, 2018. [URL](https://arxiv.org/abs/1803.02453)

Moritz Hardt, Eric Price, and Nathan Srebro. ""Equality of Opportunity in Supervised Learning."" arXiv preprint, submitted October 7, 2016. [URL](https://arxiv.org/abs/1610.02413)

Alekh Agarwal, Miroslav Dudik, Zhiwei Steven Wu. ""Fair Regression: Quantitative Definitions and Reduction-Based Algorithms."" Proceedings of the 36th International Conference on Machine Learning, PMLR 97:120-129, 2019. [URL](http://proceedings.mlr.press/v97/agarwal19d.html)

Andrew D. Selbst, Danah Boyd, Sorelle A. Friedler, Suresh Venkatasubramanian, and Janet Vertesi. “Fairness and Abstraction in Sociotechnical Systems.” FAT* '19: Proceedings of the Conference on Fairness, Accountability, and Transparency, January 29, 2019, 59–68. [URL](https://doi.org/10.1145/3287560.3287598)

Matthew Kay, Cynthia Matuszek, and Sean A. Munson. “Unequal Representation and Gender Stereotypes in Image Search Results for Occupations.” CHI '15: Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems, April 18, 2015, 3819–28. [URL](https://doi.org/10.1145/2702123.2702520)

### Software Resources

- [aequitas](https://github.com/dssg/aequitas)
- AI Fairness 360:
    - [Python](https://github.com/Trusted-AI/AIF360) 
    - [R](https://github.com/Trusted-AI/AIF360/tree/master/aif360/aif360-r)
- [algofairness](https://github.com/algofairness)
- [fairlearn](https://github.com/fairlearn/fairlearn)
- [fairml](https://github.com/adebayoj/fairml)
- [fairmodels](https://github.com/ModelOriented/fairmodels)
- [fairness](https://cran.r-project.org/web/packages/fairness/index.html)
- [solas-ai-disparity](https://github.com/SolasAI/solas-ai-disparity)
- [tensorflow/fairness-indicators](https://github.com/tensorflow/fairness-indicators)
- [Themis](https://github.com/LASER-UMASS/Themis)","Organisation for Economic Co-operation and Development (OECD). ""Measuring the environmental impacts of artificial intelligence compute and applications: The AI footprint.” OECD Digital Economy Papers, No. 341, OECD Publishing, Paris. [URL](https://doi.org/10.1787/7babf571-en)

Victor Schmidt, Alexandra Luccioni, Alexandre Lacoste, and Thomas Dandres. “Machine Learning CO2 Impact Calculator.” ML CO2 Impact, n.d. [URL](https://mlco2.github.io/impact/)

Alexandre Lacoste, Alexandra Luccioni, Victor Schmidt, and Thomas Dandres. ""Quantifying the Carbon Emissions of Machine Learning."" arXiv preprint, submitted November 4, 2019. [URL](https://arxiv.org/abs/1910.09700)

Matthew Hutson. “Measuring AI’s Carbon Footprint: New Tools Track and Reduce Emissions from Machine Learning.” IEEE Spectrum, November 22, 2022. [URL](https://spectrum.ieee.org/ai-carbon-footprint)

Association for Computing Machinery (ACM). ""TechBriefs: Computing and Climate Change."" ACM Technology Policy Council, November 2021. [URL](https://dl.acm.org/doi/pdf/10.1145/3483410)

Roy Schwartz, Jesse Dodge, Noah A. Smith, and Oren Etzioni. “Green AI.” Communications of the ACM 63, no. 12 (December 2020): 54–63. [URL](https://doi.org/10.1145/3381831)","Arvind Narayanan. ""The limits of the quantitative approach to discrimination."" 2022 James Baldwin lecture, Princeton University, October 11, 2022. [URL](https://www.cs.princeton.edu/~arvindn/talks/baldwin-discrimination/baldwin-discrimination-transcript.pdf)

Devansh Saxena, Karla Badillo-Urquiola, Pamela J. Wisniewski, and Shion Guha. “A Human-Centered Review of Algorithms Used within the U.S. Child Welfare System.” CHI ‘20: Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems, April 23, 2020, 1–15. [URL](https://doi.org/10.1145/3313831.3376229)

Rachel Thomas and David Uminsky. “Reliance on Metrics Is a Fundamental Challenge for AI.” Patterns 3, no. 5 (May 13, 2022): 100476. [URL](https://doi.org/10.1016/j.patter.2022.100476)

Momin M. Malik. ""A Hierarchy of Limitations in Machine Learning."" arXiv preprint, submitted February 29, 2020. [URL](https://arxiv.org/abs/2002.05193","ISO. ""ISO 9241-210:2019 Ergonomics of human-system interaction — Part 210: Human-centred design for interactive systems."" 2nd ed. ISO Standards, July 2019. [URL](https://www.iso.org/standard/77520.html)

Larysa Visengeriyeva, et al. “Awesome MLOps.“ GitHub. [URL](https://github.com/visenger/awesome-mlops)","ISO. ""ISO 9241-210:2019 Ergonomics of human-system interaction — Part 210: Human-centred design for interactive systems."" 2nd ed. ISO Standards, July 2019. [URL](https://www.iso.org/standard/77520.html)

Mark C. Paulk, Bill Curtis, Mary Beth Chrissis, and Charles V. Weber. “Capability Maturity Model, Version 1.1.” IEEE Software 10, no. 4 (1993): 18–27. [URL](https://doi.org/10.1109/52.219617. Note: Copy available via the University of Arizona at https://uweb.engr.arizona.edu/~ece473/readings/22-Capability%20MAturity%20Model.pdf)

Jeff Patton, Peter Economy, Martin Fowler, Alan Cooper, and Marty Cagan. User Story Mapping: Discover the Whole Story, Build the Right Product. O'Reilly, 2014. [URL](https://www.oreilly.com/library/view/user-story-mapping/9781491904893/)

Rumman Chowdhury and Jutta Williams. ""Introducing Twitter’s first algorithmic bias bounty challenge."" Twitter Engineering Blog, July 30, 2021. [URL](https://blog.twitter.com/engineering/en_us/topics/insights/2021/algorithmic-bias-bounty-challenge)

HackerOne. ""Twitter Algorithmic Bias."" HackerOne, August 8, 2021. [URL](https://hackerone.com/twitter-algorithmic-bias?type=team)

Josh Kenway, Camille François, Sasha Costanza-Chock, Inioluwa Deborah Raji, and Joy Buolamwini. ""Bug Bounties for Algorithmic Harms?"" Algorithmic Justice League, January 2022. [URL](https://www.ajl.org/bugs)

Microsoft. “Community Jury.” Microsoft Learn's Azure Application Architecture Guide, 2023. [URL](https://learn.microsoft.com/en-us/azure/architecture/guide/responsible-innovation/community-jury/)

Margarita Boyarskaya, Alexandra Olteanu, and Kate Crawford. ""Overcoming Failures of Imagination in AI Infused System Development and Deployment."" arXiv preprint, submitted December 10, 2020. [URL](https://arxiv.org/abs/2011.13416)","Sasha Costanza-Chock. Design Justice: Community-Led Practices to Build the Worlds We Need. Cambridge: The MIT Press, 2020. [URL](https://direct.mit.edu/books/book/4605/Design-JusticeCommunity-Led-Practices-to-Build-the)

David G. Robinson. Voices in the Code: A Story About People, Their Values, and the Algorithm They Made. New York: Russell Sage Foundation, 2022. [URL](https://www.russellsage.org/publications/voices-code)

Fernando Delgado, Stephen Yang, Michael Madaio, and Qian Yang. ""Stakeholder Participation in AI: Beyond 'Add Diverse Stakeholders and Stir.'"" arXiv preprint, submitted November 1, 2021. [URL](https://arxiv.org/abs/2111.01122)

George Margetis, Stavroula Ntoa, Margherita Antona, and Constantine Stephanidis. “Human-Centered Design of Artificial Intelligence.” In Handbook of Human Factors and Ergonomics, edited by Gavriel Salvendy and Waldemar Karwowski, 5th ed., 1085–1106. John Wiley & Sons, 2021. [URL](https://onlinelibrary.wiley.com/doi/10.1002/9781119636113.ch42)

Ben Shneiderman. Human-Centered AI. Oxford: Oxford University Press, 2022

Batya Friedman, David G. Hendry, and Alan Borning. “A Survey of Value Sensitive Design Methods.” Foundations and Trends in Human-Computer Interaction 11, no. 2 (November 22, 2017): 63–125. [URL](https://doi.org/10.1561/1100000015)
 
Batya Friedman, Peter H. Kahn, Jr., and Alan Borning. ""Value Sensitive Design: Theory and Methods."" University of Washington Department of Computer Science & Engineering Technical Report 02-12-01, December 2002. [URL](https://faculty.washington.edu/pkahn/articles/vsd-theory-methods-tr.pdf)

Emanuel Moss, Elizabeth Watkins, Ranjit Singh, Madeleine Clare Elish, and Jacob Metcalf. “Assembling Accountability: Algorithmic Impact Assessment for the Public Interest.” SSRN, July 8, 2021. [URL](https://doi.org/10.2139/ssrn.3877437)

Alexandra Reeve Givens, and Meredith Ringel Morris. “Centering Disability Perspectives in Algorithmic Fairness, Accountability, & Transparency.” FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency, January 27, 2020, 684-84. [URL](https://doi.org/10.1145/3351095.3375686)","Batya Friedman, and David G. Hendry. Value Sensitive Design: Shaping Technology with Moral Imagination. Cambridge, MA: The MIT Press, 2019. [URL](https://mitpress.mit.edu/9780262039536/value-sensitive-design/)

Batya Friedman, David G. Hendry, and Alan Borning. “A Survey of Value Sensitive Design Methods.” Foundations and Trends in Human-Computer Interaction 11, no. 2 (November 22, 2017): 63–125. [URL](https://doi.org/10.1561/1100000015)

Steven Umbrello, and Ibo van de Poel. “Mapping Value Sensitive Design onto AI for Social Good Principles.” AI and Ethics 1, no. 3 (February 1, 2021): 283–96. [URL](https://doi.org/10.1007/s43681-021-00038-3)

Karen Boyd. “Designing Up with Value-Sensitive Design: Building a Field Guide for Ethical ML Development.” FAccT '22: 2022 ACM Conference on Fairness, Accountability, and Transparency, June 20, 2022, 2069–82. [URL](https://doi.org/10.1145/3531146.3534626)

Janet Davis and Lisa P. Nathan. “Value Sensitive Design: Applications, Adaptations, and Critiques.” In Handbook of Ethics, Values, and Technological Design, edited by Jeroen van den Hoven, Pieter E. Vermaas, and Ibo van de Poel,  January 1, 2015, 11–40. [URL](https://doi.org/10.1007/978-94-007-6970-0_3)

Ben Shneiderman. Human-Centered AI. Oxford: Oxford University Press, 2022. 

Shneiderman, Ben. “Human-Centered AI.” Issues in Science and Technology 37, no. 2 (2021): 56–61. [URL](https://issues.org/human-centered-ai/)

Shneiderman, Ben. “Tutorial: Human-Centered AI: Reliable, Safe and Trustworthy.” IUI '21 Companion: 26th International Conference on Intelligent User Interfaces - Companion, April 14, 2021, 7–8. [URL](https://doi.org/10.1145/3397482.3453994)

George Margetis, Stavroula Ntoa, Margherita Antona, and Constantine Stephanidis. “Human-Centered Design of Artificial Intelligence.” In Handbook of Human Factors and Ergonomics, edited by Gavriel Salvendy and Waldemar Karwowski, 5th ed., 1085–1106. John Wiley & Sons, 2021. [URL](https://onlinelibrary.wiley.com/doi/10.1002/9781119636113.ch42)

Caitlin Thompson. “Who's Homeless Enough for Housing? In San Francisco, an Algorithm Decides.” Coda, September 21, 2021. [URL](https://www.codastory.com/authoritarian-tech/san-francisco-homeless-algorithm/)

John Zerilli, Alistair Knott, James Maclaurin, and Colin Gavaghan. “Algorithmic Decision-Making and the Control Problem.” Minds and Machines 29, no. 4 (December 11, 2019): 555–78. [URL](https://doi.org/10.1007/s11023-019-09513-7)

Fry, Hannah. Hello World: Being Human in the Age of Algorithms. New York: W.W. Norton & Company, 2018. [URL](https://wwnorton.com/books/Hello-World)

Sasha Costanza-Chock. Design Justice: Community-Led Practices to Build the Worlds We Need. Cambridge: The MIT Press, 2020. [URL](https://direct.mit.edu/books/book/4605/Design-JusticeCommunity-Led-Practices-to-Build-the)

David G. Robinson. Voices in the Code: A Story About People, Their Values, and the Algorithm They Made. New York: Russell Sage Foundation, 2022. [URL](https://www.russellsage.org/publications/voices-code)

Diane Hart, Gabi Diercks-O'Brien, and Adrian Powell. “Exploring Stakeholder Engagement in Impact Evaluation Planning in Educational Development Work.” Evaluation 15, no. 3 (2009): 285–306. [URL](https://doi.org/10.1177/1356389009105882)

Asit Bhattacharyya and Lorne Cummings. “Measuring Corporate Environmental Performance – Stakeholder Engagement Evaluation.” Business Strategy and the Environment 24, no. 5 (2013): 309–25. [URL](https://doi.org/10.1002/bse.1819)

Hendricks, Sharief, Nailah Conrad, Tania S. Douglas, and Tinashe Mutsvangwa. “A Modified Stakeholder Participation Assessment Framework for Design Thinking in Health Innovation.” Healthcare 6, no. 3 (September 2018): 191–96. [URL](https://doi.org/10.1016/j.hjdsi.2018.06.003)

Fernando Delgado, Stephen Yang, Michael Madaio, and Qian Yang. ""Stakeholder Participation in AI: Beyond 'Add Diverse Stakeholders and Stir.'"" arXiv preprint, submitted November 1, 2021. [URL](https://arxiv.org/abs/2111.01122)

Emanuel Moss, Elizabeth Watkins, Ranjit Singh, Madeleine Clare Elish, and Jacob Metcalf. “Assembling Accountability: Algorithmic Impact Assessment for the Public Interest.” SSRN, July 8, 2021. [URL](https://doi.org/10.2139/ssrn.3877437)

Alexandra Reeve Givens, and Meredith Ringel Morris. “Centering Disability Perspectives in Algorithmic Fairness, Accountability, & Transparency.” FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency, January 27, 2020, 684-84. [URL](https://doi.org/10.1145/3351095.3375686)","Batya Friedman, and David G. Hendry. Value Sensitive Design: Shaping Technology with Moral Imagination. Cambridge, MA: The MIT Press, 2019. [URL](https://mitpress.mit.edu/9780262039536/value-sensitive-design/)

Batya Friedman, David G. Hendry, and Alan Borning. “A Survey of Value Sensitive Design Methods.” Foundations and Trends in Human-Computer Interaction 11, no. 2 (November 22, 2017): 63–125. [URL](https://doi.org/10.1561/1100000015)

Steven Umbrello, and Ibo van de Poel. “Mapping Value Sensitive Design onto AI for Social Good Principles.” AI and Ethics 1, no. 3 (February 1, 2021): 283–96. [URL](https://doi.org/10.1007/s43681-021-00038-3)

Karen Boyd. “Designing Up with Value-Sensitive Design: Building a Field Guide for Ethical ML Development.” FAccT '22: 2022 ACM Conference on Fairness, Accountability, and Transparency, June 20, 2022, 2069–82. [URL](https://doi.org/10.1145/3531146.3534626)

Janet Davis and Lisa P. Nathan. “Value Sensitive Design: Applications, Adaptations, and Critiques.” In Handbook of Ethics, Values, and Technological Design, edited by Jeroen van den Hoven, Pieter E. Vermaas, and Ibo van de Poel,  January 1, 2015, 11–40. [URL](https://doi.org/10.1007/978-94-007-6970-0_3)

Ben Shneiderman. Human-Centered AI. Oxford: Oxford University Press, 2022. 

Shneiderman, Ben. “Human-Centered AI.” Issues in Science and Technology 37, no. 2 (2021): 56–61. [URL](https://issues.org/human-centered-ai/)

Shneiderman, Ben. “Tutorial: Human-Centered AI: Reliable, Safe and Trustworthy.” IUI '21 Companion: 26th International Conference on Intelligent User Interfaces - Companion, April 14, 2021, 7–8. [URL](https://doi.org/10.1145/3397482.3453994)

George Margetis, Stavroula Ntoa, Margherita Antona, and Constantine Stephanidis. “Human-Centered Design of Artificial Intelligence.” In Handbook of Human Factors and Ergonomics, edited by Gavriel Salvendy and Waldemar Karwowski, 5th ed., 1085–1106. John Wiley & Sons, 2021. [URL](https://onlinelibrary.wiley.com/doi/10.1002/9781119636113.ch42)

Caitlin Thompson. “Who's Homeless Enough for Housing? In San Francisco, an Algorithm Decides.” Coda, September 21, 2021. [URL](https://www.codastory.com/authoritarian-tech/san-francisco-homeless-algorithm/)

John Zerilli, Alistair Knott, James Maclaurin, and Colin Gavaghan. “Algorithmic Decision-Making and the Control Problem.” Minds and Machines 29, no. 4 (December 11, 2019): 555–78. [URL](https://doi.org/10.1007/s11023-019-09513-7)

Fry, Hannah. Hello World: Being Human in the Age of Algorithms. New York: W.W. Norton & Company, 2018. [URL](https://wwnorton.com/books/Hello-World)

Sasha Costanza-Chock. Design Justice: Community-Led Practices to Build the Worlds We Need. Cambridge: The MIT Press, 2020. [URL](https://direct.mit.edu/books/book/4605/Design-JusticeCommunity-Led-Practices-to-Build-the)

David G. Robinson. Voices in the Code: A Story About People, Their Values, and the Algorithm They Made. New York: Russell Sage Foundation, 2022. [URL](https://www.russellsage.org/publications/voices-code)

Diane Hart, Gabi Diercks-O'Brien, and Adrian Powell. “Exploring Stakeholder Engagement in Impact Evaluation Planning in Educational Development Work.” Evaluation 15, no. 3 (2009): 285–306. [URL](https://doi.org/10.1177/1356389009105882)

Asit Bhattacharyya and Lorne Cummings. “Measuring Corporate Environmental Performance – Stakeholder Engagement Evaluation.” Business Strategy and the Environment 24, no. 5 (2013): 309–25. [URL](https://doi.org/10.1002/bse.1819)

Hendricks, Sharief, Nailah Conrad, Tania S. Douglas, and Tinashe Mutsvangwa. “A Modified Stakeholder Participation Assessment Framework for Design Thinking in Health Innovation.” Healthcare 6, no. 3 (September 2018): 191–96. [URL](https://doi.org/10.1016/j.hjdsi.2018.06.003)

Fernando Delgado, Stephen Yang, Michael Madaio, and Qian Yang. ""Stakeholder Participation in AI: Beyond 'Add Diverse Stakeholders and Stir.'"" arXiv preprint, submitted November 1, 2021. [URL](https://arxiv.org/abs/2111.01122)

Emanuel Moss, Elizabeth Watkins, Ranjit Singh, Madeleine Clare Elish, and Jacob Metcalf. “Assembling Accountability: Algorithmic Impact Assessment for the Public Interest.” SSRN, July 8, 2021. [URL](https://doi.org/10.2139/ssrn.3877437)

Alexandra Reeve Givens, and Meredith Ringel Morris. “Centering Disability Perspectives in Algorithmic Fairness, Accountability, & Transparency.” FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency, January 27, 2020, 684-84. [URL](https://doi.org/10.1145/3351095.3375686)","Batya Friedman, and David G. Hendry. Value Sensitive Design: Shaping Technology with Moral Imagination. Cambridge, MA: The MIT Press, 2019. [URL](https://mitpress.mit.edu/9780262039536/value-sensitive-design/)

Batya Friedman, David G. Hendry, and Alan Borning. “A Survey of Value Sensitive Design Methods.” Foundations and Trends in Human-Computer Interaction 11, no. 2 (November 22, 2017): 63–125. [URL](https://doi.org/10.1561/1100000015)

Steven Umbrello, and Ibo van de Poel. “Mapping Value Sensitive Design onto AI for Social Good Principles.” AI and Ethics 1, no. 3 (February 1, 2021): 283–96. [URL](https://doi.org/10.1007/s43681-021-00038-3)

Karen Boyd. “Designing Up with Value-Sensitive Design: Building a Field Guide for Ethical ML Development.” FAccT '22: 2022 ACM Conference on Fairness, Accountability, and Transparency, June 20, 2022, 2069–82. [URL](https://doi.org/10.1145/3531146.3534626)

Janet Davis and Lisa P. Nathan. “Value Sensitive Design: Applications, Adaptations, and Critiques.” In Handbook of Ethics, Values, and Technological Design, edited by Jeroen van den Hoven, Pieter E. Vermaas, and Ibo van de Poel,  January 1, 2015, 11–40. [URL](https://doi.org/10.1007/978-94-007-6970-0_3)

Ben Shneiderman. Human-Centered AI. Oxford: Oxford University Press, 2022. 

Shneiderman, Ben. “Human-Centered AI.” Issues in Science and Technology 37, no. 2 (2021): 56–61. [URL](https://issues.org/human-centered-ai/)

Shneiderman, Ben. “Tutorial: Human-Centered AI: Reliable, Safe and Trustworthy.” IUI '21 Companion: 26th International Conference on Intelligent User Interfaces - Companion, April 14, 2021, 7–8. [URL](https://doi.org/10.1145/3397482.3453994)

George Margetis, Stavroula Ntoa, Margherita Antona, and Constantine Stephanidis. “Human-Centered Design of Artificial Intelligence.” In Handbook of Human Factors and Ergonomics, edited by Gavriel Salvendy and Waldemar Karwowski, 5th ed., 1085–1106. John Wiley & Sons, 2021. [URL](https://onlinelibrary.wiley.com/doi/10.1002/9781119636113.ch42)

Caitlin Thompson. “Who's Homeless Enough for Housing? In San Francisco, an Algorithm Decides.” Coda, September 21, 2021. [URL](https://www.codastory.com/authoritarian-tech/san-francisco-homeless-algorithm/)

John Zerilli, Alistair Knott, James Maclaurin, and Colin Gavaghan. “Algorithmic Decision-Making and the Control Problem.” Minds and Machines 29, no. 4 (December 11, 2019): 555–78. [URL](https://doi.org/10.1007/s11023-019-09513-7)

Fry, Hannah. Hello World: Being Human in the Age of Algorithms. New York: W.W. Norton & Company, 2018. [URL](https://wwnorton.com/books/Hello-World)

Sasha Costanza-Chock. Design Justice: Community-Led Practices to Build the Worlds We Need. Cambridge: The MIT Press, 2020. [URL](https://direct.mit.edu/books/book/4605/Design-JusticeCommunity-Led-Practices-to-Build-the)

David G. Robinson. Voices in the Code: A Story About People, Their Values, and the Algorithm They Made. New York: Russell Sage Foundation, 2022. [URL](https://www.russellsage.org/publications/voices-code)

Diane Hart, Gabi Diercks-O'Brien, and Adrian Powell. “Exploring Stakeholder Engagement in Impact Evaluation Planning in Educational Development Work.” Evaluation 15, no. 3 (2009): 285–306. [URL](https://doi.org/10.1177/1356389009105882)

Asit Bhattacharyya and Lorne Cummings. “Measuring Corporate Environmental Performance – Stakeholder Engagement Evaluation.” Business Strategy and the Environment 24, no. 5 (2013): 309–25. [URL](https://doi.org/10.1002/bse.1819)

Hendricks, Sharief, Nailah Conrad, Tania S. Douglas, and Tinashe Mutsvangwa. “A Modified Stakeholder Participation Assessment Framework for Design Thinking in Health Innovation.” Healthcare 6, no. 3 (September 2018): 191–96. [URL](https://doi.org/10.1016/j.hjdsi.2018.06.003)

Fernando Delgado, Stephen Yang, Michael Madaio, and Qian Yang. ""Stakeholder Participation in AI: Beyond 'Add Diverse Stakeholders and Stir.'"" arXiv preprint, submitted November 1, 2021. [URL](https://arxiv.org/abs/2111.01122)

Emanuel Moss, Elizabeth Watkins, Ranjit Singh, Madeleine Clare Elish, and Jacob Metcalf. “Assembling Accountability: Algorithmic Impact Assessment for the Public Interest.” SSRN, July 8, 2021. [URL](https://doi.org/10.2139/ssrn.3877437)

Alexandra Reeve Givens, and Meredith Ringel Morris. “Centering Disability Perspectives in Algorithmic Fairness, Accountability, & Transparency.” FAT* '20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency, January 27, 2020, 684-84. [URL](https://doi.org/10.1145/3351095.3375686)"

############ NIST GLOSSARY ###########

Terms,Definition 1,Citation 1,Definition 2,Citation 2,Definition 3,Citation 3,Definition 4,Citation 4,Definition 5,Citation 5,Related terms and synonyms,Legal definition applicable
accountability,"1) relates to an allocated responsibility. The responsibility can be based on regulation or agreement or through assignment as part of delegation; 2) For systems, a property that ensures that actions of an entity can be traced uniquely to the entity; 3) In a governance context, the obligation of an individual or organization to account for its activities, for completion of a deliverable or task, accept the responsibility for those activities, deliverables or tasks, and to disclose the results in a transparent manner.",ISO/IEC_TS_5723:2022(en),"""accountable"" (adjective vs. noun): answerable for actions, decisions, and performance",ISO/IEC_TS_5723:2022(en),,,,,,,,
accuracy,Closeness of computations or estimates to the exact or true values that the statistics were intended to measure.,OECD,A qualitative assessment of correctness or freedom from error.,FDA_Glossary,The measure of an instrument's capability to approach a true or absolute value. It is a function of precision and bias.,FDA_Glossary,"The accuracy of a machine learning system is measured as the percentage of correct predictions or classifications made by the model over a specific data set. It is typically estimated using a test or ""hold out"" sample, other than the one(s) used to construct the model. Its complement, the error rate, is the proportion of incorrect predictions on the same data.",Raynor,"measure of closeness of results of observations, computations, or estimates to the true values or the values accepted as being true",ISO/IEC_TS_5723:2022(en),,
actionable recourse,The ability of a person to change the decision of the model through actionable input variables.,"Ustun,_Berk",affected users of the system have the ability to challenge the decision they receive,"Varshney,_Kush",Readily available independent mechanisms by which each individuals’ complaints and disputes are investigated and expeditiously resolved at no cost to the individual.,"Voight,_Paul",,,,,"recourse, counterfactual explanation, appeal and override",
active learning,"A proposed method for modifying machine learning algorithms by allowing them to specify test regions to improve their accuracy. At any point, the algorithm can choose a new point x, observe the output and incorporate the new (x, y) pair into its training base. It has been applied to neural networks, prediction functions, and clustering functions.",Raynor,"Active learning (also called “query learning,” or sometimes “optimal experimental design” in the statistics literature) is a subfield of machine learning and, more generally, artificial intelligence. The key hypothesis is that, if the learning algorithm is allowed to choose the data from which it learns—to be “curious,” if you will—it will perform better with less training. ",settles_active_2009,"the process of learning through activities and/or discussion in class, as opposed to passively listening to an expert. It emphasizes higher-order thinking and often involves group work. ",Freeman_et_al_2014,,,,,,
active learning agent,"[a machine learning algorithm that can] decide what actions to take [with regards to its training data, in contrast to a passive learning agent, which is limited to a fixed policy].",Russell_and_Norvig,,,,,,,,,passive learning agent,
activity,Work that an organization performs using business processes; can be singular or compound.,IEEE_Guide_IPA,Set of cohesive tasks of a process.,CSRC,,,,,,,,
adaptive dynamic programming,An adaptive dynamic programming (or ADP) agent takes advantage of the constraints among the utilities of states by learning the transition model that connects them and solving the corresponding Markov decision process using dynamic programming.,Russell_and_Norvig,A means of learning a model and a reward function from observations that then uses value or policy iteration to obtain the utilities or an optimal policy; makes optimal use of the local constraints on utilities of states imposed through the neighborhood structure of the environment.,Russell_and_Norvig,,,,,,,,
adaptive learning,Updating predictive models online during their operation to react to concept drifts,"Gama,_Joao", ,,,,,,,,,
adversarial action,actions characterised by mala fide (malicious) intent and/or bad faith.,FBPML_Wiki,,,,,,,,,,
adversarial example,"Machine learning input sample formed by applying a small but intentionally worst-case perturbation ... to a clean example, such that the perturbed input causes a learned model to output an incorrect answer.",NISTIR_8269_Draft,Samples generated from real samples with carefully designed imperceptible perturbations,"Zhang,_Yonggang",,,,,,,adversarial perturbation,
adversarial machine learning,"A practice concerned with the design of ML algorithms that can resist security challenges, the study of the capabilities of attackers, and the understanding of attack consequences.","Reznik,_Leon","The field to study vulnerabilities of machine learning approaches in adversarial settings and to develop techniques to make learning robust to
adversarial manipulation. ",Vorobeychik,,,,,,,,
adversary,"The agent who conducts or intends to conduct detrimental activities, perhaps by creating an adversarial example. ",NISTIR_8269_Draft,"Individual, group, organization, or government that conducts or has the intent to conduct detrimental activities.",CSRC,,,,,,,,
adverse action notice,A notification of i) a refusal to grant credit in substantially the amount or on substantially the terms requested in an application unless the creditor makes a counteroffer (to grant credit in a different amount or on other terms) and the applicant uses or expressly accepts the credit offered; ii) A termination of an account or an unfavorable change in the terms of an account that does not affect all or substantially all of a class of the creditor's accounts or iii) A refusal to increase the amount of credit available to an applicant who has made an application for an increase.,ECOA,,,,,,,,,,
adverse impact ratio,"A substantially different rate of selection [such as in hiring] which works to the disadvantage of members of a race, sex, or ethnic group.",Cadient_EEOC,"privileged and unprivileged groups receiving different outcomes irrespective of the decision maker’s intent and irrespective of the decision-making procedure. Quantified as the ratio: disparate impact ratio = ??( ??^ (??) = fav || ?? = unpr )/??( ??^ (??) = fav || ?? = priv ) where ??(??^ (??) = fav) is the favorable label, (?? = priv) is the privileged group, and (?? = unpr) is the unprivileged group.","Varshney,_Kush","Determining what constitutes disparate impact at a statistical level is also far from straightforward. Historically, statisticians and regulators have used a variety of methods to detect its occurrence under existing legal standards. Statisticians have, for example, used a group fairness metric called the “80 percent rule” (it’s also known as the “adverse impact ratio”) as one central indicator of disparate impact. Originating in the employment context in the 1970s, the ratio consists of dividing the proportion of the selected group in the disadvantaged class by the proportion of selected members of the advantaged group. A ratio below 80% is generally considered to be evidence of discrimination. Other metrics, such as standardized mean difference or marginal effects analysis, have been used to detect unfair outcomes in AI as well.",HBR_Andrew_Burt_how_to_ensure,,,,,"disparate impact ratio, relative risk ratio",
agile,a development approach that delivers software in increments by following the principles of the Manifesto for Agile Software Development.,Gartner ,"A philosophy and methodology used to describe the continuous, iterative process to develop and deliver software and other digital technologies. User requirements and feedback inform incremental development and delivery by developers.",NSCAI,,,,,,,,
AI principles,"[An overarching concept, value, belief, or norm that guides AI development, testing, and deployment across the AI lifecycle. The OECD] identifies five complementary values-based principles for the responsible stewardship of trustworthy AI and calls on AI actors to promote and implement them: inclusive growth, sustainable development and well-being; human-centred values and fairness; transparency and explainability; robustness, security and safety; and accountability.",OECD_CAI_recommendation,,,,,,,,,Are these definitions of what an AI principle is or a list of definitions?,
algorithm,"A set of step-by-step instructions. Computer algorithms can be simple (if it's 3 p.m., send a reminder) or complex (identify pedestrians).","Hutson,_Matthew","A set of computational rules to be followed to solve a mathematical problem. More recently, the term has been adopted to refer to a process to be followed, often by a computer.",Comptroller_Office,Formulae given to a computer in order for it to complete a task (i.e. a set of rules for a computer).,"Reznik,_Leon",precise rules for transforming specified inputs into specified outputs in a finite number of steps,knuth_art_1981,"algorithms are step-by-step procedures for solving problems. For concreteness, we can think of them simply as being computed programs, written in some precise computer languages",garey_computers_1979,,
algorithmic aversion,biased assessment of an algorithm which manifests in negative behaviours and attitudes towards the algorithm compared to a human agent.,Ekaterina_et_al_2020,,,,,,,,,,
algorithm-in-the-loop,"[a] framework [that] centers human decision making, providing a more precise lens for studying the social impacts of algorithmic decision making aids; . . . processes that employ algorithmic aids to enhance human decision making. In contrast to the human-in-the-loop paradigm, which privileges algorithms as the central focus and uses people to improve algorithmic performance, the algorithm-in-the-loop perspective privileges people as the central focus and uses algorithms to improve human decision making. . . . [It] emphasizes developing systems for integration into sociotechnical contexts rather than for isolated decision making. In terms of evaluation, it emphasizes the human's decisions—rather than the algorithm’s decisions—as the primary outcome of interest.",Ben_Green_Yiling_Chen,,,,,,,,,,
alignment,"ensur[ing] that powerful AI is properly aligned with human values. ... The challenge of alignment has two parts. The first part is technical and focuses on how to formally encode values or principles in artificial agents so that they reliably do what they ought to do. ... The second part of the value alignment question is normative. It asks what values or principles, if any, we ought to encode in artificial agents.",Gabriel_2020,,,,,,,,,,
amplification,"[an act of amplifying, which is] to make larger or greater (as in amount, importance, or intensity).",Merriam-Webster_amplify,"This criterion, disparity amplification, deals with the
disparity in positive classification rates, which is a widely accepted
measure of discriminatory effect in both law and computer
science. It stipulates that a disparity in the output
of the model is justified by a commensurate disparity in the
construct, thereby allowing accurate models even when the base
rates are different for different protected groups, as equalized odds,
predictive parity, and calibration do.",yeom_avoiding_2021,"Let [construct space] ?? ' and [prediction space] ??ˆ be categorical.
Then, a model exhibits disparity amplification if
??tv (??ˆ |??=0,??ˆ |??=1) > ??tv (?? ' |??=0,?? ' |??=1). dtv is the total variation distance defined as follows. Let ??0 and ??1 be
categorical random variables with finite supports Y0 and Y1. Then,
the total variation distance between ??0 and ??1 is
??tv (??0,??1) =
12S?
???Y0?Y1
  
Pr[??0=??] - Pr[??1=??]
  
.
In the special case where ??0,??1 ? {0, 1}, the total variation distance
can also be expressed as | Pr[??0=1] - Pr[??1=1] |.",yeom_avoiding_2021,,,,,,
analytics,"Analytics is the application of scientific & mathematical methods to the study & analysis of problems involving complex systems. There are three distinct types of analytics:
* Descriptive Analytics gives insight into past events, using historical data. 
* Predictive Analytics provides insight on what will happen in the future.
* Prescriptive Analytics helps with decision making by providing actionable advice.",informs_analytics_2022,,,,,,,,,,
annotation,Further documentation accompanying a requirement.,IEEE_Soft_Vocab,[the act of] mak[ing] or furnish[ing] critical or explanatory notes or comment,Merriam-Webster_annotate,,,,,,,,
anomaly,"Anything observed in the documentation or operation of a system that deviates from expectations based on  previously verified system, software, or hardware products or reference documents.",IEEE_Soft_Vocab,"Condition that deviates from expectations, based on requirements specifications, design documents, user documents, or standards, or from someone's perceptions or experiences.",SP800-160,,,,,,,,
anonymization,"The process in which individually identifiable data is altered in such a way that it no longer can be related back to a given individual. Among many techniques, there are three primary ways that data is anonymized. Suppression is the most basic version of anonymization and it simply removes some identifying values from data to reduce its identifiability. Generalization takes specific identifying values and makes them broader, such as changing a specific age (18) to an age range (18-24). Noise addition takes identifying values from a given data set and switches them with identifying values from another individual in that data set. Note that all of these processes will not guarantee that data is no longer identifiable and have to be performed in such a way that does not harm the usability of the data.",IAPP_Privacy_Glossary,process that removes the association between the identifying dataset and the data subject,CSRC,,,,,,,,
anthropomorphism,"the attribution of distinctively human-like feelings, mental states, and behavioral characteristics to inanimate objects, animals, and in general to natural phenomena and supernatural entities",Anthropomorphism_in_AI_2020,a particular human-like interpretation of existing physical features and behaviors that goes beyond what is directly observable,Anthropomorphism_in_AI_2020,,,,,,,,
application,A software program hosted by an information system.,SP800-37,A hardware/software system implemented to satisfy a particular set of requirements.,CSRC,"software or a program that is specific to
the solution of an application problem",aime_measurement_2022 citing ISO/IEC TR 24030,,,,,,
application programming interface (API),"a software contract between the application and client, expressed as a collection of methods or functions. . . it defines the available functions you can execute; . . . the intermediary interface between the client and the application.",Hands-On_Smart_Contract_Dev,,,,,,,,,,
artificial general intelligence (AGI),Algorithms that perform a wide variety of tasks and switch simultaneously from one activity to another in the manner that humans do.,Brookings_Institution,a machine that’s as intelligent as a human and equally capable of solving the broad range of problems that require learning and reasoning,"Wallace,_Brian","Human-like intelligence, which can be applied widely as opposed to narrow AI, which can only be applied to one particular problem or task. Also called ""strong"" AI as opposed to ""weak"" AI.",AI_Ethics_Mark_Coeckelbergh,,,,,strong AI,
artificial intelligence (AI),"Interdisciplinary field, usually regarded as a branch of computer science, dealing with models and systems for the performance of functions generally associated with human intelligence, such as reasoning and learning.","Reznik,_Leon","the field concerned with developing techniques to allow computers to act in a manner that seems like an intelligent organism, such as a human would. The aims vary from the weak end, where a program seems ""a little smarter"" than one would expect, to the strong end, where the attempt is to develop a fully conscious, intelligent, computer-based entity. The lower end is continually disappearing into the general computing background, as the software and hardware evolves.",Raynor,"the study of ideas to bring into being machines that respond to stimulation consistent with traditional responses from humans, given the human capacity for contemplation, judgment and intention. Each such machine should engage in critical appraisal and selection of differing opinions within itself. Produced by human skill and labor, these machines should conduct themselves in agreement with life, spirit and sensitivity, though in reality, they are imitations.",Shubendhu_and_Vijay,a field of study that is adept at applying intelligence to vast amounts of data and deriving meaningful results,"Wallace,_Brian",The application of computational tools to address tasks traditionally requiring human analysis.,Comptroller_Office,machine learning; data science,
artificial intelligence learning,"The ingestion of a corpus, application of semantic mapping, and relevant ontology of structured and/or unstructured data that yields inference and correlation leading to the creation of useful conclusive or predictive capabilities in a given knowledge domain. Strong AI learning also includes the capability of creating unique hypotheses, attributing data relevance, processing data relationships, and updating its own lines of inquiry to further the usefulness of its purpose. ",IEEE_Guide_IPA,,,,,,,,,,
artificial narrow intelligence (ANI),[an AI system that] is designed to accomplish a specific problem-solving or reasoning task.,OECD_Artificial_Intelligence_in_Society,"Artificial Narrow Intelligence, also known as weak or applied intelligence, represents most of the current artificial intelligent systems which usually focus on a specific task. Narrow AIs are mostly much better than humans at the task they were made for: for example, look at face recognition, chess computers, calculus, and translation. The definition of artificial narrow intelligence is in contrast to that of strong AI or artificial general intelligence, which aims at providing a system with consciousness or the ability to solve any problems. Virtual assistants and AlphaGo are examples of artificial narrow intelligence systems.",AI_in_Medical_Imaging_glossary,,,,,,,weak intelligence; applied intelligence,
artificial neural networks,"A computing system, made up of a number of simple, highly interconnected processing elements, which processes information by its dynamic state response to external inputs.","Reznik,_Leon","A good definition of ANN, is given by Haykin [1] describing ANN as a massively parallel combination of simple processing unit which can acquire knowledge from environment through a learning process and store the knowledge in its connections.",guresen_definition_2011,"Definition 1. A directed graph is called an Artificial Neural Network (ANN) if it has x at least one start node (or Start Element; SE), x at least one end node (or End Element; EE), x at least one Processing Element (PE), x all the nodes used must be Processing Elements (PEs), except start nodes and end nodes, x a state variable ni associated with each node i, x a real valued weight wki associated with each link (ki) from node k to node i, x a real valued bias bi associated with each node i, x at least two of the multiple PEs connected in parallel, x a learning algorithm that helps to model the desired output for given input. x a flow on each link (ki) from node k to node i, that carries exactly the same flow which equals to nk caused by the output of node k , x each start node is connected to at least one end node, and each end node is connected to at least one start node, x no parallel edges (each link (ki) from node k to node i is unique).",,,,,,,
assessment,"Action of applying specific documented criteria to a specific software module, package or product for the purpose of determining acceptance or release of the software module, package or product.",IEEE_Soft_Vocab,the action or an instance of making a judgment about something : the act of assessing something : APPRAISAL,Merriam-Webster_assessment,,,,,,,,
asset,"Item, thing, or entity that has potential or actual value to an organization. Item that has been designed for use in multiple contexts. ",IEEE_Soft_Vocab,,,,,,,,,,
attack,Action targeting a learning system to cause malfunction. ,NISTIR_8269_Draft,"Any kind of malicious activity that attempts to collect, disrupt, deny, degrade, or destroy information system resources or the information itself.",CSRC,,,,,,,,
attribute,Property associated with a a set of real or abstract things that is some characteristic of interest.  ,IEEE_Soft_Vocab,"A quantity describing an instance. An attribute has a domain defined by the attribute type, which denotes the values that can be taken by an attribute.","Kohavi,_Ron",property or characteristic of an object that can be distinguished quantitatively or qualitatively by human or automated means,"aime_measurement_2022, citing ISO/IEC TR 24029-1",,,,,,
audit,"Systematic, independent, documented process for obtaining records, statements of fact, or other relevant information and assessing them objectively, to determine the extent to which specified requirements are fulfilled.",IEEE_Soft_Vocab,"To conduct an independent review and examination of system records and activities in order to test the adequacy and effectiveness of data security and data integrity procedures, to ensure compliance with established policy and operational procedures, and to recommend any necessary changes.",FDA_Glossary,"Independent examination of a software product, software process, or set of software processes to assess compliance with specifications, standards, contractual agreements, or other criteria",NASA_Soft_Standards,"Independent review conducted to compare the various aspects of the laboratory’s performance with a standard for that performance. Also defined as a systematic, independent and documented process for obtaining audit evidence and evaluating it objectively to determine the extent to which audit criteria are fulfilled.",UNODC_Glossary_QA_GLP,,,,
audit log,"A chronological record of system activities, including records of system accesses and operations performed in a given period. ",SP800-37,,,,,,,,,,
authenticity,property that an entity is what it claims to be,ISO/IEC_TS_5723:2022(en),,,,,,,,,,
automation,Independent machine-managed choreography of the operation of one or more digital systems.,IEEE_Guide_IPA,"conversion of processes or equipment to automatic operation, or the results of the conversion",IEEE_Soft_Vocab,"The system functions with no/little human operator involvement; however, the system performance is limited to the specific actions it has been designed to do. Typically these are well-defined tasks that have predetermined responses (i.e., simple rule-based responses).",DOD_TEVV,,,,,,
automation bias,over-relying on the outputs of AI systems,David_Leslie_Morgan_Briggs,"It refers to a well-documented human propensity to automatically defer to automated systems, despite warning signals or contradictory information from other sources. In other words, human actors are found to uncritically abdicate their decision making to automation.",alon-barkat_human_2023,,,,,,,,
autonomic,"A monitor-analyze-plan-execute (MAPE) computer system capable of sensing environments, interpreting policy, accessing knowledge (data --- information --- knowledge), making decisions, and initiating dynamically assembled routines of choreographed activity to both complete a process and update the set of environmental variables that enables the autonomic system to self-manage its own operation and the processes it oversees. An autonomic system is identified by eight characteristics:

a)        Knows the resources to which it has access, what its capabilities and limitations are, and how and why it is connected to other systems.
b)        Is able to configure and reconfigure itself depending on the changing computing environment.
c)        Is able to optimize its performance to ensure the most efficient computing process.
d)        Is able to work around encountered problems either by repairing itself or routing functions away from
the trouble.
e)        Is able to detect, identify, and protect itself against various types of attacks to maintain overall system
security and integrity.
f)        Is able to adapt to its environment as it changes by interacting with neighboring systems and establishing communication protocols.
g)        Relies on open standards and requires access to proprietary environments to achieve full performance.
h)        Is able to anticipate the demand on its resources transparently to users.
 ",IEEE_Guide_IPA,,,,,,,,,,
autonomous vehicle,"[an] automobile, bus, tractor, combine, boat, forklift, etc. . . . capable of sensing its environment and moving safely with little or no human input.",Introduction_to_Information_Systems,,,,,,,,,,
autonomy,"The system has a set of intelligence-based capabilities that allows it to respond to situations that were not pre-programmed or anticipated (i.e., decision-based responses) prior to system deployment. Autonomous systems have a degree of self-government and self-directed behavior (with the human’s proxy for decisions).  ",DOD_TEVV,"1. a state of independence and self-determination in an individual, a group, or a society. According to some theories, an inordinate focus on self-determination and achievement represents a risk factor for the development of major depressive disorder.

2. in self-determination theory more specifically, the experience of acting from choice, rather than feeling pressured to act. This form of autonomy is considered a fundamental psychological need that predicts well-being.",APA_autonomy,,,,,,,,
availability,Ensuring timely and reliable access to and use of information.,SP800-37,The property that data or information is accessible and usable upon demand by an authorized person.,NIST_SP_800,property of being accessible and usable on demand by an authorized entity,ISO/IEC_TS_5723:2022(en),,,,,,
back test,the quantitative evaluation of a model’s performance both from a statistical and trading perspective,The_Science_of_Algorithmic_Trading_and_Portfolio_Management,,,,,,,,,,
backpropagation,"The way many neural nets learn. They find the difference between their output and the desired output, then adjust the calculations in reverse order of execution.","Hutson,_Matthew","A classical method for error propagation when training Artificial Neural Networks (ANNs). For standard backpropagation, the parameters of each node are changed accoring to the local error gradient. The method can be very slow to converge although it can be improved through the use of methods that slow the error propagation and by batch processing. Many alternate methods such as the conjugate gradient and Levenberg-Marquardt algorithms are more effective and reliable.",Raynor,,,,,,,,
bad actor,"individuals or entities who are responsible for cyber incidents against enterprises, governments, and users. ",Mark_Ciampa_2021,someone with objectives of studying and using cyber security techniques and tools for personal or private gain through malicious or threat activity.,Thomas_Edgar,,,,,,,"black hat, threat actor",
bagging,Bagging predictors is a method for generating multiple versions of a predictor and using these to get an aggregated predictor.,Breiman_Leo,"In this approach we generate 
B different bootstrapped training data sets. We then train our method on
the bth bootstrapped training set in order to get ˆ f*b(x), and finally average
all the predictions, to obtain
ˆ fbag(x) =
1
B
 B
b=1
ˆ f
*b(x).
 This is called bagging.",hastie_introduction_2014,,,,,,,,
back-testing,"A form of outcomes analysis that involves the comparison of actual outcomes with modeled forecasts during a development sample time period (in-sample back-testing) and during a sample period not used in model development (out-of-time back-testing), and at an observation frequency that matches the forecast horizon or performance window of the model.",Comptroller_Office,,,,,,,,,,
batched automation,Process automation execution of intentionally segregated work processes that are able to be processed irrespective of their contextual placement within a service.,IEEE_Guide_IPA,,,,,,,,,,
benchmark,"Standard against which results can be measured or assessed; Procedure, problem, or test that can be used to compare systems or components to each other or to a standard.",IEEE_Soft_Vocab,An alternative prediction or approach used to compare a model’s inputs and outputs to estimates from alternative internal or external data or models.,Comptroller_Office,"The term benchmarking is used in machine learning (ML) to refer to the evaluation
and comparison of ML methods regarding their ability to learn patterns in ‘benchmark’
datasets that have been applied as ‘standards’. Benchmarking could be thought of simply
as a sanity check to confirm that a new method successfully runs as expected and can
reliably find simple patterns that existing methods are known to identify.",olson_pmlb_2017,,,,,,
bias,"A systematic error. In the context of fairness, we are concerned with unwanted bias that places privileged groups at systematic advantage and unprivileged groups at systematic disadvantage.",AI_Fairness_360,"[move to ""computational bias""?] An effect which deprives a statistical result of representativeness by systematically distorting it, as distinct from a random error which may distort on any one occasion but balances out on the average.",OECD,"Discrimination against or in favor of particular individuals or groups. In the context of ethics and politics, the question arises whether a particular bias is unjust or unfair.",AI_Ethics_Mark_Coeckelbergh,"[move to systemic bias] systematic difference in treatment of certain objects, people or groups in comparison to others",measurement_iso22989_2022,"A point estimator \theta_hat is said to be an unbiased estimator fo \theta if E(\theta_hat) = \theta for every possible value of \theta. If \theta_hat is not unbiased, the differen ce E(\theta_hat) - \theta is called the bias of \theta",devore_probability_2004,,
bias mitigation algorithm,A procedure for reducing unwanted bias in training data or models.,AI_Fairness_360,,,,,,,,,,
bias testing,"As it relates to disparate impact, courts and regulators have utilized or considered as acceptable various statistical tests to evaluate evidence of disparate impact. Traditional methods of statistical bias testing look at differences in predictions across protected classes, such as race or sex. In particular, courts have looked to statistical significance testing to assess whether the challenged practice likely caused the disparity and was not the result of chance or a nondiscriminatory factor.",SP1270,,,,,,,,,,
big data,"Extremely large data sets that are statistically analyzed to gain detailed insights. The data can involve billions of records and require substantial computer-processing power. Datasets are sometimes linked together to see how patterns in one domain affect other areas. Data can be structured into fixed fields or unstructured as free-flowing information. The analysis of big datasets, often using AI, can reveal patterns, trends, or underlying relationships that were not previously apparent to researchers.",Brookings_Institution,"consists of extensive datasets primarily in the characteristics of volume, variety, velocity, and/or variability?that require a scalable architecture for efficient storage, manipulation, and analysis",NIST_1500,,,,,,,,
binning,"a technique of lumping small ranges of values together into categories, or ""bins,"" for the purpose of reducing the variability (removing some of the fine structure) in a data set.","Pyle,_Dorian_Data_Preparation_as_a_Process",,,,,,,,,,
biometric data,"personal data resulting from specific technical processing relating to the physical, physiological or behavioural characteristics of a natural person, which allow or confirm the unique identification of that natural person, such as facial images or dactyloscopic data;",GDPR,"an individual’s physiological, biological, or behavioral characteristics, including information pertaining to an individual’s deoxyribonucleic acid (DNA), that is used or is intended to be used singly or in combination with each other or with other identifying data, to establish individual identity. Biometric information includes, but is not limited to, imagery of the iris, retina, fingerprint, face, hand, palm, vein patterns, and voice recordings, from which an identifier template, such as a faceprint, a minutiae template, or a voiceprint, can be extracted, and keystroke patterns or rhythms, gait patterns or rhythms, and sleep, health, or exercise data that contain identifying information.",CCPA,"A measurable physical characteristic or personal behavioral trait used to recognize the identity, or verify the claimed identity, of an applicant. Facial images, fingerprints, and iris scan samples are all examples of biometrics.",SP800-12,,,,,personal data; processing,
boosting ,"Boosting works by sequentially applying a classification
algorithm to reweighted versions of the training data and then taking
a weighted majority vote of the sequence of classifiers thus produced.",friedman_additive_2000,"A machine learning technique that iteratively combines a set of simple and not very accurate classifiers (referred to as ""weak"" classifiers) into a classifier with high accuracy (a ""strong"" classifier) by upweighting the examples that the model is currently misclassifying","aime_measurement_2022, citing Machine Learning Glossary by Google",,,,,,,,
breach,"The loss of control, compromise, unauthorized disclosure, unauthorized acquisition, or any similar occurrence where: a person other than an authorized user accesses or potentially accesses personally identifiable information; or an authorized user accesses personally identifiable information for another than authorized purpose.",CSRC,,,,,,,,,,
broad artificial intelligence (broad AI),"Complex, computational, cognitive automation system capable of providing descriptive, predictive, prescriptive, and limited deductive analytics with relevance and accuracy exceeding human expertise in a broad, logically related set of knowledge domains.",IEEE_Guide_IPA,"a sophisticated and adaptive system, which successfully performs any cognitive task by virtue of its sensory perception, previous experience, and learned skills.","Hochreiter,_Sepp ",,,,,,,,
built-in test,"Equipment or software embedded in the operational components or systems, as opposed to external support units, which perform a test or sequence of tests to verify mechanical or electrical continuity of hardware, or the proper automatic sequencing, data processing, and readout of hardware or software systems.  ",SP1011,,,,,,,,,,
bug-bounty,"Reward given to independent security researchers, penetrations testers, and white hat hackers for discovering exploitable software vulnerabilities and sharing this knowledge with the operator of a particular bug-bounty program (BBP). ","Kuehn,_Andreas",,,,,,,,,,
business process,"A defined set of business activities that represent the steps or tasks required to achieve a business objective, including the flow and use of information, participants, and human or digital resources.",IEEE_Guide_IPA,,,,,,,,,,
business process management,"Discipline involving any combination of modeling, automation, execution, control, measurement and optimization of business activity flows, in support of enterprise goals, spanning systems, employees, customers, and partners within and beyond the enterprise boundaries.",IEEE_Guide_IPA,,,,,,,,,,
business rule,"Definition, constraint, dependency, or decision criteria that determine the method of execution of a task or tasks, or influences the order of execution of a task or tasks. Business rules assert control, or influence the behavior, of a business process within computing systems.",IEEE_Guide_IPA,,,,,,,,,,
calibration,"A comparison between a device under test and an established standard, such as UTC(NIST). When the calibration is finished, it should be possible to state the estimated time offset and/or frequency offset of the device under test with respect to the standard, as well as the measurement uncertainty.",CSRC,"operation that, under specified conditions, in a first step, establishes a relation between the quantity values with measurement uncertainties provided by measurement standards and corresponding indications with associated measurement uncertainties and, in a second step, uses this information to establish a relation for obtaining a measurement result from an indication","aime_measurement_2022, citing ISO/IEC Guide 99","Set of operations that establish, under specified conditions, the relationship between values indicated by a measuring instrument or measuring system, or values represented by a material measure, and the corresponding known values of a measurand.",UNODC_Glossary_QA_GLP,,,,,,
capability,"measure of capacity and the ability of an entity, person or organization to achieve its objectives",ISO/IEC_TS_5723:2022(en),,,,,,,,,,
case,"Single entry, single exit multiple way branch that defines a control expression, specifies the processing to be performed for each value of the control expression, and returns control in all instances to the statement immediately following the overall construct.",IEEE_Soft_Vocab,,,,,,,,,,
causal inference,"an intellectual discipline that considers the assumptions, study designs, and estimation strategies that allow researchers to draw causal conclusions based on data. The term ‘causal conclusion’ used here refers to a conclusion regarding the effect of a causal variable (often referred to as the ‘treatment’ under a broad conception of the word) on some outcome(s) of interest.","Jennifer,_Hill",,,,,,,,,,
causative,acting as the cause of something,cambridge_causative_2023,,,,,,,,,,
chatbot,"Conversational agent that dialogues with its user (for example: empathic robots available to patients, or automated conversation services in customer relations).",COE_AI_Glossary,A chatbot is a computer program which responds like an intelligent entity when conversed with. The conversation may be through text or voice. Any chatbot program understands one or more human languages by Natural Language Processing,"Khanna,_Anirudh",,,,,,,,
choreography,"An ordered sequence of system-to-system message exchanges between two or more participants. In choreography, there is no central controller, responsible entity, or observer of the process.",IEEE_Guide_IPA,,,,,,,,,,
classification,"When the output is one of a finite set of values (such as sunny, cloudy or rainy), the learning problem is called classification, and is called Boolean or binary classification if there are only two values.",AIMA,task of assigning collected data to target categories or classes.,"aime_measurement_2022, citing ISO/IEC TR 24030",,,,,,,,
classifier,A model that predicts categorical labels from features.,AI_Fairness_360,,,,,,,,,,
clustering,Detecting potentially useful clusters of input examples.,AIMA,"The basic problem of clustering may be stated as follows: 
Given a set of data points, partition them into a set of groups which are as similar as possible.",aggarwal_clustering_2013,"the tendency for items to be consistently grouped together in the course of recall. This grouping typically occurs for related items. It is readily apparent in memory tasks in which items from the same category, such as nonhuman animals, are recalled together.",APA_clustering,,,,,,
cognitive automation,"The identification, assessment, and application of available machine learning algorithms for the purpose of leveraging domain knowledge and reasoning to further automate the machine learning already present in a manner that may be thought of as cognitive. With cognitive automation, the system performs corrective actions driven by knowledge of the underlying analytics tool itself, iterates its own automation approaches and algorithms for more expansive or more thorough analysis, and is thereby able to fulfill its purpose. The automation of the cognitive process refines itself and dynamically generates novel hypotheses that it can likewise assess against its existing corpus and other information resources.",IEEE_Guide_IPA,,,,,,,,,,
cognitive computing,"Complex computational systems designed to

—        Sense (perceive the world and collect data);
—        Comprehend (analyze and understand the information collected);
- Act (make informed decisions and provide guidance based on this analysis in an independent way);
and
—        Adapt (adapt capabilities based on experience) in ways comparable to the human brain.
",IEEE_Guide_IPA,,,,,,,,,,
column,"In the context of relational databases, a column is a set of data values, all of a single type, in a table.",techopedia_column_2022,,,,,,,,,,
COMPAS controversy,"A canonical example [of algorithmic bias] comes from a tool used by courts in the United States to make pretrial detention and release decisions. The software, Correctional Offender Management Profiling for Alternative Sanctions (COMPAS), measures the risk of a person to recommit another crime. Judges use COMPAS to decide whether to release an offender or to keep him or her in prison. An investigation into the software found a bias against African-Americans: COMPAS is more likely to have higher false positive rates for African-American offenders than Caucasian offenders in falsely predicting them to be at a higher risk of recommitting a crime or recidivism","Mehrabi,_Ninareh",,,,,,,,,,
computer vision,The digital process of perceiving and learning visual tasks in order to interpret and understand the world through cameras and sensors.,NSCAI,"An image understanding task that automatically builds a description not only of the image itself, but of the three dimensional scene that it depicts.",NBSIR_82-2582,,,,,,,,
concept drift,"Use of a system outside the planned domain of application, and a common cause of performance gaps between laboratory settings and the real world.",SP1270,an online supervised learning scenario when the relation between the input data and the target variable changes over time.,"Gama,_Joao","Systems that classify or predict a concept (e.g., credit ratings or computer intrusion monitors) over time can suffer performance loss when the concept they are tracking changes. This is referred to as concept drift. This can either be a natural process that occurs without a reference to the system, or an active process, where others are reacting to the system (e.g., virus detection).",Raynor,,,,,,
confidentiality,"Data confidentiality is a property of data, usually resulting from legislative measures, which prevents it from unauthorized disclosure. ",OECD,"Preserving authorized restrictions on information access and disclosure, including means for protecting personal privacy and proprietary information.",CSRC,The property that data or information is not made available or disclosed to unauthorized persons or processes.,NIST_SP_800,"A property that information is not disclosed to users, processes, or devices unless they have been authorized to access the information.",CISA,,,,
confusion matrix,"A matrix showing the predicted and actual classifications. A confusion matrix is of size LxL, where L is the number of different label values","Kohavi,_Ron",,,,,,,,,,
consent,"‘Consent’ of the data subject means any freely given, specific, informed and unambiguous indication of the data subject's wishes by which he or she, by a statement or by a clear affirmative action, signifies agreement to the processing of personal data relating to him or her.",GDPR,"“Consent” means any freely given, specific, informed, and unambiguous indication of the consumer’s wishes by which the consumer, or the consumer’s legal guardian, a person who has power of attorney, or a person acting as a conservator for the consumer, including by a statement or by a clear affirmative action, signifies agreement to the processing of personal information relating to the consumer for a narrowly defined particular purpose. Acceptance of a general or broad terms of use, or similar document, that contains descriptions of personal information processing along with other, unrelated information, does not constitute consent. Hovering over, muting, pausing, or closing a given piece of content does not constitute consent. Likewise, agreement obtained through use of dark patterns does not constitute consent.",CCPA,,,,,,,personal data,
constituent system,"independent system that forms part of a system of systems (SoS) (note: Constituent systems can be part of one or more SoS. Each constituent system is a useful system by itself, having its own development, management, utilization, goals, and resources, but interacts within the SoS to provide the unique capability of the SoS).",ISO/IEC_TS_5723:2022(en),,,,,,,,,,
constraint,"Specification of what may be contained in a data or metadata set in terms of the content or, for data only, in terms of the set of key combinations to which specific attributes (defined by the data structure) may be attached.",OECD,A limitation or implied requirement that constrains the design solution or implementation of the systems engineering process and is not changeable by the enterprise,IEEE_Soft_Vocab,,,,,,,,
construct validity,the degree to which the application of constructs to phenomena is warranted with respect to the  research goals and questions.,"Wieringa,_Roel_J.","Construct validation is involved whenever a test is to be interpreted as a measure of some attribute or quality which is not “operationally defined.” The problem faced by the investigator is, “What constructs account for variance in test performance?”",cronbach_construct_1955,"Established experimentally to
 demonstrate that a survey distinguishes between 
people who do and do not have certain 
characteristics. It is usually established 
experimentally.",fink_survey_2010,"Establishing construct
 validity means demonstrating, in a variety of ways, that the measurements
 obtained from measurement model are both meaningful
 and useful.",jacobs_measurement_2023,,,,
content harms,"the psychological, social, physical, or other harms experienced by someone while they are interacting with content that is algorithmically recommended to them.","Chi,_Gao,_Ma",,,,,,,,,harms of representation,
content validity,"Refers to the extent to which a 
measure thoroughly and appropriately assesses the 
skills or characteristics it is intended to measure.",fink_survey_2010,"the extent to which a test measures a representative sample of the subject matter or behavior under investigation. For example, if a test is designed to survey arithmetic skills at a third-grade level, content validity indicates how well it represents the range of arithmetic operations possible at that level. Modern approaches to determining content validity involve the use of exploratory factor analysis and other multivariate statistical procedures.",APA_content_validity,,,,,,,,
contestability,"A contestable statement, claim, legal decision, etc. is one that is possible to argue about or try to have changed because it may be wrong",cambridge_contestable_2023,,,,,,,,,,
context,"The context is the circumstances, purpose, and perspective under which an object is defined or used. ",OECD,The immediate environment in which a function (or set of functions in a diagram) operates,IEEE_Soft_Vocab,the interrelated conditions in which something exists or occurs.,Merriam-Webster_context,,,,,,
context control,,,,,,,,,,,,
contextual learning,"A computing system with sufficient knowledge regarding its purpose that it understands the source, relevance, and utility of data and inputs.",IEEE_Guide_IPA,,,,,,,,,,
context-of-use,"The Context of Use is the actual conditions under which a given artifact/software product is used, or will be used in a normal day to day working situation. ",interaction_context_2023,"comprises a combination of users, goals, tasks, resources, and the technical, physical and social, cultural and organizational environments in which a system, product or service is used[; ...] can include the interactions and interdependencies between the object of interest and other systems, products or services.",ISO_9241-11:2018,,,,,,,,
controllability,property of a system that allows a human or another external agent to intervene in the system’s functioning; such a system is heteronomous.,ISO/IEC_TS_5723:2022(en),,,,,,,,,,
control class,"(control group) the set of observations in an experiment or prospective study that do not receive the experimental treatment(s). These observations serve (a) as a comparison point to evaluate the magnitude and significance of each experimental treatment, (b) as a reality check to compare the current observations with previous observation history, and (c) as a source of data for establishing the natural experimental error.",nist_statistics_2012,,,,,,,,,,
controller,"‘Controller’ means the natural or legal person, public authority, agency or other body which, alone or jointly with others, determines the purposes and means of the processing of personal data; where the purposes and means of such processing are determined by Union or Member State law, the controller or the specific criteria for its nomination may be provided for by Union or Member State law;",GDPR,,,,,,,,,personal data; processor,
corpus (corpora),"A deliberately assembled collection of knowledge and data (structured and/or unstructured) believed to contain relevant information on a topic or topics to be used by software systems for which useful analysis, prediction, or outcome is being sought. ",IEEE_Guide_IPA,,,,,,,,,,
correlation,In its most general sense correlation denoted the interdependence between quantitative or qualitative data. In this sense it would include the association of dichotomised attributes and the contingency of multiply-classified attributes.,OECD,"The correlation coefficient of two random variables y_1, and y_2, denoted \rho(y_1,y_2) is: \rho(y_1, y_2) = Cov(y_1, y_2)/\sqrt{Var(y_1)*Var(y_2)}",box_statistics_2005,,,,,,,,
counterfactual explanation,"Statements taking the form: Score p was returned because variables V had values (v1, v2,...) associated with them. If V instead had values (v1', v2',...) score p' would have been returned.",wachter_counterfactual_2018,,,,,,,,,,
counterfactual fairness,Our definition of counterfactual fairness captures the intuition that a decision is fair towards an individual if it the same in (a) the actual world and (b) a counterfactual world where the individual belonged to a different demographic group.,kusner_counterfactual_2017,"Given a predictive problem with fairness considerations, where A, X and Y represent the protected
attributes, remaining attributes, and output of interest respectively, let us assume that we are given a
causal model (U; V; F), where V = A \cup X. We postulate the following criterion for predictors of Y .
Definition 5 (Counterfactual fairness). Predictor ^Y is counterfactually fair if under any context
 X = x and A = a, 
P( ^Y_{A <- a} (U) = y | X = x; A = a) = P( ^Y_{A <- a')(U) = y | X = x;A = a); (1) 
for all y and for any value a' attainable by A.",kusner_counterfactual_2017,"A fairness
metric that checks whether a classifier produces the same result for one individual
as it does for another individual who is identical to the first, except with
respect to one or more sensitive attributes. Evaluating a classifier for counterfactual
fairness is one method for surfacing potential sources of bias in a
model","aime_measurement_2022, citing Machine Learning Glossary by Google",,,,,,
countermeasure,"Actions, devices, procedures, techniques, or other measures that reduce the vulnerability of a system. Synonymous with security controls and safeguards.",SP800-37,"Actions, devices, procedures, or techniques that meet or oppose (i.e., counters) a threat, a vulnerability, or an attack by eliminating or preventing it, by minimizing the harm it can cause, or by discovering and reporting it so that corrective action can be taken.",GWUC,,,,,,,safeguard; security control,
criterion validity,"compares responses to future performance 
or to those obtained from other, more well-established 
surveys. Criterion validity is made up two
 subcategories: predictive and concurrent. Predictive 
validity refers to the extent to which a survey measure 
forecasts future performance. A graduate school entry
 examination that predicts who will do well in graduate
 school has predictive validity. Concurrent validity is 
demonstrated when two assessments agree or a new measure
is compared favorably with one that is already considered 
valid.",fink_survey_2010,"an index of how well a test correlates with an established standard of comparison (i.e., a criterion). Criterion validity is divided into three types: predictive validity, concurrent validity, and retrospective validity. For example, if a measure of criminal behavior is valid, then it should be possible to use it to predict whether an individual (a) will be arrested in the future for a criminal violation, (b) is currently breaking the law, and (c) has a previous criminal record.",APA_criterion_validity,,,,,,,criterion-referenced validity; criterion-related validity,
crowdsource,"a type of participative online activity in which an individual, an institution, a non-profit organization, or company proposes to a group of individuals of varying knowledge, heterogeneity, and number, via a flexible open call, the voluntary undertaking of a task. The undertaking of the task, of variable complexity and modularity, and in which the crowd should participate bringing their work, money, knowledge and/or experience, always entails mutual benefit. The user will receive the satisfaction of a given type of need, be it economic, social recognition, self-esteem, or the development of individual skills, while the crowdsourcer will obtain and utilize to their advantage what the user has brought to the venture, whose form will depend on the type of activity undertaken.",Enrique,,,,,,,,,,
customer,"The beneficiary of the execution of an automated task, process, or service.",IEEE_Guide_IPA,,,,,,,,,,
cybersecurity,"Prevention of damage to, protection of, and restoration of computers, electronic communications systems, electronic communications services, wire communication, and electronic communication, including information contained therein, to ensure its availability, integrity, authentication, confidentiality, and nonrepudiation.",SP800-37,,,,,,,,,,
dark pattern,"“Dark pattern” means a user interface designed or manipulated with the substantial effect of subverting or impairing user autonomy, decisionmaking, or choice, as further defined by regulation.",CCPA,,,,,,,,,,
data,"Characteristics or information, usually numerical, that are collected through observation. ",OECD,"re-interpretable representation of information in
a formalized manner suitable for communication, interpretation or processing","aime_measurement_2022, citing ISO/IEC TR 24029-1",,,,,,,,
data analytics,"The analysis of data to gather substantive insights. Researchers use statistical techniques to find trends or patterns in the data, which give them a better understanding of a range of different topics. Data analytic approaches are used in many businesses and organizations to track day-to-day activities and improve operational efficiency.",Brookings_Institution,"Data analysis is the process of transforming raw data into usable information, often presented in the form of a published analytical article, in order to add value to the statistical output. ",OECD,"the process of applying graphical, statistical, or quantitative techniques to a set of observations or measurements in order to summarize it or to find general patterns.",APA_data_analysis,,,,,,
data cleaning,"Data Cleaning is the process of identifying, correcting, or removing inaccurate or corrupt data records","Ranschaert,_Erik",,,,,,,,,,
data control,management oversight of information policies for an organization’s information; observing and reporting on how processes are working and managing issues.,Egnyte,,,,,,,,,,
data dredging,A statistical bias in which testing huge numbers of hypotheses of a dataset may appear to yield statistical significance even when the results are statistically nonsignificant.,SP1270,,,,,,,,,statistical bias; p-hacking,
data drift,The change in model input data that leads to model performance degradation.,Microsoft_Azure_documentation,,,,,,,,,,
data-driven,Data-driven decision making  (DDD) refers to the practice of basing decisions on the analysis of data rather than purely on intuition.,provost_data_2013,,,,,,,,,,
data fabric,"A data corpus, after the application of semantic mapping, relevant ontologies, and data seeding sufficient for artificial intelligence (AI) or machine learning algorithms to provide meaningful insight, prediction, and/or prescription. ",IEEE_Guide_IPA,,,,,,,,,,
data fusion ,"A process in which data, generated by multiple sensory sources, is integrated and/or correlated to create information, knowledge, and/or intelligence that may be displayed for user or be actionable to accomplish the tasks.",SP1011,"The process of combining data from multiple sources to produce more accurate, consistent, and concise information than that provided by any individual data source.","Munir,_Arslan",,,,,,,,
data governance,A set of processes that ensures that data assets are formally managed throughout the enterprise. A data governance model establishes authority and management and decision making parameters related to the data produced or managed by the enterprise.,CSRC,"refers to a system, including policies, people, practices, and technologies, necessary to ensure data management within an organization",NIST_1500,,,,,,,,
data mining ,"Techniques that analyze large amounts of information to gain insights, spot trends, or uncover substantive patterns. These approaches are used to help businesses and organizations improve their processes or identify associations that shed light on relevant questions. Data mining often involves more use of algorithms, especially machine learning, than traditional statistics.",Brookings_Institution,"Data Mining is the process of data analysis and information extraction from large amounts of datasets with machine learning, statistical approaches. and many others.","Ranschaert,_Erik","computational process that extracts patterns
by analysing quantitative data from different perspectives and dimensions, categorizing
them, and summarizing potential relationships and impacts",aime_measurement_2022 citinig ISO/IEC 22989,,,,,,
data point,a discrete unit of information.,TechTarget_data_point,the information we feed into the machine learning model.,Morris_John_data_point,,,,,,,,
data poisoning,"Machine learning systems trained on user-provided data are susceptible to data poisoning attacks, whereby malicious users inject false training data with the aim ofcorrupting the learned model","Steinhardt,_Jacob",,,,,,,,,,
data preparation,"We define data preparation as the set of preprocessing operations performed in early stages of a data processing pipeline, i.e., data transformations at the structural and syntactical levels",hameed_data_2020,,,,,,,,,,
data proxy,Data that are closely related to and serve in place of data that are either unobservable or immeasurable.,Comptroller_Office,,,,,,,,,,
data quality,degree to which the characteristics of data satisfy stated and implied needs when used under specified conditions,IEEE_Soft_Vocab,"The dimensions of the IMF definition of ""data quality"" are:

- integrity;
- methodological soundness;
- accuracy and reliability;
- serviceability;
- accessibility.

There are a number of prerequisites for quality. These comprise:

- legal and institutional environment;
- resources;
- quality awareness.",OECD,,,,,,,,
data science,"The field that combines domain expertise, programming skills, and knowledge of mathematics and statistics to extract meaningful insights from data.","Reznik,_Leon","Data-Driven Science, or Data Science, is an interdisciplinary field of employing computing algorithms to extract knowledge or insights from data acquired from different sources.","Ranschaert,_Erik",Methodology for the synthesis of useful knowledge directly from data through a process of discovery or of hypothesis formulation and hypothesis testing.,NIST_1500,"Interdisciplinary science that uses statistics, algorithms, and other methods to extract meaningful and useful patterns from data sets—sometimes known as “big data.” Today, machine learning is often used in this field. Next to analysis of data, data science is also concerned with the capturing, preparation, and interpretation of data.",AI_Ethics_Mark_Coeckelbergh,,,artificial intelligence (AI); machine learning (ML),
data scientist,"A practitioner who has sufficient knowledge in the overlapping regimes of business needs, domain knowledge, analytical skills, and software and systems engineering to manage the end-to-end data processes in the analytics life cycle.",NIST_1500,,,,,,,,,,
data seeding,"The intentional introduction of initial state conditions, influencing factors, and outcomes (both successful and unsuccessful) in a data fabric to create sufficient machine learning analysis signals to enable encouragement/discouragement to enrich deterministic relationships between data elements in a given information domain. ",IEEE_Guide_IPA,,,,,,,,,,
data wrangling ,"process by which the data required by an application is identified, extracted, cleaned and integrated, to yield a data set that is suitable for exploration and analysis.","Furche,_Tim",,,,,,,,,,
decision,A conclusion reached after consideration of business rules and relevant data within a given process.,IEEE_Guide_IPA,Types of statements in which a choice between two or more possible outcomes controls which set of actions will result.,IEEE_Soft_Vocab,,,,,,,,
decision point,"A point within a business process where the process flow can take one of several alternative paths, including recursive.",IEEE_Guide_IPA,,,,,,,,,,
decision subject,,,,,,,,,,,,
decision tree,"Tree-structure resembling a flowchart, where every node represents a test to an attribute, each branch represents the possible outcomes of that test, and the leaves represent the class labels.","Reznik,_Leon","In this chapter, we describe tree-based methods for regression and
classification. These involve stratifying or segmenting the predictor space
into a number of simple regions. In order to make a prediction for a given
observation, we typically use the mean or the mode of the training observations
in the region to which it belongs. Since the set of splitting rules used
to segment the predictor space can be summarized in a tree, these types of
approaches are known as decision tree methods.",james_statistical_2014,,,,,,,,
decision-making,"the cognitive process resulting in the selection of a belief or a course of action among several possible alternative options. It could be either rational or irrational. The decision-making process is a reasoning process based on assumptions of values, preferences and beliefs of the decision-maker. Every decision-making process produces a final choice, which may or may not prompt action.",Wikipedia_Decision-making,"the cognitive process of choosing between two or more alternatives, ranging from the relatively clear cut (e.g., ordering a meal at a restaurant) to the complex (e.g., selecting a mate). Psychologists have adopted two converging strategies to understand decision making: (a) statistical analysis of multiple decisions involving complex tasks and (b) experimental manipulation of simple decisions, looking at the elements that recur within these decisions.",APA_decision_making,Decision making requires that the decision maker make a choice between two or more alternatives (note that doing nothing can be viewed as making a choice). The alternative selected results in real or imaginary consequences to the decision maker. Judgment is a closely related process in which a person rates or assigns values to attributes of the alternatives considered. A rational decision maker seeks desirable consequences and attempts to avoid undesirable consequences.,Lehto_Nanda_2021,"a choice of action – of what to do or not do. Decisions are made to achieve goals, and they are based on beliefs about what actions will achieve the goals.",Baron_Thinking_and_Deciding,,,,
decision support system,a computer program application used to improve a company's decision-making capabilities. It analyzes large amounts of data and presents an organization with the best possible options available[; they] bring together data and knowledge from different areas and sources to provide users with information beyond the usual reports and summaries. This is intended to help people make informed decisions.,TechTarget_decision_support_system,"a model- or knowledge-based system intended to support managerial decision making in semistructured or unstructured situations. A DSS is not meant to replace a decision maker, but to extend his/her decision making capabilities. It uses data, provides a clear user interface, and can incorporate the decision maker’s own insights.",Burstein_Holsapple,,,,,,,,
decommission,"the total or partial removal of existing components and their corresponding sub-components from Production and any relevant environment, minimizing risks and impacts, ensuring policy compliance, and maximizing the financial benefits (i.e., optimizing the cost reduction). ",IG1190M_AIOps_Decommission_v1.0.0,,,,,,,,,,
deductive analytics,"Insights, reporting, and information answering the question, ""What would likely happen IF…?” Deductive analytics evaluates causes and outcomes of possible future events.",IEEE_Guide_IPA,,,,,,,,,deductive reasoning,
deep learning,"A subset of machine learning that relies on neural networks with many layers of neurons. In so doing, deep learning employs statistics to spot underlying trends or data patterns and applies that knowledge to other layers of analysis. Some have labeled this as a way to “learn by example” and a technique that “perform[s] classification tasks directly from images, text, or sound” and then applies that knowledge independently.[4] Deep learning requires extensive computing power and labeled data, and is used in medical research, automated vehicles, electronics, and manufacturing, among other areas.",Brookings_Institution,"Deep learning is a broad family of techniques for machine learning in which hypotheses take the form of complex algebraic circuits with tunable connection strengths. The word “deep” refers to the fact that the circuits are typically organized into many layers, which means that computation paths from inputs to outputs have many steps. Deep learning is currently the most widely used approach for applications such as visual object recognition, machine translation, speech recognition, speech synthesis, and image synthesis; it also plays a significant role in reinforcement learning applications.",Russell_and_Norvig,Machine learning method based on characterization of data learning.,"Reznik,_Leon","A form of machine learning that uses neural networks with several layers of ""neurons"": simple interconnected processing units that interact.",AI_Ethics_Mark_Coeckelbergh,"[an approach to AI that allows] computers to learn from experience and understand the world in terms of a hierarchy of concepts, with each concept defined through its relation to simpler concepts. By gathering knowledge from experience, this approach avoids the need for human operators to formally specify all the knowledge that the computer needs. The hierarchy of concepts enables the computer to learn complicated concepts by building them out of simpler ones. If we draw a graph showing how these concepts are built on top of each other, the graph is deep, with many layers.",deeplearningbook_intro,,
deepfake,"Digital images and audio that are artificially altered or manipulated by AI and/or deep learning often to make someone do or say something he or she did not actually do or say. Pictures or videos can be edited to put someone in a compromising position or to have someone make a controversial statement, even though the person did not actually do or say what is shown. Increasingly, it is becoming difficult to distinguish artificially manufactured material from actual videos and images.",Brookings_Institution,A digital picture or video that has been maliciously edited using an algorithm in a way that makes the video appear authentic.,GWUC,,,,,,,,
deletion,"Of an <X>, the action of destroying an instantiated <X>.",IEEE_Soft_Vocab,,,,,,,,,,
denial-of-service,"The prevention of authorized access to resources or the delaying of time-critical operations. (Time-critical may be milliseconds or it maybe hours, depending upon the service provided).",SP800-12,An attack that prevents or impairs the authorized use of information system resources or services.,CISA,"when legitimate users are unable to access information systems, devices, or other network resources due to the actions of a malicious cyber threat actor. Services affected may include email, websites, online accounts (e.g., banking), or other services that rely on the affected computer or network. A denial-of-service condition is accomplished by flooding the targeted host or network with traffic until the target cannot respond or simply crashes, preventing access for legitimate users. DoS attacks can cost an organization both time and money while their resources and services are inaccessible.",ST04-015,,,,,,
denigration,(denigrate) to attack the reputation of,merriam-webster_dictionary_2022,(denigrate) to deny the importance or validity of,merriam-webster_dictionary_2022,,,,,,,,
dependability,"<of an item> ability to perform as and when required (note 1: includes availability, reliability, recoverability, maintainability, and maintenance support performance, and, in some cases, other characteristics such as durability, safety and security. Note 2: used as a collective term for the time-related quality characteristics of an item).",ISO/IEC_TS_5723:2022(en),,,,,,,,,,
deployment,Phase of a project in which a system is put into operation and cutover issues are resolved,IEEE_Soft_Vocab,,,,,,,,,,
descriptive analytics,"Insights, reporting, and information answering the question, “Why did something happen?” Descriptive analytics determines information useful to understanding the cause(s) of an event(s).",IEEE_Guide_IPA,,,,,,,,,,
deterministic,"modelling [that] produces consistent outcomes for a given set of inputs, regardless of how many times the model is recalculated. The mathematical characteristics are known in this case. None of them is random, and each problem has just one set of specified values as well as one answer or solution. The unknown components in a deterministic model are external to the model. It deals with the definitive outcomes as opposed to random results and doesn’t make allowances for error.",Sourabh_Mehta_deterministic,,,,,,,,,,
deterministic algorithm,"An algorithm that, given the same inputs, always produces the same outputs.",CSRC,,,,,,,,,,
developer,"A general term that includes developers or manufacturers of systems, system components, or system services; systems integrators; vendors; and product resellers. Development of systems, components, or services can occur internally within organizations or through external entities. ",SP800-37,"Individual or organization that performs development activities (including requirements analysis, design, testing through acceptance) during the system or software life-cycle process.",IEEE_Soft_Vocab,,,,,,,,
diagnostic analytics,"Insights, reporting, and information answering the question, “Why did something happen?” Diagnostic analytics determines information useful to understanding the cause(s) of an event(s). ",IEEE_Guide_IPA,,,,,,,,,,
diagnostics,Pertaining to the detection and isolation of faults or failures,IEEE_Software_Vocab,,,,,,,,,,
differential privacy,"Differential privacy is a method for measuring how much information the output of a computation reveals about an individual. It is based on the randomised injection of ""noise"". Noise is a random alteration of data in a dataset so that values such as direct or indirect identifiers of individuals are harder to reveal. An important aspect of differential privacy is the concept of “epsilon” or ?, which determines the level of added noise. Epsilon is also known as the “privacy budget” or “privacy parameter”.",privacy-enhancing_technologies,"For two datasets D and D' that differ in at most one element, a randomized algorithm $M$ guarantees \emph{$(\epsilon, \delta)$-differential privacy} for any subset of the output $S$ if $M$ satisfies: \begin{equation}
    Pr[M(D) \in S] \leq exp(\epsilon)*Pr[M(D') \in S] + \delta
\end{equation}
Furthermore, when $\delta = 0$ an algorithm M is said to guarantee \emph{$\epsilon$-differential privacy}",gong_differential_2020,,,,,,,,
differential validity,"Differential validity states that the validities in two applicant populations are unequal, that is, pi != pa.",hunter_differential_1979,,,,,,,,,,
digital labor,Digital automation of information technology systems and/or business processes that successfully delivers work output previously performed by human labor or new work output that would typically or alternatively have been performed by human labor.,IEEE_Guide_IPA,,,,,,,,,,
digital workforce,The collective suite of automation technologies delivering existing or new work output as applied in a business; the manifestation of digital labor.,IEEE_Guide_IPA,,,,,,,,,,
dimension,"The dimension of an object is a topological measure of the size of its covering properties. Roughly speaking, it is the number of coordinates needed to specify a point on the object.",wolfram_math_2022,Distinct components that a multidimensional construct encompasses,IEEE_Soft_Vocab,,,,,,,,
dimension reduction,Dimensionality reduction is the process of taking data in a high dimensional space and mapping it into a new space whose dimensionality is much smaller,"Shalev-Shwartz,_Shai",,,,,,,,,,
discrimination,Disadvantageous treatment of a person based on belonging to a category rather than on individual merit.,Žliobaite_Indre,,,,,,,,,,
disparate impact,Facially neutral practices that might nevertheless have an unjustified adverse impact on members of a protected class.,"Lipton,_Zachary","For Predictor Y and Sensitive Impact S. Definition 6.2 Disparate Impact (DI) =
P[Yˆ = 1 | S != 1]/P[Yˆ = 1 | S = 1]
",friedler_comparative_2019,,,,,,,,
disparate treatment,"Intentional discrimination, including (i) decisions explicitly based on protected characteristics; and (ii) intentional discrimination via proxy variables (e.g literacy tests for voting eligibility).","Lipton,_Zachary",,,,,,,,,,
distributional robustness,Optimizing the predictive accuracy for a whole class of distributions instead of just a single target distribution.,"Meinshausen,_Nicolai",,,,,,,,,,
diversity,"Diversity refers to anything that sets one individual apart from another, including the full spectrum of human demographic differences as well as the different ideas, backgrounds, and opinions people bring.",Seth_Boden_2020,"Diversity: The term diversity is used to describe individual differences (e.g. life experiences, learning and working styles, personality types) and group/social differences (e.g. race, socio-economic status, class, gender, sexual orientation, country of origin, ability, intellectual traditions and perspectives, as well as cultural, political, religious, and other affiliations) that can be engaged to achieve excellence in teaching, learning, research, scholarship, and administrative and support services.",GWU_diversity_and_inclusion,"any dimension that can be used to differentiate groups and people from one another. It means respect for and appreciation of differences. But it’s more than this. We all bring with us diverse perspectives, work experiences, life styles and cultures. Here in ODE we know the power of diversity is unleashed when we respect and value differences. Diversity is defined by who we are as individuals. HUD recognizes that its strength comes from the dedication, experience, talents, and perspectives of every employee. Diversity encompasses the range of similarities and differences each individual brings to the workplace, including but not limited to national origin, language, race, color, disability, ethnicity, gender, age, religion, sexual orientation, gender identity, socioeconomic status, veteran status, and family structures. We define workforce diversity as a collection of individual attributes that together help us pursue organizational objectives efficiently and effectively. In simple terms, diversity is the mix.",HUD_diversity_and_inclusion,"the practice of including the many communities, identities, races, ethnicities, backgrounds, abilities, cultures, and beliefs of the American people, including underserved communities.",EO_DEIA_2021,,,inclusion,
documentation,"Collection of documents on a given subject; written or pictorial information describing, defining, specifying, reporting, or certifying activities, requirements, procedures, or results.",IEEE_Soft_Vocab,,,,,,,,,,
domain,"Distinct scope, within which common characteristics are exhibited, common rules observed, and over which a distribution transparency is preserved.",IEEE_Soft_Vocab,"A set of elements, data, resources, and functions that share a commonality in combinations of: (1) roles supported, (2) rules governing their use, and (3) protection needs.",SP800-160,"<artificial intelligence> specific field of knowledge
or expertise","aime_measurement_2022, citing ISO/IEC 2382",,,,,,
domain expertise,Domain expertise implies knowledge and understanding of the essential aspects of a specific field of inquiry.,McCue_Colleen,,,,,,,,,,
domain shift,Differences between the source and target domain data,"Stacke,_Karin",,,,,,,,,distributional shift,
drinking your own champagne,The practice in which tech workers use their own product consistently to see how well it works and where improvements can be made.,kelley_dogfooding_2022,,,,,,,,,"dogfooding, eating your own dogfood",
dynamic process,The process in which one or more paths are defined and may be utilized based on the conditions present at the time of execution.,IEEE_Guide_IPA,,,,,,,,,,
eavesdropping,An attack in which an attacker listens passively to the authentication protocol to capture information that can be used in a subsequent active attack to masquerade as the claimant.,"Reznik,_Leon",An attack in which an attacker listens passively to the authentication protocol to capture information that can be used in a subsequent active attack to masquerade as the claimant.,CSRC,A form of active wiretapping attack in which the attacker intercepts and selectively modifies communicated data to masquerade as one or more of the entities involved in a communication association.,NIST_CSRC_man-in-the-middle_attack,"An attack in which an attacker is positioned between two communicating parties in order to intercept and/or alter data traveling between them. In the context of authentication, the attacker would be positioned between claimant and verifier, between registrant and CSP during enrollment, or between subscriber and CSP during authenticator binding.",NIST_CSRC_man-in-the-middle_attack,An attack where the adversary positions himself in between the user and the system so that he can intercept and alter data traveling between them.,NIST_CSRC_man-in-the-middle_attack,man-in-the-middle; interception attack,
edge case,"a problem or situation, especially in computer programming, that only happens at the highest or lowest end of a range of possible values or in extreme situations:",cambridge_dictionary_2022,,,,,,,,,,
effective challenge,"The concept of effective challenge is used to improve AI implementation at large financial services organizations in the US. An interpretation of an effective challenge is that, when building AI systems, one of the best ways to guarantee good results is to actively challenge and review each step of the development process. Of course, a culture of effective challenge must apply to everyone developing an AI system, even so-called “rock-star” engineers and data scientists.",Darnell_Coss_Hall,,,,,,,,,,
embedding,"An embedding is a representation of a topological object, manifold, graph, field, etc. in a certain space in such a way that its connectivity or algebraic properties are preserved. For example, a field embedding preserves the algebraic structure of plus and times, an embedding of a topological space preserves open sets, and a graph embedding preserves connectivity.

One space X is embedded in another space Y when the properties of Y restricted to X are the same as the properties of X. ",wolfram_math_2022,,,,,,,,,,
emergent risks,,,,,,,,,,,ontological uncertainty,
emulation,"The use of a data processing system to imitate another data processing system, so that the imitating system accepts the same data, executes the same programs, and achieves the same results as the imitated system.",IEEE_Soft_Vocab,,,,,,,,,,
end event,"An activity, task, or output that describes or defines the conclusion of a process.",IEEE_Guide_IPA,,,,,,,,,,
engineer,"n. 3a: a designer or builder of engines; b: a person who is trained in or follows as a profession a branch of engineering; c: a person who carries through an enterprise by skillful or artful contrivance; 4: a person who runs or supervises an engine or an apparatus.

v. 1: to lay out, construct, or manage as an engineer.",Merriam-Webster_engineer,,,,,,,,,,
ensemble,a machine learning paradigm where multiple models (often called “weak learners”) are trained to solve the same problem and combined to get better results. The main hypothesis is that when weak models are correctly combined we can obtain more accurate and/or robust models.,Joseph_Rocca_Ensemble_methods,,,,,,,,,,
environment,"Anything affecting a subject system or affected by a subject system through interactions with it, or anything sharing an interpretation of interactions with a subject system",IEEE_Soft_Vocab,,,,,,,,,,
equality of odds,"(Equalized odds). We say that a predictor bY satisfies equalized odds with respect to protected attribute A and outcome Y, if bY and A are independent conditional on Y.",hardt_equality_2016,"The probability of a person in the positive class being correctly assigned a positive outcome and the probability of a person in a negative class being incorrectly assigned a positive outcome should both be the same for the protected and unprotected group members. In other words, the protected and unprotected groups should have equal rates for true positives and false positives.","Mehrabi,_Ninareh",,,,,,,,
equality of opportunity,(Equal opportunity). We say that a binary predictor bY satisfies equal opportunity with respect to A and Y if Pr{bY = 1 | A = 0; Y = 1} = Pr{bY = 1 | A = 1; Y = 1}.,hardt_equality_2016,"The probability of a person in positive class being assigned to a positive outcome should be equal for both protected and unprotected group members. In other words, the protected and unprotected groups should have equal true positive rates.","Mehrabi,_Ninareh",,,,,,,,
error,The difference between the observed value of an index and its “true” value. Errors maybe random or systematic. Random errors are generally referred to as “errors”. Systematic errors are called “biases”.,OECD,"Difference between a computed, observed, or measured value or condition and the true, specified, or theoretically correct value or condition.",IEEE_Soft_Vocab,measured quantity value minus a reference quantity value,"aime_measurement_2022, citing ISO/IEC Guide 99",,,,,,
error propagation,the way in which uncertainties in the variables affect the uncertainty in the calculated results.,Dorf_2018,,,,,,,,,propgation of uncertainty; proprgation of error,
ethics,"definition 1a: ""a set of moral principles : a theory or system of moral values""; definition 1b: ""the principles of conduct governing an individual or a group""; definition 1c: ""a consciousness of moral importance""; definition 1d: ""a guiding philosophy""; definition 2: ""a set of moral issues or aspects (such as rightness)""; definition 3: ""the discipline dealing with what is good and bad and with moral duty and obligation""",Merriam-Webster_ethic,"n.
1. the branch of philosophy that investigates both the content of moral judgments (i.e., what is right and what is wrong) and their nature (i.e., whether such judgments should be considered objective or subjective). The study of the first type of question is sometimes termed normative ethics and that of the second metaethics. Also called moral philosophy.

2. the principles of morally right conduct accepted by a person or a group or considered appropriate to a specific field. In psychological research, for example, proper ethics requires that participants be treated fairly and without harm and that investigators report results and findings honestly. See code of ethics; professional ethics; research ethics. —ethical adj.",APA_ethics,,,,,,,,
ethics by design,"An approach to technology ethics and a key component of responsible innovation that aims to integrate ethics in the design and development stage of the technology. Sometimes formulated as ""embedding values in design."" Similar terms are ""value-sensitive design"" and ""ethically aligned design.""",AI_Ethics_Mark_Coeckelbergh,,,,,,,,,,
evaluation,"(1) systematic determination of the extent to
which an entity meets its specified criteria; (2) action that assesses the value of
something","aime_measruement_2022, citing ISO/IEC 24765",,,,,,,,,"Test, Evaluation, Verification and Validation (TEVV)",
evasion,"In Evasion Attacks, the adversary solves a constrained optimization problem to find a small input perturbation that causes a large change in the loss function and results in output misclassification. ",tabassi_adversarial_2019,,,,,,,,,,
example,"definition 1: ""one that serves as a pattern to be imitated or not to be imitated""; definition 3: ""one that is representative of all of a group or type""; definition 4: ""a parallel or closely similar case especially when serving as a precedent or model""; definition 5: ""an instance (such as a problem to be solved) serving to illustrate a rule or precept or to act as an exercise in the application of a rule""",Merriam-Webster_example,,,,,,,,,,
exception,"An event that occurs during the performance of the process that causes a diversion from the normal flow of the process. Exceptions are generated by an unanticipated event within a process due to an undefined or unknown input, undefined or unexpected outcome, or unforeseen sequencing of a task or event.",IEEE_Guide_IPA,,,,,,,,,,
execute,"To carry out a plan, a task command, or another instruction",SP1011,"To carry out an instruction, process, or computer program; directing, managing, performing, and accomplishing the project work, providing the deliverables, and providing work performance information.",IEEE_Soft_Vocab,,,,,,,,
executive,one that exercises administrative or managerial control,Merriam-Webster_executive,,,,,,,,,,
ex-nomination,"Ex-nomination is the harm of eliminating social identity by almost ignoring its existence. This term comes from Barthes where he coined it to describe what the bourgeoisie do to hide their name and identity by not referring to themselves as such to naturalize bourgeois ideology. This can show up in some of the same examples as mentioned above, as ex-nomination can present itself in technology not recognizing a certain class of people with facial recognition technology or by having implicit biases towards certain adjectives to describe certain classes","Blank,_Abagayle_Lee",,,,,,,,,,
experiment,"a series of observations conducted under controlled conditions to study a relationship with the purpose of drawing causal inferences about that relationship. An experiment involves the manipulation of an independent variable, the measurement of a dependent variable, and the exposure of various participants to one or more of the conditions being studied. Random selection of participants and their random assignment to conditions also are necessary in experiments. ",apa_experiment_2023,"A study of a fundamental physical process by the use of one or more computer simulators. Like empirical experiments, input variables (factors) are systematically changed to assess their impact upon simulator outputs (responses). Unlike empirical experiments, the simulator responses are deterministic, and this has implications: Computer experiments can appropriately have their factors with intermediate levels and the scope, especially the number of runs, can be more ambitious. Further, modeling methods based on interpolators (especially kriging) emerge as a viable approach. Good practice is to use Latin hypercubes for computer experiments, and advanced nonparametric modeling methods such as kriging, neural networks, and multivariate adaptive regression splines (MARS) in the data analysis stage. Important applications of computer experimental methods are for determining process optima and for evaluating process tolerances.",nist_statistics_2012,,,,,,,,
expert system,"A form of AI that attempts to replicate a human's expertise in an area, such as medical diagnosis. It combines a knowledge base with a set of hand-coded rules for applying that knowledge. Machine-learning techniques are increasingly replacing hand coding.","Hutson,_Matthew",Intelligent computer program that uses knowledge and inference procedures to solve problems that are difficult enough to require significant human expertise for their solution.,"Reznik,_Leon",An expert system is an intelligent computer program that uses knowledge and inference procedures to solve problems that are difficult enough to require significant human expertise for their solution.,OECD,Computer system that provides for expertly solving problems in a given field or application area by drawing inferences from a knowledge base developed from human expertise.,IEEE_Soft_Vocab,"A computer system emulating the decision-making ability of a human expert through the use of reasoning, leveraging an encoding of domain-specific knowledge most commonly represented by sets of if-then rules rather than procedural code. The term “expert system” was used largely during the 1970s and ’80s amidst great enthusiasm about the power and promise of rule-based systems that relied on a “knowledge base” of domain-specific rules and rule-chaining procedures that map observations to conclusions or recommendations.",NSCAI,,
expertise,The accumulation of specialized knowledge is often called expertise. Passive expertise is a type of knowledge-based specialization that arises from experiences in life and one's position in a society or culture. Formal expertise is the result of a self-selection of a domain of knowledge that is mastered deliberately and for which there are clear benchmarks of success.,Schneider_McGrew_in_Flanagan_McDonough_2018,,,,,,,,,,
explainability,"The ability to provide a human interpretable explanation for a machine learning prediction and produce insights about the causes of decisions, potentially to line up with human reasoning. ",NISTIR_8269_Draft,"Within the context of AI, the extent to which AI decisioning processes and outcomes are reasonably understood.",Comptroller_Office,"The ability to explain or be explained. In the context of ethics, it refers to the ability to explain to others why you have done something or why you have made a decision; this is part of what it means to be responsible.",AI_Ethics_Mark_Coeckelbergh,"A characteristic of an AI system in which there is provision of accompanying evidence or reasons for system output in a manner that is meaningful or understandable to individual users (as well as to developers and auditors) and reflects the system’s process for generating the output (e.g., what alternatives were considered, but not proposed, and why not).",NSCAI,,,interpretability,
explainable artificial intelligence (XAI),"XAI seeks to make AI more understandable and interpretable, and therefore trustworthy. One of the complaints about artificial intelligence is the lack of transparency in how it operates. Many algorithm developers don’t reveal the data that go into applications or how various factors are weighted and analyzed. That leads to a situation where outsiders cannot understand or explain how AI reached the outcome or decision that it did. That lack of explainability can lead people to suspect the worst about AI, and thus not trust AI in general or certain AI applications in particular. XAI seeks to help describe either the overall function of AI or the specific way it reaches decisions.",Brookings_Institution,"AI that can explain to humans its actions, decisions, or recommendations, or can provide sufficient information about how it came to its result.",AI_Ethics_Mark_Coeckelbergh,,,,,,,,
explainer,Functionality for providing details on or causes for fairness metric results.,AI_Fairness_360,,,,,,,,,,
explanation,Systems deliver accompanying evidence or reason(s) for all outputs.,NISTIR_8269_Draft,"The explanation principle obligates AI systems to supply evidence, support, or reasoning for each output.",NISTIR_8312,,,,,,,,
exploratory,Exploratory Data Analysis (EDA) is an approach/philosophy for data analysis that employs a variety of techniques (mostly graphical) to 1. maximize insight into a data set; 2. uncover underlying structure; 3. extract important variables; 4. detect outliers and anomalies; 5. test underlying assumptions; 6. develop parsimonious models; and 7. determine optimal factor settings.,nist_statistics_2012,,,,,,,,,,
external validity,A study has external validity to the degree that its results can be extended (generalized) beyond the limited research setting and sample in which they were obtained,bordens_Research_2011,"the extent to which the results of research or testing can be generalized beyond the sample that generated them. The more specialized the sample, the less likely will it be that the results are highly generalizable to other individuals, situations, and time periods.",APA_external_validity,,,,,,,,
facial recognition (FR),"A technology for identifying specific people based on pictures or videos. It operates by analyzing features such as the structure of the face, the distance between the eyes, and the angles between a person’s eyes, nose, and mouth. It is controversial because of worries about privacy invasion, malicious applications, or abuse by government or corporate entities. In addition, there have been well-documented biases by race and gender with many facial recognition algorithms.",Brookings_Institution,"Records the spatial geometry of distinguishing features of the face. Different vendors use different methods of facial recognition, however, all focus on measures of key features of the face.",Woodward,"Face recognition algorithms, however, have no built-in notion of a particular person. They are not built to identify particular people; instead they include a face detector followed by a feature extraction algorithm that converts one or more images of a person into a vector of values that relate to the identity of the person. The extractor typically consists of a neural network that has been trained on ID-labeled images available to the developer. In operations, they act as generic extractors of identity-related information from photos of persons they have usually never seen before. Recognition proceeds as a differential operator: Algorithms compare two feature vectors and emit a similarity score. This is a vendor-defined numeric value expressing how similar the parent faces are. It is compared to a threshold value to decide whether two samples are from, or represent, the same person or not. Thus, recognition is mediated by persistent identity information stored in a feature vector (or “template”).",NISTIR_8280,,,,,,
fair-washing,promoting the false perception that a machine learning model respects some ethical values,aivodji_fairwashing_2019,,,,,,,,,,
"fairness (another entry for ""algorithmic fairness""?)","""“cultural assumptions” regarding “the regulation of [human] life effected by stated and unstated rules of interaction,” rules that most interactants see as “generally applicable” and “reasonable.” (We have to get the full definition from the book...","Anna Wierzbicka, English: Meaning and Culture (Oxford: Oxford University Press, 2006), 152–54",,,,,,,,,,
fairness metric,A quantification of unwanted bias in training data or models.,AI_Fairness_360,"A mathematical definition of “fairness” that is measurable. Some commonly used fairness metrics include:

equalized odds
predictive parity
counterfactual fairness
demographic parity
Many fairness metrics are mutually exclusive; see incompatibility of fairness metrics.",google_glossary_2023,,,,,,,,
false negative,An example in which the predictive model mistakenly classifies an item as in the negative class.,NSCAI,an outcome where the model incorrectly predicts the negative class.,google_dev_classification-true-false-positive-negative,A false negative is denying an applicant who should be approved,"Varshney,_Kush","1. An instance in which a security tool intended to detect a particular threat fails to do so.
2. Incorrectly classifying malicious activity as benign.",CSRC_false_negative,,,Type II error (in statistics),
false positive,An example in which the model mistakenly classifies an item as in the positive class,NSCAI,an outcome where the model incorrectly predicts the positive class.,google_dev_classification-true-false-positive-negative, A false positive is approving an applicant who should be denied,"Varshney,_Kush","1. An alert that incorrectly indicates that a vulnerability is present.
2. An alert that incorrectly indicates that malicious activity is occurring.
3. An instance in which a security tool incorrectly classifies benign content as malicious.
4. Incorrectly classifying benign activity as malicious.
5. An erroneous acceptance of the hypothesis that a statistically significant event has been observed. This is also referred to as a type 1 error. This is also referred to as a type 1 error. When “health-testing” the components of a device, it often refers to a declaration that a component has malfunctioned – based on some statistical test(s) – despite the fact that the component was actually working correctly.",CSRC_false_positive,,,Type I error (in statistics),
fault tolerance ,The ability of a system or component to continue normal operation despite the presence of hardware or software faults,SP1011,,,,,,,,,,
favorable label,A label whose value corresponds to an outcome that provides an advantage to the recipient. The opposite is an unfavorable label.,AI_Fairness_360,,,,,,,,,,
feature,An attribute containing information for predicting the label.,AI_Fairness_360,,,,,,,,,,
feature extraction,a more general method in which one tries to develop a transformation of the input space onto the lowdimensional subspace that preserves most of the relevant information,khalid_feature_2014,,,,,,,,,,
feature importance,"how important the feature was for the classification performance of the model; 
a measure of the individual contribution of the corresponding feature for a particular classifier, regardless of the
shape (e.g., linear or nonlinear relationship) or direction of the feature effect",saarela_feature_2021,,,,,,,,,,
feature shift,"Unlike joint distribution shift detection, which cannot localize which features caused the shift, we define a new hypothesis test for each feature individually. Naïvely, the simplest test would be to check if the marginal distributions have changed for each feature (as explored by [25]); however, the marginal distribution would be easy for an adversary to simulate (e.g., by looping the sensor values from a previous day). Thus, marginal tests are not sufficient for our purpose. Therefore, we propose to use conditional distribution tests. More formally, our null and alternative hypothesis for the j-th feature is that its full conditional distribution (i.e., its distribution given all other features) has not shifted for all values of the other features.",kulinski_feature_2020,,,,,,,,,,
federated learning,a learning model which addresses the problem of data governance and privacy by training algorithms collaboratively without transferring the data to another location.,Public_Health_and_Informatics_MIE_2021,,,,,,,,,,
feedback loop,"describes the process of leveraging the output of an AI system and corresponding end-user actions in order to retrain and improve models over time. The AI-generated output (predictions or recommendations) are compared against the final decision (for example, to perform work or not) and provides feedback to the model, allowing it to learn from its mistakes.",C3.ai_feedback_loop,,,,,,,,,closed-loop learning,
fitting,Fitting is the process of verifying whether the data item value is in the previously specified interval.,OECD,,,,,,,,,,
firmware,Computer programs and data stored in hardware - typically in read-only memory (ROM) or programmable read-only memory (PROM) - such that the programs and data cannot be dynamically written or modified during execution of the programs.,SP800-37,Combination of a hardware device and computer instructions or computer data that reside as read only software on the hardware device.,IEEE_Soft_Vocab,,,,,,,,
Forecasting ,"Estimate or prediction of conditions and events in the project's future based on information and knowledge available at the time of the forecast. The information is based on the project's past performance and expected future performance, and includes information that could impact the project in the future, such as estimate at completion and estimate to complete.",IEEE_Soft_Vocab,"Predicting the future as accurately as possible, given all of the information available, including historical data and knowledge of any future events that might impact the forecasts.","Hyndman,_Rob",,,,,,,,
four-fifths rule,"a rule of thumb under which [federal enforcement agencies] will generally consider a selection rate for any race, sex, or ethnic group which is less than four-fifths (4/5ths) or eighty percent (80%) of the selection rate for the group with the highest selection rate as a substantially different rate of selection. . . . This ""4/5ths"" or ""80%"" rule of thumb is not intended as a legal definition, but is a practical means of keeping the attention of the enforcement agencies on serious discrepancies in rates of hiring, promotion and other selection decisions.",EEOC_Q&A_Employee_Selection,,,,,,,,,,
fraud detection,"Monitoring the behavior of populations of users in order to estimate, detect, or avoid undesirable behavior.","Kou,_Yufeng",detecting and recognizing fraudulent activities as they enter systems and report them to a system manager.,Behdad,,,,,,,,
fully autonomous ,"Accomplishes its assigned mission, within a defined scope, without human intervention while adapting to operational and environmental conditions",SP1011,,,,,,,,,,
generative adversarial network (GAN),"Generative Adversarial Networks, or GANs for short, are an approach to generative modeling using deep learning methods, such as convolutional neural networks. Generative modeling is an unsupervised learning task in machine learning that involves automatically discovering and learning the regularities or patterns in input data in such a way that the model can be used to generate or output new examples that plausibly could have been drawn from the original dataset.","Brownlee,_Jason","A pair of jointly trained neural networks that generates realistic new data and improves through competition. One net creates new examples (fake Picassos, say) as the other tries to detect the fakes.","Hutson,_Matthew","Generative adversarial networks (GANs) consist of two competing neural networks—a generator network that tries to create fake outputs (such as pictures), and a discriminator network that tries to determine whether the outputs are real or fake. A major advantage of this structure is that GANs can learn from less data than other deep learning algorithms.",CRS_AI,"An approach to training AI models useful for applications like data synthesis, augmentation, and compression where two neural networks are trained in tandem: one is designed to be a generative network (the forger) and the other a discriminative network (the forgery detector). The objective is for each network to train and better itself off the other, reducing the need for big labeled training data.",NSCAI,,,,
generative artificial intelligence,"[a kind of artificial intelligence] capable of generating new content such as code, images, music, text, simulations, 3D objects, videos, and so on. It is considered an important part of AI research and development, as it has the potential to revolutionize many industries, including entertainment, art, and design.",Arham_Islam_History_2023,"describes algorithms (such as ChatGPT) that can be used to create new content, including audio, code, images, text, simulations, and videos.",McKinsey_generative_AI,,,,,,,,
global,[An approach that] [t]ries to understand the model as a whole. ,arun_opportunities_2020,A global explanation produces a model that approximates the non-interpretable model.,NISTIR_8312_Full,,,,,,,,
governance,"The actions to ensure stakeholder needs, conditions, and options are evaluated to determine balanced, agreed-upon enterprise objectives; setting direction through prioritization and decision-making; and monitoring performance and  ompliance against agreed-upon directions and objectives. AI governance may include policies on the nature of AI applications developed and deployed versus those limited or withheld.",NSCAI,"A framework of policies, rules, and processes for ensuring direction, management and accountability.",SP1270,,,,,,,,
graph,Diagram that represents the variation of a variable in comparison with that of one or more other variables. Diagram or other representation consisting of a finite set of nodes and internode connections called edges or arcs.,IEEE_Soft_Vocab,"A graph (sometimes called an undirected graph to distinguish it from a directed graph, or a simple graph to distinguish it from a multigraph) is a pair G = (V, E), where V is a set whose elements are called vertices (singular: vertex), and E is a set of paired vertices, whose elements are called edges (sometimes links or lines).",wikipedia_graph_2023,,,,,,,,
graphical processing unit (GPU),A specialized chip capable of highly parallel processing. GPUs are well-suited for running machine learning and deep learning algorithms. GPUs were first developed for efficient parallel processing of arrays of values used in computer graphics. Modern-day GPUs are designed to be optimized for machine learning.,NSCAI,,,,,,,,,,
graphical user interface (GUI),"A GUI is a type of computer human interface on a computer. It solves the blank screen problem that confronted early computer users.  These early users sat down in front of a computer and faced a blank screen, with only a prompt. The computer gave the user no indication what the user was to do next. GUIs are an attempt to solve this blank screen problem. At a conceptual level, a computer human interface is a ""means by which people and computers communicate with each other""",jansen_graphical_1998,,,,,,,,,,
ground truth,information provided by direct observation as opposed to information provided by inference,Collins_Dictionary_ground_truth,"value of the target variable for a particular item of labelled
input data","aime_measurement_2022, citing ISO/IEC 22989","In most accounts of supervised (machine) learning, the ground truth is considered to be the “dependent variable” that is predicted by a collection of features (independent variables).","Muller,_Michael ",,,,,,
group fairness,The goal of groups defined by protected attributes receiving similar treatments or outcomes.,AI_Fairness_360,Treat different groups equally ,"Mehrabi,_Ninareh",,,,,,,,
hacker,Unauthorized user who attempts to or gains access to an information system.,"Reznik,_Leon",Technically sophisticated computer enthusiast who uses his or her knowledge and means to gain unauthorized access to protected resources.,IEEE_Soft_Vocab,,,,,,,,
hallucination,"generated content that is nonsensical or unfaithful to the provided source content[; ...] there are two main types of hallucinations, namely intrinsic hallucination and extrinsic hallucination. [An intrinsic hallucination is a] generated output that contradicts the source content; [an extrinsic hallucination is a] generated output that cannot be verified from the source content (i.e., output can neither be supported nor contradicted by the source).",Survey_of_Hallucination_in_NLG,when a bot confidently says something that is not true.,Liam_Tung_2022_Meta_hallucination,,,,,,,,
hardware,"Physical equipment used to process, store, or transmit computer programs or data",IEEE_Soft_Vocab,,,,,,,,,,
harm,"An undesired outcome [whose] cost exceeds some threshold[; ...] the key points in the definition of safety are that: costs have to be sufficiently high in some human sense for events to be harmful, and that safety involves reducing both the probability of expected harms and the possibility of unexpected harms.",Engineering_safety_in_machine_learning,"to damage, injure or hurt.",Black's_Law_Dictionary_harm,,,,,,,,
harmful bias,"Harmful bias can be either conscious or unconscious. Unconscious, also known as implicit bias, involves associations outside conscious awareness that lead to a negative evaluation of a person on the basis of characteristics such as race, gender, sexual orientation, or physical ability.3,14 Discrimination is behavior; discriminatory actions perpetrated by individuals or institutions refer to inequitable treatment of members of certain social groups that results in social advantages or disadvantages",humphrey_addressing_2020,,,,,,,,,,
harms of allocation,"unfairly assigned opportunities or resources due to algorithmic intervention[; ...] when a system [distributes] or withholds certain groups an opportunity or a resource. [They are] immediate, easily quantifiable, discrete, and transactional.",Lim_Swee_Kiat_harms,,,,,,,,,,
harms of bias,a skew that produces a type of harm[; ...] further classifie[d] into harms of allocation and harms of representation.,Lim_Swee_Kiat_harms,,,,,,,,,,
harms of representation,"algorithmically filtered depictions that are discriminatory. [They are] long term, difficult to formalize, diffuse, and cultural.",Lim_Swee_Kiat_harms,,,,,,,,,content harms,
human activity recognition (HAR),the art of identifying and naming activities using [a]rtificial [i]ntelligence (AI) from the gathered activity raw data by utilizing various sources (so-called devices).,Gupta_et_al_HAR_2022,,,,,,,,,,
human-assisted,"The type of human-robot-interaction that that refers to situations during which human interactions are needed at the level of detail of task plans, i.e., during the execution of a task",SP1011,,,,,,,,,,
human capital management,"A C-suite business discipline that develops enterprise human capital strategies and ensures the human capital portfolio is effectively managed. Human capital management provides decision support by combining business and workforce intelligence to the development of enterprise human capital strategies: how to leverage people and their ideas effectively to achieve bottom-line business goals such as growing the business, increasing market share, margins, share price, and decreasing SG&A costs, as well as improving business processes, benefiting from technology investments, and increasing productivity.

","Merritt,_Linda",,,,,,,,,,
human-computer interaction (HCI),methods and approaches for designing and architecting user interfaces and the interactions between humans and computer (or information) technology.,Poore_Lawrence_ARLIS_2023-01,,,,,,,,,,
human-cognitive bias,"Human-cognitive biases relate to how an individual or group perceives AI system information to make a decision or fill in missing information, or how humans think about purposes and functions of an AI system. Human biases are omnipresent in decision-making processes across the AI lifecycle and system use, including the design, implementation, operation, and maintenance of AI.",NIST_AI_RMF_1.0,"Systematic error in judgment and decision-making common to all human beings which can be due to cognitive limitations, motivational factors, and/or adaptations to natural environments.",,,,,,,,,
human-enabled machine learning,"Detection, correlation, and pattern recognition generated through machine-based observation of human operation of software systems capturing successful or unsuccessful operations to enable the creation of a useful predictive analytics capability. ",IEEE_Guide_IPA,,,,,,,,,,
human experiment,anything done to an individual to learn how it will affect [that person].,Bassiouni_Baffes_Evrard,,,,,,,,,,
human-in-the-loop,An AI system that requires human interaction.,DOD_Modeling_and_Simulation_Glossary,,,,,,,,,,
human-machine teaming (HMT),"The ability of humans and AI systems to work together to undertake complex, evolving tasks in a variety of environments with seamless handoff both ways between human and AI team members. Areas of effort include developing effective policies for controlling human and machine initiatives, computing methods that ideally complement people, methods that optimize goals of teamwork, and designs that enhance human-AI interaction.",NSCAI,"methods and approaches for coordinating the functions and actions of (semi) autonomous machine capabilities and human users, which are granted equal weighting.",Poore_Lawrence_ARLIS_2023-01,,,,,,,human-AI teaming,
human-operator-intervention ,The need for human interaction in a normally fully autonomous behavior due to some extenuating circumstances.,SP1011,,,,,,,,,,
human subjects,"a living individual about whom an investigator (whether professional or student) conducting research: (i) Obtains information or biospecimens through intervention or interaction with the individual, and uses, studies, or analyzes the information or biospecimens; or (ii) Obtains, uses, studies, analyzes, or generates identifiable private information or identifiable biospecimens.",45_CFR_46_2018_Requirements_(2018_Common_Rule),,,,,,,,,participant,
human system integration (HSI),methods and approaches for testing and optimizing all human-related considerations from a “whole-system” or “system-of-systems” level.,Poore_Lawrence_ARLIS_2023-01,,,,,,,,,,
human values,"Artificial intelligence systems use data we generate in our daily lives and as such are a mirror of our interests, weaknesses, and differences. Artificial intelligence, like any other technology, is not value-neutral. Understanding the values behind the technology and deciding on how we want our values to be incorporated in AI systems requires that we are also able to decide on how and what we want AI to mean in our societies. It implies deciding on ethical guidelines, governance policies, incentives, and regulations. And it also implies that we are aware of differences in interests and aims behind AI systems developed by others according to other cultures and principles. *See note.
",Virginia_Dignum_Responsibility_and_Artificial_Intelligence,The quantity which a function f takes upon application to a given quantity.,wolfram_math_2022,"what is important to people in their lives, with a focus on ethics and morality. Work to date has emphasized human well-being, dignity, and justice",Batya_Friedman_VSD_Introduction,,,,,,
hyperparameters,"the parameters that are used to either configure a ML model (e.g., the penalty parameter C in a support vector machine, and the learning rate to train a neural network) or to specify the algorithm used to minimize the loss function (e.g., the activation function and optimizer types in a neural network, and the kernel type in a support vector machine).",On_Hyperparameter_Optimization,,,,,,,,,,
hypothesis testing,A term used generally to refer to testing significance when specific alternatives to the null hypothesis are considered.,OECD,,,,,,,,,,
impact,the force of impression of one thing on another : a significant or major effect,Merriam-Webster_impact,"a powerful effect that something, especially something new, has on a situation or person",cambridge_impact_2023,,,,,,,,
impact assessment,"a risk management tool that seeks to ensure an organization has sufficient considered a system's relative benefits and costs before implementation. In the context of AI, an impact assessment helps to answer a simple question: alongside this system’s intended use, for whom could it fail?",Bipartisan_Policy_Center_impact_assessments,,,,,,,,,,
impersonation,A malicious individual is able to impersonate a legitimate data subject to the data controller. The adversary forges a valid access request and goes through the identity verification enforced by the data controller. The data controller sends to the adversary the data of a legitimate data subject. Defeating impersonation is the primary objective of any authentication protocol. The result of this attack is a data breach (e.g. blaggers [sic] pretend to be someone they are not in order to wheedle out the information they are seeking obtaining information illegaly which they then sell for a specified price).,Security_Analysis_of_Subject_Access,,,,,,,,,,
in-processing,Techniques that modify the algorithms in order to mitigate bias during model training. Model training processes could incorporate changes to the objective (cost) function or impose a new optimization constraint.,SP1270,Techniques that try to modify and change state-of-the-art learning algorithms to remove discrimination during the model training process.,"Mehrabi,_Ninareh",,,,,,,,
in-processing algorithm,A bias mitigation algorithm that is applied to a model during its training.,AI_Fairness_360,,,,,,,,,,
incident,"a situation in which AI systems caused, or nearly caused, real-world harm.",AI_Incident_Database,the occurrence of a technical event that affects the integrity of a Product and/or Model.,FBPML_Wiki,"an alleged harm or near harm event to people, property, or the environment where an AI system is implicated.",AI_Incident_Editors,"Adverse event(s) in a computer system or networks caused by a failure of a security mechanism, or an attempted or threatened breach of these mechanisms.","Hasan,_Raza",,,,
incident response,"a public official response to an incident ... from an entity (i.e. company, organization, individual) allegedly responsible for developing or deploying the AI or AI system involved in said incident.",AIID_incident_response,,,,,,,,,,
inclusion,"a cultural and environmental feeling of belonging and sense of uniqueness. It represents the extent to which employees feel valued, respected, encouraged to fully participate, and able to be their authentic selves.",Seth_Boden_2020,"describ[ing] the active, intentional, and ongoing engagement with diversity -- in people, in the curriculum, in the co-curriculum, and in communities (e.g. intellectual, social, cultural, geographic) with which individuals might connect.",GWU_diversity_and_inclusion,"a state of being valued, respected and supported. It’s about focusing on the needs of every individual and ensuring the right conditions are in place for each person to achieve his or her full potential. Inclusion should be reflected in an organization’s culture, practices and relationships that are in place to support a diverse workforce. Inclusion is the process of creating a working culture and environment that recognizes, appreciates, and effectively utilizes the talents, skills, and perspectives of every employee; uses employee skills to achieve the agency’s objectives and mission; connects each employee to the organization; and encourages collaboration, flexibility, and fairness. We define inclusion as a set of behaviors (culture) that encourages employees to feel valued for their unique qualities and experience a sense of belonging.",HUD_diversity_and_inclusion,"the recognition, appreciation, and use of the talents and skills of employees of all backgrounds. ",EO_DEIA_2021,,,diversity,
independence,"Of software quality assurance (SQA), situation in which SQA is free from technical, managerial, and financial influences, intentional or unintentional",IEEE_Soft_Vocab,"Two events are independent if the occurrence of one event does not affect
the chances of the occurrence of the other event. The mathematical
formulation of the independence of events A and B is the probability of the
occurrence of both A and B being equal to the product of the probabilities
of A and B (i.e., P(A and B) = P(A)P(B))",nist_800_2010,"In simple terms, inclusion is getting the mix to work together.",,,,,,,
individual fairness,The goal of similar individuals receiving similar treatments or outcomes.,AI_Fairness_360,Give similar predictions to similar individuals,"Mehrabi,_Ninareh","A fairness metric
that checks whether similar individuals are classified similarly",aime_measurement_2022 citing Machine Learning Glossary by Google,,,,,,
inference,"The stage of ML in which a model is applied to a task. For example, a classifier model produces the classification of a test sample.",NISTIR_8269_Draft,,,,,,,,,,
information input component,One of the three components of a model. This component delivers assumptions and data to the model.,Comptroller_Office,,,,,,,,,,
information security,"preservation of confidentiality, integrity and availability of information; in addition, other properties, such as authenticity, accountability, non-repudiation, and reliability can also be involved.",ISO/IEC_TS_5723:2022(en),,,,,,,,,,
input,Data received from an external source,IEEE_Soft_Vocab,,,,,,,,,,
insider attack,"Those who are within [an] organisation may have authorised access to vast amounts of sensitive company records that are essential for maintaining competitiveness and market position, and knowledge of information services and procedures that are crucial for daily operations. . . .[and] should an individual choose to act against the organisation, then with their privileged access and their extensive knowledge, they are well positioned to cause serious damage.",IEEE_Caught_in_the_Act,,,,,,,,,,
in silico,carrying out some experiment by means of a computer simulation,World_Wide_Words_In_silico,,,,,,,,,computer simulation testing,
instance,"Discrete, bounded thing with an intrinsic, immutable, and unique identity. Individual occurrence of a type",IEEE_Soft_Vocab,"A single object of the world from which a model will be learned, or on which a model will be used (e.g., for prediction).","Kohavi,_Ron",,,,,,,,
instance weight  ,A numerical value that multiplies the contribution of a data point in a model.,AI_Fairness_360,,,,,,,,,,
integrity,"Degree to which a system, product, or component prevents unauthorized access to, or modification of, computer programs or data.",IEEE_Soft_Vocab,"Guarding against improper information modification or destruction, and includes ensuring information non-repudiation and authenticity.",CSRC,"The property whereby information, an information system, or a component of a system has not been modified or destroyed in an unauthorized manner.",CISA,"<data> property whereby data have not been altered in an unauthorized manner since they were created, transmitted, or stored; <systems> property of accuracy and completeness",ISO/IEC_TS_5723:2022(en),"the quality of moral consistency, honesty, and truthfulness with oneself and others.",APA_integrity,,
intelligent process automation,"A preconfigured software instance that combines business rules, experience- based context determination logic, and decision criteria to initiate and execute multiple interrelated human and automated processes in a dynamic context. The goal is to complete the execution of a combination of processes, activities, and tasks in one or more unrelated software systems that deliver a result or service with minimal or no human intervention. ",IEEE_Guide_IPA,,,,,,,,,,
interaction,Action that takes place with the participation of the environment of the object.,IEEE_Soft_Vocab,,,,,,,,,,
internal validity,The ability of your research design to adequately test your hypotheses,bordens_research_2010,"the degree to which a study or experiment is free from flaws in its internal structure and its results can therefore be taken to represent the true nature of the phenomenon. In other words, internal validity pertains to the soundness of results obtained within the controlled conditions of a particular study, specifically with respect to whether one can draw reasonable conclusions about cause-and-effect relationships among variables.",APA_internal_validity,,,,,,,,
interoperability,The ability of software or hardware systems or components to operate together successfully with minimal effort by end user,SP1011,"Degree to which two or more systems, products or components can exchange information and use the information that has been exchanged.",IEEE_Soft_Vocab,"The ability for tools to work together in execution, communication, and data exchange under specific conditions.",NIST_1500,,,,,,
interpretability,The ability to understand the value and accuracy of system output. Interpretability refers to the extent to which a cause and effect can be observed within  a system or to which what is going to happen given a change in input or algorithmic parameters can be predicted. ,NSCAI,The ability to explain or to present an ML model’s reasoning in understandable terms to a human,"aime_measurement_2022, citing Machine Learning Glossary by Google",,,,,,,explainability,
interpretable model,"An interpretable machine learning model obeys a domain-specific set of constraints to allow it (or its predictions, or the data) to be more easily understood by humans. These constraints can differ dramatically depending on the domain.",rudin_interpretable_2022,,,,,,,,,,
intervenability,the property that intervention is possible concerning all ongoing or planned privacy relevant data processing[; ...] the data subjects themselves should be able to intervene with regards to the processing of their own data ... [to ensure] that data subjects have the ability to control how their data is processed and by whom.,Covert_et_al,,,,,,,,,,
kill switch,a form of safety mechanism used to completely shut off a device in case of an emergency situation where it cannot be shut off using the normal process or if immediate shut off is required.,Techopedia_kill_switch,,,,,,,,,,
knowledge,"The sum of all information derived from diagnostic, descriptive, predictive, and prescriptive analytics embedded in or available to or from a cognitive computing system.",IEEE_Guide_IPA,"<artificial intelligence> abstracted information
about objects, events, concepts or rules, their relationships and properties, organized
for goal-oriented systematic use","aime_measurement_2022, citinig ISO/IEC 22989",,,,,,,,
label,A value corresponding to an outcome.,AI_Fairness_360,target variable assigned to a sample,"aime_measurement_2022, citing ISO/IEC 22989",,,,,,,,
label shift,"Under label shift, the label distribution p(y) might change but the class-conditional
distributions p(x|y) do not. ... We work with the label shift assumption, i.e., ps(x|y) = pt(x|y)",saurabh_label_2020,,,,,,,,,,
large language model (LLM),"a class of language models that use deep-learning algorithms and are trained on extremely large textual datasets that can be multiple terabytes in size. LLMs can be classed into two types: generative or discriminatory. Generative LLMs are models that output text, such as the answer to a question or even writing an essay on a specific topic. They are typically unsupervised or semi-supervised learning models that predict what the response is for a given task. Discriminatory LLMs are supervised learning models that usually focus on classifying text, such as determining whether a text was made by a human or AI.",AI_Assurance_2022,,,,,,,,,language model,
language model,A language model is an approximative description that captures patterns and regularities present in natural language and is used for making assumptions on previously unseen language fragments.,"Gustavii,_Ebba ",,,,,,,,,large language model (LLM),
learning,A procedure in artificial intelligence by which an artificial intelligence program improves its performance by gaining knowledge.,Dennis_Mercadal,"the acquisition of novel information, behaviors, or abilities after practice, observation, or other experiences, as evidenced by change in behavior, knowledge, or brain function. Learning involves consciously or nonconsciously attending to relevant aspects of incoming information, mentally organizing the information into a coherent cognitive representation, and integrating it with relevant existing knowledge activated from long-term memory.",APA_learning,,,,,,,,
least privilege,The principle that a security architecture should be designed so that each entity is granted the minimum system resources and authorizations that the entity needs to perform its function.,CSRC,The security objective of granting users only those accesses they need to perform their official duties.,SP-800-12,,,,,,,,
lemmatization,the process of grouping together the different inflected forms of a word so they can be analyzed as a single item.,Artasanchez_Joshi_AI_with_Python,"in natural language processing[, ...] working with words according to their root lexical components",Techopedia_lemmatization,grouping together words with the same root or lemma but with different inflections or derivatives of meaning so they can be analyzed as one item.,Techslang_lemmatization,the grouping together of different forms of the same word.,TechTarget_lemmatization,,,,
linear model,[a supervised learning algorithm that uses] a simple formula to find a best-fit line through a set of data points.,dataiku_ML_and_linear_models,"(linear) An operator L^~ is said to be linear if, for every pair of functions f and g and scalar t,

 L^~(f+g)=L^~f+L^~g 
and

 L^~(tf)=tL^~f. ",wolfram_mathworld_2022,,,,,,,,
local,"Mainly focus on
 explanation of individual
 data instances. Generates 
one explanation map g per
data x \in X.
",arun_opportunities_2020,A local explanation explains a subset of decisions or is a per-decision explanation.,NISTIR_8312_Full,,,,,,,,
localization,Creation of a national or specific regional version of a product.,IEEE_Soft_Vocab,,,,,,,,,,
logistic model,"(logistic equation) The continuous version of the logistic model is described by the differential equation
 (dN)/(dt)=(rN(K-N))/K, 	
(1)
where r is the Malthusian parameter (rate of maximum population growth) and K is the so-called carrying capacity (i.e., the maximum sustainable population). Dividing both sides by K and defining x=N/K then gives the differential equation
 (dx)/(dt)=rx(1-x), 	
(2)
which is known as the logistic equation and has solution
 x(t)=1/(1+(1/(x_0)-1)e^(-rt)). 	
(3)
The function x(t) is sometimes known as the sigmoid function.",wolfram_mathworld_2022,,,,,,,,,,
machine learning,A general approach for determining models from data.,AI_Fairness_360,Machine Learning is the study of computer algorithms that improve automatically through experience.,"Mitchell,_Tom",Machine learning is based on algorithms that can learn from data without relying on rules-based programming.,Pyle_and_San_José,"The study or the application of computer algorithms that improve automatically through experience. Machine learning algorithms build a model based on training data in order to perform a specific task, like aiding in prediction or decision-making processes, without necessarily being explicitly programmed to do so",NSCAI,A subcategory of artificial intelligence; a method of designing a sequence of actions to solve a problem that optimizes automatically through experience and with limited or no human intervention.,Comptroller_Office,,
machine observation,Machine detection and interpretation of relevant and meaningful events and conditions that impact operation of the computer system itself or other dependent mechanisms or processes essential to the purpose of the system.,IEEE_Guide_IPA,,,,,,,,,,
malicious actor,See bad actor.,,,,,,,,,,,
malware,"Hardware, firmware, or software that is intentionally included or inserted in a system for a harmful purpose.","Reznik,_Leon",Software that compromises the operation of a system by performing an unauthorized function or process.,CISA,,,,,,,trojan horse,
materiality,"Refers to the significance of a matter in relation to a set of financial or performance information. If a matter is material to the set of information, then it is likely to be of significance to a user of that information",OECD,,,,,,,,,,
McNamara fallacy,presum[ing] that (A) quantitative models of reality are always more accurate than other models; (B) the quantitative measurements that can be made most easily must be the most relevant; and (C) factors other than those currently being used in quantitative metrics must either not exist or not have a significant influence on success. This flawed approach to reasoning is also known as the quantitative fallacy.,McNamara_Fallacy,,,,,,,,,quantitative fallacy,
measurement,"(Quantitative) (1) act or process of assigning a number or category to an
entity to describe an attribute of that entity; (2) assignment of numbers to
objects in a systematic way to represent properties of the object; (3) use of
a metric to assign a value (e.g., a number or category) from a scale to an
attribute of an entity; (4) set of operations having the object of determining a
value of a measure; (5) assignment of values and labels to aspects of software
engineering work products, processes, and resources plus the models that are
derived from them, whether these models are developed using statistical or
other techniques; (6) figure, extent, or amount obtained by measuring","aime_measurement_2022, citing ISO/IEC 24765","(Qualitative) (1) a way of learning about social reality [...][that uses] approaches [...] to explore, describe, or explain social phenomen[a]; unpack the meaning people ascribe to activities, situations, events, or [artifacts]; build a depth of understanding about some aspect of social life; build ""thick descriptions"" (see Clifford Geertz, 1973) of people in naturalistic settings; explore new or underresearched areas; or make micro-macro links (illuminate connections between individuals-groups and institutional and/or cultural contexts). (2) [approaches that] can make visible and unpick the mechanisms which link particular variables, by looking at the explanations, or accounts, provided by those involved. ",Leavy_OHQR_Intro,"Qualitative measurement engages research methods and techniques to provide information about the nature of phenomenon. Qualitative methods are designed for systematic collection,  organization, description and interpretation of non-numeric (textual, verbal or visual) data (Hammarberg et. al, 2016). Qualitative measurement generally answers questions about why, for whom, when, and how something is (or is not) observed, whereas quantitative measurement answers questions about what is observed.  Elements assessed using qualitative measurement may include contextual norms or meaning, socio-cultural dynamics, individual or collective beliefs, and complex multi-component interactions or interventions (Busetto et. al, 2020).",Hammarberg_2016_Busetto_2020,"Documentation of assumptions and methods used is a foundational element of qualitative measurement, as the choice of single or combined methods is made based on the phenomenon and its context (Russell & Gregory, 2003).  When appropriately paired, qualitative and quantitative measurement can provide corroboration or elaboration, demonstrate use cases, and/or identify conditions for complementarity or contradiction (Brannen, 2005).",Russell_2003_Brannen_2005,,,,
measurement method,generic description of a logical organization of operations used in a measurement,"aime_measurement_2022, citing ISO/IEC Guide 99","logical sequence of operations, described generically, used
in quantifying an attribute with respect to a specified scale","aime_measurement_2022, citing ISO/IEC 24765",,,,,,,,
measurement model,The initial confirmatory factory analysis (CFA) model that underlies the structural model [that] tests the adequacy (as indexed by model fit) of the specified relations whereby indicators are linked to their underlying construct.,Little_2013,"A statistical model that links unobservable theoretical constructs, operationalized as latent variables, and observable properties—i.e., data about the world",jackman_oxford_2008,,,,,,,,
measurability,"ability to assess an attribute of an entity against a metric (note 1: ""measurable"" is the adjective form of ""measurability"")",ISO/IEC_TS_5723:2022(en),,,,,,,,,,
membership inference,"given a machine learning model and a record, determining whether the record was used as part of the model's training dataset or not.",,,,,,,,,,,
metadata,Metadata is data that defines and describes other data.,OECD,Data that describe other data.,IEEE_Soft_Vocab,"Data employed to annotate other data with descriptive information, possibly including their data descriptions, data about data ownership, access paths, access rights, and data volatility.",,,,,,,
metric,defined measurement method and measurement scale,ISO/IEC_TS_5723:2022(en),"(1) quantitative measure of the degree to which a system, component, or process possesses a given attribute; (2) defined measurement method and the measurement scale; c.f., measure in this section above","aime_measurement_2022, citing ISO/IEC 24765",,,,,,,,
minimization,"(Part of the ICO framework for auditing AI) AI systems generally require large amounts of data. However, organisations must comply with the minimisation principle under data protection law if using personal data. This means ensuring that any personal data is adequate, relevant and limited to what is necessary for the purposes for which it is processed. […] The default approach of data scientists in designing and building AI systems will not necessarily take into account any data minimisation constraints. Organisations must therefore have in place risk management practices to ensure that data minimisation requirements, and all relevant minimisation techniques, are fully considered from the design phase, or, if AI systems are bought or operated by third parties, as part of the procurement process due diligence",ICO_data_minimisation,"a data controller should limit the collection of personal information to what is directly relevant and necessary to accomplish a specified purpose. They should also retain the data only for as long as is necessary to fulfil that purpose. In other words, data controllers should collect only the personal data they really need, and should keep it only for as long as they need it. The data minimisation principle is expressed in Article 5(1)(c) of the GDPR and Article 4(1)(c) of Regulation (EU) 2018/1725, which provide that personal data must be ""adequate, relevant and limited to what is necessary in relation to the purposes for which they are processed"".",EDPS_data_minimization,,,,,,,,
mitigation,the process of lessening the severity of stereotypical/unjust associations and disparate model performance.,Arjun_Subramonian_bias_mitigation,,,,,,,,,,
mixed methods,"In mixed methods, the researcher collects and analyzes both qualitative and quantitative data rigorously in response to research questions and hypotheses; integrates the two forms of data and their results; organizes these procedures into specific research designs that provide the logic and procedures for conducting the study; and frames these procedures within theory and philosophy.",Creswell_Clark_mixed_methods,"research in which the inquirer or investigator collects and analyzes data, integrates the findings, and draws inferences using both qualitative and quantitative approaches or methods in a single study or a program of study. ",Lisa_M._Given_SAGE,,,,,,,,
MLOPS,MLOps (machine learning operations) stands for the collection of techniques and tools for the deployment of ML models in production. ,symeonidis_MLOps_2022,,,,,,,,,,
model,A function that takes features as input and predicts labels as output.,AI_Fairness_360,"Machine Learning algorithms and data processing designed, developed, trained and implemented to achieve set outputs, inclusive of datasets used for said purposes unless otherwise stated.",FBPML_Wiki,"A model is a formalised expression of a theory or the causal situation which is regarded as having generated observed data. In statistical analysis the model is generally expressed in symbols, that is to say in a mathematical form, but diagrammatic models are also found. The word has recently become very popular and possibly somewhat over-worked.",OECD,"A conceptual, mathematical, or physical representation of phenomenon observed in a system of ideas, events, or processes. In computationally-based models used in AI, phenomenon are often abstracted for mathematical representation, which means that characteristics that can not be represented mathematically may not be captured in the model.",SP1270,"A quantitative method, system, or approach that applies statistical, economic, financial, or mathematical theories, techniques, and assumptions to process input data into quantitative estimates. A model consists of three components: an information input component, which delivers assumptions and data to the model; a processing component, which transforms inputs into estimates; and a reporting component, which translates the estimates into useful business information.",Comptroller_Office,,
model assertion,Model assertions are arbitrary functions over a model’s input and output that indicate when errors may be occurring,"Kang,_Daniel ",,,,,,,,,,
model card,"short documents accompanying trained machine learning models that provide benchmarked evaluation in a variety of conditions, such as across different cultural, demographic, or phenotypic groups (e.g., race, geographic location, sex, Fitzpatrick skin type) and intersectional groups (e.g., age and race, or sex and Fitzpatrick skin type) that are relevant to the intended application domains. [They] also disclose the context in which models are intended to be used, details of the performance evaluation procedures, and other relevant information.",Model_Cards_for_Model_Reporting,,,,,,,,,,
model debugging,Model debugging aims to diagnose a model’s failures.,Jain_Saachi,,,,,,,,,,
model decay,Model decay depicts that the performance of the model is degrading over time,"Nayak,_Pragati",,,,,,,,,,
model editing,"An area of research that aims to enable fast, data-efficient updates to a pre-trained base model’s behavior for only a small region of the domain, without damaging model performance on other inputs of interest","Mitchell,_Eric",,,,,,,,,,
model extraction,"Adversaries maliciously exploiting the query interface to steal the model. More precisely, in a model extraction attack, a good approximation of a sensitive or proprietary model held by the server is extracted (i.e. learned) by a dishonest user who interacts with the server only via the query interface.","Chandrasekaran,_Varun ",,,,,,,,,model inversion; model stealing,
model governance,"Model Governance is the name for the overall internal framework of a firm or organization that controls the processes for Model Development, Model Validation and Model Usage, assign responsibilities and roles etc.",open_risk_2022,,,,,,,,,,
model inventory,"in the context of Risk Management, [...] a database/[management information system] developed for the purpose of aggregating quantitative model related information that is in use by a firm or organization.",ORM_model_inventory,,,,,,,,,,
model overlay,"Judgmental or qualitative adjustments to model inputs or outputs to compensate for model, data, or other known limitations. A model overlay is a type of override.",Comptroller_Office,,,,,,,,,,
model risk management,"model risk management encompasses governance and control mechanisms such as board and senior management oversight, policies and procedures, controls and compliance, and an appropriate incentive and organizational structure",Fed_Reserve,,,,,,,,,,
model suite,A group of models that work together.,Comptroller_Office,,,,,,,,,,
model training,the phase in the data science development lifecycle where practitioners try to fit the best combination of weights and bias to a machine learning algorithm to minimize a loss function over the prediction range,C3.ai_Model_Training,"process to determine or to improve the parameters of a machine learning model, based on a machine learning algorithm, by using training data","aime_measurement_2022, citing ISO/IEC 22989",,,,,,,,
model validation,the set of processes and activities intended to verify that models are performing as expected.,yields.io_model_validation,"the set of principles, practices and organizational arrangements supporting a rigorous (audited) model development and validation cycle.",Open_Risk_Manual_model_validation,,,,,,,,
monitoring,Examination of the status of the activities of a supplier and of their results by the acquirer or a third party.,IEEE_Soft_Vocab,"Continual checking, supervising, critically observing or determining the status in order to identify change from the performance level required or expected.",SP800-160,,,,,,,,
moral agency,"The capacity for moral action, reasoning, judgment, and decision making, as opposed to merely having moral consequences.",AI_Ethics_Mark_Coeckelbergh,,,,,,,,,,
moral patiency,The moral standing of an entity in the sense of how that entity should be treated.,AI_Ethics_Mark_Coeckelbergh,,,,,,,,,,
naive Bayes ,"The naive Bayes classifier is a Bayesian learning method that has been found to be useful in many practical applications. It is called ""naive"" because it incorporates the simplifying assumption that attribute values are conditionally independent, given the classification of the instance. The naive Bayes classifier applies to learning tasks where each instance x is described by a conjunction of attribute values and where the target function f(x) can take on any value from some finite set V.","Mitchell,_Tom",,,,,,,,,,
natural language processing,"A computer's attempt to “understand” spoken or written language. It must parse vocabulary, grammar, and intent, and allow for variation in language use. The process often involves machine learning.","Hutson,_Matthew","The ability of a machine to process, analyze, and mimic human language, either spoken or written.",NSCAI,,,,,,,,
neural network,"Neural networks Also known as artificial neural network, neural net, deep neural net; a computer system inspired by living brains.","Ranschaert,_Erik","A model that, taking inspiration from the brain, is composed of layers (at least one of which is hidden) consisting of simple connected units or neurons followed by nonlinearities","aime_measurement_2022, citing Machine Learnign Glossary by Google","A highly abstracted and simplified model of the human brain used in machine learning. A set of units receives pieces of an input (pixels in a photo, say), performs simple computations on them, and passes them on to the next layer of units. The final layer represents the answer.","Hutson,_Matthew",,,,,,
nondiscrimination,"the practice of treating people, companies, countries, etc. in the same way as others in order to be fair:",Cambridge Dictionary,"In the context of machine learning non-discrimination can be defined as follows: (1) people that are similar in terms non-protected characteristics should receive similar predictions, and (2) differences in predictions across groups of people can only be as large as justified by non-protected characteristics.",Žliobaite_Indre,"the elimination of all existing discriminatory conditions, whether purposeful or inadvertent.",FWS_062_FW_1,"the practice of treating people, companies, countries, etc. in the same way as others in order to be fair",Cambridge_Dictionary_non-discrimination,,,,
normal flow,"The intended flow of a process originating from a start event, continuing through all defined activities, and concluding successfully to its defined end event.",IEEE_Guide_IPA,,,,,,,,,,
normalization,Conceptual procedure in database design that removes redundancy in a complex database by establishing dependencies and relationships between database entities. Normalization reduces storage requirements and avoids database inconsistencies.,OECD,"The process of converting
an actual range of values into a standard range of values, typically -1
to +1 or 0 to 1","aime_measurement_2022, citing Machine Learning Glossary by Google",,,,,,,,
objective evidence,"data supporting the existence or verity of something (note: can be obtained through observation, measurement, test, or other means).",ISO/IEC_TS_5723:2022(en),,,,,,,,,,
observation,"a piece of information received online from users, sensors, or other knowledge sources",poole_mackworth_observation,"the careful, close examination of an object, process, or other phenomenon for the purpose of collecting data about it or drawing conclusions.",APA_observation,,,,,,,,
off-label use,"[when] data published for one task are used for another[; ...] off-label usage could lead to biased, overly optimistic results of machine-learning algorithms. The underlying cause is that public data are processed with hidden processing pipelines that alter the data features.",misuse_of_public_data,,,,,,,,,,
offline learning,implies ... a static dataset that [one] know[s] from the start and the parameters of [one's] machine learning algorithm are adjusted to the whole dataset at once often loading the whole dataset into memory or in batches.,Ben_Auffarth_2021,,,,,,,,,,
online learning,fitting [one's] model incrementally as the data flows in (streaming data).,Ben_Auffarth_2021,,,,,,,,,,
ontology,"A set of concepts and categories in a subject area or knowledge domain that shows their properties and the relationships among them to enable interoperability among disparate elements and systems and specify interfaces to independent, knowledge-based services for the purpose of enabling certain kinds of automated reasoning. ",IEEE_Guide_IPA,,,,,,,,,,
opacity,[to receive] the output of [an] algorithm (the classification decision) [and to not] have any concrete sense of how or why a particular classification has been arrived at from inputs.,Jenna_Burrell,"A description of some deep learning systems [that] take an input and provide an output, but the calculations that occur in between are not easy for humans to interpret.","Hutson,_Matthew","The nature of some AI techniques whereby the inferential operations are complex, hidden, or otherwise opaque to their developers and end users in terms of providing an understanding of how classifications, recommendations, or actions are generated and what overall performance will be.",NSCAI,"When AI system processes, functions, output or behavior are unavailable to all stakeholders - usually an antonym for transparency",,,,black box; unexplainable,
operationalization,Putting AI systems or related concepts into use so they can be measured.,,,,,,,,,,,
operator ,"A role assumed by the person performing remote control or teleoperation, semi-autonomous operations, or other human-in-the-loop types of operations",SP1011,Individual or organization that performs the operations of a system.,IEEE_Soft_Vocab,Individual or organization that performs the operations of a system.,SP800-160,,,,,,
opt-in,an individual makes an active affirmative indication of choice via a user interface signaling a desire to share their information with third parties.,IAPP_Privacy_Glossary,,,,,,,,,privacy; consent; opt-out,
opt-out,an individual makes an active affirmative indication of choice via a user interface signaling a desire not to share their information with third parties.,IAPP_Privacy_Glossary,,,,,,,,,privacy; consent; opt-in,
outcome,something that follows as a result or consequence,merriam_webster_outcome,,,,,,,,,,
outlier,An outlier is a data point that is far from other points.,Russell_and_Norvig,An outlier is a data value that lies in the tail of the statistical distribution of a set of data values.,OECD,"Values distant from most
other values. In machine learning, any of the following are outliers:
• Weights with high absolute values
• Predicted values relatively far away from the actual values
• Input data whose values are more than roughly 3 standard deviations from
the mean
Outliers often cause problems in model training. Clipping is one way of managing
outliers",aime_measurement_2022 citing Machine Learning Glossary by Google,,,,,,
output,Data transmitted to an external destination,IEEE_Soft_Vocab,"Process by which an information processing system, or any of its parts, transfers data outside of that system or part",IEEE_Soft_Vocab,,,,,,,,
overfitting,"In statistics and machine learning, overfitting occurs when a model tries to predict a trend in data that is too noisy. Overfitting is the result of an overly complex model with too many parameters. A model that is overfitted is inaccurate because the trend does not reflect the reality of the data. An overfitted model is a model with a trend line that reflects the errors in the data that it is trained with, instead of accurately predicting unseen data.","Ranschaert,_Erik","Given a hypothesis space H, a hypothesis h element of H is said to overfit the training data if there exists some alternative hypothesis h' element of H, such that h has smaller error than h' over the training examples, but h' has a smaller error than h over the entire distribution of instance.","Mitchell,_Tom",,,,,,,,
package,a folder with all the code and metadata needed to train and serve a machine learning model.,about_ML_packages,,,,,,,,,,
parametric,"A learning model that summarizes data with a set of parameters of
fixed size (independent of the number of training examples)",Russell_and_Norvig,,,,,,,,,,
parent process,"A process that may contain one or more sub-processes, activities, and tasks.",IEEE_Guide_IPA,,,,,,,,,,
parity,Bit(s) used to determine whether a block of data has been altered. Rationale: Term has been replaced by the term “parity bit”.,NIST_CSRC_parity, the quality or state of being equal or equivalent,Merriam-Webster_parity,,,,,,,,
participation,engag[ing] multiple stakeholders in deliberative processes in order to achieve consensus.,Sloane_et_al_2020,,,,,,,,,,
participant,"A computer system, data, input, business rule, human intervention, and other contributor to the flow of a process.",IEEE_Guide_IPA,"a living individual about whom an investigator (whether professional or student) conducting research:
(i) Obtains information or biospecimens through intervention or interaction with the individual, and uses, studies, or analyzes the information or biospecimens; or
(ii) Obtains, uses, studies, analyzes, or generates identifiable private information or identifiable biospecimens.",45_CFR_46_2018_Requirements_(2018_Common_Rule),,,,,,,human subject,
passive learning agent,A passive learning agent has a fixed policy that determines its behavior. An active learning agent gets to decide what actions to take.,Russell_and_Norvig,,,,,,,,,active learning agent,
personal data,"‘Personal data’ means any information relating to an identified or identifiable natural person (‘data subject’); an identifiable natural person is one who can be identified, directly or indirectly, in particular by reference to an identifier such as a name, an identification number, location data, an online identifier or to one or more factors specific to the physical, physiological, genetic, mental, economic, cultural or social identity of that natural person.",GDPR,"(1) “Personal information” means information that identifies, relates to, describes, is reasonably capable of being associated with, or could reasonably be linked, directly or indirectly, with a particular consumer or household. Personal information includes, but is not limited to, the following if it identifies, relates to, describes, is reasonably capable of being associated with, or could be reasonably linked, directly or indirectly, with a particular consumer or household:
(A) Identifiers such as a real name, alias, postal address, unique personal identifier, online identifier, Internet Protocol address, email address, account name, social security number, driver’s license number, passport number, or other similar identifiers.
(B) Any personal information described in subdivision (e) of Section 1798.80.
(C) Characteristics of protected classifications under California or federal law.
(D) Commercial information, including records of personal property, products or services purchased, obtained, or considered, or other purchasing or consuming histories or tendencies.
(E) Biometric information.
(F) Internet or other electronic network activity information, including, but not limited to, browsing history, search history, and information regarding a consumer’s interaction with an internet website application, or advertisement.
(G) Geolocation data.
(H) Audio, electronic, visual, thermal, olfactory, or similar information.
(I) Professional or employment-related information.
(J) Education information, defined as information that is not publicly available personally identifiable information as defined in the Family Educational Rights and Privacy Act (20 U.S.C. Sec. 1232g; 34 C.F.R. Part 99).
(K) Inferences drawn from any of the information identified in this subdivision to create a profile about a consumer reflecting the consumer’s preferences, characteristics, psychological trends, predispositions, behavior, attitudes, intelligence, abilities, and aptitudes.
(L) Sensitive personal information.
(2) “Personal information” does not include publicly available information or lawfully obtained, truthful information that is a matter of public concern. For purposes of this paragraph, “publicly available” means: information that is lawfully made available from federal, state, or local government records, or information that a business has a reasonable basis to believe is lawfully made available to the general public by the consumer or from widely distributed media; or information made available by a person to whom the consumer has disclosed the information if the consumer has not restricted the information to a specific audience. “Publicly available” does not mean biometric information collected by a business about a consumer without the consumer’s knowledge.
(3) “Personal information” does not include consumer information that is deidentified or aggregate consumer information.",CCPA,,,,,,,,
policy,"The general principles by which a government is guided in its management of public affairs, or the legislature in its measures. This term, as applied to a law, ordinance, or rule of law, denotes its general purpose or tendency considered as directed to the POLICY",law_policy_2023,A policy defines the learning agent’s way of behaving at a given time,sutton_reinforcement_2018,,,,,,,,
positionality,"Awareness and discussion of ones’ social and institutional position with regards to research, particularly of power imbalances, and limitations the researcher may have because of differences in lived experience.",Malik_2021,"the researcher's starting points and standpoints before and during inquiry, as well as the conditions shaping the research situation, process, and product.",Charmaz_Henwood,,,,,,,reflexivity,
post-hoc explanation,"also known as the reverse engineering approach[;] tries to reconstruct explanations for decisions made by a black-box[;] . . . can be further divided into global and local methods. Global explanations concern understanding the overall logic and behavior of a black-box model, while local explanations try to find correlations between feature values of a record and an outcome. Global methods may use different types of explanators to open various black-boxes for different problems, e.g. decision trees for explaining random forests, prototypes to explain ensemble models, feature importance scores, decision trees for identifying training samples responsible for mispredictions , and minimal feature adjustment for reverting a class label.",Moradi_Samwald,"Post-hoc explainability targets models that are not readily interpretable by design by resorting to diverse means to enhance their interpretability, such as text explanations, visual explanations, local explanations, explanations by example, explanations by simplification and feature relevance explanations techniques. Each of these techniques covers one of the most common ways humans explain systems and processes by themselves.",NISTIR_8312_Full,"Post-hoc explainability targets models that are not readily inter- pretable by design by resorting to diverse means to enhance their in- terpretability, such as text explanations, visual explanations, local expla- nations, explanations by example, explanations by simplification and feature relevance explanations techniques. Each of these techniques covers one of the most common ways humans explain systems and processes by themselves.",barredo_explainable_2020,,,,,,
post-processing,"Typically performed with the help of a holdout dataset (data not used in the training of the model). Here, the learned model is treated as a black box and its predictions are altered by a function during the post-processing phase. The function is deduced from the performance of the black box model on the holdout dataset. ",SP1270,"Performed after training by accessing a holdout set that was not involved during the training of the model. If the algorithm can only treat the learned model as a black box without any ability to modify the training data or learning algorithm, then only post-processing can be used in which the labels assigned by the black-box model initially get reassigned based on a function during the post-processing phase.","Mehrabi,_Ninareh",,,,,,,,
post-processing algorithm,A bias mitigation algorithm that is applied to predicted labels.,AI_Fairness_360,,,,,,,,,,
practical significance,speaks to the magnitude of the relationship [between two variables] and whether or not that magnitude is important.,Mind_on_Statistics,a conceptual framework for evaluating discrimination cases developed primarily on statistical evidence that is the subject of increasing interest and discussion by some in the equal employment opportunity (EEO) field.,DOL_Practical_Significance,,,,,,,statistical significance (often paired in contrast to this); substantive significance (synonym),
pre-processing algorithm,A bias mitigation algorithm that is applied to training data.,AI_Fairness_360,,,,,,,,,,
precision,A metric for classification models. Precision identifies the frequency with which a model was correct when classifying the positive class.,NSCAI,"closeness of agreement between indications or measured
quantity values obtained by replicate measurements on the same or similar
objects under specified conditions","aime_measurement_2022, citing ISO/IEC Guide 99","A metric for classification models.
Precision identifies the frequency with which a model was correct when predicting
the positive class. That is:
Precision = 
True Positive
/(True Positive + False Positive)","aime_measurement_2022, citing Machine Learning Glossary by Google","Closeness of agreement between independent test results obtained under prescribed conditions. It is generally dependent on analyte concentration, and this dependence should be determined and documented. The measure of precision is usually expressed in terms of imprecision and computed as a standard deviation of the test results. Higher imprecision is reflected by a larger standard deviation. Independent test results means results obtained in a manner not influenced by any previous results on the same or similar material. Precision covers repeatability and reproducibility [19]. Alternatively, precision is a measure for the reproducibility of measurements within a set, that is, of the scatter or dispersion of a set about its central value. Precision depends only on the distribution of random errors and does not relate to the true value or specified value. ",UNODC_Glossary_QA_GLP,,,,
prediction,"Forecasting quantitative or qualitative outputs through function approximation, applied on input data or measurements.",NSCAI,primary output of an AI system when provided with input data or information,"aime_measurement_2022, citing ISO/IEC 22989",,,,,,,,
predictive analysis,The organization of analyses of structured and unstructured data for inference and correlation that provides a useful predictive capability to new circumstances or data.,IEEE_Guide_IPA,,,,,,,,,,
predictive analytics,"Insights, reporting, and information answering the question, ""What is likely to happen?"" Predictive analytics support high confidence foretelling of future event(s).",IEEE_Guide_IPA,,,,,,,,,,
preprocessing,Transforming the data so that the underlying discrimination is mitigated. This method can be used if a modeling pipeline is allowed to modify the training data.,SP1270,"Techniques that try to transform the data so the underlying discrimination is removed. If the algorithm is allowed to modify the training data, then pre-processing can be used.","Mehrabi,_Ninareh",,,,,,,,
prescriptive analytics,"Insights, reporting, and information answering the question, “What should I do about it?"" Prescriptive analytics determines information that provides high confidence actions necessary to recover from an event or fulfill a need.",IEEE_Guide_IPA,,,,,,,,,,
privacy,freedom from intrusion into the private life or affairs of an individual,ISO/IEC_TS_5723:2022(en),"freedom from intrusion into the private life
or affairs of an individual when that intrusion results from undue or illegal
gathering and use of data about that individual","aime_measurement_2022, citing ISO/IEC TR 24029-1",,,,,,,,
privacy-by-design,Embedding privacy measures and privacy enhancing technologies directly into the design of information technologies and systems.,ENISA,,,,,,,,,data-protection-by-design (def: https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX%3A02016R0679-20160504&qid=1532348683434),
privacy-enhancing technology,"A coherent system of ICT (Information and Communications Technology) measures that protects privacy by eliminating or reducing personal data or by preventing unnecessary and/or undesired processing of personal data, all without losing the functionality of the information system.",PET_Handbook,,,,,,,,,,
privileged protected attribute,A value of a protected attribute indicating a group that has historically been at systematic advantage.,AI_Fairness_360,,,,,,,,,,
procedure,"Information item that presents an ordered series of steps to perform a process, activity, or task.",IEEE_Soft_Vocab,,,,,,,,,,
process,"A sequence or flow of activities in an organization with the objective of carrying out work, which may include a set of activities, events, tasks, and decisions in a sequenced flow that adhere to finite execution semantics. Process levels will generally follow structure at the capability maturity model integration (CMMI) level.",IEEE_Guide_IPA,Set of interrelated or interacting activities that transforms inputs into outputs,IEEE_Soft_Vocab,,,,,,,,
process flow,"The defined representation of the overall progression of how a process is intended to be performed, including all exceptions.",IEEE_Guide_IPA,,,,,,,,,,
processing,"‘Processing’ means any operation or set of operations which is performed on personal data or on sets of personal data, whether or not by automated means, such as collection, recording, organisation, structuring, storage, adaptation or alteration, retrieval, consultation, use, disclosure by transmission, dissemination or otherwise making available, alignment or combination, restriction, erasure or destruction.",GDPR,"“Processing” means any operation or set of operations that are performed on personal information or on sets of personal information, whether or not by automated means.",CCPA,,,,,,,personal data; processing,
processing environment,the combination of software and hardware on which the Application runs.,Law_Insider_processing_environment,,,,,,,,,,
processor,"‘Processor’ means a natural or legal person, public authority, agency or other body which processes personal data on behalf of the controller.",GDPR,"“Processing” means any operation or set of operations that are performed on personal information or on sets of personal information, whether or not by automated means.",CCPA,,,,,,,personal data; processing; controller,
product manager,"a specialized product management professional whose job is to manage the planning, development, launch, and success of products/solutions powered by AI, machine learning, and deep learning technologies.",productmanagerHQ_Josh_Fechter,,,,,,,,,,
product owner,"[person who is] focused on providing direction and prioritization for the cross-functional AI team, ensuring everyone remains focused on the overall vision and road map. This role is responsible for unifying individuals with diverse skills and backgrounds toward a common goal. ",Forbes_Tracy_Kemp,,,,,,,,,,
product velocity,how fast a product can be delivered to the market,Cost_Management_ch15,,,,,,,,,,
productization,"[turning the best performing model] into an actual ""data product,"" ready to be used in live services.",Towards_Productizing,,,,,,,,,,
profiling,"‘Profiling’ means any form of automated processing of personal data consisting of the use of personal data to evaluate certain personal aspects relating to a natural person, in particular to analyse or predict aspects concerning that natural person's performance at work, economic situation, health, personal preferences, interests, reliability, behaviour, location or movements.",GDPR,"“Profiling” means any form of automated processing of personal information, as further defined by regulations pursuant to paragraph (16) of subdivision (a) of Section 1798.185, to evaluate certain personal aspects relating to a natural person and in particular to analyze or predict aspects concerning that natural person’s performance at work, economic situation, health, personal preferences, interests, reliability, behavior, location, or movements.",CCPA,Measuring the characteristics of expected activity so that changes to it can be more easily identified.,CSRC,,,,,personal data; processing,
protected attribute,"An attribute that partitions a population into groups whose outcomes should have parity. Examples include race, gender, caste, and religion. Protected attributes are not universal, but are application specific.",AI_Fairness_360,,,,,,,,,,
protected class,"[a feature] that may not be used as the basis for decisions [and] could be chosen because of legal mandates or because of organizational values. Some common protected [classes] include race, religion, national origin, gender, marital status, age, and socioeconomic status.",MIT_Protected_Attributes,A group of people with a common characteristic who are legally protected from [...] discrimination on the basis of that characteristic. Protected classes are created by both federal and state law.,Practical_Law_protected_class,,,,,,,,
prototype,A prototype is an original model constructed to include all the technical characteristics and performances of the new product.,OECD,,,,,,,,,,
provisioning,The granting of access rights and executional privilege to an agent (human or machine) within an application(s) or system(s).,IEEE_Guide_IPA,,,,,,,,,,
proxy,"A variable that can stand in for another, usually not directly observable or measurable, variable.",SP1270,,,,,,,,,,
proxy discrimination,"a particularly pernicious subset of disparate impact. Like all forms of disparate impact, it involves a facially neutral practice that disproportionately harms members of a protected class. But a practice producing a disparate impact only amounts to proxy discrimination when a second condition is met. In particular, proxy discrimination requires that the usefulness to the discriminator of a facially neutral practice derives, at least in part, from the very fact that it produces a disparate impact. This condition can be met either when the discriminator intends to disparately impact a protected group or when a legally-prohibited characteristic is predictive of the discriminator’s goals in ways that cannot be captured more directly by non-suspect data.",Proxy_Discrimination,,,,,,,"A variable V in a causal graph exhibits unresolved discrimination if there exists a directed path from A to V that is not blocked by a resolving variable, and V itself is non-resolving.","Mehrabi,_Ninareh (this definiiton is quite technical, though)",,
pseudo-anonymization (pseudonymization),"‘Pseudonymisation’ means the processing of personal data in such a manner that the personal data can no longer be attributed to a specific data subject without the use of additional information, provided that such additional information is kept separately and is subject to technical and organisational measures to ensure that the personal data are not attributed to an identified or identifiable natural person;",GDPR,"“Pseudonymize” or “Pseudonymization” means the processing of personal information in a manner that renders the personal information no longer attributable to a specific consumer without the use of additional information, provided that the additional information is kept separately and is subject to technical and organizational measures to ensure that the personal information is not attributed to an identified or identifiable consumer.",CCPA,A data management technique to strip identifiers linking data to an individual.,NSCAI,,,,,personal data; processing,
pseudoscience,"a system of theories, assumptions, and methods erroneously regarded as scientific",Merriam-Webster_pseudoscience,,,,,,,,,,
quality,The totality of features and characteristics of a product or service that bear on its ability to satisfy stated or implied needs.,OECD,"<data> degree to which the characteristics of data satisfy stated and implied needs when used under specified conditions; <system> degree to which a set of inherent characteristics of an object fulfils requirements (an object can be a product, process or service)",ISO/IEC_TS_5723:2022(en),,,,,,,,
racialized,"A socio-political process by which groups are ascribed a racial identity, whether or not members of the group self-identify as such",AAAS_AI_and_Bias_2022-09,,,,,,,,,,
ranking,a type of machine learning that sorts data in a relevant order[; often used by companies] to optimize search and recommendations.,DEV_ranking,"position, order, or standing within a group : RANK",Merriam-Webster_ranking,,,,,,,,
reason code,"Response functions created by linear regression algorithms are probably the most popular, accountable, and transparent class of machine learning models. These models will be referred to here as linear and monotonic. They are transparent because changing any given input feature (or sometimes a combination or function of an input feature) changes the response function output at a defined rate, in only one direction, and at a magnitude represented by a readily available coefficient. Monotonicity also enables accountability through intuitive, and even automatic, reasoning about predictions. For instance, if a lender rejects a credit card application, they can say exactly why because their probability of default model often assumes that credit scores, account balances, and the length of credit history are linearly and monotonically related to the ability to pay a credit card bill. When these explanations are created automatically and listed in plain English, they are typically called reason codes.",Machine_Learning_Interpretability_with_H20_Driverless_AI,logical and actionable [response] in the event credit is denied or other adverse action is taken; emerge[s] directly from the model that generates the credit score and serve[s] the purpose of explaining to consumers why the consumer was denied credit.,Comparing_scores_and_reason_codes,,,,,,,,
recall ,A metric for classification models; identifies the frequency with which a model correctly classifies the true positive items.,NSCAI,"A metric for classification models
that answers the following question: Out of all the possible positive labels,
how many did the model correctly identify? That is:
Recall = 
True Positive/(
True Positive + false Negative)","aime_measurement_2022, citing Machine Learning Glossary by Google",,,,,,,,
recognition,the automatic discovery of regularities in data through the use of computer algorithms and with the use of these regularities to take actions such as classifying the data into different categories.,Pattern_Recognition_and_Machine_Learning,"a sense of awareness and familiarity experienced when one encounters people, events, or objects that have been encountered before or when one comes upon material that has been learned in the past.",APA_recognition,"to transfer prior learning or past experience to current consciousness: that is, to retrieve and reproduce information; to remember.",APA_recall,,,,,,
recommendation system,A software tool and techniques that provide suggestion based on the customer's taste to discover new appropriate thing for them by filtering personalized information based on the user's preferences from a large volume of information,"Das,_Debashis","A subclass of information filtering system that seek to predict ‘rating’ or ‘preference’ that a user would give to an item (such as music, books or movies) or social element (e.g. people or group) they had not yet considered, using a model built from the characteristics of an item (content based approaches) or the user’s social environment (collaborative filtering approaches)","Sharma,_Lalita",,,,,,,,
rectification,An individual’s right to have personal data about them corrected or amended by a business or other organization if it is inaccurate.,IAPP_Privacy_Glossary,,,,,,,,,,
red-lining,the creation and maintenance of technology practices that further entrench discriminatory practices against already marginalized groups.,Banking_on_Your_Data_Christopher_Gilliard,,,,,,,,,,
red-team,"A group of people authorized and organized to emulate a potential adversary’s attack or exploitation capabilities against an enterprise’s security posture. The Red Team’s objective is to improve enterprise cybersecurity by demonstrating the impacts of successful attacks and by demonstrating what works for the defenders (i.e., the Blue Team) in an operational environment. Also known as Cyber Red Team.",CSRC,a role-playing exercise in which a problem is examined from an adversary’s or enemy’s perspective.,Annie_Jacobsen_2015,"The objective of red-teaming exercises is to “obtain a realistic level of risk and vulnerabilities against your technology, people and physical/facilities” [RedTeam Security Consulting, 2016, p. 1]. [See note.]",Pamela_Goh_2021,,,,,,
reference class,"A class which is intended to describe structure and behavior of object identifiers. Its instances, called references, are passed by-value and indirectly represent objects by substituting for some primitive reference.",IGI_Global_reference_class,,,,,,,,,,
reflexivity,"A form of critical thinking that prompts us to consider the ‘whys’ and ‘hows’ of research, critically questioning the utility, ethics, and value of what, whom, and how we study",Jamieson_Govaart_Pownall,"in qualitative research, the self-referential quality of a study in which the researcher reflects on the assumptions behind the study and especially the influence of his or her own motives, history, and biases on its conduct.",APA_reflexivity,,,,,,,positionality,
regression,"Regression is a process of predicting the value to a yes or no label provided it falls on a continuous spectrum of input values, subcategory of supervised learning.","Ranschaert,_Erik","Refers to the process of developing an empirical (data-driven) model to predict and/or explain one or more attributes in a database or set of data. It is most frequently associated with the simple linear model (y=mx+b) taught in most introductory statistics courses; the same ideas have been extended in many directions, including classification problems. When the emphasis is on hypothesis testing and simple models, the regression output is typically a few parameters that provide a direct linkage from the input variables to the predicted variables (or classification). In other situations the emphasis is on explaining as much of the variability in the output variables as is ""reasonable"" from the input variables. In this case, there are a number of ""advanced"" techniques, such as smoothing splines, decision trees, neural nets, and so forth, for which there are many ""free"" parameters. The meaning of any one of these parameters can be obscure. Many Data Mining techniques are, at their core, variations on well-known regression techniques.",Raynor,the prediction of an exact value using a given set of data,Saleh_Alkhalifa_ML_in_Biotech,,,,,,
reinforcement learning,"A type of machine learning in which the algorithm learns by acting toward an abstract goal, such as “earn a high video game score” or “manage a factory efficiently.” During training, each effort is evaluated based on its contribution toward the goal.","Hutson,_Matthew","Algorithms, in which an agent decides what to do to perform the given task to maximize the given function.","Reznik,_Leon","A method of training algorithms to make suitable actions by maximizing rewarded behavior over the course of its actions.61 This type of learning can take place in simulated environments, such as game-playing, which reduces the need for real-world data. ",NSCAI,,,,,,
reliability,Reliability refers to the closeness of the initial estimated value(s) to the subsequent estimated values.,OECD,"ability of an item to perform as required, without failure, for a given time interval, under given conditions. Note 1 to <system> definition: The time interval duration can be expressed in units appropriate to the item concerned (e.g. calendar time, operating cycles, distance run, etc.) and the units should always be clearly stated. Note 2 to <system> definition: Given conditions include aspects that affect reliability, such as: mode of operation, stress levels, environmental conditions, and maintenance.",ISO/IEC_TS_5723:2022(en),"property of consistent
intended behaviour and results","aime_measurement_2022, citing ISO/IEC 22989",,,,,,
remediation,"The process of treating data by cleaning, organizing, and migrating it to a safe and secure environment for optimized usage is called data remediation. Generally [understood] as a process involving deleting unnecessary or unused data. However, the actual process . . . is very detailed and includes several steps, including replacing, updating, or modifying data along with cleaning it, organizing it, and getting rid of unnecessary data.",CPO_Magazine_Amar_Kanagaraj,,,,,,,,,,
representativeness,,,,,,,,,,,,
reproducibility,Closeness of the agreement between the results of measurements of the same measurand carried out under changed conditions of measurement.,IEEE_Soft_Vocab,,,,,,,,,,
requirement,something essential to the existence or occurrence of something else : CONDITION,Merriam-Webster_requirement,,,,,,,,,,
residual,"Residuals are differences between the one-step-predicted output from the model and the measured output from the validation data set. Thus, residuals represent the portion of the validation data not explained by the model.",MathWorks_Residual,,,,,,,,,,
residual analysis,a powerful [statistical] tool to detect the problems associated with the violation of the ANOVA [analysis of variance] assumptions.,fernandez_residual_1992,"Residual analysis consists of two tests: the whiteness test and the independence test.

According to the whiteness test criteria, a good model has the residual autocorrelation function inside the confidence interval of the corresponding estimates, indicating that the residuals are uncorrelated.

According to the independence test criteria, a good model has residuals uncorrelated with past inputs. Evidence of correlation indicates that the model does not describe how part of the output relates to the corresponding input.",MathWorks_Residual,,,,,,,,
resilience,"The ability to prepare for and adapt to changing conditions and withstand and recover rapidly from disruptions. Resilience includes the ability to withstand and recover from deliberate attacks, accidents, or naturally occurring threats or incidents. The ability of a system to adapt to and recover from adverse conditions.",NISTIR_8269_Draft,"<governance> ability to anticipate and adapt to, resist, or quickly recover from a potentially disruptive event, whether natural or man-made; <system> capability of a system to maintain its functions and structure in the face of internal and external change, and to degrade gracefully when this is necessary",ISO/IEC_TS_5723:2022(en),"ability of a system to recover operational condition
quickly following an incident","aime_measurement_2022, citing ISO/IEC 22989",,,,,,
responsible AI,An AI system that aligns development and behavior to goals and values. This includes developing and fielding AI technology in a manner that is consistent with democratic values.,NSCAI,,,,,,,,,,
result,The consequential outcome of completing a process.,IEEE_Guide_IPA,,,,,,,,,,
retention limit,"refers to the amount of information that is stored long-term, and can be measured in volume (the size of the total collected logs in bytes) and time (the number of months or years that logs are stored for).",Industrial_Network_Security_2011,,,,,,,,,,
risk,"When considering the negative impact of a potential event, risk is a function of 1) the negative impact, or magnitude of harm, that would arise if the circumstance or event occurs and 2) the likelihood of occurrence (Adapted from: OMB Circular A-130:2016). Negative impact or harm can be experienced by individuals, groups, communities, organizations, society, the environment, and the planet.",NIST_AI_RMF_1.0,"The risk to organizational operations (including mission, functions, image, reputation), organizational assets, individuals, other organizations, and the Nation due to the potential for unauthorized access, use, disclosure, disruption, modification, or destruction of information and/or a system.","Reznik,_Leon","A measure of the extent to which an entity is threatened by a potential circumstance or event, and typically a function of: (i) the adverse impacts that would arise if the circumstance or event occurs; and (ii) the likelihood of occurrence.",SP800-12,"An uncertain event or condition that, if it occurs, has a positive or negative effect on a project's objectives",IEEE_Soft_Vocab,effect of uncertainty on objectives,ISO_IEC_38507,,
risk control,"mechanisms at the design, implementation, and evaluation stages [that can be taken] into consideration when developing responsible AI for organizations that includes security risks (cyber intrusion risks, privacy risks, and open source software risk), economic risks (e.g., job displacement risks), and performance risks (e.g., risk of errors and bias and risk of black box, and risk of explainability). ",Toward_an_understanding_of_responsible_artificial_intelligence_practices,,,,,,,,,,
risk tiering,"when evaluating the desired level of explainability[, h]igh-risk models are worthy of deeper consideration: models with an elevated level of risk and/or models which are related to decisions about individuals (especially decisions which can be impactful), to privacy, to potential predetermination of individual’s choice; models using sensitive variables (even if using them is formally legitimate); models related to decisions in healthcare, human resources, allocation of benefits. All of these are examples that should drive a higher model risk rating, and they should meet more stringent explainability expectations. Existing MRM policies need to be updated to ensure they take into consideration the relevant risks when rating AI/ML models and deciding what the desired level of interpretability emphasis to be in any given case. ",pwc_Model_Risk_Management_of_AI_and_ML_Systems,,,,,,,,,,
risk tolerance,"Risk tolerance refers to the organization’s or AI actor’s ... readiness to bear the risk in order to achieve its objectives. Risk tolerance can be influenced by legal or regulatory
requirements.",NIST_AI_RMF_1.0,,,,,,,,,,
robotic desktop automation (RDA),"The computer application that makes available to a human operator a suite of predefined activity choreography to complete the execution of processes, activities, transactions, and tasks in one or more unrelated software systems to deliver a result or service in the course of human-initiated or -managed workflow.",IEEE_Guide_IPA,,,,,,,,,,
robotic process automation (RPA),"A preconfigured software instance that uses business rules and predefined activity choreography to complete the autonomous execution of a combination of processes, activities, transactions, and tasks in one or more unrelated software systems to deliver a result or service with human exception management. ",IEEE_Guide_IPA,"Software to help in the automation of tasks, especially those that are tedious and repetitive.",NSCAI,,,,,,,,
robust AI ,"An AI system that is resilient in real-world settings, such as an object-recognition application that is robust to significant changes in lighting. The phrase also refers to resilience when it comes to adversarial attacks on AI components.",NSCAI,,,,,,,,,,
robustness,ability of a system to maintain its level of performance under a variety of circumstances,ISO/IEC_TS_5723:2022(en),"The ability of a machine learning model/algorithm to maintain correct and reliable performance under different conditions (e.g., unseen, noisy, or adversarially manipulated data).",NISTIR_8269_Draft,,,,,,,,
root-mean-square deviation (RMSD),"of an estimator of a parameter[; ...] the square-root of the mean squared error (MSE) of the estimator. In symbols, if X is an estimator of the parameter t, then
RMSE(X) = ( E( (X-t)2 ) )½.
The RMSE of an estimator is a measure of the expected error of the estimator. The units of RMSE are the same as the units of the estimator.",Glossary_of_Statistical_Terms,a frequently used measure of the differences between values (sample or population values) predicted by a model or an estimator and the values observed,Wikipedia_RMSD,,,,,,,root-mean-square error (RMSE),
rounding,the transformation of a number expressed in a particular base to a number with fewer digits.,Nick_Higham_1,"In finite precision arithmetic[, the process by which] the result of an elementary arithmetic operation [that] does not generally lie in the underlying number system, F, . . . must be mapped back into F.",Nick_Higham_2,,,,,,,,
row,"describes a single entity or observation and the columns describe properties about that entity or observation. The more rows you have, the more examples from the problem domain that you have.",Machine_Learning_Mastery_Jason_Brownlee,,,,,,,,,,
safety,"property of a system such that it does not, under defined conditions, lead to a state in which human life, health, property, or the environment is endangered; [safety involves reducing both the probability of expected harms and the possibility of unexpected harms].",ISO/IEC_TS_5723:2022(en),freedom from risk which is not tolerable,"aime_measurement_2022, citinig ISO/IEC TR 24029-1",,,,,,,,
scalability,"The ability to increase or decrease the computational resources required to execute a varying volume of tasks, processes, or services.",IEEE_Guide_IPA,,,,,,,,,,
score  ,A continuous value output from a classifier. Applying a threshold to a score results in a predicted label.,AI_Fairness_360,,,,,,,,,,
screen out,"Screen-out discrimination occurs when “a disability prevents a job applicant or employee from meeting—or lowers their performance on—a selection criterion, and the applicant or employee loses a job opportunity as a result.” ",EEOC_ADA_AI,1) to remove (someone or something that is not suitable for a particular purpose) from a group that is being examined; 2) to prevent (something harmful) from passing through,Merriam-Webster_screen_out,,,,,,,,
security,"resistance to intentional, unauthorized act(s) designed to cause harm or damage to a system",ISO/IEC_TS_5723:2022(en),"degree to which a product or system (3.38)
protects information (3.20) and data (3.11) so that persons or other products
or systems have the degree of data access appropriate to their types and levels
of authorization","aime_measurement_2022, citing ISO/IEC TR 24029-1",,,,,,,,
segmentation,The process of identifying homogeneous subgroups within a data table.,Raynor,,,,,,,,,,
selective adherence,"Decision-makers are more likely to follow advice 
(human or algorithmic-based) that matches stereotypical
views of the decision subjects.",alon-barkat_human_2023,,,,,,,,,,
self-aware system,"A computing platform imbued with sufficient knowledge and analytic capability to make useful conclusions about its inputs, its own processing, and the use of its output so that it is capable of self- judgment and improvement consistent with its purpose.",IEEE_Guide_IPA,,,,,,,,,,
self-diagnosis ,"Ability of a system to adequately take measurement information from sensors, validate the data, and communicate the processes and results to other devices",SP1011,,,,,,,,,,
self-healing system,"A computing system able to perceive that it is not operating correctly and, without human intervention, make the necessary adjustments to restore itself to normalcy.",IEEE_Guide_IPA,,,,,,,,,,
semantic mapping,"A strategic schema or framework of metadata labels applied to all data, data groups, data fields, data types, or data content used to introduce new or raw data into a corpus or data fabric to give machine learning algorithms direction for investigating known or potential relationships between data. A semantic map provides a structure for the introduction of new data, information, or knowledge ",IEEE_Guide_IPA,,,,,,,,,,
sensitivity analysis,"A “what-if” type of analysis to determine the sensitivity of the outcomes to changes in parameters. If a small change in a parameter results in relatively large changes in the outcomes, the outcomes are said to be sensitive to that parameter.",OECD,Sensitivity analysis varies settings of a model’s input parameters and assesses resulting changes in model outputs.,Raynor,Sensitivity analysis varies settings of a model’s input parameters and assesses resulting changes in model outputs.,mills_study_2010,,,,,,
sensitive data,"a specific set of ""special categories"" that must be treated with extra security[, including] information about: [r]acial or ethnic origin; [p]olitical opinions; [r]eligious or philosophical beliefs; [t]rade union membership; [g]enetic data; [d]ata related to a person's sex life or sexual orientation; and [b]iometric data (where processed to uniquely identify someone). ",IT_Governance_Blog_Luke_Irwin,,,,,,,,,,
sensory digitization,"The conversion of typically analog or human sensory perception (e.g., vision, speech) to a digital format useful for machine-to-human interaction or machine processing of traditionally analog sensory information [e.g., optical character recognition (OCR)].",IEEE_Guide_IPA,,,,,,,,,,
service,"A collection of coordinated processes that takes one or more kinds of input, performs a value-added transformation, and creates an output that fulfills the needs of a customer [or shareholder].",IEEE_Guide_IPA,,,,,,,,,,
signal detection theory,a framework for interpreting data from experiments in which accuracy is measured.,Signal_Detection_Theory,,,,,,,,,,
shallow learning,Techniques that separate the process of feature extraction from learning itself.,"Reznik,_Leon",,,,,,,,,,
situational awareness,"Perception of elements in the system and/or environment and a comprehension of their meaning, which could include a projection of the future status of perceived elements and the uncertainty associated with that status. ",SP800-160,,,,,,,,,,
snake oil,"Something proposed as a solution to a problem, but which is of little real or practical value; speech or action which is superficially attractive or convincing but of no real substance or effectiveness.",OED_snake_oil,,,,,,,,,,
socio-technical system,how humans interact with technology within the broader societal context,NIST SP1270,system that includes a combination of technical and human or natural elements ,ISO/IEC_TS_5723:2022(en),,,,,,,,
software testing,"Activity in which a system or component is executed under specified conditions, the results are observed or recorded, and an evaluation is made of some aspect of the system or component.",IEEE_Soft_Vocab,,,,,,,,,,
sparsity,refers to a matrix of numbers that includes many zeros or values that will not significantly impact a calculation.,Dave_Salvator_sparsity,,,,,,,,,,
specification,"A document that specifies, in a complete, precise, verifiable manner, the requirements, design, behavior, or other characteristics of a system or component and often the procedures for determining whether these provisions have been satisfied.",SP800-37,,,,,,,,,,
stakeholder,"Individual or organization having a right, share, claim, or interest in a system or in its possession of characteristics that meet their needs and expectations. An individual, group, or organization who may affect, be affected by, or perceive itself to be affected by a decision, activity, or outcome of a project.",IEEE_Soft_Vocab,"any individual, group, or organization that can affect, be affected by, or perceive itself to be affected by a decision or activity",ISO/IEC_TS_5723:2022(en),,,,,,,,
standard deviation,The most widely used measure of dispersion of a frequency distribution introduced by K. Pearson (1893). It is equal to the positive square root of the variance. The standard deviation should not be confused with the root mean square deviation.,OECD,,,,,,,,,,
start event,"An activity, task, or input that describes or defines the beginning of a process.",IEEE_Guide_IPA,,,,,,,,,,
statistical bias,"A systematic tendency for estimates or measurements to be above or below their true values. Statistical biases arise from systematic as opposed to random error. Statistical bias can occur in the absence of prejudice, partiality, or discriminatory intent.",SP1270,,,,,,,,,,
statistical parity,The independence between the protected attribute and the outcome of the decision rule,"Besse,_Philippe",,,,,,,,,,
statistical significance,"When the probability of obtaining a statistic of a given size due strictly to random sampling error, or chance, is less than the selected alpha level [or the probability of a type I error]; also represents a rejection of the null hypothesis.",Statistics_in_Plain_English,refers to whether a relationship between two or more variables exists beyond a probability expected by chance,The_SAGE_Encyclopedia_of_Communication_Research_Methods,,,,,,,,
statistics,"Numerical data relating to an aggregate of individuals; the science of collecting, analysing and interpreting such data",OECD,,,,,,,,,,
stereotype,"Definition 3b (figurative): A preconceived and oversimplified idea of the characteristics which typify a person, situation, etc.; an attitude based on such a preconception. Also, a person who appears to conform closely to the idea of a type.",OED_stereotype,"a set of cognitive generalizations (e.g., beliefs, expectations) about the qualities and characteristics of the members of a group or social category. Stereotypes, like schemas, simplify and expedite perceptions and judgments, but they are often exaggerated, negative rather than positive, and resistant to revision even when perceivers encounter individuals with qualities that are not congruent with the stereotype.",APA_stereotype,"Contemporary social psychology typically defines stereotypes as mental representations of a group and its members, and stereotyping as the cognitive activity of treating individual elements in terms of higher level categorial properties ",Augoustinos_Walker_1998,,,,,,
stochastic,The adjective “stochastic” implies the presence of a random variable; e.g. stochastic variation is variation in which at least one of the elements is a variate and a stochastic process is one wherein the system incorporates an element of randomness as opposed to a deterministic system.,OECD,,,,,,,,,,
straight-through processing (STP),"The successful execution of a service, process, or transaction performed entirely through traditional application platforms with predefined interfaces (i.e., application programming interfaces [APIs]).",IEEE_Guide_IPA,,,,,,,,,,
strawperson,"a fallacious argument which irrelevantly attacks a position that appears similar to, but is actually different from, an opponent's position, and concludes that the opponent's real position has thereby been refuted.",Hughes_Lavery_Critical_Thinking,,,,,,,,,,
stress test,"Type of performance efficiency testing conducted to evaluate a test item's behavior under conditions of loading above anticipated or specified capacity requirements, or of resource availability below minimum specified requirements",IEEE_Soft_Vocab,,,,,,,,,,
strong AI,AI that is capable of solving almost all tasks that humans can solve,Shevlin_et_al_2019,"AI that is as smart and well-rounded as a human. Some say it's impossible. Current AI is weak, or narrow. It can play chess or drive but not both, and lacks common sense.","Hutson,_Matthew",,,,,,,artificial general intelligence (AGI),
structured data,Data that has a predefined data model or is organized in a predefined way.,NIST_1500,,,,,,,,,,
sub-process,A subordinate process that can be included within a parent process. It can be present and/or repeated within other parent processes.,IEEE_Guide_IPA,,,,,,,,,,
supervised learning,"A type of machine learning in which the algorithm compares its outputs with the correct outputs during training. In unsupervised learning, the algorithm merely looks for patterns in a set of data.","Hutson,_Matthew","Algorithms, which develop a mathematical model from the input data and known desired outputs.","Reznik,_Leon","For a computer to process a set of data whose attributes have been divided into two groups and derive a relationship between the values of one and the values of the other. These two groups are sometimes called predictor and targets, respectively. In statistical terminology, they are called independent and dependent variables. Respectively. The learning Is ""supervised because the distinction between the predictors and the target variables is chosen by the investigator or some other outside agency.",Raynor,"a general subset of machine learning in which data, like its associated labels, is used to train models that can learn or generalize from the data to make predictions, preferably with a high degree of certainty.",Saleh_Alkhalifa_ML_in_Biotech,,,,
support vector machines,A supervised machine learning model for data classification and regression analysis. One of the most used classifiers in machine learning. It optimizes the width of the gap between the points of separate categories in feature space.,"Ranschaert,_Erik",,,,,,,,,,
surveillance,"an outcome of establishing information infrastructures as the basis for administration, prodduction, marketing, entertainment and law enforcement[, involving] garnering personal data for a variety of purposes in a quest for greater efficiency, convenience or safety. Its ethics and politics are inherently ambiguous, but at the same time surveillance is never neutral.",David_Lyon_2007,,,,,,,,,,
system,combination of interacting elements organized to achieve one or more stated purposes,ISO/IEC_TS_5723:2022(en),,,,,,,,,,
systemic bias,Systemic biases result from procedures and practices of particular institutions that operate in ways which result in certain social groups being advantaged or favored and others being disadvantaged or devalued. This need not be the result of any conscious prejudice or discrimination but rather of the majority following existing rules or norms.,"D. Chandler and R. Munday, A Dictionary of Media and Communication. Oxford
University Press, Jan. 2011, publication Title: A Dictionary of Media and
Communication",,,,,,,,,,
system of systems,set of systems and system elements that interact to provide a unique capability that none of the constituent systems can accomplish on its own (note: can be necessary to facilitate interaction of the constituent systems in the system of systems),ISO/IEC_TS_5723:2022(en),,,,,,,,,,
talent acquisition,the process of finding and acquiring skilled human labor for organizational needs and to meet any labor requirement.,IJMAE,,,,,,,,,,
target,"a method for solving a problem that an AI algorithm parses its training data to find. Once an algorithm finds its target function, that function can be used to predict results (predictive analysis). The function can then be used to find output data related to inputs for real problems where, unlike training sets, outputs are not included.",TechTarget_target_function,,,,,,,,,"target variable, target value ",
task,"The performance of a discrete activity with a defined start, stop, and outcome that cannot be broken down to a finer level of detail.",IEEE_Guide_IPA,"Required, recommended, or permissible action, intended to contribute to the achievement of one or more outcomes of a process",IEEE_Soft_Vocab,"set of activities undertaken in order to achieve a specific
goal","aime_measurement_2022, citing ISO/IEC TR 24030",,,,,,
taxonomy,Taxonomy refers to classification according to presumed natural relationships among types and their subtypes.,OECD,,,,,,,,,,
tech-washing,"the practice of slapping a trendy, new label on legacy solutions.",Forbes_Kayvan_Alikhani,,,,,,,,,,
technical control,"Security controls (i.e., safeguards or countermeasures) for an information system that are primarily implemented and executed by the information system through mechanisms contained in the hardware, software, or firmware components of the system.",NIST_SP_800-30_Rev_1,,,,,,,,,,
technochauvinism,The belief that technology is always the solution,"M. Broussard, Artificial Unintelligence: How Computers Misunderstand the World. MIT Press, 2018.",,,,,,,,,techno-solutionism,
techno-solutionism,See technochauvinism.,,,,,,,,,,,
test,"Technical operation to determine one or more characteristics of or to evaluate the performance of a given product, material, equipment, organism, physical phenomenon, process or service according to a specified procedure.",UNODC_Glossary_QA_GLP,any activity aimed at evaluating an attribute or capability of a program or system and deteermining that it meets its required results.,William_Hetzel,"(1) activity in which a system or component is executed
under specified conditions, the results are observed or recorded, and an evaluation
is made of some aspect of the system or component; (2) to conduct an
activity as in (1); (3) set of one or more test cases and procedures.","aime_measurement_2022, citing ISO/IEC 24765",the process of executing a program with the intent of finding errors.,The_Art_of_Software_Testing,,,"Test, Evaluation, Verification and Validation (TEVV)",
"Test and Evaluation, Verification and Validation (TEVV) ","A framework for assessing, incorporating methods and metrics to determine that a technology or system satisfactorily meets its design specifications and requirements, and that it is sufficient for its intended use.",NSCAI_Report,,,,,,,,,,
third party,"an entity that is involved in some way in an interaction that is primarily between two other entities. [Please see note, especially regarding NIST CSRC terms that we might incorporate into this definition.]",TechTarget_third_party,,,,,,,,,,
threat actor,See bad actor.,,,,,,,,,,,
three lines of defense,"Most financial institutions follow a three-lines-of-defense model, which separates front line groups, which are generally accountable for business risks (the First Line), from other risk oversight and independent challenge groups (the Second Line) and assurance (the Third Line)",AIRS_Penn,,,,,,,,,,
traceability,"Ability to trace the history, application or location of an entity by means of recorded identification. [""Chain of custody"" is a related term.] Alternatively, traceability is a property of the result of a measurement or the value of a standard whereby it can be related with a stated uncertainty, to stated references, usually national or international standards, i.e. through an unbroken chain of comparisons. In this context, The standards referred to here are measurement standards rather than written standards.",UNODC_Glossary_QA_GLP,"A characteristic of an AI system enabling a person to understand the technology, development processes, and operational capabilities (e.g., with transparent and auditable methodologies along with documented data sources and design procedures).",NSCAI,,,,,,,,
training data,A dataset from which a model is learned.,AI_Fairness_360,"a sample from a population of possible examples, and the statistical similarities of each class extracted, or more precisely the significant differences between classes are found.","Ripley,_Brian","samples for training used to fit a machine learning
model","aime_measurement_2022, citing ISO/IEC 22989",,,,,,
transaction,"Enactment of a process represented by a set of coordinated activities carried out by multiple systems and/or participants in accordance with defined relationships. This coordination leads to an intentional, consistent, and verifiable result across all participants.",IEEE_Guide_IPA,,,,,,,,,,
transfer learning,"A technique in machine learning in which an algorithm learns to perform one task, such as recognizing cars, and builds on that knowledge when learning a different but related task, such as recognizing cats.","Hutson,_Matthew",,,,,,,,,,
transformer,A procedure that modifies a dataset.,AI_Fairness_360,,,,,,,,,,
transparency,"<information> open, comprehensive, accessible, clear and understandable presentation of information; <systems> property of a system or process to imply openness and accountability",ISO/IEC_TS_5723:2022(en),Understanding the working logic of the model.,NISTIR_8269_Draft,"<organization> property of an organization that appropriate activities and decisions are communicated to relevant stakeholders (3.5.13) in a comprehensive, accessible and understandable manner
Note 1 to entry: Inappropriate communication of activities and decisions can violate security, privacy or confidentiality requirements.",iso_22989_2022,"<system> property of a system that appropriate information about the system is made available to relevant stakeholders (3.5.13)
Note 1 to entry: Appropriate information for system transparency can include aspects such as features, performance, limitations, components, procedures, measures, design goals, design choices and assumptions, data sources and labelling protocols.
Note 2 to entry: Inappropriate disclosure of some aspects of a system can violate security, privacy or confidentiality requirements.",iso_22989_2022,,,,
trojan horse ,"A computer program that appears to have a useful function, but also has a hidden and potentially malicious function that evades security mechanisms, sometimes by exploiting legitimate authorizations of a system entity that invokes the program.","Reznik,_Leon",,,,,,,,,malware,
true negative,outcome where the model correctly predicts the negative class.,google_dev_classification-true-false-positive-negative,,,,,,,,,,
true positive,an outcome where the model correctly predicts the positive class. ,google_dev_classification-true-false-positive-negative,,,,,,,,,,
trust,the system status in the mind of human beings based on their perception of and experience with the system; concerns the attitude that a person or technology will help achieve specific goals in a situation characterized by uncertainty and vulnerability.,DOD_TEVV,degree to which a user or other stakeholder has confidence that a product or system will behave as intended,"aime_measurement_2022, citing ISO/IEC TR 24029-1",the attitude that an agent will help achieve an individual’s goals in a situation characterized by uncertainty and vulnerability.,,,,,,,
trustworthiness,"The degree to which an information system (including the information technology components that are used to build the system) can be expected to preserve the confidentiality, integrity, and availability of the information being processed, stored, or transmitted by the system across the full range of threats and individuals’ privacy.",SP800-37,"Worthy of being trusted to fulfill whatever critical requirements may be needed for a particular component, subsystem, system, network, application, mission, enterprise, or other entity.",SP800-160,"ability to meet stakeholders' expectations in a verifiable way; an attribute that can be applied to services, products, technology, data and information as well as to organizations.",ISO/IEC_TS_5723:2022(en),,,,,,
trustworthy AI,"Characteristics of trustworthy AI
 systems include: valid and reliable, safe, secure and resilient, accountable and transparent, 
explainable and interpretable, privacy-enhanced, and fair with harmful bias 
managed.",NIST_AI_RMF_1.0,"AI that can be trusted by humans. Conditions for such trust can refer to (other) ethical principles such as human dignity, respect for human rights, and so on, and/or to social and technical factors that influence whether people will want to use the technology. The use of the term ""trust"" with regard to technologies is controversial.",AI_Ethics_Mark_Coeckelbergh,"
Trustworthy AI has three components: (1) it should be lawful, ensuring compliance with all applicable laws and
regulations (2) it should be ethical, demonstrating respect for, and ensure adherence to, ethical principles and
values and (3) it should be robust, both from a technical and social perspective, since, even with good intentions, AI
systems can cause unintentional harm. Trustworthy AI concerns not only the trustworthiness of the AI system itself
but also comprises the trustworthiness of all processes and actors that are part of the system’s life cycle.",european_ethics_2019,,,,,,
type I error,"The null hypothesis H0 is rejected, even though it is [true]",berthold_guide_2020,false positive rate,james_statistical_2014,,,,,,,,
type II error,"The null hypothesis H0 is accepted, even though it is [false]",berthold_guide_2020,true positive rate,james_statistical_2014,,,,,,,,
uncertainty,"Result of not having accurate or sufficient knowledge of a situation; state, even partial, of deficiency of information related to understanding or knowledge of an event, its consequence, or likelihood",IEEE_Soft_Vocab,,,,,,,,,,
underfitting,Underfitting occurs when a statistical model cannot adequately capture the underlying structure of the data.,"Ranschaert,_Erik",,,,,,,,,,
underrepresentation,inadequately represented. (See note.),Merriam-Webster_underrepresented,when members of discernible groups are not consistently present in representative bodies and among measures of well-being in numbers roughly proportionate to their numbers within the population. ,Encyclopedia.com_underrepresentation,,,,,,,,
unexplainable,impossibility of providing an explanation for certain decisions made by an intelligent system which is both 100% accurate and comprehensible.,Roman_V._Yampolskiy_Unexplainability,,,,,,,,,black box; opacity,
unstructured data,Data that does not have a predefined data model or is not organized in a predefined way,,,,,,,,,,,
unsupervised learning,"Algorithms, which take a set of data consisting only of inputs and then they attempt to cluster the data objects based on the similarities or dissimilarities in them.","Reznik,_Leon",Learning techniques that group instances without a pre-specified dependent attribute.,"Kohavi,_Ron","A learning strategy that consists in observing and analyzing different entities and determining that some of their subsets can be grouped into certain classes, without any correctness test being performed on acquired knowledge through feedback from external knowledge sources.
Note 1 to entry: Once a concept is formed, it is given a name that may be used in subsequent learning of other concepts.",iso_2382_1997,,,,,,
usability,"extent to which a system product or service can be used by specified users to achieve specified goals with effectiveness, efficiency and satisfaction in a specified context of use (note 1: The “specified” users, goals and context of use refer to the particular combination of users, goals and context of use for which usability is being considered; note 2: used as a qualifier to refer to the design knowledge, competencies, activities and design attributes that contribute to usability, such as usability expertise, usability professional, usability engineering, usability method, usability evaluation, usability heuristic). [See also: ISO/IEC 9241-11 Ergonomic of Human-System Interaction — Part 11: Usability: Definitions and Concepts. ISO, Geneva, Switzerland, 2018, https://www.iso.org/standard/63500.html.]",ISO/IEC_TS_5723:2022(en),,,,,,,,,,
usability testing,"refers to evaluating a product or service by testing it with representative users. Typically, during a test, participants will try to complete typical tasks while observers watch, listen and takes notes.  The goal is to identify any usability problems, collect qualitative and quantitative data and determine the participant's satisfaction with the product.",Usabilitygov,,,,,,,,,,
user,individual or group that interacts with a system or benefits from a system during its utilization,IEEE_Soft_Vocab,"A person, organization, or other entity which requests access to and uses the resources of a computer system or network.",CSRC,,,,,,,,
user-centered design,"the practice of the following principles, the active involvement of users for a clear understanding of user and task requirements, iterative design and evaluation, and a multi-disciplinary approach","Vredenburg,_Karel","Approach to system design and development that aims to make interactive systems more usable by focusing on the use of the system; applying human factors, ergonomics and usability knowledge and techniques.",IEEE_Soft_Vocab,,,,,,,,
validation,Confirmation by examination and provision of objective evidence that the particular requirements for a specific intended use are fulfilled.,UNODC_Glossary_QA_GLP,"Confirmation, through the provision of objective evidence, that the requirements for a specific intended use or application have been fulfilled.",IEEE_Soft_Vocab,"provides objective evidence that the capability provided by the system complies with stakeholder performance requirements, achieving its use in its intended operational environment; answers the question, ""Is it the right solution to the problem?"" [C]onsists of evaluating the operational effectiveness, operational suitability, sustainability, and survivability of the system or system elements under operationally realistic conditions.",DOD_TEVV,A continuous monitoring of the process of compilation and of the results of this process.,OECD,,,"Test and Evaluation, Verification, and Validation (TEVV)",
value sensitive design,a theoretically grounded approach to the design of technology that accounts for human values in a principled and systematic manner throughout the design process.,Friedman_et_al_2017,,,,,,,,,,
variable,A variable is a characteristic of a unit being observed that may assume more than one of a set of values to which a numerical measure or a category from a classification can be assigned.,OECD,Quantity or data item whose value can change,IEEE_Soft_Vocab,,,,,,,,
variable importance,Represents the statistical significance of each variable in the data in terms of its affect on the model.,H20.ai_glossary,,,,,,,,,,
variance,The variance is the mean square deviation of the variable around the average value. It reflects the dispersion of the empirical values around its mean.,OECD,"A quantifiable deviation, departure, or divergence away from a known baseline or expected value",IEEE_Soft_Vocab,,,,,,,,
verifiable,can be checked for correctness by a person or tool,ISO/IEC_TS_5723:2022(en),"provides evidence that the system or system element performs its intended functions and meets all performance requirements listed in the system performance specification and functional and allocated baselines; answers the question, ""Did you build the system correctly?"" ",DOD_TEVV,"the goal of designing AI systems that have strong, ideally provable, assurances of correctness with respect to mathematically specified requirements.",Seshia_et_al_2022,,,,,"Test and Evaluation, Verification and Validation (TEVV)",
washing,"a marketing effort designed to imply that a company's brands and products involve artificial intelligence technologies, even though the connection may be tenuous or non-existent.",TechTarget_Ivy_Wigmore,,,,,,,,,,
word embedding,"a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. . . . A word embedding, trained on word co-occurrence in text corpora, represents each word (or common phrase) w as a d-dimensional word vector w~ 2 Rd. It serves as a dictionary of sorts for computer programs that would like to use word meaning. First, words with similar semantic meanings tend to have vectors that are close together. Second, the vector differences between words in embeddings have been shown to represent relationships between words.",Bolukbasi_et_al_Debiasing_Word_Embeddings,,,,,,,,,,










































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































