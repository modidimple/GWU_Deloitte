# -*- coding: utf-8 -*-
"""
jpatrickhall@gmail.com
11.9.14
Educational use only.
"""

import sys
import getopt
import re
import os
import numpy
import multiprocessing
import time
import collections
import ast
from multiprocessing import Process
from datetime import datetime
from nltk.stem import wordnet

class Preprocess(object):

    """ Text preprocessing for szl.it.

    Chunks raw input files and preprocesses each in a separate thread.

    Each function can be used seperately, but the default 'in_prefix' options
    assume the following order:

    - lower
    - regex_replace_common_terms
    - lemmatize
    - resolve_replacements
    - replace_terms
    - apply_stoplist
    - remove_short_terms
    - remove_infrequent_terms
    - get_unique_terms
    - write_tbd

    Each function only runs if its speficied output file does not exist. This
    allows you to run the job to a certain point and pick back up where it left
    off ... if you are careful. Thus, significant speed-ups can be attained by
    saving the files generated by regex_replace_common_terms and reusing them
    whenever possible. It is also slightly faster to use a premade file of
    replacements, preventing lemmatize and resolve_replacements from running.

    Attributes:
        nthread: Number of threads to use during execution.
        working_dir: A directory that will contain all intermediate and final
            results.
        raw_file: The input file containing raw text.
        stop_dir: Directory containing stoplists (and only stoplists).
        threshold: Terms of length >= threshold will be removed.
        custom_replacements: Can be None; in that case replacements are
            generated using only the nltk lemmatizer.
            Can be a file of space separated key-value pairs;
            in that case, lemmatize and resolve_replacements do
            not run. (This is a little faster.)

    """

    def create_out_dirs(self):

        """ Creates nthread number of output directories. """

        print '---------------------------------------------------------------'
        print 'Creating working directory structure ... '

        for i in range(0, int(self.nthread)):

            chunk_outdir = self.working_dir + os.sep + '_chunk_dir' + str(i)
            try:
                if not os.path.exists(chunk_outdir):
                    os.mkdir(chunk_outdir)
                    print 'Created ' + chunk_outdir + ' ...'
            except:
                print 'Failed to locate or create ' + chunk_outdir + '!'
                sys.exit(-2)

        print 'Done.'

    def chunk_files(self):

        """ Separates input file into nthread roughly equal chunks.

        Each in a separate directory (created in create_out_dirs) for thread
        safety.

        """

        print '---------------------------------------------------------------'
        print 'Chunking ' + self.raw_file + ' ...'

        nline = sum(1 for line in open(self.raw_file))
        chunk_size = numpy.floor(nline/int(self.nthread))

        nthread = int(self.nthread)
        j = 0
        with open(self.raw_file) as file_:
            for i, line in enumerate(file_):
                if (i + 1 == j * chunk_size and j != nthread) or i == nline:
                    out.close()
                if i + 1 == 1 or (j != nthread and i + 1 == j * chunk_size):
                    chunk_outdir = self.working_dir + os.sep + '_chunk_dir'\
                        + str(j)
                    chunk_file = chunk_outdir + os.sep + '_raw' + str(j) + '.txt'
                    if os.path.isfile(chunk_file):
                        break
                    out = open(chunk_file, 'w+')
                    j = j + 1
                if out.closed != True:
                    out.write(line)
                if i % 10000 == 0 and i != 0:
                    print 'Processing line %i ...' % (i)

            print 'Done.'


    def __init__(self, nthread, working_dir, raw_file, stop_dir, threshold,\
                 custom_replacements):

        """ Initializes class attributes and creates working dir structure. """

        self.nthread = nthread
        self.working_dir = 'C:\\Users\\imdim\\OneDrive\\Documents\\SEM3\\BPA\\nmf_data'
        self.raw_file = raw_file
        self.stop_dir = stop_dir
        self.threshold = threshold
        if custom_replacements != None:
            self.custom_replacements = custom_replacements
        else:
            self.custom_replacements = self.working_dir + os.sep +\
            'replacements.txt'
        self.terms = {}

        self.create_out_dirs()
        self.chunk_files()

        self.row_fname = None
        self.col_fname = None
        self.val_fname = None
        self.terms_fname = None

    def lower(self, i, in_prefix='_raw'):

        """ Set all lines in chunk file to lower case.

        Args:
            i: Thread index.
            in_prefix: The filename prefix for locating input files; can be
                changed to alter the order of execution.

        """

        chunk_dir = self.working_dir + os.sep + '_chunk_dir' + str(i)
        in_file = chunk_dir + os.sep + in_prefix + str(i) + '.txt'
        out_file = chunk_dir + os.sep + in_prefix + '_lower' + str(i)  + '.txt'

        if not os.path.isfile(out_file):

            nline = sum(1 for l in open(in_file))
            decile = int(numpy.floor(nline/10.))

            process_name = multiprocessing.current_process().name
            print process_name + ': Attempting to process ' + str(nline) +\
                ' documents ...'

            k = 1
            out = open(out_file, 'wb')
            with open(in_file) as in_f:
                for j, line in enumerate(in_f):
                    out.write(line.lower())
                    if j == k*decile:
                        if k*10 <= 100:
                            print process_name + ': Completed ' + str(k*10) +\
                                '% ...'
                            k = k + 1

    def regex_replace_common_terms(self, i, in_prefix='_raw_lower'):

        """ Use regular expressions to compress common pop-culture phrases into
            single word entities.

        Very slow; avoid if possible by saving the created out_file's and using
        them in subsequent runs.

        Args:
            i: Thread index.
            in_prefix: The filename prefix for locating input files; can be
                changed to alter the order of execution.

        """
        chunk_dir = self.working_dir + os.sep + '_chunk_dir' + str(i)
        in_file = chunk_dir + os.sep + in_prefix + str(i) + '.txt'
        out_file = chunk_dir + os.sep + in_prefix + '_rgx_entity' + str(i) +\
            '.txt'

        if not os.path.isfile(out_file):

            nline = sum(1 for l in open(in_file))
            decile = int(numpy.floor(nline/10.))

            process_name = multiprocessing.current_process().name
            print process_name + ': Attempting to process ' + str(nline) +\
                ' documents ...'

            k = 1
            out = open(out_file, 'wb')
            with open(in_file) as in_f:
                for j, line in enumerate(in_f):

                    # data cleaning
                    line = line.rstrip()
                    line = re.sub(r'http\:\/\/[^\s]*?(\s|$)', r' ', line, flags=re.I)
                    line = re.sub(r'https\:\/\/[^\s]*?(\s|$)', r' ', line, flags=re.I)
                    line = re.sub(r'\xc3\xa9', r'e', line)
                    line = re.sub(r'\xe9', r'e', line)
                    line = re.sub(r'Ã©', r'e', line)
                    line = re.sub(r'\_', r' ', line)
                    line = re.sub(r'&AGRAVE;|&#192;|&#xC0;', r'a', line, flags=re.I)
                    line = re.sub(r'&AACUTE;|&#193;|&#xC1;', r'a', line, flags=re.I)
                    line = re.sub(r'&ACIRC;|&#194;|&#xC2;', r'a', line, flags=re.I)
                    line = re.sub(r'&ATILDE;|&#195;|&#xC3;', r'a', line, flags=re.I)
                    line = re.sub(r'&AUML;|&#196;|&#xC4;', r'a', line, flags=re.I)
                    line = re.sub(r'&ARING;|&#197;|&#xC5;', r'a', line, flags=re.I)
                    line = re.sub(r'&AELIG;|&#198;|&#xC6;', r'ae', line, flags=re.I)
                    line = re.sub(r'&CCEDIL;|&#199;|&#xC7;', r'c', line, flags=re.I)
                    line = re.sub(r'&EGRAVE;|&#200;|&#xC8;', r'e', line, flags=re.I)
                    line = re.sub(r'&EACUTE;|&#201;|&#xC9;', r'e', line, flags=re.I)
                    line = re.sub(r'&ECIRC;|&#202;|&#xCA;', r'e', line, flags=re.I)
                    line = re.sub(r'&EUML;|&#203;|&#xCB;', r'e', line, flags=re.I)
                    line = re.sub(r'&IGRAVE;|&#204;|&#xCC;', r'i', line, flags=re.I)
                    line = re.sub(r'&IACUTE;|&#205;|&#xCD;', r'i', line, flags=re.I)
                    line = re.sub(r'&ICIRC;|&#206;|&#xCE;', r'i', line, flags=re.I)
                    line = re.sub(r'&IUML;|&#207;|&#xCF;', r'i', line, flags=re.I)
                    line = re.sub(r'&NTILDE;|&#209;|&#xD1;', r'n', line, flags=re.I)
                    line = re.sub(r'&OGRAVE;|&#210;|&#xD2;', r'o', line, flags=re.I)
                    line = re.sub(r'&OACUTE;|&#211;|&#xD3;', r'o', line, flags=re.I)
                    line = re.sub(r'&OCIRC;|&#212;|&#xD4;', r'o', line, flags=re.I)
                    line = re.sub(r'&OTILDE;|&#213;|&#xD5;', r'o', line, flags=re.I)
                    line = re.sub(r'&OUML;|&#214;|&#xD6;', r'o', line, flags=re.I)
                    line = re.sub(r'&OSLASH;|&#216;|&#xD8;', r'o', line, flags=re.I)
                    line = re.sub(r'&UGRAVE;|&#217;|&#xD9;', r'u', line, flags=re.I)
                    line = re.sub(r'&UACUTE;|&#218;|&#xDA;', r'u', line, flags=re.I)
                    line = re.sub(r'&UCIRC;|&#219;|&#xDB;', r'u', line, flags=re.I)
                    line = re.sub(r'&UUML;|&#220;|&#xDC;', r'u', line, flags=re.I)
                    line = re.sub(r'&YACUTE;|&#221;|&#xDD;', r'y', line, flags=re.I)
                    line = re.sub(r'&AGRAVE;|&#224;|&#xE0;', r'a', line, flags=re.I)
                    line = re.sub(r'&AACUTE;|&#225;|&#xE1;', r'a', line, flags=re.I)
                    line = re.sub(r'&ACIRC;|&#226;|&#xE2;', r'a', line, flags=re.I)
                    line = re.sub(r'&ATILDE;|&#227;|&#xE3;', r'a', line, flags=re.I)
                    line = re.sub(r'&AUML;|&#228;|&#xE4;', r'a', line, flags=re.I)
                    line = re.sub(r'&ARING;|&#229;|&#xE5;', r'a', line, flags=re.I)
                    line = re.sub(r'&AELIG;|&#230;|&#xE6;', r'ae', line, flags=re.I)
                    line = re.sub(r'&CCEDIL;|&#231;|&#xE7;', r'c', line, flags=re.I)
                    line = re.sub(r'&EGRAVE;|&#232;|&#xE8;', r'e', line, flags=re.I)
                    line = re.sub(r'&EACUTE;|&#233;|&#xE9;', r'e', line, flags=re.I)
                    line = re.sub(r'&ECIRC;|&#234;|&#xEA;', r'e', line, flags=re.I)
                    line = re.sub(r'&EUML;|&#235;|&#xEB;', r'e', line, flags=re.I)
                    line = re.sub(r'&IGRAVE;|&#236;|&#xEC;', r'i', line, flags=re.I)
                    line = re.sub(r'&IACUTE;|&#237;|&#xED;', r'i', line, flags=re.I)
                    line = re.sub(r'&ICIRC;|&#238;|&#xEE;', r'i', line, flags=re.I)
                    line = re.sub(r'&IUML;|&#239;|&#xEF;', r'i', line, flags=re.I)
                    line = re.sub(r'&NTILDE;|&#241;|&#xF1;', r'n', line, flags=re.I)
                    line = re.sub(r'&OGRAVE;|&#242;|&#xF2;', r'o', line, flags=re.I)
                    line = re.sub(r'&OACUTE;|&#243;|&#xF3;', r'o', line, flags=re.I)
                    line = re.sub(r'&OCIRC;|&#244;|&#xF4;', r'o', line, flags=re.I)
                    line = re.sub(r'&OTILDE;|&#245;|&#xF5;', r'o', line, flags=re.I)
                    line = re.sub(r'&OUML;|&#246;|&#xF6;', r'o', line, flags=re.I)
                    line = re.sub(r'&OSLASH;|&#248;|&#xF8;', r'o', line, flags=re.I)
                    line = re.sub(r'&UGRAVE;|&#249;|&#xF9;', r'u', line, flags=re.I)
                    line = re.sub(r'&UACUTE;|&#250;|&#xFA;', r'u', line, flags=re.I)
                    line = re.sub(r'&UCIRC;|&#251;|&#xFB;', r'u', line, flags=re.I)
                    line = re.sub(r'&UUML;|&#252;|&#xFC;', r'u', line, flags=re.I)
                    line = re.sub(r'&YACUTE;|&#253;|&#xFD;', r'y', line, flags=re.I)
                    line = re.sub(r'&YUML;|&#255;|&#xFF;', r'y', line, flags=re.I)
                    line = re.sub(r'&NBSP;|&#160;|&#xA0;', r' ', line, flags=re.I)
                    line = re.sub(r'&ENSP;|&#8194;|&#x2002;', r' ', line, flags=re.I)
                    line = re.sub(r'&EMSP;|&#8195;|&#x2003;', r' ', line, flags=re.I)
                    line = re.sub(r'&THINSP;|&#8201;|&#x2009;', r' ', line, flags=re.I)
                    line = re.sub(r'&ZWNJ;|&#8204;|&#x200C;', r' ', line, flags=re.I)
                    line = re.sub(r'&ZWJ;|&#8205;|&#x200D;', r' ', line, flags=re.I)
                    line = re.sub(r'&LRM;|&#8206;|&#x200E;', r' ', line, flags=re.I)
                    line = re.sub(r'&RLM;|&#8207;|&#x200F;', r' ', line, flags=re.I)
                    line = re.sub(r'&#45;|&#x2D;', r'-', line, flags=re.I)
                    line = re.sub(r'&NDASH;|&#8211;|&#x2013;', r'-', line, flags=re.I)
                    line = re.sub(r'&MDASH;|&#8212;|&#x2014;', r'-', line, flags=re.I)
                    line = re.sub(r'&QUOT;|&#34;|&#x22;', r'\"', line, flags=re.I)
                    line = re.sub(r'&LSQUO;|&#8216;|&#x2018;', r'\"', line, flags=re.I)
                    line = re.sub(r'&RSQUO;|&#8217;|&#x2019;', r"\'", line, flags=re.I)
                    line = re.sub(r'&SBQUO;|&#8218;|&#x201A;', r"\'", line, flags=re.I)
                    line = re.sub(r'&LDQUO;|&#8220;|&#x201C;', r'\"', line, flags=re.I)
                    line = re.sub(r'&RDQUO;|&#8221;|&#x201D;', r'\"', line, flags=re.I)
                    line = re.sub(r'&BDQUO;|&#8222;|&#x201E;', r'\"', line, flags=re.I)
                    line = re.sub(r'&AMP;|&#38;|&#x26;', r'&', line, flags=re.I)
                    line = re.sub(r'&GT;|&#62;|&#x3E;', r'>', line, flags=re.I)
                    line = re.sub(r'&LT;|&#60;|&#x3C;', r'<', line, flags=re.I)
                    line = re.sub(r'&#33;|&#x21;', r'\!', line, flags=re.I)
                    line = re.sub(r'&#40;|&#x28;', r'\(', line, flags=re.I)
                    line = re.sub(r'&#41;|&#x29;', r'\)', line, flags=re.I)
                    line = re.sub(r'&#42;|&#x2A;', r'\*', line, flags=re.I)
                    line = re.sub(r'&#43;|&#x2B;', r'\+', line, flags=re.I)
                    line = re.sub(r'&#47;|&#x2F;', r'\/', line, flags=re.I)
                    line = re.sub(r'&#58;|&#x3A;', r':', line, flags=re.I)
                    line = re.sub(r'&#59;|&#x3B;', r';', line, flags=re.I)
                    line = re.sub(r'&#46;|&#x2E;', r'\.', line, flags=re.I)
                    line = re.sub(r'&#63;|&#x3F;', r'\?', line, flags=re.I)
                    line = re.sub(r'&#61;|&#x3D;', r'=', line, flags=re.I)
                    line = re.sub(r'&#35;|&#x23;', r'#', line, flags=re.I)

                    # entity extraction

                    # line = re.sub(r'artificial intelligence', r'artificialintelligence', line, flags=re.I)
                    # line = re.sub(r'\sai\s', r' artificialintelligence ', line, flags=re.I)
                    # line = re.sub(r'achieve.+ment', r'application', line, flags=re.I)
                    # line = re.sub(r'acci.+dent', r'accident', line, flags=re.I)
                    # line = re.sub(r'adver.+sarial', r'adversarial', line, flags=re.I)
                    # line = re.sub(r'appli.+cation', r'application', line, flags=re.I)
                    # line = re.sub(r'computer.+vision', r'computervision', line, flags=re.I)
                    # line = re.sub(r'data.+scientist', r'datascientist', line, flags=re.I)
                    # line = re.sub(r'data.+scientists', r'datascientist', line, flags=re.I)
                    # line = re.sub(r'equal.+ized', r'equalized', line, flags=re.I)
                    # line = re.sub(r'expla.+nation', r'explanation', line, flags=re.I)
                    # line = re.sub(r'impor.+tant', r'important', line, flags=re.I)
                    # line = re.sub(r'impor.+tance', r'importance', line, flags=re.I)
                    # line = re.sub(r'inci.+dent', r'incident', line, flags=re.I)
                    # line = re.sub(r'large.+language.+model', r'languagemodel', line, flags=re.I)
                    # line = re.sub(r'layer.+wise', r'layerwise', line, flags=re.I)
                    # line = re.sub(r'machine learning', r'machinelearning', line, flags=re.I)
                    # line = re.sub(r'\sml\s', r' machinelearning ', line, flags=re.I)
                    # line = re.sub(r'perfor.+mance', r'performance', line, flags=re.I)
                    # line = re.sub(r'vice.+versa', r'viceversa', line, flags=re.I)

                
                    line = re.sub(r'physical\s*data\s*breach', r'physicalbreach', line, flags=re.I)
                    line = re.sub(r'physical\s*theft', r'physicaltheft', line, flags=re.I)
                    line = re.sub(r'physical\s*loss', r'physicalloss', line, flags=re.I)
                    line = re.sub(r'missing', r'physicalloss', line, flags=re.I)
                    line = re.sub(r'cyber\s*attack', r'cyberattack', line, flags=re.I)
                    line = re.sub(r'unauthorized\s*access', r'unauthorizedaccess', line, flags=re.I)
                    line = re.sub(r'unintended\s*disclosure', r'unintendeddisclosure', line, flags=re.I)
                    line = re.sub(r'theft\s*of\s*laptop', r'theftlaptop', line, flags=re.I)
                    line = re.sub(r'stolen\s*laptop', r'theftlaptop', line, flags=re.I)
                    line = re.sub(r'lost\s*laptop', r'lostlaptop', line, flags=re.I)
                    line = re.sub(r'posting\s*of\s*information', r'postinginformation', line, flags=re.I)
                    line = re.sub(r'portable\s*device\s*breach', r'PORT', line, flags=re.I)
                    line = re.sub(r'portable\s*device\s*category', r'PORT', line, flags=re.I)
                    line = re.sub(r'stationary\s*computer\s*breach', r'STAT', line, flags=re.I)
                    line = re.sub(r'insider', r'insider', line, flags=re.I)
                    line = re.sub(r'outside\s*party', r'external', line, flags=re.I)
                    line = re.sub(r'brea.+ch', r'breach', line, flags=re.I)
                    line = re.sub(r'physi.+cal', r'physical', line, flags=re.I)
                    line = re.sub(r'uninten.+ded', r'unintended', line, flags=re.I)
                    line = re.sub(r'acci.+dent', r'accident', line, flags=re.I)
                    line = re.sub(r'perso.+nal', r'personal', line, flags=re.I)
                    line = re.sub(r'mist.+ake', r'mistake', line, flags=re.I)

                    # Catch-All

                    line = re.sub(r'&#?[a-zA-Z0-9]*;', '', line, flags=re.I)

                    out.write(line + '\n')

                    if j == k*decile:
                        if k*10 <= 100:
                            print process_name + ': Completed ' + str(k*10) +\
                                '% ...'
                            k = k + 1

            out.close()

    def lemmatize(self, i, in_prefix='_raw_lower_rgx_entity'):
        """ Stem terms into an infinitive or singular form.

        Creates a space-separated dictionary of word-lemma key-value pairs.
        Only runs if replacements are not specified in the -r option.

        Args:
            i: Thread index.
            in_prefix: The filename prefix for locating input files; can be
                changed to alter the order of execution.

        """

        chunk_dir = self.working_dir + os.sep + '_chunk_dir' + str(i)
        in_file = chunk_dir + os.sep + in_prefix + str(i) + '.txt'
        chunk_replace_list = chunk_dir + os.sep + 'replacements' + str(i) + '.txt'

        if not (os.path.isfile(chunk_replace_list) or\
            os.path.isfile(self.custom_replacements)):

            lemmatizer = wordnet.WordNetLemmatizer()
            nline = sum(1 for l in open(in_file))
            decile = int(numpy.floor(nline/10.))

            process_name = multiprocessing.current_process().name
            print process_name + ': Attempting to process ' + str(nline) +\
                ' documents ...'

            k = 0
            replacements = open(chunk_replace_list, 'wb')
            with open(in_file) as dictionary:
                for j, line in enumerate(dictionary):
                    for word in re.split(r'-|\/|;|,|\s|\.|<\/|<|>|&|\!|:|\"|\)|\(|\*|\+|\?|=|#|_', line):
                        word = re.sub(r'[\W\d]', "", word)
                        if word == '':
                            continue
                        lemma = lemmatizer.lemmatize(word, 'v')
                        if word == lemma:                                   # if the word is not a verb
                            lemma = lemmatizer.lemmatize(word, 'n')         # see if the word is also not a noun
                            if word != lemma:                               # if the word is not also a noun, then make replacement
                                replacements.write(word + " " + lemma + "\n")
                        else:                                               # if the word is not a verb, then make replacement
                            replacements.write(word + " " + lemma + "\n")

                    if j == k*decile:
                        if k*10 <= 100:
                            print process_name + ': Completed ' + str(k*10) +\
                                '% ...'
                            k = k + 1

            dictionary.close()

    def resolve_replacements(self, in_prefix='replacements'):
        """ Create a unique list of replaced terms from each thread.

        Only runs if replacements are not specified in the -r option.

        Args:
            i: Thread index.
            in_prefix: The filename prefix for locating input files; can be
                changed to alter the order of execution.

        """

        chunk_dir = self.working_dir + os.sep + '_chunk_dir0'
        in_file = chunk_dir + os.sep + in_prefix + '0.txt'

        if not os.path.isfile(self.custom_replacements):
            replace_list = {}
            with open(in_file) as file_:
                for line in file_:
                    (key_, value_) = line.split()
                    replace_list[key_.strip().lower()] = value_.strip().lower()
            if self.nthread > 1:
                for i in range(1, int(self.nthread)):
                    chunk_outdir = self.working_dir + os.sep + '_chunk_dir' +\
                        str(i)
                    in_file = chunk_outdir + os.sep + in_prefix + str(i) + '.txt'
                    dict_ = {}
                    with open(in_file) as file_:
                        for line in file_:
                            (key_, value_) = line.split()
                            dict_[key_.strip().lower()] =\
                                value_.strip().lower()
                    # Overwrites previously existing k, v pairs
                    replace_list.update(dict_)

            out_file = self.working_dir + os.sep + 'replacements.txt'
            with open(out_file, 'wb') as out:
                out.write('### word lemma\n')
                for key_, value_ in sorted(replace_list.items()):
                    out.write(str(key_))
                    out.write(' ')
                    out.write(str(value_))
                    out.write('\n')

    def replace_terms(self, i, in_prefix='_raw_lower_rgx_entity'):
        """ Replaces terms using the list created by lemmatization.

            All replacements comes from the replacements.txt file
            created by lemmatize and resolve_replacements; replacements.txt can
            be created manually as well.

            Utility script update_stemlist.py can be used to update the
            replacements.

        Args:
            i: Thread index.
            in_prefix: The filename prefix for locating input files; can be
                changed to alter the order of execution.

        """

        chunk_dir = self.working_dir + os.sep + '_chunk_dir' + str(i)
        in_file = chunk_dir + os.sep + in_prefix + str(i) + '.txt'
        out_file = chunk_dir + os.sep + in_prefix + '_stemmed' + str(i) + '.txt'

        if not os.path.isfile(out_file):

            process_name = multiprocessing.current_process().name
            print process_name +\
                ': Attempting to load custom replacement list from ' +\
                self.custom_replacements + ' ...'

            # Load custom replacment list
            replace_list = {}
            with open(self.custom_replacements) as file_:
                for line in file_:
                    if not re.split(r'\s', line)[0].startswith('#') or \
                        re.split(r'\s|', line)[0] in ['', '\r\n', '\n']:
                        pair = re.split(r'\s', line)
                        key_ = pair[0].strip().lower()
                        value_ = pair[1].strip().lower()
                        replace_list[key_] = value_
            print 'Done.'

            nline = sum(1 for l in open(in_file))
            decile = int(numpy.floor(nline/10.))

            process_name = multiprocessing.current_process().name
            print process_name + ': Attempting to process ' + str(nline) +\
                ' documents ...'

            k = 0
            keep_list = []
            out = open(out_file, 'wb')
            with open(in_file) as dictionary:
                for j, line in enumerate(dictionary):
                    keep_list = [replace_list[re.sub(r'[\W\d]', '', word)] \
                        if re.sub(r'[\W\d]', '', word) in replace_list else re.sub(r'[\W\d]', '', word) \
                            for word in re.split(r'-|\/|;|,|\s|\.|<\/|<|>|&|\!|:|\"|\)|\(|\*|\+|\?|=|#|_', line)]
                    out.write(' '.join(keep_list) + '\n')
                    keep_list = []

                    if j == k*decile:
                        if k*10 <= 100:
                            print process_name + ': Completed ' + str(k*10) +\
                                '% ...'
                            k = k + 1

            dictionary.close()
            out.close()

    def apply_stoplist(self, i, in_prefix='_raw_lower_rgx_entity_stemmed'):
        """ Remove stoplist terms.

        Args:
            i: Thread index.
            in_prefix: The filename prefix for locating input files; can be
                changed to alter the order of execution.

        """

        chunk_dir = self.working_dir + os.sep + '_chunk_dir' + str(i)
        in_file = chunk_dir + os.sep + in_prefix + str(i) + '.txt'
        out_file = chunk_dir + os.sep + in_prefix + '_stopped' + str(i) + '.txt'

        if not os.path.isfile(out_file):

            process_name = multiprocessing.current_process().name
            print process_name + ': Attempting to load stoplist from ' +\
                self.stop_dir + ' ...'

            # Load stoplists
            stop_list = set([])
            for file_ in os.listdir(self.stop_dir):
                stop_list.update(set(line.strip().lower()\
                    for line in open(self.stop_dir + os.sep + file_)))
            print 'Done.'

            nline = sum(1 for l in open(in_file))
            decile = int(numpy.floor(nline/10.))

            process_name = multiprocessing.current_process().name
            print process_name + ': Attempting to process ' + str(nline) +\
                ' documents ...'

            k = 0
            stop_dict = open(out_file, 'wb')
            stop_line = ''
            with open(in_file) as dictionary:
                for j, line in enumerate(dictionary):
                    stop_line = [word for word in line.split()\
                        if word.lower() not in stop_list and word.isalpha()]
                    stop_dict.write(' '.join(stop_line) + '\n')
                    stop_line = []

                    if j == k*decile:
                        if k*10 <= 100:
                            print process_name + ': Completed ' + str(k*10) +\
                                '% ...'
                            k = k + 1

            dictionary.close()
            stop_dict.close()
                    
    def remove_short_terms(self, i,\
        in_prefix='_raw_lower_rgx_entity_stemmed_stopped'):
        """ Removes words of length less than 3. (Hard-coded)

        Args:
            i: Thread index.
            in_prefix: The filename prefix for locating input files; can be
                changed to alter the order of execution.

        """

        chunk_dir = self.working_dir + os.sep + '_chunk_dir' + str(i)
        in_file = chunk_dir + os.sep + in_prefix + str(i) + '.txt'
        out_file = chunk_dir + os.sep + in_prefix + '_long' + str(i) + '.txt'

        if not os.path.isfile(out_file):

            nline = sum(1 for l in open(in_file))
            decile = int(numpy.floor(nline/10.))

            process_name = multiprocessing.current_process().name
            print process_name + ': Attempting to process ' + str(nline) +\
                ' documents ...'

            k = 0
            out = open(out_file, 'wb')
            with open(in_file) as dictionary:
                for j, line in enumerate(dictionary):
                    long_line = [word for word in line.split()\
                                 if len(word.strip()) > 2 and word.isalpha()]
                    out.write(' '.join(long_line) + '\n')
                    long_line = []

                    if j == k*decile:
                        if k*10 <= 100:
                            print process_name + ': Completed ' + str(k*10) +\
                                '% ...'
                            k = k + 1

            dictionary.close()
            out.close()

    def get_counts(self, nthread,\
                   in_prefix='_raw_lower_rgx_entity_stemmed_stopped_long'):
        
        """ Count words from all threads; More accurate, but slower and needs 
            more memory. No free lunch ... 
            
        Args:
            nthread: Total number of threads.
            in_prefix: The filename prefix for locating input files; can be
                changed to alter the order of execution.    
                
        Returns: The counts of all the words in the preprocessed in_files. 
        
        """
        
        words = []
        for i in range(0, int(nthread)):
            chunk_dir = self.working_dir + os.sep + '_chunk_dir' + str(i)
            in_file = chunk_dir + os.sep + in_prefix + str(i) + '.txt'
            with open(in_file) as dictionary:
                for line in dictionary:
                    words += line.split()
                    
        return collections.Counter(words) 

    def remove_infrequent_terms(self, i, counts,\
        in_prefix='_raw_lower_rgx_entity_stemmed_stopped_long'):
        """ Removes terms that occur less than or equal to 'threshold'
            number of times.

        Args:
            i: Thread index.
            in_prefix: The filename prefix for locating input files; can be
                changed to alter the order of execution.

        """

        chunk_dir = self.working_dir + os.sep + '_chunk_dir' + str(i)
        in_file = chunk_dir + os.sep + in_prefix + str(i) + '.txt'
        out_file = chunk_dir + os.sep + in_prefix + '_freq' + str(i) + '.txt'

        if not os.path.isfile(out_file):

            nline = sum(1 for l in open(in_file))
            decile = int(numpy.floor(nline/10.))

            process_name = multiprocessing.current_process().name
            print process_name + ': Attempting to process ' + str(nline) +\
                ' documents ...'

            keep_set = set([k for k in counts.keys()\
                if counts[k] >= int(self.threshold)])

            nline = sum(1 for l in open(in_file))
            decile = int(numpy.floor(nline/10.))

            k = 0
            freq_dict = open(out_file, 'wb')
            freq_list = []
            with open(in_file) as dictionary:
                for j, line in enumerate(dictionary):
                    line_list = line.split()
                    freq_list = [word for word in line_list if word in\
                                 keep_set]
                    freq_dict.write(' '.join(freq_list) + '\n')

                    if j == k*decile:
                        if k*10 <= 100:
                            print process_name + ': Completed ' + str(k*10) + \
                                '% ...'
                            k = k + 1

            dictionary.close()
            freq_dict.close()

    def get_unique_terms(self,\
        in_prefix='_raw_lower_rgx_entity_stemmed_stopped_long_freq'):
        """ Finds unique terms from the cleaned data generated in each
            sepearate thread.

        Args:
            in_prefix: The filename prefix for locating input files; can be
                changed to alter the order of execution.

        Returns: An indexed dictionary of the unique terms in alphabetical
            order.

        """

        unique_terms = set([])
        total_term_counts = collections.Counter()
        for i in range(0, int(self.nthread)):
            chunk_outdir = self.working_dir + os.sep + '_chunk_dir' + str(i)
            in_file = chunk_outdir + os.sep + in_prefix + str(i) + '.txt'
            for line in open(in_file):
                words = [word for word in line.split()]
                unique_terms.update(set(words))
                total_term_counts.update(collections.Counter(words))

        term_dict = {}
        term_dict = dict.fromkeys(unique_terms)
        index_ = 0
        for key in sorted(term_dict):
            term_dict[key] = index_
            index_ += 1

        time_stamp =\
            datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d_%H-%M-%S')
        self.set_terms_fname(self.working_dir + os.sep + 'terms_' + time_stamp +\
            '.txt')
        term_file = open(self.get_terms_fname(), 'wb')
        for term in sorted(term_dict):
            term_file.write(str(term_dict[term]) + ' ' + term + ' ' +\
                str(total_term_counts[term]) + '\n')
        term_file.close()

        self.set_terms(term_dict)

        return term_dict

    def write_partial_tbd(self, i, in_prefix, offsets):
        """ Writes the part of the TBD that corresponds to the terms found
            in a thread.

            Helper function that allows write_tbd() to execute in parallel.

        Args:
            i: Thread index.
            in_prefix: The filename prefix for locating input files; can be
                changed to alter the order of execution.
            offsets: Contains the row offset (within the TBD) of a given
                thread.

        """

        chunk_outdir = self.working_dir + os.sep + '_chunk_dir' + str(i)
        in_file = chunk_outdir + os.sep + in_prefix + str(i) + '.txt'
        tbd_row_file = chunk_outdir + os.sep + '_row' + str(i) + '.txt'
        tbd_col_file = chunk_outdir + os.sep + '_col' + str(i) + '.txt'
        tbd_val_file = chunk_outdir + os.sep + '_val' + str(i) + '.txt'

        nline = sum(1 for l in open(in_file))
        decile = int(numpy.floor(nline/10.))

        if (not os.path.isfile(tbd_row_file)) \
            or (not os.path.isfile(tbd_col_file)) \
            or (not os.path.isfile(tbd_val_file)):

            process_name = multiprocessing.current_process().name
            print process_name + ': Attempting to process ' + str(nline) +\
                ' documents ...'

            k = 0
            row_f = open(tbd_row_file, 'wb')
            col_f = open(tbd_col_file, 'wb')
            val_f = open(tbd_val_file, 'wb')

            with open(in_file) as dictionary:
                for j, line in enumerate(dictionary):
                    counts = collections.Counter(line.split())
                    for word in counts.keys():
                        row_f.write(str(self.terms.get(word)) + '\n')
                        col_f.write(str(j+offsets[i]) + '\n')
                        val_f.write(str(counts[word]) + '\n')

                    if j == k*decile:
                        if k*10 <= 100:
                            print process_name + ': Completed ' + str(k*10) +\
                                '% ...'
                            k = k + 1

    def write_tbd(self,\
        in_prefix='_raw_lower_rgx_entity_stemmed_stopped_long_freq'):
        """ Writes TBD matrix as COO in three files. """

        offset = 0
        offsets = [offset]
        for i in range(0, int(self.nthread)-1):
            chunk_outdir = self.working_dir + os.sep + '_chunk_dir' + str(i)
            in_file = chunk_outdir + os.sep + in_prefix + str(i) + '.txt'
            offset += sum(1 for l in open(in_file))
            offsets.append(offset)

        # Multithreading here is causing problems on Eric's Machine.
        # Moved function definition to a higher level in the class
        processes = []
        try:
            for i in range(0, int(self.nthread)):
                process_name = 'Process_' + str(i)
                process = Process(target=self.write_partial_tbd,\
                    name=process_name, args=(i, in_prefix, offsets,))
                process.start()
                processes.append(process)
            for process_ in processes:
                process_.join()
        except:
            print 'ERROR: Could not create TBD chunk.'
            print sys.exc_info()
            sys.exit(-3)

        time_stamp =\
            datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d_%H-%M-%S')
        self.set_row_fname(self.working_dir + os.sep + 'row_' + time_stamp +\
            '.txt')
        self.set_col_fname(self.working_dir + os.sep + 'col_' + time_stamp +\
            '.txt')
        self.set_val_fname(self.working_dir + os.sep + 'val_' + time_stamp +\
            '.txt')
        row_f = open(self.get_row_fname(), 'wb')
        col_f = open(self.get_col_fname(), 'wb')
        val_f = open(self.get_val_fname(), 'wb')
        row_f.write('row\n')
        col_f.write('col\n')
        val_f.write('val\n')
        for i in range(0, int(self.nthread)):
            row_f.write(open(self.working_dir + os.sep + '_chunk_dir' + str(i) +\
                os.sep + '_row' + str(i) + '.txt').read())
            col_f.write(open(self.working_dir + os.sep + '_chunk_dir' + str(i) +\
                os.sep + '_col' + str(i) + '.txt').read())
            val_f.write(open(self.working_dir + os.sep + '_chunk_dir' + str(i) +\
                os.sep + '_val' + str(i) + '.txt').read())

        row_f.close()
        col_f.close()
        val_f.close()

    def get_terms(self):
        """ Getter for terms."""
        return self.terms

    def set_terms(self, terms):
        """ Setter for terms."""
        self.terms = terms

    def get_terms_fname(self):
        """ Getter for terms_fname."""
        return self.terms_fname

    def set_terms_fname(self, name):
        """ Setter for terms_fname."""
        self.terms_fname = name

    def get_row_fname(self):
        """ Getter for row_fname."""
        return self.row_fname

    def set_row_fname(self, name):
        """ Setter for row_fname."""
        self.row_fname = name

    def get_col_fname(self):
        """ Getter for col_fname."""
        return self.col_fname

    def set_col_fname(self, name):
        """ Setter for col_fname."""
        self.col_fname = name

    def get_val_fname(self):
        """ Getter for val_fname."""
        return self.val_fname

    def set_val_fname(self, name):
        """ Setter for val_fname."""
        self.val_fname = name


def main(argv):
    """ For running standalone.

    Args:
        argv: Command line args.

    Raises:
        GetoptError: Error parsing command line options.
    """

    nthread = None
    working_dir = None
    raw_file = None
    stop_dir = None
    custom_replacements = None
    threshold = 10

    try:
        opts, _ = getopt.getopt(argv, "f:t:d:s:p:r:h")
        for opt, arg in opts:
            if opt == '-t':
                threshold = arg
            elif opt == '-d':
                working_dir = arg
            elif opt == '-f':
                raw_file = arg
            elif opt == '-s':
                stop_dir = arg
            elif opt == '-p':
                nthread = arg
            elif opt == '-r':
                if os.path.isfile(arg.strip()):
                    custom_replacements = arg.strip()
                else:
                    custom_replacements = ast.literal_eval(arg)
            elif opt == '-h':
                print 'Preprocess.py -f <input file> -p <number of processes> -d <output directory> -s <stoplist directory> -t <threshold>'
                sys.exit(0)
    except getopt.GetoptError:
        print 'Preprocess.py -f <input file> -p <number of processes> -d <output directory> -s <stoplist directory> -t <threshold> -r <custom replacements>'
        sys.exit(-1)
    if None in [nthread, working_dir, raw_file, stop_dir, threshold]:
        print 'Preprocess.py -f <input file> -p <number of processes> -d <output directory> -s <stoplist directory> -t <threshold> -r <custom replacements>'
        print 'None in [nthread, working_dir, raw_file, stop_dir, threshold]'
        print 'Proceeding with options: '
        print 'Processes (-p)           = %s' % (nthread)
        print 'Output directory (-d)    = %s' % (working_dir)
        print 'Input file (-f)          = %s' % (raw_file)
        print 'Stoplist directory       = %s' % (stop_dir)
        print 'Threshold (-t)           = %s' % (threshold)
        print 'Custom replacements (-r) = %s' % (custom_replacements)
        print sys.exc_info()
        sys.exit(-1)

    print '-------------------------------------------------------------------'
    print 'Proceeding with options: '
    print 'Processes (-p)           = %s' % (nthread)
    print 'Output directory (-d)    = %s' % (working_dir)
    print 'Input file (-f)          = %s' % (raw_file)
    print 'Stoplist directory       = %s' % (stop_dir)
    print 'Threshold (-t)           = %s' % (threshold)
    print 'Custom replacements (-r) = %s' % (custom_replacements)
    pre = Preprocess(nthread, working_dir, raw_file, stop_dir, threshold,\
        custom_replacements)

    big_tic = time.time()

    print '-------------------------------------------------------------------'
    print 'Converting to lowercase ... '
    tic = time.time()
    processes = []
    try:
        for i in range(0, int(nthread)):
            process_name = 'Process_' + str(i)
            process = Process(target=pre.lower, name=process_name, args=(i,))
            process.start()
            processes.append(process)
        for process_ in processes:
            process_.join()
        toc = time.time()-tic
        print 'Converted to lowercase in %.2f s.' % (toc)
    except:
        print 'ERROR: Could not convert to lowercase.'
        print sys.exc_info()
        sys.exit(-2)

    print '-------------------------------------------------------------------'
    print 'Concatenating common pop-culture terms ... '
    tic = time.time()
    processes = []
    try:
        for i in range(0, int(nthread)):
            process_name = 'Process_' + str(i)
            process = Process(target=pre.regex_replace_common_terms,\
                              name=process_name, args=(i,))
            process.start()
            processes.append(process)
        for process_ in processes:
            process_.join()
        toc = time.time()-tic
        print 'Terms replaced in %.2f s.' % (toc)
    except:
        print 'ERROR: Could not concatenating common pop-culture terms.'
        print sys.exc_info()
        sys.exit(-2)

    print '-------------------------------------------------------------------'
    print 'Generating stems ... '
    tic = time.time()
    processes = []
    try:
        for i in range(0, int(nthread)):
            process_name = 'Process_' + str(i)
            process = Process(target=pre.lemmatize,
                              name=process_name, args=(i,))
            process.start()
            processes.append(process)
        for process_ in processes:
            process_.join()
        toc = time.time()-tic
        print 'Stems generated in %.2f s.' % (toc)
    except:
        print 'ERROR: Could not generating stems.'
        print sys.exc_info()
        sys.exit(-2)

    print '-------------------------------------------------------------------'
    print 'Resolving unique terms from different threads ... '
    tic = time.time()
    pre.resolve_replacements()
    toc = time.time()-tic
    print 'Resolved unique terms in %.2f s.' % (toc)

    print '-------------------------------------------------------------------'
    print 'Replacing stemmed terms ... '
    tic = time.time()
    processes = []
    try:
        for i in range(0, int(nthread)):
            process_name = 'Process_' + str(i)
            process = Process(target=pre.replace_terms, name=process_name,\
                              args=(i,))
            process.start()
            processes.append(process)
        for process_ in processes:
            process_.join()
        toc = time.time()-tic
        print 'Terms replaced in %.2f s.' % (toc)
    except:
        print 'ERROR: Could not replace stems.'
        print sys.exc_info()
        sys.exit(-2)

    print '-------------------------------------------------------------------'
    print 'Applying stop list ... '
    tic = time.time()
    processes = []
    try:
        for i in range(0, int(nthread)):
            process_name = 'Process_' + str(i)
            process = Process(target=pre.apply_stoplist, name=process_name,\
                              args=(i,))
            process.start()
            processes.append(process)
        for process_ in processes:
            process_.join()
        toc = time.time()-tic
        print 'Stopped terms removed in %.2f s.' % (toc)
    except:
        print 'ERROR: Could not apply stop list.'
        print sys.exc_info()
        sys.exit(-2)

    print '-------------------------------------------------------------------'
    print 'Removing short terms ... '
    tic = time.time()
    processes = []
    try:
        for i in range(0, int(nthread)):
            process_name = 'Process_' + str(i)
            process = Process(target=pre.remove_short_terms,\
                              name=process_name, args=(i,))
            process.start()
            processes.append(process)
        for process_ in processes:
            process_.join()
        toc = time.time()-tic
        print 'Short terms removed in %.2f s.' % (toc)
    except:
        print 'ERROR: Could not .'
        print sys.exc_info()
        sys.exit(-2)

    print '-------------------------------------------------------------------'
    print 'Counting all terms ... '
    tic = time.time()
    counts = pre.get_counts(nthread)
    toc = time.time()-tic
    print 'Terms counted in %.2f s.' % (toc)
    
    print '-------------------------------------------------------------------'
    print 'Removing infrequent terms ... '
    tic = time.time()
    processes = []    
    try:
        for i in range(0, int(nthread)):
            process_name = 'Process_' + str(i)
            process = Process(target=pre.remove_infrequent_terms,\
                              name=process_name, args=(i, counts))
            process.start()
            processes.append(process)
        for process_ in processes:
            process_.join()
        toc = time.time()-tic
        print 'Infrequent terms removed in %.2f s.' % (toc)
    except:
        print 'ERROR: Could not remove infrequent terms.'
        print sys.exc_info()
        sys.exit(-2)

    print '-------------------------------------------------------------------'
    print 'Processing unique terms ... '
    tic = time.time()
    terms = pre.get_unique_terms()
    pre.set_terms(terms)
    toc = time.time()-tic
    print 'Terms processed in %.2f s.' % (toc)
    print 'Number of unique terms: %i' % (len(terms))

    print '-------------------------------------------------------------------'
    print 'Writing COO term-by-document matrix ... '
    tic = time.time()
    pre.write_tbd()
    toc = time.time()-tic
    print 'Matrix written in %.2f s.' % (toc)

    big_toc = time.time()-big_tic
    print '-------------------------------------------------------------------'
    print 'Total run time: %.2f s.' % (big_toc)

if __name__ == '__main__':
    main(sys.argv[1:])
